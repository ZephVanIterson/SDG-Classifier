Repository,Description,Readme Content,Topics,Last Contribution,Number of Contributors,Number of Stars,Number of Subscribers,Contributors
openfarmcc/OpenFarm,A free and open database for farming and gardening knowledge. You can grow anything!,"![OpenFarm](https://github.com/openfarmcc/OpenFarm/blob/master/app/assets/images/OpenFarm%20f%20logo%20-%20green%20%23219653.svg)

[![Coverage Status](https://img.shields.io/coveralls/openfarmcc/OpenFarm.svg)](https://coveralls.io/r/openfarmcc/OpenFarm)
[![Code Climate](https://codeclimate.com/github/openfarmcc/OpenFarm/badges/gpa.svg)](https://codeclimate.com/github/FarmBot/OpenFarm)
[![OpenCollective](https://opencollective.com/openfarm/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/openfarm/sponsors/badge.svg)](#sponsors)

# Security Concerns

We take security seriously and value the input of independent researchers. Please email `security@farmbot.io` for issues that require immediate attention. Please follow [responsible disclosure](). **Do not use Slack or Github issues to discuss security vulnerabilities.**

# The Community of Contributors: How it Works

### About

[OpenFarm](http://openfarm.cc) is a free and open database and web application for farming and gardening knowledge. One might think of it as the Wikipedia for growing plants, though it functions more like a cooking recipes site.

The main content are Growing Guides: creative, crowd-sourced, single-author, structured documents that include all of the necessary information for a person or machine to grow a plant, i.e.: seed spacing and depth, watering regimen, recommended soil composition and companion plants, sun/shade requirements, etc. In this Freebase platform, gardeners can find answers to questions like &ldquo;How do I grow tomatoes?&rdquo;

### Start by Joining Existing Contributors

To start the discussion, get involved, and meet OpenFarm core community of contributors, we strongly recommend joining [our Slack room](http://slack.openfarm.cc/)! This is where you'll find the latest conversation about Openfarm and the most active contributors.

Check also the [FAQ](http://openfarm.cc/pages/faq) for some frequently asked questions about contributing (Angular, Issue Trackers, IRC Channels).

Check the [ongoing issues](https://github.com/openfarmcc/OpenFarm/projects) that need work on in the priority list.

### Look for Something You Want to Work On

For [front-end](https://github.com/openfarmcc/OpenFarm/projects/1) and [back-end](https://github.com/openfarmcc/OpenFarm/projects/3) code contributions, we aim at maintaining and prioritizing the Github issues through Github Projects, the Trello-like web-based project management board of Github: [OpenFarm Projects](https://github.com/openfarmcc/OpenFarm/projects).

Need to use OpenFarm Assets? [Here they are](https://drive.google.com/open?id=0B-wExYzQcnp3cGphOGZQS1lBRFk)!

We have few more languages missing for the website content to be translated: help us [translate the website](https://www.transifex.com/projects/p/openfarm/)!

### Who Can Contribute

Everyone is welcome to bring value to the Open Source community of OpenFarm. Time is our most valuable assets here, so any minute of your time counts to make things happen! ""Better done, than perfect!""
We strive for diversity in our community and want to ensure we provide a safe and inclusive space for everyone by adopting a [Code of Conduct](https://openfarm.cc/pages/code_of_conduct?locale=en).

Our community is composed of tech and non-tech folks, newbie as well as experts in gardening, overall great people willing to take actions for a better future and sharing knowledge and growing our own food.

### Our problem-solving process

On the way we work together, we aim at:
- having transparency in reasoning behind actions: taking time for documentation, questions and answers
- prefering done, than perfect: breaking down tasks so that anyone can contribute few minutes of their time on a regular basis
- taking shortcuts: what's the most obvious for a better usability? what's the shortest way to build a feature? What's the most valuable inputs for a feedback?

## Development

### Getting Started (The Easy Way)

You should use Vagrant to get the OpenFarm system running on your computer. It will avoid having to install the things listed in The Hard Way below.

1. Install [Vagrant](https://www.vagrantup.com/docs/installation/).
2. Install [VirtualBox](https://www.virtualbox.org/wiki/Downloads).
3. Open your terminal.
4. `$ git clone https://github.com/openfarmcc/OpenFarm.git` - this tells your computer to fetch the data stored in this repository using git.
5. `$ cd OpenFarm` - change to the OpenFarm directory.
6. `$ vagrant up` This will take a long time. We're downloading a whole bunch of stuff. Go make yourself a pot of coffee, or brew some tea. If something goes wrong at this point, reach out to us directly via GitHub issue.

#### Accessing Vagrant

Once Vagrant is set up on your system, you might want to actually access it. For example, if you want to start up the server (though vagrant up should run `rails s` for you):

8. `$ vagrant ssh` - this makes you access the new virtual server we just created to run OpenFarm on.
9. `cd /vagrant` - the `vagrant` directory is mirrored in your own computer. If you add a file there, you'll see it appear here.
10. `rails s` - actually run the Rails server!
11. you should now be able to access OpenFarm on your local system at http://localhost:3000. If all went well, you will have a seeded database and can use the account `admin@admin.com` with password `admin123`.

The above is still being patched, so please reach out to us if something went wrong!

### Getting Started (The Hard Way)

You will need to install [Ruby](http://www.ruby-lang.org/en/), [Rails](http://rubyonrails.org/), [ElasticSearch](http://www.elasticsearch.org/) [v6.5.0](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/release-notes-6.5.0.html), and [Mongodb](http://docs.mongodb.org/manual/installation/) before you can get an OpenFarm server up and running on your local machine. Once you have these prerequisites to get started with a local copy of the project, run:

```bash
$ git clone https://github.com/openfarmcc/OpenFarm.git
$ cd OpenFarm
$ bundle install
$ rake db:setup
$ echo ""ENV['SECRET_KEY_BASE'] = '$(rake secret)'"" >> config/app_environment_variables.rb
$ echo ""ENV['GOOGLE_MAPS_API_KEY'] = ''"" >> config/app_environment_variables.rb # or get an actual API key at https://console.developers.google.com/flows/enableapi?apiid=maps_backend&keyType=CLIENT_SIDE&reusekey=true&pli=1
$ rails s
```

Then, visit [http://127.0.0.1:3000/](http://127.0.0.1:3000/) in your browser to see the OpenFarm web application running on your local machine. If all went well, you will have a seeded database and can use the account `admin@admin.com` with password `admin123`.

**Note about ElasticSearch**: Some Linux users have noted issues installing ElasticSearch onto a host machine. One workaround is to install ElasticSearch via Docker:

```
sudo docker pull elasticsearch:6.5.0
sudo docker pull mongo
```

```
sudo sysctl -w vm.max_map_count=262144 # <= Some linux users must run this
sudo docker run -p 9300:9300 -p 9200:9200 elasticsearch:6.5.0
sudo docker run -p 27017:27017 mongo

```


**If you had any problem** installing bundles getting up and running etc see the [Common Issues Page](https://github.com/openfarmcc/OpenFarm/wiki/Common-Issues).

Remember that `/vagrant` folder in the Vagrant VM is largely for convenience, and working in it can cause unexpected behavior with other tools - you should do your work in your own non-vagrant environment. Use the environment you're most familiar with to program, and Vagrant will do the rest.


#### Become a Core Contributor

If you've made two PRs, we'll add you as a core contributor.

For core-code contributors, here are a few basic ground-rules:

* No --force pushes or modifying the Git history in any way.
* Non-master branches ought to be used for ongoing work.
* External API changes and significant modifications ought to be subject to an internal pull-request to solicit feedback from other contributors.
* Internal pull-requests to solicit feedback are encouraged for any other non-trivial contribution but left to the discretion of the contributor.
* Contributors should attempt to adhere to the prevailing code-style.

([based on the OPEN open source model](https://github.com/Level/community/blob/master/CONTRIBUTING.md))

[Further reading](https://medium.com/the-javascript-collection/healthy-open-source-967fa8be7951#.alkpecsnd)

### Actual Code Contributors

Here are some of the [Github contributors](https://github.com/openfarmcc/OpenFarm/graphs/contributors).

Outside of Github, there's a whole host of people who also contributed financially, by building gardening content on the website, on providing more visibility for OpenFarm in any ways!

### Donate to OpenFarm as a Backer

Support us with a monthly donation and help us continue our activities. [[Become a backer](https://opencollective.com/openfarm#backer)]

<a href=""https://opencollective.com/openfarm"" target=""_blank""><img src=""https://opencollective.com/openfarm/backers.svg?avatarHeight=36&width=600""></a>

### Support OpenFarm as a Sponsor

Become a sponsor and get your logo on our README on Github with a link to your site. [[Become a sponsor](https://opencollective.com/openfarm#sponsor)]

<a href=""https://opencollective.com/openfarm"" target=""_blank""><img src=""https://opencollective.com/openfarm/sponsors.svg?avatarHeight=36&width=600""></a>

### Software License

The MIT License (MIT)

Copyright (c) 2019 OpenFarm [(http://openfarm.cc/)](http://openfarm.cc/).

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

### Data License

All data within the OpenFarm.cc database is in the [Public Domain (CC0)](http://creativecommons.org/publicdomain/zero/1.0/).
","'farming', 'gardening', 'open-source', 'plants', 'rails'",2024-03-10T13:38:40Z,30,1496,63,"('simonv3', 1116), ('RickCarlino', 701), ('dependabotbot', 201), ('warpling', 170), ('dependabot-previewbot', 161), ('CloCkWeRX', 119), ('TanSA05', 94), ('roryaronson', 88), ('Br3nda', 64), ('TheChirpyWitch', 25), ('broder', 14), ('gabrielburnworth', 12), ('RaceFPV', 11), ('md5', 10), ('kelcecil', 10), ('speizerj', 9), ('digital-dreamer', 9), ('guitsaru', 8), ('nickedwards109', 7), ('Cynaria', 6), ('roselynemakena', 6), ('code-factor', 5), ('pnob32', 5), ('mo22de', 5), ('robbrit', 4), ('rickr', 4), ('Katy310', 4), ('cpursley', 4), ('ignaciots', 3), ('sophiakc', 3)"
sleepbusorg/sleepbus.org,Repository not found,,,,0,0,0,
openfoodfacts/openfoodfacts-androidapp,Native version of Open Food Facts on Android - Coders & Decoders welcome ü§≥ü•´ ,"<img height='175' src=""https://static.openfoodfacts.org/images/svg/openfoodfacts-logo-en.svg"" align=""left"" hspace=""1"" vspace=""1"">

Open Food Facts - Legacy Android app (still used for Open Beauty, Pet Food and Products Facts)
=============================

Open Food Facts is collaborative food products database made by everyone, for everyone. Open Food Facts contributors gathers information and data on food products from around the world, using mobile apps.

<br>

> ### ‚ö†Ô∏è WARNING
> 
> The new Open Food Facts app is located [HERE](https://github.com/openfoodfacts/smooth-app)
> 
> **Note: This codebase is currently only deployed for Open Beauty Facts, Open Pet Food Facts and Open Products Facts apps.**

[![Project Status](https://opensource.box.com/badges/active.svg)](https://opensource.box.com/badges)
[![Quality Gate](https://sonarcloud.io/api/project_badges/measure?project=openfoodfacts_openfoodfacts-androidapp&metric=alert_status)](https://sonarcloud.io/dashboard/index/openfoodfacts_openfoodfacts-androidapp)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/openfoodfacts/localized.svg)](https://crowdin.com/project/openfoodfacts)
![Build](https://github.com/openfoodfacts/openfoodfacts-androidapp/workflows/Android%20Integration/badge.svg)
[![Open Source Helpers](https://www.codetriage.com/openfoodfacts/openfoodfacts-androidapp/badges/users.svg)](https://www.codetriage.com/openfoodfacts/openfoodfacts-androidapp)
<br>

- [Open Beauty Facts](https://play.google.com/store/apps/details?id=org.openbeautyfacts.scanner), [Open Pet Food Facts](https://play.google.com/store/apps/details?id=org.openpetfoodfacts.scanner) and [Open Products Facts](https://play.google.com/store/apps/details?id=org.openproductsfacts.scanner) are also built from this codebase.

<details><summary><h2> What is Open Food Facts? </h2></summary>

### A food products database

Open Food Facts is a database of food products with ingredients, allergens, nutrition facts‚Ä¶ which allow us to compute scores like Nutri-Score, NOVA groups and Eco-Score.

### Made by everyone

Open Food Facts is a non-profit association of volunteers.
25000+ contributors like you have added 3M+ products from 150 countries using our Android or iPhone apps to scan barcodes and upload pictures of products and their labels.

### For everyone

Data about food is of public interest and has to be open. The complete database is published as open data and can be reused by anyone.

</details>

## User flows
[Visual documentation of the App on Figma](https://www.figma.com/file/BQ7CSyFvl7D9ljcXT0ay0u/Navigation-within-the-app)

## Documentation of the source code
The documentation is generated automatically from the source code and your improvements to code documentation are published automatically.
[Code documentation on GitHub pages](https://openfoodfacts.github.io/openfoodfacts-androidapp/)

## Helping with our next release
Here are issues and feature requests you can work on:
- [ ] [3.6.6 milestone](https://github.com/openfoodfacts/openfoodfacts-androidapp/milestone/36)

<details><summary><h2> What can I work on ? </h2></summary>

Open Food Facts on Android has 0,5M users and 1,6M products. *Each contribution you make will have a large impact on food transparency worldwide.* Finding the right issue or feature will help you have even more more impact. Feel free to ask for feedback on the #android channel before you start work, and to document what you intend to code.

- [Here are issues and feature requests you can work on](https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/4169)
- [P1 issues](https://github.com/openfoodfacts/openfoodfacts-androidapp/labels/p1)
- [Small issues (Hacktoberfest)](https://github.com/openfoodfacts/openfoodfacts-androidapp/labels/hacktoberfest)


If you don't have time to contribute code, you're very welcome to
* Scan new products
* [**Make a donation** to help pay for the hosting and general costs](https://donate.openfoodfacts.org) 

## Help translate Open Food Facts in your language

You can help translate Open Food Facts and the app at (no technical knowledge required, takes a minute to signup): <br>
https://translate.openfoodfacts.org

</details>

## Installation

| Choose the right flavor | Install steps|
| ------------- | ------------- |
|<img src=""https://user-images.githubusercontent.com/1689815/39445509-8064b2f8-4cbb-11e8-908d-86bcd61cb4f5.png"" height=""300""> | * Download the latest [Android Studio](https://developer.android.com/studio) stable build. <br>* If you are running the app for the first time, Android Studio will ask you to install the Gradle dependencies. <br>* If you are a new contributor to open-source, we recommend you read our [Setup Guidelines](https://github.com/openfoodfacts/openfoodfacts-androidapp/blob/master/SETUP_GUIDELINES.md) <br>* In Android Studio, make sure to select `OFF` as the default flavor for Open Food Facts (`OBF` is Open Beauty Facts, `OPF` - Open Products Facts, `OPFF` - Open Pet Food Facts) <br>* You should be able to install Open Food Facts on your phone using an USB cable, or run it in an emulator. <br>* <b>The package name on the Play Store is org.openfoodfacts.scanner.</b> For historic reasons, it's openfoodfacts.github.scrachx.openfood in the code and on F-Droid.|

## Running a Fastlane lane
The project uses Fastlane to automate release and screenshots generation.
* First time you checkout, run `bundle install` at the root of the project.
* Then launch lanes using `bundle exec fastlane release` (for example the release lane).
* We're moving Fastlane related things to https://github.com/openfoodfacts/fastlane-descriptions.

### Who do I talk to?

* Any member of the Android team or contact@openfoodfacts.org
* Join our #android and #android-alerts discussion room on Slack (Get an invite: <https://slack.openfoodfacts.org/>)

### Contributing 

If you're new to open-source, we recommend to checkout our [Contributing Guidelines](https://github.com/openfoodfacts/openfoodfacts-androidapp/blob/master/CONTRIBUTING.md). Feel free to fork the project and send a pull request.

<details><summary><h2> Libraries Used </h2></summary>
We use the following libraries, and we're not closed to changes where relevant :-)

<b> If you spot any libraries we added or we don't use anymore, feel free to update this list using a Pull Request. </b>

- [Dagger 2](https://github.com/google/dagger) - A fast dependency injector for Android and Java
- [Retrofit](https://square.github.io/retrofit/) - Retrofit turns your REST API into a Java interface
- [OkHttp](https://github.com/square/okhttp) - An HTTP+SPDY client for Android and Java applications
- [Mockito](https://github.com/mockito/mockito) - Most popular Mocking framework for unit tests written in Java
- [Apache](https://github.com/apache/commons-io) - The Apache Commons IO library contains utility classes, stream implementations, file filters, file comparators, endian transformation classes, and much more.
- [Kotlin Coroutines](https://developer.android.com/kotlin/coroutines) - A coroutine is a concurrency design pattern that you can use on Android to simplify code that executes asynchronously.  
- [Hilt](https://developer.android.com/training/dependency-injection/hilt-android) - Hilt is a dependency injection library for Android that reduces the boilerplate of doing manual dependency injection in your project. 
- [Dagger](https://developer.android.com/training/dependency-injection/dagger-android) - Manual dependency injection or service locators in an Android app can be problematic depending on the size of your project. You can limit your project's complexity as it scales up by using Dagger to manage dependencies. Dagger automatically generates code that mimics the code you would otherwise have hand-written.
- [Jackson](https://github.com/FasterXML/jackson) - Core part of Jackson that defines Streaming API as well as basic shared abstractions
- [journeyapps/zxing-android-embedded](https://github.com/journeyapps/zxing-android-embedded) - Barcode scanner library for Android, based on the ZXing decoder
- GreenDao
- [mikepenz/MaterialDrawer](https://github.com/mikepenz/MaterialDrawer) - The flexible, easy to use, all in one drawer library for your Android project.

Big thanks to their contributors!

</details>

<details> <summary><h2>Copyright and License</h2></summary>

    Copyright 2016-2022 Open Food Facts

    Licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       https://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an ""AS IS"" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and 
    limitations under the License.
</details>

<br><br>

## Contributors

The project was initially started by [Scot Scriven](https://github.com/itchix), other contributors include:
- [Aur√©lien Leboulanger](https://github.com/herau)
- [Pierre Slamich](https://github.com/teolemon)
- [Friedger M√ºffke](https://github.com/friedger)
- [Qian Jin](https://github.com/jinqian)
- [Fred Deniger](https://github.com/deniger)
- [VaiTon](https://github.com/VaiTon)

<br><br>
<a href=""https://github.com/openfoodfacts/openfoodfacts-androidapp/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=openfoodfacts/openfoodfacts-androidapp"" />
</a>
","'android', 'crowdsourcing', 'environment', 'food', 'gsoc', 'hacktoberfest', 'java', 'kotlin', 'kotlin-android', 'nutrition', 'openfoodfacts'",2024-04-26T09:48:55Z,30,747,37,"('teolemon', 5637), ('VaiTon', 930), ('dependabotbot', 372), ('itchix', 206), ('herau', 153), ('deniger', 140), ('huzaifaiftikhar', 85), ('PrajwalM2212', 62), ('naivekook', 55), ('dobriseb', 46), ('philippeauriach', 45), ('kartikaysharma01', 44), ('ripetrescu', 40), ('g123k', 36), ('subhanjansaha', 35), ('jaindiv26', 34), ('mustaqmustu', 33), ('Karljoones', 32), ('Shubham-vish', 28), ('ross-holloway94', 26), ('github-actionsbot', 26), ('mrudultora', 24), ('jaztriumph', 24), ('dependabot-previewbot', 23), ('balaji-ramavathu', 22), ('brandenfung', 22), ('raphael0202', 19), ('RajaVamsi11', 13), ('prashantkh19', 12), ('iml-v', 12)"
electricitymaps/electricitymaps-contrib,A real-time visualisation of the CO2 emissions of electricity consumption,"<p align=""center"">
  <a href=""https://www.app.electricitymaps.com"">
    <img alt=""Electricity Maps"" src=""https://raw.githubusercontent.com/electricitymaps/electricitymaps-contrib/master/web/public/images/electricitymaps-icon.svg"" width=""100"" />
  </a>
</p>
<h1 align=""center"">
  Electricity Maps
</h1>

<p align=""center"">
A real time and historical visualisation of the Greenhouse Gas Intensity (in terms of CO<sub>2</sub> equivalent) of electricity production and consumption around the world.<br>
  <strong><a href=""https://app.electricitymaps.com"">app.electricitymaps.com ¬ª</a></strong>
</p>

<p align=""center"">
  <img alt=""GitHub last commit"" src=""https://img.shields.io/github/last-commit/electricitymaps/electricitymaps-contrib"">
  <a href=""https://github.com/electricitymaps/electricitymaps-contrib/releases"">
    <img alt=""GitHub release (latest by date)"" src=""https://img.shields.io/github/v/release/electricitymaps/electricitymaps-contrib""></a>
  <a href=""https://github.com/electricitymaps/electricitymaps-contrib/CONTRIBUTING.md"">
    <a href=""https://github.com/electricitymaps/electricitymaps-contrib/blob/master/LICENSE.md"">
    <img src=""https://img.shields.io/github/license/electricitymaps/electricitymaps-contrib"" alt=""Electricity Maps is released under the GNU-AGPLv3 license."" /></a>
  <a href=""https://slack.electricitymaps.com"">
    <img src=""https://img.shields.io/badge/slack-3700%2B-%23126945"" alt=""Join our Slack"" /></a>
  <a href=""https://twitter.com/intent/follow?screen_name=ElectricityMaps"">
    <img src=""https://img.shields.io/twitter/follow/ElectricityMaps"" alt=""Twitter Follow"" /></a>
</p>

![image](web/public/images/electricitymap_social_image.png#gh-light-mode-only)
![image](web/public/images/electricitymap_social_image_dark.png#gh-dark-mode-only)

## Introduction

This project aims to provide a free, open-source, and transparent visualisation of the carbon intensity of electricity consumption around the world.

We fetch the raw production data from public, free, and official sources. They include official government and transmission system operators' data. We then run [our flow-tracing algorithm](https://www.electricitymaps.com/blog/flow-tracing) to calculate the actual carbon intensity of a country's electricity consumption.

_Try it out at [app.electricitymaps.com](https://app.electricitymaps.com), or download the app on [Google Play](https://play.google.com/store/apps/details?id=com.tmrow.electricitymap&utm_source=github) or [App store](https://itunes.apple.com/us/app/electricity-map/id1224594248&utm_source=github)._

## Contributing

The Electricity Maps app is a community project and we welcome contributions from anyone!

We are always looking for help to build parsers for new countries, fix broken parsers, improve the frontend app, improve accuracy of data sources, discuss new potential data sources, update region capacities, and much more.

Read our [contribution guidelines](/CONTRIBUTING.md) to get started.

## Community & Support

Use these channels to be part of the community, ask for help while using Electricity Maps, or just learn more about what's going on:

- [Slack](https://slack.electricitymaps.com): This is the main channel to join the community. You can ask for help, showcase your work, and stay up to date with everything happening.
- [GitHub Issues](https://github.com/electricitymaps/electricitymaps-contrib/issues): Raise any issues you encounter with the data or bugs you find while using the app.
- [GitHub Discussions](https://github.com/electricitymaps/electricitymaps-contrib/discussions): Join discussions and share new ideas for features.
- [GitHub Wiki](https://github.com/electricitymaps/electricitymaps-contrib/wiki): Learn more about methodology, guides for how to set up development environment, etc.
- [FAQ](https://app.electricitymaps.com/FAQ): Get your questions answered in our FAQ.
- [Our Commercial Website](https://electricitymaps.com/): Learn more about how you or your company can use the data too.
- [Our Blog](https://electricitymaps.com/blog/): Read about the green transition and how Electricity Maps is helping to accelerate it.
- [Twitter](https://twitter.com/electricitymaps): Follow for latest news
- [LinkedIn](https://www.linkedin.com/company/electricitymaps): Follow for latest news

## License

This repository is licensed under GNU-AGPLv3 since v1.5.0, find our license [here](https://github.com/electricitymaps/electricitymaps-contrib/blob/master/LICENSE.md). Contributions prior to commit [cb9664f](https://github.com/electricitymaps/electricitymaps-contrib/commit/cb9664f43f0597bedf13e832047c3fc10e67ba4e) were licensed under [MIT license](https://github.com/electricitymaps/electricitymaps-contrib/blob/master/LICENSE_MIT.txt)

## Frequently asked questions

_We also have a lot more questions answered on [app.electricitymaps.com/faq](https://app.electricitymaps.com/faq)!_

**Where does the data come from?**
The data comes from many different sources. You can check them out [here](https://github.com/electricityMaps/electricitymaps-contrib/blob/master/DATA_SOURCES.md)

**Why do you calculate the carbon intensity of _consumption_?**
In short, citizens should not be responsible for the emissions associated with all the products they export, but only for what they consume.
Consumption-based accounting (CBA) is a very important aspect of climate policy and allows assigning responsibility to consumers instead of producers.
Furthermore, this method is robust to governments relocating dirty production to neighboring countries in order to green their image while still importing from it.
You can read more in our blog post [here](https://electricitymaps.com/blog/flow-tracing/).

**Why don't you show emissions per capita?**
A country that has few inhabitants but a lot of factories will appear high on CO<sub>2</sub>/capita.
This means you can ""trick"" the numbers by moving your factory abroad and import the produced _good_ instead of the electricity itself.
That country now has a low CO<sub>2</sub>/capita number because we only count CO<sub>2</sub> for electricity (not for imported/exported goods).
The CO<sub>2</sub>/capita metric, by involving the size of the population, and by not integrating all CO<sub>2</sub> emission sources, is thus an incomplete metric.
CO<sub>2</sub> intensity on the other hand only describes where is the best place to put that factory (and when it is best to use electricity), enabling proper decisions.

**CO<sub>2</sub> emission factors look high ‚Äî what do they cover exactly?**
The carbon intensity of each type of power plant takes into account emissions arising from the whole life cycle of the plant (construction, fuel production, operational emissions and decommissioning). Read more on the [Emissions Factor Wiki page](https://github.com/electricitymaps/electricitymaps-contrib/wiki/Emission-factors).

**How can I get access to historical data or the live API?**
All this and more can be found **[here](https://electricitymaps.com/)**.
You can also visit our **[data portal](https://www.electricitymaps.com/data-portal)** to download historical datasets.
","'climate-change', 'data-visualization', 'hacktoberfest', 'sustainability'",2024-05-02T14:21:03Z,30,3291,76,"('corradio', 1760), ('VIKTORVAV99', 305), ('dependabotbot', 231), ('madsnedergaard', 205), ('systemcatch', 154), ('tonypls', 120), ('unitrium', 119), ('mathilde-daugy', 118), ('pierresegonne', 113), ('alixunderplatz', 83), ('nessie2013', 71), ('electricitymapsbot', 69), ('fbarl', 67), ('maxbellec', 60), ('FelixDQ', 59), ('tmrow-bot', 57), ('skovhus', 56), ('magol', 53), ('brunolajoie', 52), ('jarek', 52), ('q--', 49), ('Raffox97', 45), ('KabelWlan', 42), ('BLACKLEG', 42), ('jkopb', 36), ('lorrieq', 28), ('ovbm', 28), ('veqtrus', 28), ('silkeholmebonnen', 24), ('amv213', 17)"
WorldBank-Transport/DRIVER,"DRIVER - Data for Road Incident Visualization, Evaluation, and Reporting ","# DRIVER
DRIVER - Data for Road Incident Visualization, Evaluation, and Reporting

[![Build Status](https://travis-ci.org/WorldBank-Transport/DRIVER.svg?branch=develop)](https://travis-ci.org/WorldBank-Transport/DRIVER)

## Deploying

1. Follow the Installation instructions below
2. Follow the instructions in doc/system-administration.md

## Developing

### Installation

1. Install Vagrant 1.5+

1. Install Ansible 1.8+

1. Install `vagrant-hostmanager` plugin via:

    ```bash
    vagrant plugin install vagrant-hostmanager
    ```

1. Prevent changes in `group_vars/development` from being tracked by git.

    - You will likely make changes to `group_vars/development` to configure your local environment. To make sure you don't commit those changes unless you need to change the default development settings, you can make git not track changes to that file. To do this, run `git update-index --assume-unchanged deployment/ansible/group_vars/development`.
    - To revert back to tracking changes, run `git update-index --no-assume-unchanged deployment/ansible/group_vars/development`.

1. Create `gradle/data/driver.keystore`

    - To run in development without support for JAR file building:
      ```bash
      touch gradle/data/driver.keystore
      ```
      (If you just want to install the DRIVER web interface, do this. You can add Android integration later.)

    - To build schema model JAR files for the Android app, copy the signing keystore to `gradle/data/driver.keystore`
    and set the password for the keystore under `keystore_password` in `deployment/ansible/group_vars/development`.

1. (Optional) To enable geocoding, [set up Pickpoint in `group_vars/development`](#pickpoint)

1. Install [NFS](https://en.wikipedia.org/wiki/Network_File_System). On Debian/Ubuntu, run:

    ```bash
    sudo apt-get install nfs-common nfs-kernel-server
    ```

1. Start the Vagrant VM
    ```bash
    vagrant up
    ```

    If you run into issues provisioning the VMs or forget a step, try re-provisioning as needed:
    ```bash
    vagrant provision <vm-name>
    ```

### Pickpoint

Pickpoint is a geocoding service used by DRIVER to obtain lat/lon coordinates from input addresses. DRIVER can work without Pickpoint configured, but to enable geocoding, obtain a pickpoint API key from https://pickpoint.io and enter the key in `deployment/ansible/group_vars/development` under `web_js_nominatim_key`.

### Running & Configuration

The app is available on http://localhost:7000/, and the schema editor at http://localhost:7000/editor/.

In development environments a default Django superuser will be created for you:
  - Username: `admin`
  - Password: `admin`

### Google OAuth

To configure Google OAuth for development, follow [these steps](https://support.google.com/googleapi/answer/6158849?hl=en&ref_topic=7013279) to create a web application and credentials for your local DRIVER instance.

When creating a client ID for your web application, use these URLs:

**Authorized JavaScript origins**:

http://localhost:7000

**Authorized redirect URIs**:

http://localhost:7000/openid/callback/login/

Once you have the client ID and client secret, add those values to `deployment/ansible/group_vars/development` and reprovision the `app` VM  as needed:
```bash
vagrant provision app
```

### Frontend
Both Angular apps can be run in development mode via:
```bash
./scripts/grunt.sh editor serve
```
and
```bash
./scripts/grunt.sh web serve
```
You will need to run these commands in separate terminals if you'd like to have both running at the same time.

The frontend app will be available on port 7002 at http://localhost:7002 and the schema editor will be available on port
7001 at http://localhost:7001. Both will reload automatically as changes are made.

To make requests to a Django runserver directly (for example, to perform interactive debugging in
the request-response cycle), run:
```bash
./scripts/manage.sh runserver 0.0.0.0:8000
```
You should then be able to access the Django runserver on port 3001 of the `app` VM at http://localhost:3001.

Front end files are mounted inside the `app` Vagrant VM at `/opt/schema_editor` for the Angular editor and `/opt/web` for the Angular interface.

#### Updating existing translation files
New Angular translation tokens should be added to i18n/exclaim.json with a value of ""!<english>"".
The English translation (en-us.json) is automatically built from exclaim.json. New tokens are also
propagated to other translations via a grunt task:

```bash
./scripts/grunt.sh web translate
```

#### Adding a new translation file
Place the new JSON file in the i18n folder. Add the file to the i18nForeignLanguages var in Gruntfile.js.
To enable the language to be selected via the language picker, add an item to the `languages` list in
`deployment/ansible/group_vars/development`. Setting `rtl` to true will enable right-to-left CSS changes.


### Docker
To update the Docker container images to reflect environment changes (Such as changed Python packages), provision the `app` VM:
```bash
vagrant provision app
```

### Testing

#### Javascript
To run the Javascript automated tests, use:
```bash
./scripts/grunt.sh web test
```

## Testing Data

### Boundaries
Geographic boundaries are used to filter records to a defined area, such as a region or state. These boundaries are created by uploading shape files to the editor, http://localhost:7000/editor under ""Add new geographies"".

For developers at Azavea, use the `regions.zip` and `states.zip` files available in the DRIVER project folder on the fileshare. For non-Azavea users, upload a zipped shapefile containing the boundaries of the jurisdictions where you plan to operate DRIVER. If you don't have such a shapefile, [Natural Earth](https://www.naturalearthdata.com/features/) is a good place to start.""

After uploading each the file, select `name` as the display field, then hit save. Either refresh the page or navigate somewhere else in between uploads.

### Records
Record data can be populated from a CSV file that contains named columns for `""lat""`, `""lon""`, and `""record_date""`. A file with semi-realistic data can be found in `scripts/sample_data/sample_traffic.csv` for use. For developers at Azavea, CSV files containing historical data can be downloaded from the `/data` folder of the project's directory in the fileshare, with names of the format `<city or agency>_traffic.csv`.

In order to import record data you will have to obtain an Authorization header and its API token. To do this, log in to the web application, then open the network tab in web developer tools and reload the page. Inspect the request headers
from an API request and pull out the value of the `Authorization` header, for example
`Token f1acac96cc79c4822e9010d23ab425231d580875`.

Using the API token, run:
```bash
python scripts/load_incidents_v3.py --authz 'Token <YOUR_AUTH_TOKEN>' scripts/sample_data/
```
Note that the import process will take roughly two hours for the full data set; you can cut down the
number of records with `head` on the individual CSVs.

The `load_incidents_v3.py` script will also create a schema for you. If you already have a schema in place, and simply want to load data associated with that schema, you will need to modify the script accordingly: change the `schema_id = create_schema(...)` line with `schema_id = 'replace-this-with-the-existing-schema-id'`.

To load mock black spots, run:
```bash
python scripts/load_black_spots.py --authz 'Token <YOUR_AUTH_TOKEN>' /path/to/black_spots.json
```
Mock black spot data is available in `scripts/sample_data/black_spots.json`.

To load mock interventions, run:
```bash
python scripts/load_interventions.py --authz 'Token <YOUR_AUTH_TOKEN>' /path/to/interventions_sample_pts.geojson
```
Mock intervention data is available in `scripts/sample_data/interventions_sample_pts.geojson`.

To generate black spot and load forecast training inputs, run:
```bash
python scripts/generate_training_input.py /path/to/roads.shp /path/to/records.csv
```

More information on the requirements for loading data can be found in the [`scripts/`
directory](./scripts/README.md).

### Costs

You can't request records with associated costs successfully until you configure some costs.
To do this, navigate to your editor (by default on http://localhost:7000/editor/), select ""Incident"" from
record types in the menu on the left. (If there are multiple record types named ""Incident"", delete all but one.) Select ""Cost aggregation settings"", then:

- Choose a currency prefix in ""Cost Prefix"" (e.g., `$`, but anything is fine)
- Select ""Incident Details"" in ""Related Content Type""
- Choose ""Severity"" in ""Field""
- Then decide how much money you think human lives, human physical security, and property are worth

## Production

TODO: Notes on creating a production superuser and adding a production OAuth2 application


## Using OAuth2 / Getting tokens

Get a token:
```bash
curl -X POST -d ""grant_type=password&username=<user_name>&password=<password>"" -u""<client_id>:<client_secret>"" http://localhost:7000/o/token/
```

Returns:
```json
{
    ""access_token"": ""<your_access_token>"",
    ""token_type"": ""Bearer"",
    ""expires_in"": 36000,
    ""refresh_token"": ""<your_refresh_token>"",
    ""scope"": ""read write groups""
}
```

Note: If you're experiencing SSL errors with cURL, your version of cURL may not have the right certificate authorities installed. Try passing the `-k` parameter to `curl`.

Making requests with a token:
```bash
# GET
curl -H ""Authorization: Bearer <your_access_token>"" http://localhost:7000:/api/record/
curl -H ""Authorization: Bearer <your_access_token>"" http://localhost:7000:/api/recordschema/
```

Restricted access (disabled in development to allow access to the browsable API):

Add an additional `scope` parameter to token request:
```bash
curl -X POST -d ""grant_type=password&username=<user_name>&password=<password>&scope=read"" -u""<client_id>:<client_secret>"" http://localhost:7000/o/token/
```

Now, this token will have read-only access to the API.

## Releases

Releases use a `github_changelog_generator` tool written in `ruby`.

- Make sure your `develop` is up-to-date
- Start the Gitflow release:
    ```bash
    git flow release start <your release version>
    ```
-
    ```bash
    docker run -ti --rm -v ${PWD}:/changelog -w /changelog ruby:2.5 /bin/bash
    ```
- From the container:
    ```bash
    gem install github_changelog_generator
    ```
- Then, to generate the changelog since the last release:
    ```bash
    $ export RELEASE_VERSION=<your release version>
    $ export LAST_RELEASE=<the most recent tag>
    $ export GITHUB_TOKEN=<your github personal access token>
    $ github_changelog_generator ""WorldBank-Transport/DRIVER"" \
        --token ${GITHUB_TOKEN} \
        --since-tag ${LAST_RELEASE} \
        --future-release ${RELEASE_VERSION} \
        --base CHANGELOG.md \
        --no-issues \
        --no-issues-wo-labels \
        --no-author
    ```

It's important to include the `since-tag` argument, since without it, the changelog generator
will include everything that went into 1.0.0, which is a lot of stuff and not super meaningful,
since `1.0.0` is ""what was there when we decided to start using semantic versioning.""
Note: We've had some problems with the `since-tag` argument not being respected; if this happens,
manually delete the duplicate entries and update the GitHub diff link.

- Include the CHANGELOG in your release branch
- Git flow publish the release:
    ```
    git flow release publish <your release version>
    ```
- Open a PR for your release
- Wait for a successful build and approval (from whom?), then:
    ```bash
    $ git flow release finish <your release version>
    $ git checkout master
    $ git tag -f <your version>  # git-flow puts the tag on `develop`
    $ git push origin master
    $ git checkout develop
    $ git push origin develop
    $ git push [-s] --tags
    ```

:tada:
",,2022-12-21T07:37:29Z,17,34,15,"('kshepard', 478), ('flibbertigibbet', 361), ('ddohler', 236), ('moradology', 170), ('pcaisse', 96), ('KlaasH', 88), ('CloudNiner', 47), ('shreshthkhilani', 45), ('fungjj92', 36), ('rmartz', 31), ('mtedeschi', 30), ('jisantuc', 20), ('hectcastro', 16), ('sharph', 12), ('notthatbreezy', 11), ('jeancochrane', 6), ('jerheff', 1)"
mysociety/pombola,,"# Pombola

This web app allows you to store and share information on public figures,
especially politicians.

For detailed background about the project as of 2018 please see
[docs/BACKGROUND.md](docs/BACKGROUND.md). For an overview of the system see
[docs/OVERVIEW.md](docs/OVERVIEW.md).


## Installing

Please see [docs/INSTALL.md](docs/INSTALL.md)

To change your site's look and feel please see the [styling notes](docs/STYLING_NOTES.md).

For software developers, you can see information about running
the tests in [docs/TESTING.md](docs/TESTING.md).

## Troubleshooting

Please see [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) file

## Future

The Pombola codebase was originally written for the Kenyan site
[mzalendo.com](http://info.mzalendo.com). It has since been modified to support
other sites around the world, notably several in Africa.

We want the code to be easy to use for other installations, but currently there are some rough edges.

## Acknowledgements

We are very grateful for the funding that has made development
of Pombola possible, in particular the generous support from:

* [The Indigo Trust](http://indigotrust.org.uk/)
* [The Omidyar Network](https://www.omidyar.com/)

Thanks also to [Browserstack](https://www.browserstack.com/) who
let us use their web-based cross-browser testing tools for this
project.

## mySociety.org Software Licensing

Most of the software in this directory is Copyright (c) 2004-2019 UK
Citizens Online Democracy.

Unless otherwise stated in particular files or directories, this
software is free software; you can redistribute it and/or modify it
under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Can you explain briefly what the GNU Affero GPL is? We offer the
source code of our websites to our users. The GNU Affero GPL has the
requirement that anyone else using that code for their own websites
also does the courtesy of offering the source code to their users.

Why not use the GPL? The GPL guarantees that anyone who gets a binary
version of the software also gets the source code so they can modify
it. Since users of websites never get the binary, just HTML pages, it
is no better a license than a BSD style license would be for them.
For this reason, we use the GNU Affero GPL.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Affero General Public License for more details.

Information about the GNU Affero GPL:
http://www.fsf.org/licensing/licenses/agpl-3.0.html

A copy of the GNU Affero General Public License can be found in [LICENSE.txt](/LICENSE.txt).
",,2020-01-30T14:09:49Z,30,65,21,"('mhl', 1530), ('chrismytton', 540), ('evdb', 412), ('wrightmartin', 233), ('zarino', 170), ('jacksonj04', 156), ('idesouza', 125), ('osfameron', 97), ('eokyere', 95), ('lizconlan', 67), ('dracos', 66), ('geoffkilpin', 64), ('Jedidiah', 27), ('octopusinvitro', 23), ('JenMysoc', 22), ('sagepe', 18), ('tmtmtmtm', 16), ('teuben', 14), ('davea', 12), ('struan', 11), ('xybrnet', 9), ('stevenday', 8), ('ajparsons', 4), ('dependabot-previewbot', 3), ('paullenz', 3), ('longhotsummer', 2), ('MikeMuli', 2), ('dependabot-support', 2), ('robinkiplangat', 1), ('duncanparkes', 1)"
orcasound/orcanode,"Software for live-streaming and recording lossy (AAC) or lossless compressed audio (HLS, DASH, FLAC) via AWS S3 buckets. :star:","# Orcasound's orcanode code for live-streaming audio data

The `orcanode` software repository contains audio tools and scripts for capturing, reformatting, transcoding and uploading audio data at each node of a network. Orcanode live-streaming should work on Intel (amd64) or Raspberry Pi (arm32v7) platforms using any soundcard.  The most common hardware used by Orcasound is the [Pisound HAT](https://blokas.io/pisound/) on a Raspberry Pi (3B+ or 4) single-board computer.

There is a `base` set of tools and a couple of specific projects in the `node` and `mseed` directories. The node directory is for new locations streaming within the Orcasound listening network (primary nodes).

The mseed directory has code for converting audio data in the mseed format to the live-streaming audio format used by primary nodes. This conversion code is mainly used for audio data collected by the [Ocean Observatories Initiative or OOI](https://oceanobservatories.org/ ""OOI"") network.  See the README in the `mseed` directory for more info. Transcoding from other audio formats should likely go in new directories by encoding scheme, similar to the mseed directory... 

You can also gain some bioacoustic context for the project in the [orcanode wiki](https://github.com/orcasound/orcanode/wiki).

## Background & motivation

This code was developed for live-streaming from source nodes in the [Orcasound](http://orcasound.net) hydrophone network (WA, USA). Thus, the repository names begin with ""orca""! Our primary motivation is to make it easy for community scientists to listen for whales via the [Orcasound web app](https://live.orcasound.net) using their favorite device/OS/browser.

We also aspire to use open source software as much as possible. We rely heavily on [FFmpeg](https://www.ffmpeg.org/). One of our long-term goals is to stream lossless [FLAC](https://xiph.org/flac/)-encoded audio within [DASH](https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP) segments to a player that works optimally on as many listening devices as possible. For now (2018-2023) we have found the best end-to-end performance across the broadest range of web browsers is acheived by streaming AAC-encoded audio within [HLS](https://developer.apple.com/streaming/) segments. 

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See the deployment section (below) for notes on how to deploy the project on a live system like [live.orcasound.net](https://live.orcasound.net).

If you want to set up your hardware to host a hydrophone within the Orcasound network, take a look at [how to join Orcasound](http://www.orcasound.net/join/) and [our prototype built from a Raspberry Pi with the Pisound ADC HAT](http://www.orcasound.net/2018/04/27/orcasounds-new-live-audio-solution-from-hydrophone-to-headphone-with-a-raspberry-pi-computer-and-hls-dash-streaming-software/).

The general scheme is to acquire audio data from a sound card within a Docker container via ALSA or Jack and FFmpeg, and then stream the audio data with minimal latency to cloud-based storage (as of Oct 2021, we use AWS S3 buckets). Errors/etc are logged to LogDNA via a separate Docker container.

### Prerequisites

An ARM or X86 device with a sound card (or other audio input devices) connected to the Internet (via wireless network or ethernet cable) that has [Docker-compose](https://docs.docker.com/compose/install/) installed and an AWS account with some S3 buckets set up.

### Installing

Create a base docker image for your architecture by running the script in /base/rpi or /base/amd64 as appropriate.  You will need to create a .env file as appropriate for your projects.  Common to to all projects are the need for AWS keys

```
AWSACCESSKEYID=YourAWSaccessKey
AWSSECRETACCESSKEY=YourAWSsecretAccessKey
 
SYSLOG_URL=syslog+tls://syslog-a.logdna.com:YourLogDNAPort
SYSLOG_STRUCTURED_DATA='logdna@YourLogDNAnumber key=""YourLogDNAKey"" tag=""docker""
```

(You can request keys via the #hydrophone-nodes channel in the Orcasound Slack. As of October, 2023, we are continuing to use AWS S3 for storage and LogDNA for live-logging and troubleshooting.)

Here are explanations of some of the .env fields:

* NODE_NAME should indicate your device and it's location, ideally in the form `device_location` (e.g. we call our Raspberry Pi staging device in Seattle `rpi_seattle`. 
* NODE_TYPE determines what audio data formats will be generated and transferred to their respective AWS buckets. 
* AUDIO_HW_ID is the card, device providing the audio data. Note: you can find your sound device by using the command ""arecord -l"".  For Raspberry Pi hardware with pisound just use AUDIO_HW_ID=pisound
* CHANNELS indicates the number of audio channels to expect (1 or 2). 
* FLAC_DURATION is the amount of seconds you want in each archived lossless file. 
* SEGMENT_DURATION is the amount of seconds you want in each streamed lossy segment.


## Supported combinations


| NODE ARCHITECTURE | node | mseed |
|-------------------|------|-------|
| arm32v7           |  Y   |  N    |
| amd64             |  Y   |  Y    |



| NODE ARCHITECTURE | hls-only | research | dev-virt |
|-------------------|----------|----------|----------|
| arm32v7           | Y        | Y        | N        |
| amd64             | Y        | N        | Y        |



| NODE Hardware     | hls-only | research |
|-------------------|----------|----------|
| RPI4              | Y        | Y        |
| RPI3 B-           | Y        | N        |



## Running local tests

In the repository directory (where you also put your .env file) first copy the compose file you want to docker-compose.yml.  For example if you are raspberry pi and you want to use the prebuilt image then copy docker-compose.rpi-pull.yml to docker-compose.  Then run `docker-compose up -d`. Watch what happens using `htop`. If you want to verify files are being written to /tmp or /mnt directories, get the name of your streaming service using `docker-compose ps` (in this case `orcanode_streaming_1`) and then do `docker exec -it orcanode_streaming_1 /bin/bash` to get a bash shell within the running container.

## Running an end-to-end test

Once you've verified files are making it to your S3 bucket (with public read access), you can test the stream using a browser-based reference player.  For example, with [Bitmovin HLS/MPEG/DASH player](https://bitmovin.com/demos/stream-test?format=hls&manifest=) you can use select HLS and then paste the URL for your current S3-based manifest (`.m3u8` file) to listen to the stream (and observe buffer levels and bitrate in real-time).

Your URL should look something like this:
```
https://s3-us-west-2.amazonaws.com/dev-streaming-orcasound-net/rpi_seattle/hls/1526661120/live.m3u8
```
For end-to-end tests of Orcasound nodes, this schematic describes how sources map to the `dev`, `beta`, and `live` subdomains of orcasound.net --

![Schematic of Orcasound source-subdomain mapping](http://orcasound.net/img/orcasound-app/Orcasound-software-evolution-model.png ""Orcasound software evolution model"")

([Google draw source](https://drive.google.com/file/d/1YFTAQPqgtcTl6ubac0mgyQ7fvg0BZqzH/view?usp=sharing) and [archived schematics](https://orcasound.net/img/orcasound-app/)) -- and you can monitor your development stream via the web-app using this URL structure:

```dev.orcasound.net/dynamic/node_name``` 

For example, with node_name = rpi_orcasound_lab the test URL would be [dev.orcasound.net/dynamic/rpi_orcasound_lab](http://dev.orcasound.net/dynamic/rpi_orcasound_lab).


## Deployment

If you would like to add a node to the Orcasound hydrophone network, read through our [Administrative Handbook](https://github.com/orcasound/.github/wiki#3-administrative-handbook) and then contact admin@orcasound.net if you have any questions. 

## Built With

* [FFmpeg](https://www.ffmpeg.org/) - Uses ALSA to acquire audio data, then generates lossy streams and/or lossless archive files
* [rsync](https://rsync.samba.org/) - Transfers files locally from /tmp to /mnt directories
* [s3fs](https://github.com/s3fs-fuse/s3fs-fuse) - Used to transfer audio data from local device to S3 bucket(s)

## Contributing

Please read [CONTRIBUTING.md](https://github.com/orcasound/orcanode/blob/master/CONTRIBUTING) for details on our code of conduct, and the process for submitting pull requests.

## Authors

* **Steve Hicks** - *Raspberry Pi expert* - [Steve on Github](https://github.com/mcshicks)
* **Paul Cretu** - *Lead developer* - [Paul on Github](https://github.com/paulcretu)
* **Scott Veirs** - *Project manager* - [Scott on Github](https://github.com/scottveirs)
* **Val Veirs** - *Hydrophone expert* - [Val on Github](https://github.com/veirs)

See also the list of [orcanode contributors](https://github.com/orcasound/orcanode/graphs/contributors) who have helped this project and the [Orcasound Hacker Hall of Fame](https://www.orcasound.net/hacker-hall-of-fame/) who have advanced both Orcasound open source code and the hydrophone network in the habitat of the endangered Southern Resident killer whales.

## License

This project is licensed under the GNU Affero General Public License v3.0 - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Thanks to the backers of the 2017 Kickstarter that funded the development of this open source code.
* Thanks to the makers of the Raspberry Pi, the Pisound HAT (Blokas in Lithuania), and the manufacturers who supply us with long-lasting, cost-effective hydrophones.
* Thanks to the many friends and backers who helped improve maintain nodes and improve the [Orcasound app](https://github.com/orcasound/orcasite).
","'audio-recorder', 'audio-streaming', 'aws-s3', 'boto', 'dash', 'hls', 'hls-live-streaming', 'hls-server', 'hls-stream', 'mpeg-dash', 'mseed', 'python'",2023-11-13T07:07:18Z,9,32,8,"('scottveirs', 136), ('paulcretu', 38), ('orcasoundapp', 31), ('karan2704', 17), ('mcshicks', 12), ('joyliao07', 3), ('evanjscallan', 1), ('valentina-s', 1), ('kunakl07', 1)"
onaio/onadata,"Collect, Analyze and Share","Ona Platform
============

Collect, Analyze and Share Data!

.. image:: https://github.com/onaio/onadata/actions/workflows/ci.yml/badge.svg
  :target: https://github.com/onaio/onadata/actions/workflows/ci.yml

.. image:: https://app.codacy.com/project/badge/Grade/68c96351c8b24d5c9062a9c8247142f2
   :target: https://www.codacy.com/gh/onaio/onadata/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=onaio/onadata&amp;utm_campaign=Badge_Grade

.. image:: https://img.shields.io/badge/License-BSD_3--Clause-blue.svg
   :target: https://opensource.org/licenses/BSD-3-Clause


About
-----

Ona is derived from the excellent `formhub <http://github.com/SEL-Columbia/formhub>`_ platform developed by the Sustainable Engineering Lab at Columbia University.

Installation
------------

See the `installation documentation <https://api.ona.io/static/docs/install.html>`_.

Docker
------

Install `Docker <https://www.docker.com/get-docker>`_ and `Docker Compose <https://docs.docker.com/compose/>`_.

.. code-block:: sh

    docker-compose up

    # create super user
    # -----------------
    docker exec -it onadata_web_1 bash

    # activate virtual envirenment
    source /srv/.virtualenv/bin/activate

    python manage.py createsuperuser

It should be accessible via http://localhost:8000. The settings are in
`onadata/settings/docker.py <onadata/settings/docker.py>`_.

On registration check the console for the activation links, the default email
backend is ``django.core.mail.backends.console.EmailBackend``. See
`Django Docs <https://docs.djangoproject.com/en/1.11/topics/email/>`_ for details.

Contributing
------------

If you would like to contribute code please read
`Contributing Code to Ona Data <CONTRIBUTING.MD>`_.

Edit top level requirements in the file `requirements/base.in <requirements/base.in>`_. Use
 `pip-compile <https://github.com/nvie/pip-tools>`_ to update `requirements/base.pip <requirements/base.pip>`_.
 You will need to update `requirements.pip` and set `lxml==3.6.0`, for some unknown reason `pip-compile` seems to
 pick a lower version of lxml when `openpyxl` requires `lxml>=3.3.4`.

.. code-block:: sh

    pip-compile --output-file requirements/base.pip requirements/base.in

Copy `pre-commit.sh <pre-commit.sh>`_ into `.git/hooks/pre-commit`, it ensures staged python flake8 are in acceptable code style and conventions.

.. code-block:: sh

    cp pre-commit.sh .git/hooks/pre-commit
    chmod +x .git/hooks/pre-commit

**Security Acknowledgments**

We would like to thank the following security researchers for responsibly disclosing security issues:

============= ================  ==========  ==============
 Name          Date              Severity    Contribution
============= ================  ==========  ==============
Danish Tariq   1st April 2018     Medium     `Users able to create projects in other user accounts <https://github.com/onaio/onadata/commit/bdcd53922940739d71bc554ca86ab484de5feab8>`_
============= ================  ==========  ==============

Code Structure
--------------

* **api** - This app provides the API functionality mostly made up of viewsets

* **logger** - This app serves XForms to and receives submissions from
  ODK Collect and Enketo.

* **viewer** - This app provides a csv and xls export of the data stored in
  logger. This app uses a data dictionary as produced by pyxform. It also
  provides a map and single survey view.

* **main** - This app is the glue that brings logger and viewer
  together.

Localization
------------

To generate a locale from scratch (ex. Spanish)

.. code-block:: sh

    django-admin.py makemessages -l es -e py,html,email,txt ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -l es && cd - ; done

To update PO files

.. code-block:: sh

    django-admin.py makemessages -a ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -a && cd - ; done

To compile MO files and update live translations

.. code-block:: sh

    django-admin.py compilemessages ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py compilemessages && cd - ; done

Api Documentation
-----------------

Generate the API documentation and serve via Django using:

.. code-block:: sh

    cd docs
    make html
    python manage.py collectstatic

Generate sphinx docs for new code using
`autodoc <http://www.sphinx-doc.org/en/stable/invocation.html#invocation-of-sphinx-apidoc>`_.

Run sphinx in autobuild mode using:

.. code-block:: sh

    sphinx-autobuild docs docs/_build/html

Requires sphinx-autobuild, install with ``pip install sphinx-autobuild``.


Django Debug Toolbar
--------------------

* `$ pip install django-debug-toolbar`
* Use/see `onadata/settings/debug_toolbar_settings/py`
* Access api endpoint on the browser and use `.debug` as the format extension e.g `/api/v1/projects.debug`

Upgrading existing installation to django 1.9+
----------------------------------------------

**Requirements**

* Postgres 9.4 or higher
* xcode-select version 2343 or higher

**Upgrading from a pervious Ona setup**
Ensure you upgrade all your pip requirements using the following command:

.. code-block:: sh

    pip install -r requirements/base.pip

Fake initial migration of `guardian`, `django_digest`, `registration`. Migrate `contenttypes` app first.

.. code-block:: sh

    python manage.py migrate contenttypes
    python manage.py migrate --fake-initial django_digest
    python manage.py migrate --fake-initial guardian
    python manage.py migrate --fake-initial registration
    python manage.py migrate


**Major django changes affecting Ona**
* The DATABASES settings key depricates the use of the *autocommit* setting in the *OPTIONS* dictionary.
",,2024-05-02T10:07:47Z,30,181,55,"('ukanga', 2937), ('pld', 1700), ('larryweya', 1250), ('dorey', 1011), ('DavisRayM', 854), ('amarder', 816), ('KipSigei', 426), ('ivermac', 367), ('mejymejy', 281), ('denniswambua', 234), ('lincmba', 230), ('moshthepitt', 160), ('rgaudin', 151), ('prabhasp', 140), ('WinnyTroy', 132), ('mberg', 115), ('modilabs-bumblebee', 108), ('prajjwol', 101), ('FrankApiyo', 96), ('Wambere', 94), ('TomCoder', 84), ('royrutto', 77), ('mrmoje', 50), ('bmarika', 50), ('geoffreymuchai', 50), ('antonatem', 49), ('katembu', 46), ('modilabs-starscream', 44), ('kelvin-muchiri', 41), ('urbanslug', 40)"
opendatateam/udata,Customizable and skinnable social platform dedicated to open data.,"<p align=""center""><img src=""https://i.imgur.com/rlRox1c.png""></p>

udata
=====

Customizable and skinnable social platform dedicated to (open)data.

The [full documentation][readthedocs-url] is hosted on Read the Docs.

udata is maintained by [Etalab](https://www.etalab.gouv.fr/), the
french public agency in charge of open data.  Etalab is responsible
for publishing udata's roadmap and for building consensus around it.

It is collectively taken care of by members of the
[OpenDataTeam](https://github.com/opendatateam).

[readthedocs-url]: https://udata.readthedocs.io/en/latest/
","'flask', 'flask-restplus', 'opendata', 'portal', 'python', 'vue', 'vuejs'",2024-05-02T14:16:25Z,30,229,14,"('noirbizarre', 3719), ('davidbgk', 579), ('abulte', 366), ('crowdin-opendatateam', 334), ('pyup-bot', 326), ('maudetes', 220), ('quaxsze', 134), ('jphnoel', 51), ('taniki', 48), ('ThibaudDauce', 43), ('vinyll', 33), ('jdesboeufs', 22), ('micael-grilo', 20), ('bzg', 18), ('l-vincent-l', 16), ('nicolaskempf57', 11), ('pblayo', 10), ('udata-bot', 10), ('grischard', 7), ('AntoineAugusti', 6), ('kojicz983', 5), ('tboye', 5), ('yohanboniface', 4), ('lepture', 4), ('JulienParis', 3), ('petzlux', 3), ('teleboas', 2), ('geoffreyaldebert', 2), ('eraviart', 2), ('ThomasG77', 2)"
