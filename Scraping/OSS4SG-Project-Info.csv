Repository,Description,Readme Content,Topics,Last Contribution,Number of Contributors,Number of Stars,Number of Subscribers,Contributors,SDG
openfarmcc/OpenFarm,A free and open database for farming and gardening knowledge. You can grow anything!,"![OpenFarm](https://github.com/openfarmcc/OpenFarm/blob/master/app/assets/images/OpenFarm%20f%20logo%20-%20green%20%23219653.svg)

[![Coverage Status](https://img.shields.io/coveralls/openfarmcc/OpenFarm.svg)](https://coveralls.io/r/openfarmcc/OpenFarm)
[![Code Climate](https://codeclimate.com/github/openfarmcc/OpenFarm/badges/gpa.svg)](https://codeclimate.com/github/FarmBot/OpenFarm)
[![OpenCollective](https://opencollective.com/openfarm/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/openfarm/sponsors/badge.svg)](#sponsors)

# Security Concerns

We take security seriously and value the input of independent researchers. Please email `security@farmbot.io` for issues that require immediate attention. Please follow [responsible disclosure](). **Do not use Slack or Github issues to discuss security vulnerabilities.**

# The Community of Contributors: How it Works

### About

[OpenFarm](http://openfarm.cc) is a free and open database and web application for farming and gardening knowledge. One might think of it as the Wikipedia for growing plants, though it functions more like a cooking recipes site.

The main content are Growing Guides: creative, crowd-sourced, single-author, structured documents that include all of the necessary information for a person or machine to grow a plant, i.e.: seed spacing and depth, watering regimen, recommended soil composition and companion plants, sun/shade requirements, etc. In this Freebase platform, gardeners can find answers to questions like &ldquo;How do I grow tomatoes?&rdquo;

### Start by Joining Existing Contributors

To start the discussion, get involved, and meet OpenFarm core community of contributors, we strongly recommend joining [our Slack room](http://slack.openfarm.cc/)! This is where you'll find the latest conversation about Openfarm and the most active contributors.

Check also the [FAQ](http://openfarm.cc/pages/faq) for some frequently asked questions about contributing (Angular, Issue Trackers, IRC Channels).

Check the [ongoing issues](https://github.com/openfarmcc/OpenFarm/projects) that need work on in the priority list.

### Look for Something You Want to Work On

For [front-end](https://github.com/openfarmcc/OpenFarm/projects/1) and [back-end](https://github.com/openfarmcc/OpenFarm/projects/3) code contributions, we aim at maintaining and prioritizing the Github issues through Github Projects, the Trello-like web-based project management board of Github: [OpenFarm Projects](https://github.com/openfarmcc/OpenFarm/projects).

Need to use OpenFarm Assets? [Here they are](https://drive.google.com/open?id=0B-wExYzQcnp3cGphOGZQS1lBRFk)!

We have few more languages missing for the website content to be translated: help us [translate the website](https://www.transifex.com/projects/p/openfarm/)!

### Who Can Contribute

Everyone is welcome to bring value to the Open Source community of OpenFarm. Time is our most valuable assets here, so any minute of your time counts to make things happen! ""Better done, than perfect!""
We strive for diversity in our community and want to ensure we provide a safe and inclusive space for everyone by adopting a [Code of Conduct](https://openfarm.cc/pages/code_of_conduct?locale=en).

Our community is composed of tech and non-tech folks, newbie as well as experts in gardening, overall great people willing to take actions for a better future and sharing knowledge and growing our own food.

### Our problem-solving process

On the way we work together, we aim at:
- having transparency in reasoning behind actions: taking time for documentation, questions and answers
- prefering done, than perfect: breaking down tasks so that anyone can contribute few minutes of their time on a regular basis
- taking shortcuts: what's the most obvious for a better usability? what's the shortest way to build a feature? What's the most valuable inputs for a feedback?

## Development

### Getting Started (The Easy Way)

You should use Vagrant to get the OpenFarm system running on your computer. It will avoid having to install the things listed in The Hard Way below.

1. Install [Vagrant](https://www.vagrantup.com/docs/installation/).
2. Install [VirtualBox](https://www.virtualbox.org/wiki/Downloads).
3. Open your terminal.
4. `$ git clone https://github.com/openfarmcc/OpenFarm.git` - this tells your computer to fetch the data stored in this repository using git.
5. `$ cd OpenFarm` - change to the OpenFarm directory.
6. `$ vagrant up` This will take a long time. We're downloading a whole bunch of stuff. Go make yourself a pot of coffee, or brew some tea. If something goes wrong at this point, reach out to us directly via GitHub issue.

#### Accessing Vagrant

Once Vagrant is set up on your system, you might want to actually access it. For example, if you want to start up the server (though vagrant up should run `rails s` for you):

8. `$ vagrant ssh` - this makes you access the new virtual server we just created to run OpenFarm on.
9. `cd /vagrant` - the `vagrant` directory is mirrored in your own computer. If you add a file there, you'll see it appear here.
10. `rails s` - actually run the Rails server!
11. you should now be able to access OpenFarm on your local system at http://localhost:3000. If all went well, you will have a seeded database and can use the account `admin@admin.com` with password `admin123`.

The above is still being patched, so please reach out to us if something went wrong!

### Getting Started (The Hard Way)

You will need to install [Ruby](http://www.ruby-lang.org/en/), [Rails](http://rubyonrails.org/), [ElasticSearch](http://www.elasticsearch.org/) [v6.5.0](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/release-notes-6.5.0.html), and [Mongodb](http://docs.mongodb.org/manual/installation/) before you can get an OpenFarm server up and running on your local machine. Once you have these prerequisites to get started with a local copy of the project, run:

```bash
$ git clone https://github.com/openfarmcc/OpenFarm.git
$ cd OpenFarm
$ bundle install
$ rake db:setup
$ echo ""ENV['SECRET_KEY_BASE'] = '$(rake secret)'"" >> config/app_environment_variables.rb
$ echo ""ENV['GOOGLE_MAPS_API_KEY'] = ''"" >> config/app_environment_variables.rb # or get an actual API key at https://console.developers.google.com/flows/enableapi?apiid=maps_backend&keyType=CLIENT_SIDE&reusekey=true&pli=1
$ rails s
```

Then, visit [http://127.0.0.1:3000/](http://127.0.0.1:3000/) in your browser to see the OpenFarm web application running on your local machine. If all went well, you will have a seeded database and can use the account `admin@admin.com` with password `admin123`.

**Note about ElasticSearch**: Some Linux users have noted issues installing ElasticSearch onto a host machine. One workaround is to install ElasticSearch via Docker:

```
sudo docker pull elasticsearch:6.5.0
sudo docker pull mongo
```

```
sudo sysctl -w vm.max_map_count=262144 # <= Some linux users must run this
sudo docker run -p 9300:9300 -p 9200:9200 elasticsearch:6.5.0
sudo docker run -p 27017:27017 mongo

```


**If you had any problem** installing bundles getting up and running etc see the [Common Issues Page](https://github.com/openfarmcc/OpenFarm/wiki/Common-Issues).

Remember that `/vagrant` folder in the Vagrant VM is largely for convenience, and working in it can cause unexpected behavior with other tools - you should do your work in your own non-vagrant environment. Use the environment you're most familiar with to program, and Vagrant will do the rest.


#### Become a Core Contributor

If you've made two PRs, we'll add you as a core contributor.

For core-code contributors, here are a few basic ground-rules:

* No --force pushes or modifying the Git history in any way.
* Non-master branches ought to be used for ongoing work.
* External API changes and significant modifications ought to be subject to an internal pull-request to solicit feedback from other contributors.
* Internal pull-requests to solicit feedback are encouraged for any other non-trivial contribution but left to the discretion of the contributor.
* Contributors should attempt to adhere to the prevailing code-style.

([based on the OPEN open source model](https://github.com/Level/community/blob/master/CONTRIBUTING.md))

[Further reading](https://medium.com/the-javascript-collection/healthy-open-source-967fa8be7951#.alkpecsnd)

### Actual Code Contributors

Here are some of the [Github contributors](https://github.com/openfarmcc/OpenFarm/graphs/contributors).

Outside of Github, there's a whole host of people who also contributed financially, by building gardening content on the website, on providing more visibility for OpenFarm in any ways!

### Donate to OpenFarm as a Backer

Support us with a monthly donation and help us continue our activities. [[Become a backer](https://opencollective.com/openfarm#backer)]



### Support OpenFarm as a Sponsor

Become a sponsor and get your logo on our README on Github with a link to your site. [[Become a sponsor](https://opencollective.com/openfarm#sponsor)]



### Software License

The MIT License (MIT)

Copyright (c) 2019 OpenFarm [(http://openfarm.cc/)](http://openfarm.cc/).

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

### Data License

All data within the OpenFarm.cc database is in the [Public Domain (CC0)](http://creativecommons.org/publicdomain/zero/1.0/).
","'farming', 'gardening', 'open-source', 'plants', 'rails'",2024-03-10T13:38:40Z,30,1496,63,"('simonv3', 1116), ('RickCarlino', 701), ('dependabotbot', 201), ('warpling', 170), ('dependabot-previewbot', 161), ('CloCkWeRX', 119), ('TanSA05', 94), ('roryaronson', 88), ('Br3nda', 64), ('TheChirpyWitch', 25), ('broder', 14), ('gabrielburnworth', 12), ('RaceFPV', 11), ('md5', 10), ('kelcecil', 10), ('speizerj', 9), ('digital-dreamer', 9), ('guitsaru', 8), ('nickedwards109', 7), ('Cynaria', 6), ('roselynemakena', 6), ('code-factor', 5), ('pnob32', 5), ('mo22de', 5), ('robbrit', 4), ('rickr', 4), ('Katy310', 4), ('cpursley', 4), ('ignaciots', 3), ('sophiakc', 3)","[9, 'Industry, Innovation and Infrastructure']"
openfoodfacts/openfoodfacts-androidapp,Native version of Open Food Facts on Android - Coders & Decoders welcome 🤳🥫 ,"

Open Food Facts - Legacy Android app (still used for Open Beauty, Pet Food and Products Facts)
=============================

Open Food Facts is collaborative food products database made by everyone, for everyone. Open Food Facts contributors gathers information and data on food products from around the world, using mobile apps.



> ### ⚠️ WARNING
> 
> The new Open Food Facts app is located [HERE](https://github.com/openfoodfacts/smooth-app)
> 
> **Note: This codebase is currently only deployed for Open Beauty Facts, Open Pet Food Facts and Open Products Facts apps.**

[![Project Status](https://opensource.box.com/badges/active.svg)](https://opensource.box.com/badges)
[![Quality Gate](https://sonarcloud.io/api/project_badges/measure?project=openfoodfacts_openfoodfacts-androidapp&metric=alert_status)](https://sonarcloud.io/dashboard/index/openfoodfacts_openfoodfacts-androidapp)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/openfoodfacts/localized.svg)](https://crowdin.com/project/openfoodfacts)
![Build](https://github.com/openfoodfacts/openfoodfacts-androidapp/workflows/Android%20Integration/badge.svg)
[![Open Source Helpers](https://www.codetriage.com/openfoodfacts/openfoodfacts-androidapp/badges/users.svg)](https://www.codetriage.com/openfoodfacts/openfoodfacts-androidapp)


- [Open Beauty Facts](https://play.google.com/store/apps/details?id=org.openbeautyfacts.scanner), [Open Pet Food Facts](https://play.google.com/store/apps/details?id=org.openpetfoodfacts.scanner) and [Open Products Facts](https://play.google.com/store/apps/details?id=org.openproductsfacts.scanner) are also built from this codebase.

 What is Open Food Facts? 

### A food products database

Open Food Facts is a database of food products with ingredients, allergens, nutrition facts… which allow us to compute scores like Nutri-Score, NOVA groups and Eco-Score.

### Made by everyone

Open Food Facts is a non-profit association of volunteers.
25000+ contributors like you have added 3M+ products from 150 countries using our Android or iPhone apps to scan barcodes and upload pictures of products and their labels.

### For everyone

Data about food is of public interest and has to be open. The complete database is published as open data and can be reused by anyone.



## User flows
[Visual documentation of the App on Figma](https://www.figma.com/file/BQ7CSyFvl7D9ljcXT0ay0u/Navigation-within-the-app)

## Documentation of the source code
The documentation is generated automatically from the source code and your improvements to code documentation are published automatically.
[Code documentation on GitHub pages](https://openfoodfacts.github.io/openfoodfacts-androidapp/)

## Helping with our next release
Here are issues and feature requests you can work on:
- [ ] [3.6.6 milestone](https://github.com/openfoodfacts/openfoodfacts-androidapp/milestone/36)

 What can I work on ? 

Open Food Facts on Android has 0,5M users and 1,6M products. *Each contribution you make will have a large impact on food transparency worldwide.* Finding the right issue or feature will help you have even more more impact. Feel free to ask for feedback on the #android channel before you start work, and to document what you intend to code.

- [Here are issues and feature requests you can work on](https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/4169)
- [P1 issues](https://github.com/openfoodfacts/openfoodfacts-androidapp/labels/p1)
- [Small issues (Hacktoberfest)](https://github.com/openfoodfacts/openfoodfacts-androidapp/labels/hacktoberfest)


If you don't have time to contribute code, you're very welcome to
* Scan new products
* [**Make a donation** to help pay for the hosting and general costs](https://donate.openfoodfacts.org) 

## Help translate Open Food Facts in your language

You can help translate Open Food Facts and the app at (no technical knowledge required, takes a minute to signup): 
https://translate.openfoodfacts.org



## Installation

| Choose the right flavor | Install steps|
| ------------- | ------------- |
| | * Download the latest [Android Studio](https://developer.android.com/studio) stable build. * If you are running the app for the first time, Android Studio will ask you to install the Gradle dependencies. * If you are a new contributor to open-source, we recommend you read our [Setup Guidelines](https://github.com/openfoodfacts/openfoodfacts-androidapp/blob/master/SETUP_GUIDELINES.md) * In Android Studio, make sure to select `OFF` as the default flavor for Open Food Facts (`OBF` is Open Beauty Facts, `OPF` - Open Products Facts, `OPFF` - Open Pet Food Facts) * You should be able to install Open Food Facts on your phone using an USB cable, or run it in an emulator. * The package name on the Play Store is org.openfoodfacts.scanner. For historic reasons, it's openfoodfacts.github.scrachx.openfood in the code and on F-Droid.|

## Running a Fastlane lane
The project uses Fastlane to automate release and screenshots generation.
* First time you checkout, run `bundle install` at the root of the project.
* Then launch lanes using `bundle exec fastlane release` (for example the release lane).
* We're moving Fastlane related things to https://github.com/openfoodfacts/fastlane-descriptions.

### Who do I talk to?

* Any member of the Android team or contact@openfoodfacts.org
* Join our #android and #android-alerts discussion room on Slack (Get an invite: )

### Contributing 

If you're new to open-source, we recommend to checkout our [Contributing Guidelines](https://github.com/openfoodfacts/openfoodfacts-androidapp/blob/master/CONTRIBUTING.md). Feel free to fork the project and send a pull request.

 Libraries Used 
We use the following libraries, and we're not closed to changes where relevant :-)

 If you spot any libraries we added or we don't use anymore, feel free to update this list using a Pull Request. 

- [Dagger 2](https://github.com/google/dagger) - A fast dependency injector for Android and Java
- [Retrofit](https://square.github.io/retrofit/) - Retrofit turns your REST API into a Java interface
- [OkHttp](https://github.com/square/okhttp) - An HTTP+SPDY client for Android and Java applications
- [Mockito](https://github.com/mockito/mockito) - Most popular Mocking framework for unit tests written in Java
- [Apache](https://github.com/apache/commons-io) - The Apache Commons IO library contains utility classes, stream implementations, file filters, file comparators, endian transformation classes, and much more.
- [Kotlin Coroutines](https://developer.android.com/kotlin/coroutines) - A coroutine is a concurrency design pattern that you can use on Android to simplify code that executes asynchronously.  
- [Hilt](https://developer.android.com/training/dependency-injection/hilt-android) - Hilt is a dependency injection library for Android that reduces the boilerplate of doing manual dependency injection in your project. 
- [Dagger](https://developer.android.com/training/dependency-injection/dagger-android) - Manual dependency injection or service locators in an Android app can be problematic depending on the size of your project. You can limit your project's complexity as it scales up by using Dagger to manage dependencies. Dagger automatically generates code that mimics the code you would otherwise have hand-written.
- [Jackson](https://github.com/FasterXML/jackson) - Core part of Jackson that defines Streaming API as well as basic shared abstractions
- [journeyapps/zxing-android-embedded](https://github.com/journeyapps/zxing-android-embedded) - Barcode scanner library for Android, based on the ZXing decoder
- GreenDao
- [mikepenz/MaterialDrawer](https://github.com/mikepenz/MaterialDrawer) - The flexible, easy to use, all in one drawer library for your Android project.

Big thanks to their contributors!



 Copyright and License

    Copyright 2016-2022 Open Food Facts

    Licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       https://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an ""AS IS"" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and 
    limitations under the License.




## Contributors

The project was initially started by [Scot Scriven](https://github.com/itchix), other contributors include:
- [Aurélien Leboulanger](https://github.com/herau)
- [Pierre Slamich](https://github.com/teolemon)
- [Friedger Müffke](https://github.com/friedger)
- [Qian Jin](https://github.com/jinqian)
- [Fred Deniger](https://github.com/deniger)
- [VaiTon](https://github.com/VaiTon)



  

","'android', 'crowdsourcing', 'environment', 'food', 'gsoc', 'hacktoberfest', 'java', 'kotlin', 'kotlin-android', 'nutrition', 'openfoodfacts'",2024-05-03T09:20:24Z,30,747,37,"('teolemon', 5637), ('VaiTon', 930), ('dependabotbot', 372), ('itchix', 206), ('herau', 153), ('deniger', 140), ('huzaifaiftikhar', 85), ('PrajwalM2212', 62), ('naivekook', 55), ('dobriseb', 46), ('philippeauriach', 45), ('kartikaysharma01', 44), ('ripetrescu', 40), ('g123k', 36), ('subhanjansaha', 35), ('jaindiv26', 34), ('mustaqmustu', 33), ('Karljoones', 32), ('Shubham-vish', 28), ('ross-holloway94', 26), ('github-actionsbot', 26), ('mrudultora', 24), ('jaztriumph', 24), ('dependabot-previewbot', 23), ('balaji-ramavathu', 22), ('brandenfung', 22), ('raphael0202', 19), ('RajaVamsi11', 13), ('prashantkh19', 12), ('iml-v', 12)","[12, 'Responsible Consumption and Production']"
electricitymaps/electricitymaps-contrib,A real-time visualisation of the CO2 emissions of electricity consumption,"
  
    
  


  Electricity Maps



A real time and historical visualisation of the Greenhouse Gas Intensity (in terms of CO2 equivalent) of electricity production and consumption around the world.
  app.electricitymaps.com »



  
  
    
  
    
    
  
    
  
    


![image](web/public/images/electricitymap_social_image.png#gh-light-mode-only)
![image](web/public/images/electricitymap_social_image_dark.png#gh-dark-mode-only)

## Introduction

This project aims to provide a free, open-source, and transparent visualisation of the carbon intensity of electricity consumption around the world.

We fetch the raw production data from public, free, and official sources. They include official government and transmission system operators' data. We then run [our flow-tracing algorithm](https://www.electricitymaps.com/blog/flow-tracing) to calculate the actual carbon intensity of a country's electricity consumption.

_Try it out at [app.electricitymaps.com](https://app.electricitymaps.com), or download the app on [Google Play](https://play.google.com/store/apps/details?id=com.tmrow.electricitymap&utm_source=github) or [App store](https://itunes.apple.com/us/app/electricity-map/id1224594248&utm_source=github)._

## Contributing

The Electricity Maps app is a community project and we welcome contributions from anyone!

We are always looking for help to build parsers for new countries, fix broken parsers, improve the frontend app, improve accuracy of data sources, discuss new potential data sources, update region capacities, and much more.

Read our [contribution guidelines](/CONTRIBUTING.md) to get started.

## Community & Support

Use these channels to be part of the community, ask for help while using Electricity Maps, or just learn more about what's going on:

- [Slack](https://slack.electricitymaps.com): This is the main channel to join the community. You can ask for help, showcase your work, and stay up to date with everything happening.
- [GitHub Issues](https://github.com/electricitymaps/electricitymaps-contrib/issues): Raise any issues you encounter with the data or bugs you find while using the app.
- [GitHub Discussions](https://github.com/electricitymaps/electricitymaps-contrib/discussions): Join discussions and share new ideas for features.
- [GitHub Wiki](https://github.com/electricitymaps/electricitymaps-contrib/wiki): Learn more about methodology, guides for how to set up development environment, etc.
- [FAQ](https://app.electricitymaps.com/FAQ): Get your questions answered in our FAQ.
- [Our Commercial Website](https://electricitymaps.com/): Learn more about how you or your company can use the data too.
- [Our Blog](https://electricitymaps.com/blog/): Read about the green transition and how Electricity Maps is helping to accelerate it.
- [Twitter](https://twitter.com/electricitymaps): Follow for latest news
- [LinkedIn](https://www.linkedin.com/company/electricitymaps): Follow for latest news

## License

This repository is licensed under GNU-AGPLv3 since v1.5.0, find our license [here](https://github.com/electricitymaps/electricitymaps-contrib/blob/master/LICENSE.md). Contributions prior to commit [cb9664f](https://github.com/electricitymaps/electricitymaps-contrib/commit/cb9664f43f0597bedf13e832047c3fc10e67ba4e) were licensed under [MIT license](https://github.com/electricitymaps/electricitymaps-contrib/blob/master/LICENSE_MIT.txt)

## Frequently asked questions

_We also have a lot more questions answered on [app.electricitymaps.com/faq](https://app.electricitymaps.com/faq)!_

**Where does the data come from?**
The data comes from many different sources. You can check them out [here](https://github.com/electricityMaps/electricitymaps-contrib/blob/master/DATA_SOURCES.md)

**Why do you calculate the carbon intensity of _consumption_?**
In short, citizens should not be responsible for the emissions associated with all the products they export, but only for what they consume.
Consumption-based accounting (CBA) is a very important aspect of climate policy and allows assigning responsibility to consumers instead of producers.
Furthermore, this method is robust to governments relocating dirty production to neighboring countries in order to green their image while still importing from it.
You can read more in our blog post [here](https://electricitymaps.com/blog/flow-tracing/).

**Why don't you show emissions per capita?**
A country that has few inhabitants but a lot of factories will appear high on CO2/capita.
This means you can ""trick"" the numbers by moving your factory abroad and import the produced _good_ instead of the electricity itself.
That country now has a low CO2/capita number because we only count CO2 for electricity (not for imported/exported goods).
The CO2/capita metric, by involving the size of the population, and by not integrating all CO2 emission sources, is thus an incomplete metric.
CO2 intensity on the other hand only describes where is the best place to put that factory (and when it is best to use electricity), enabling proper decisions.

**CO2 emission factors look high — what do they cover exactly?**
The carbon intensity of each type of power plant takes into account emissions arising from the whole life cycle of the plant (construction, fuel production, operational emissions and decommissioning). Read more on the [Emissions Factor Wiki page](https://github.com/electricitymaps/electricitymaps-contrib/wiki/Emission-factors).

**How can I get access to historical data or the live API?**
All this and more can be found **[here](https://electricitymaps.com/)**.
You can also visit our **[data portal](https://www.electricitymaps.com/data-portal)** to download historical datasets.
","'climate-change', 'data-visualization', 'hacktoberfest', 'sustainability'",2024-05-03T15:24:25Z,30,3292,76,"('corradio', 1760), ('VIKTORVAV99', 306), ('dependabotbot', 231), ('madsnedergaard', 205), ('systemcatch', 154), ('tonypls', 120), ('unitrium', 119), ('mathilde-daugy', 118), ('pierresegonne', 113), ('alixunderplatz', 83), ('nessie2013', 71), ('electricitymapsbot', 69), ('fbarl', 67), ('maxbellec', 60), ('FelixDQ', 59), ('tmrow-bot', 57), ('skovhus', 56), ('magol', 53), ('brunolajoie', 52), ('jarek', 52), ('q--', 49), ('Raffox97', 45), ('KabelWlan', 42), ('BLACKLEG', 42), ('jkopb', 36), ('lorrieq', 28), ('ovbm', 28), ('veqtrus', 28), ('silkeholmebonnen', 24), ('amv213', 17)","[7, 'Affordable and Clean Energy']"
WorldBank-Transport/DRIVER,"DRIVER - Data for Road Incident Visualization, Evaluation, and Reporting ","# DRIVER
DRIVER - Data for Road Incident Visualization, Evaluation, and Reporting

[![Build Status](https://travis-ci.org/WorldBank-Transport/DRIVER.svg?branch=develop)](https://travis-ci.org/WorldBank-Transport/DRIVER)

## Deploying

1. Follow the Installation instructions below
2. Follow the instructions in doc/system-administration.md

## Developing

### Installation

1. Install Vagrant 1.5+

1. Install Ansible 1.8+

1. Install `vagrant-hostmanager` plugin via:

    ```bash
    vagrant plugin install vagrant-hostmanager
    ```

1. Prevent changes in `group_vars/development` from being tracked by git.

    - You will likely make changes to `group_vars/development` to configure your local environment. To make sure you don't commit those changes unless you need to change the default development settings, you can make git not track changes to that file. To do this, run `git update-index --assume-unchanged deployment/ansible/group_vars/development`.
    - To revert back to tracking changes, run `git update-index --no-assume-unchanged deployment/ansible/group_vars/development`.

1. Create `gradle/data/driver.keystore`

    - To run in development without support for JAR file building:
      ```bash
      touch gradle/data/driver.keystore
      ```
      (If you just want to install the DRIVER web interface, do this. You can add Android integration later.)

    - To build schema model JAR files for the Android app, copy the signing keystore to `gradle/data/driver.keystore`
    and set the password for the keystore under `keystore_password` in `deployment/ansible/group_vars/development`.

1. (Optional) To enable geocoding, [set up Pickpoint in `group_vars/development`](#pickpoint)

1. Install [NFS](https://en.wikipedia.org/wiki/Network_File_System). On Debian/Ubuntu, run:

    ```bash
    sudo apt-get install nfs-common nfs-kernel-server
    ```

1. Start the Vagrant VM
    ```bash
    vagrant up
    ```

    If you run into issues provisioning the VMs or forget a step, try re-provisioning as needed:
    ```bash
    vagrant provision 
    ```

### Pickpoint

Pickpoint is a geocoding service used by DRIVER to obtain lat/lon coordinates from input addresses. DRIVER can work without Pickpoint configured, but to enable geocoding, obtain a pickpoint API key from https://pickpoint.io and enter the key in `deployment/ansible/group_vars/development` under `web_js_nominatim_key`.

### Running & Configuration

The app is available on http://localhost:7000/, and the schema editor at http://localhost:7000/editor/.

In development environments a default Django superuser will be created for you:
  - Username: `admin`
  - Password: `admin`

### Google OAuth

To configure Google OAuth for development, follow [these steps](https://support.google.com/googleapi/answer/6158849?hl=en&ref_topic=7013279) to create a web application and credentials for your local DRIVER instance.

When creating a client ID for your web application, use these URLs:

**Authorized JavaScript origins**:

http://localhost:7000

**Authorized redirect URIs**:

http://localhost:7000/openid/callback/login/

Once you have the client ID and client secret, add those values to `deployment/ansible/group_vars/development` and reprovision the `app` VM  as needed:
```bash
vagrant provision app
```

### Frontend
Both Angular apps can be run in development mode via:
```bash
./scripts/grunt.sh editor serve
```
and
```bash
./scripts/grunt.sh web serve
```
You will need to run these commands in separate terminals if you'd like to have both running at the same time.

The frontend app will be available on port 7002 at http://localhost:7002 and the schema editor will be available on port
7001 at http://localhost:7001. Both will reload automatically as changes are made.

To make requests to a Django runserver directly (for example, to perform interactive debugging in
the request-response cycle), run:
```bash
./scripts/manage.sh runserver 0.0.0.0:8000
```
You should then be able to access the Django runserver on port 3001 of the `app` VM at http://localhost:3001.

Front end files are mounted inside the `app` Vagrant VM at `/opt/schema_editor` for the Angular editor and `/opt/web` for the Angular interface.

#### Updating existing translation files
New Angular translation tokens should be added to i18n/exclaim.json with a value of ""!"".
The English translation (en-us.json) is automatically built from exclaim.json. New tokens are also
propagated to other translations via a grunt task:

```bash
./scripts/grunt.sh web translate
```

#### Adding a new translation file
Place the new JSON file in the i18n folder. Add the file to the i18nForeignLanguages var in Gruntfile.js.
To enable the language to be selected via the language picker, add an item to the `languages` list in
`deployment/ansible/group_vars/development`. Setting `rtl` to true will enable right-to-left CSS changes.


### Docker
To update the Docker container images to reflect environment changes (Such as changed Python packages), provision the `app` VM:
```bash
vagrant provision app
```

### Testing

#### Javascript
To run the Javascript automated tests, use:
```bash
./scripts/grunt.sh web test
```

## Testing Data

### Boundaries
Geographic boundaries are used to filter records to a defined area, such as a region or state. These boundaries are created by uploading shape files to the editor, http://localhost:7000/editor under ""Add new geographies"".

For developers at Azavea, use the `regions.zip` and `states.zip` files available in the DRIVER project folder on the fileshare. For non-Azavea users, upload a zipped shapefile containing the boundaries of the jurisdictions where you plan to operate DRIVER. If you don't have such a shapefile, [Natural Earth](https://www.naturalearthdata.com/features/) is a good place to start.""

After uploading each the file, select `name` as the display field, then hit save. Either refresh the page or navigate somewhere else in between uploads.

### Records
Record data can be populated from a CSV file that contains named columns for `""lat""`, `""lon""`, and `""record_date""`. A file with semi-realistic data can be found in `scripts/sample_data/sample_traffic.csv` for use. For developers at Azavea, CSV files containing historical data can be downloaded from the `/data` folder of the project's directory in the fileshare, with names of the format `_traffic.csv`.

In order to import record data you will have to obtain an Authorization header and its API token. To do this, log in to the web application, then open the network tab in web developer tools and reload the page. Inspect the request headers
from an API request and pull out the value of the `Authorization` header, for example
`Token f1acac96cc79c4822e9010d23ab425231d580875`.

Using the API token, run:
```bash
python scripts/load_incidents_v3.py --authz 'Token ' scripts/sample_data/
```
Note that the import process will take roughly two hours for the full data set; you can cut down the
number of records with `head` on the individual CSVs.

The `load_incidents_v3.py` script will also create a schema for you. If you already have a schema in place, and simply want to load data associated with that schema, you will need to modify the script accordingly: change the `schema_id = create_schema(...)` line with `schema_id = 'replace-this-with-the-existing-schema-id'`.

To load mock black spots, run:
```bash
python scripts/load_black_spots.py --authz 'Token ' /path/to/black_spots.json
```
Mock black spot data is available in `scripts/sample_data/black_spots.json`.

To load mock interventions, run:
```bash
python scripts/load_interventions.py --authz 'Token ' /path/to/interventions_sample_pts.geojson
```
Mock intervention data is available in `scripts/sample_data/interventions_sample_pts.geojson`.

To generate black spot and load forecast training inputs, run:
```bash
python scripts/generate_training_input.py /path/to/roads.shp /path/to/records.csv
```

More information on the requirements for loading data can be found in the [`scripts/`
directory](./scripts/README.md).

### Costs

You can't request records with associated costs successfully until you configure some costs.
To do this, navigate to your editor (by default on http://localhost:7000/editor/), select ""Incident"" from
record types in the menu on the left. (If there are multiple record types named ""Incident"", delete all but one.) Select ""Cost aggregation settings"", then:

- Choose a currency prefix in ""Cost Prefix"" (e.g., `$`, but anything is fine)
- Select ""Incident Details"" in ""Related Content Type""
- Choose ""Severity"" in ""Field""
- Then decide how much money you think human lives, human physical security, and property are worth

## Production

TODO: Notes on creating a production superuser and adding a production OAuth2 application


## Using OAuth2 / Getting tokens

Get a token:
```bash
curl -X POST -d ""grant_type=password&username=&password="" -u"":"" http://localhost:7000/o/token/
```

Returns:
```json
{
    ""access_token"": """",
    ""token_type"": ""Bearer"",
    ""expires_in"": 36000,
    ""refresh_token"": """",
    ""scope"": ""read write groups""
}
```

Note: If you're experiencing SSL errors with cURL, your version of cURL may not have the right certificate authorities installed. Try passing the `-k` parameter to `curl`.

Making requests with a token:
```bash
# GET
curl -H ""Authorization: Bearer "" http://localhost:7000:/api/record/
curl -H ""Authorization: Bearer "" http://localhost:7000:/api/recordschema/
```

Restricted access (disabled in development to allow access to the browsable API):

Add an additional `scope` parameter to token request:
```bash
curl -X POST -d ""grant_type=password&username=&password=&scope=read"" -u"":"" http://localhost:7000/o/token/
```

Now, this token will have read-only access to the API.

## Releases

Releases use a `github_changelog_generator` tool written in `ruby`.

- Make sure your `develop` is up-to-date
- Start the Gitflow release:
    ```bash
    git flow release start 
    ```
-
    ```bash
    docker run -ti --rm -v ${PWD}:/changelog -w /changelog ruby:2.5 /bin/bash
    ```
- From the container:
    ```bash
    gem install github_changelog_generator
    ```
- Then, to generate the changelog since the last release:
    ```bash
    $ export RELEASE_VERSION=
    $ export LAST_RELEASE=
    $ export GITHUB_TOKEN=
    $ github_changelog_generator ""WorldBank-Transport/DRIVER"" \
        --token ${GITHUB_TOKEN} \
        --since-tag ${LAST_RELEASE} \
        --future-release ${RELEASE_VERSION} \
        --base CHANGELOG.md \
        --no-issues \
        --no-issues-wo-labels \
        --no-author
    ```

It's important to include the `since-tag` argument, since without it, the changelog generator
will include everything that went into 1.0.0, which is a lot of stuff and not super meaningful,
since `1.0.0` is ""what was there when we decided to start using semantic versioning.""
Note: We've had some problems with the `since-tag` argument not being respected; if this happens,
manually delete the duplicate entries and update the GitHub diff link.

- Include the CHANGELOG in your release branch
- Git flow publish the release:
    ```
    git flow release publish 
    ```
- Open a PR for your release
- Wait for a successful build and approval (from whom?), then:
    ```bash
    $ git flow release finish 
    $ git checkout master
    $ git tag -f   # git-flow puts the tag on `develop`
    $ git push origin master
    $ git checkout develop
    $ git push origin develop
    $ git push [-s] --tags
    ```

:tada:
",,2022-12-21T07:37:29Z,17,34,15,"('kshepard', 478), ('flibbertigibbet', 361), ('ddohler', 236), ('moradology', 170), ('pcaisse', 96), ('KlaasH', 88), ('CloudNiner', 47), ('shreshthkhilani', 45), ('fungjj92', 36), ('rmartz', 31), ('mtedeschi', 30), ('jisantuc', 20), ('hectcastro', 16), ('sharph', 12), ('notthatbreezy', 11), ('jeancochrane', 6), ('jerheff', 1)","[16, 'Peace, Justice and Strong Institutions']"
mysociety/pombola,,"# Pombola

This web app allows you to store and share information on public figures,
especially politicians.

For detailed background about the project as of 2018 please see
[docs/BACKGROUND.md](docs/BACKGROUND.md). For an overview of the system see
[docs/OVERVIEW.md](docs/OVERVIEW.md).


## Installing

Please see [docs/INSTALL.md](docs/INSTALL.md)

To change your site's look and feel please see the [styling notes](docs/STYLING_NOTES.md).

For software developers, you can see information about running
the tests in [docs/TESTING.md](docs/TESTING.md).

## Troubleshooting

Please see [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) file

## Future

The Pombola codebase was originally written for the Kenyan site
[mzalendo.com](http://info.mzalendo.com). It has since been modified to support
other sites around the world, notably several in Africa.

We want the code to be easy to use for other installations, but currently there are some rough edges.

## Acknowledgements

We are very grateful for the funding that has made development
of Pombola possible, in particular the generous support from:

* [The Indigo Trust](http://indigotrust.org.uk/)
* [The Omidyar Network](https://www.omidyar.com/)

Thanks also to [Browserstack](https://www.browserstack.com/) who
let us use their web-based cross-browser testing tools for this
project.

## mySociety.org Software Licensing

Most of the software in this directory is Copyright (c) 2004-2019 UK
Citizens Online Democracy.

Unless otherwise stated in particular files or directories, this
software is free software; you can redistribute it and/or modify it
under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Can you explain briefly what the GNU Affero GPL is? We offer the
source code of our websites to our users. The GNU Affero GPL has the
requirement that anyone else using that code for their own websites
also does the courtesy of offering the source code to their users.

Why not use the GPL? The GPL guarantees that anyone who gets a binary
version of the software also gets the source code so they can modify
it. Since users of websites never get the binary, just HTML pages, it
is no better a license than a BSD style license would be for them.
For this reason, we use the GNU Affero GPL.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Affero General Public License for more details.

Information about the GNU Affero GPL:
http://www.fsf.org/licensing/licenses/agpl-3.0.html

A copy of the GNU Affero General Public License can be found in [LICENSE.txt](/LICENSE.txt).
",,2020-01-30T14:09:49Z,30,65,21,"('mhl', 1530), ('chrismytton', 540), ('evdb', 412), ('wrightmartin', 233), ('zarino', 170), ('jacksonj04', 156), ('idesouza', 125), ('osfameron', 97), ('eokyere', 95), ('lizconlan', 67), ('dracos', 66), ('geoffkilpin', 64), ('Jedidiah', 27), ('octopusinvitro', 23), ('JenMysoc', 22), ('sagepe', 18), ('tmtmtmtm', 16), ('teuben', 14), ('davea', 12), ('struan', 11), ('xybrnet', 9), ('stevenday', 8), ('ajparsons', 4), ('dependabot-previewbot', 3), ('paullenz', 3), ('longhotsummer', 2), ('MikeMuli', 2), ('dependabot-support', 2), ('robinkiplangat', 1), ('duncanparkes', 1)","[16, 'Peace, Justice and Strong Institutions']"
orcasound/orcanode,"Software for live-streaming and recording lossy (AAC) or lossless compressed audio (HLS, DASH, FLAC) via AWS S3 buckets. :star:","# Orcasound's orcanode code for live-streaming audio data

The `orcanode` software repository contains audio tools and scripts for capturing, reformatting, transcoding and uploading audio data at each node of a network. Orcanode live-streaming should work on Intel (amd64) or Raspberry Pi (arm32v7) platforms using any soundcard.  The most common hardware used by Orcasound is the [Pisound HAT](https://blokas.io/pisound/) on a Raspberry Pi (3B+ or 4) single-board computer.

There is a `base` set of tools and a couple of specific projects in the `node` and `mseed` directories. The node directory is for new locations streaming within the Orcasound listening network (primary nodes).

The mseed directory has code for converting audio data in the mseed format to the live-streaming audio format used by primary nodes. This conversion code is mainly used for audio data collected by the [Ocean Observatories Initiative or OOI](https://oceanobservatories.org/ ""OOI"") network.  See the README in the `mseed` directory for more info. Transcoding from other audio formats should likely go in new directories by encoding scheme, similar to the mseed directory... 

You can also gain some bioacoustic context for the project in the [orcanode wiki](https://github.com/orcasound/orcanode/wiki).

## Background & motivation

This code was developed for live-streaming from source nodes in the [Orcasound](http://orcasound.net) hydrophone network (WA, USA). Thus, the repository names begin with ""orca""! Our primary motivation is to make it easy for community scientists to listen for whales via the [Orcasound web app](https://live.orcasound.net) using their favorite device/OS/browser.

We also aspire to use open source software as much as possible. We rely heavily on [FFmpeg](https://www.ffmpeg.org/). One of our long-term goals is to stream lossless [FLAC](https://xiph.org/flac/)-encoded audio within [DASH](https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP) segments to a player that works optimally on as many listening devices as possible. For now (2018-2023) we have found the best end-to-end performance across the broadest range of web browsers is acheived by streaming AAC-encoded audio within [HLS](https://developer.apple.com/streaming/) segments. 

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See the deployment section (below) for notes on how to deploy the project on a live system like [live.orcasound.net](https://live.orcasound.net).

If you want to set up your hardware to host a hydrophone within the Orcasound network, take a look at [how to join Orcasound](http://www.orcasound.net/join/) and [our prototype built from a Raspberry Pi with the Pisound ADC HAT](http://www.orcasound.net/2018/04/27/orcasounds-new-live-audio-solution-from-hydrophone-to-headphone-with-a-raspberry-pi-computer-and-hls-dash-streaming-software/).

The general scheme is to acquire audio data from a sound card within a Docker container via ALSA or Jack and FFmpeg, and then stream the audio data with minimal latency to cloud-based storage (as of Oct 2021, we use AWS S3 buckets). Errors/etc are logged to LogDNA via a separate Docker container.

### Prerequisites

An ARM or X86 device with a sound card (or other audio input devices) connected to the Internet (via wireless network or ethernet cable) that has [Docker-compose](https://docs.docker.com/compose/install/) installed and an AWS account with some S3 buckets set up.

### Installing

Create a base docker image for your architecture by running the script in /base/rpi or /base/amd64 as appropriate.  You will need to create a .env file as appropriate for your projects.  Common to to all projects are the need for AWS keys

```
AWSACCESSKEYID=YourAWSaccessKey
AWSSECRETACCESSKEY=YourAWSsecretAccessKey
 
SYSLOG_URL=syslog+tls://syslog-a.logdna.com:YourLogDNAPort
SYSLOG_STRUCTURED_DATA='logdna@YourLogDNAnumber key=""YourLogDNAKey"" tag=""docker""
```

(You can request keys via the #hydrophone-nodes channel in the Orcasound Slack. As of October, 2023, we are continuing to use AWS S3 for storage and LogDNA for live-logging and troubleshooting.)

Here are explanations of some of the .env fields:

* NODE_NAME should indicate your device and it's location, ideally in the form `device_location` (e.g. we call our Raspberry Pi staging device in Seattle `rpi_seattle`. 
* NODE_TYPE determines what audio data formats will be generated and transferred to their respective AWS buckets. 
* AUDIO_HW_ID is the card, device providing the audio data. Note: you can find your sound device by using the command ""arecord -l"".  For Raspberry Pi hardware with pisound just use AUDIO_HW_ID=pisound
* CHANNELS indicates the number of audio channels to expect (1 or 2). 
* FLAC_DURATION is the amount of seconds you want in each archived lossless file. 
* SEGMENT_DURATION is the amount of seconds you want in each streamed lossy segment.


## Supported combinations


| NODE ARCHITECTURE | node | mseed |
|-------------------|------|-------|
| arm32v7           |  Y   |  N    |
| amd64             |  Y   |  Y    |



| NODE ARCHITECTURE | hls-only | research | dev-virt |
|-------------------|----------|----------|----------|
| arm32v7           | Y        | Y        | N        |
| amd64             | Y        | N        | Y        |



| NODE Hardware     | hls-only | research |
|-------------------|----------|----------|
| RPI4              | Y        | Y        |
| RPI3 B-           | Y        | N        |



## Running local tests

In the repository directory (where you also put your .env file) first copy the compose file you want to docker-compose.yml.  For example if you are raspberry pi and you want to use the prebuilt image then copy docker-compose.rpi-pull.yml to docker-compose.  Then run `docker-compose up -d`. Watch what happens using `htop`. If you want to verify files are being written to /tmp or /mnt directories, get the name of your streaming service using `docker-compose ps` (in this case `orcanode_streaming_1`) and then do `docker exec -it orcanode_streaming_1 /bin/bash` to get a bash shell within the running container.

## Running an end-to-end test

Once you've verified files are making it to your S3 bucket (with public read access), you can test the stream using a browser-based reference player.  For example, with [Bitmovin HLS/MPEG/DASH player](https://bitmovin.com/demos/stream-test?format=hls&manifest=) you can use select HLS and then paste the URL for your current S3-based manifest (`.m3u8` file) to listen to the stream (and observe buffer levels and bitrate in real-time).

Your URL should look something like this:
```
https://s3-us-west-2.amazonaws.com/dev-streaming-orcasound-net/rpi_seattle/hls/1526661120/live.m3u8
```
For end-to-end tests of Orcasound nodes, this schematic describes how sources map to the `dev`, `beta`, and `live` subdomains of orcasound.net --

![Schematic of Orcasound source-subdomain mapping](http://orcasound.net/img/orcasound-app/Orcasound-software-evolution-model.png ""Orcasound software evolution model"")

([Google draw source](https://drive.google.com/file/d/1YFTAQPqgtcTl6ubac0mgyQ7fvg0BZqzH/view?usp=sharing) and [archived schematics](https://orcasound.net/img/orcasound-app/)) -- and you can monitor your development stream via the web-app using this URL structure:

```dev.orcasound.net/dynamic/node_name``` 

For example, with node_name = rpi_orcasound_lab the test URL would be [dev.orcasound.net/dynamic/rpi_orcasound_lab](http://dev.orcasound.net/dynamic/rpi_orcasound_lab).


## Deployment

If you would like to add a node to the Orcasound hydrophone network, read through our [Administrative Handbook](https://github.com/orcasound/.github/wiki#3-administrative-handbook) and then contact admin@orcasound.net if you have any questions. 

## Built With

* [FFmpeg](https://www.ffmpeg.org/) - Uses ALSA to acquire audio data, then generates lossy streams and/or lossless archive files
* [rsync](https://rsync.samba.org/) - Transfers files locally from /tmp to /mnt directories
* [s3fs](https://github.com/s3fs-fuse/s3fs-fuse) - Used to transfer audio data from local device to S3 bucket(s)

## Contributing

Please read [CONTRIBUTING.md](https://github.com/orcasound/orcanode/blob/master/CONTRIBUTING) for details on our code of conduct, and the process for submitting pull requests.

## Authors

* **Steve Hicks** - *Raspberry Pi expert* - [Steve on Github](https://github.com/mcshicks)
* **Paul Cretu** - *Lead developer* - [Paul on Github](https://github.com/paulcretu)
* **Scott Veirs** - *Project manager* - [Scott on Github](https://github.com/scottveirs)
* **Val Veirs** - *Hydrophone expert* - [Val on Github](https://github.com/veirs)

See also the list of [orcanode contributors](https://github.com/orcasound/orcanode/graphs/contributors) who have helped this project and the [Orcasound Hacker Hall of Fame](https://www.orcasound.net/hacker-hall-of-fame/) who have advanced both Orcasound open source code and the hydrophone network in the habitat of the endangered Southern Resident killer whales.

## License

This project is licensed under the GNU Affero General Public License v3.0 - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Thanks to the backers of the 2017 Kickstarter that funded the development of this open source code.
* Thanks to the makers of the Raspberry Pi, the Pisound HAT (Blokas in Lithuania), and the manufacturers who supply us with long-lasting, cost-effective hydrophones.
* Thanks to the many friends and backers who helped improve maintain nodes and improve the [Orcasound app](https://github.com/orcasound/orcasite).
","'audio-recorder', 'audio-streaming', 'aws-s3', 'boto', 'dash', 'hls', 'hls-live-streaming', 'hls-server', 'hls-stream', 'mpeg-dash', 'mseed', 'python'",2023-11-13T07:07:18Z,9,32,8,"('scottveirs', 136), ('paulcretu', 38), ('orcasoundapp', 31), ('karan2704', 17), ('mcshicks', 12), ('joyliao07', 3), ('evanjscallan', 1), ('valentina-s', 1), ('kunakl07', 1)","[14, 'Life Below Water']"
onaio/onadata,"Collect, Analyze and Share","Ona Platform
============

Collect, Analyze and Share Data!

.. image:: https://github.com/onaio/onadata/actions/workflows/ci.yml/badge.svg
  :target: https://github.com/onaio/onadata/actions/workflows/ci.yml

.. image:: https://app.codacy.com/project/badge/Grade/68c96351c8b24d5c9062a9c8247142f2
   :target: https://www.codacy.com/gh/onaio/onadata/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=onaio/onadata&amp;utm_campaign=Badge_Grade

.. image:: https://img.shields.io/badge/License-BSD_3--Clause-blue.svg
   :target: https://opensource.org/licenses/BSD-3-Clause


About
-----

Ona is derived from the excellent `formhub `_ platform developed by the Sustainable Engineering Lab at Columbia University.

Installation
------------

See the `installation documentation `_.

Docker
------

Install `Docker `_ and `Docker Compose `_.

.. code-block:: sh

    docker-compose up

    # create super user
    # -----------------
    docker exec -it onadata_web_1 bash

    # activate virtual envirenment
    source /srv/.virtualenv/bin/activate

    python manage.py createsuperuser

It should be accessible via http://localhost:8000. The settings are in
`onadata/settings/docker.py `_.

On registration check the console for the activation links, the default email
backend is ``django.core.mail.backends.console.EmailBackend``. See
`Django Docs `_ for details.

Contributing
------------

If you would like to contribute code please read
`Contributing Code to Ona Data `_.

Edit top level requirements in the file `requirements/base.in `_. Use
 `pip-compile `_ to update `requirements/base.pip `_.
 You will need to update `requirements.pip` and set `lxml==3.6.0`, for some unknown reason `pip-compile` seems to
 pick a lower version of lxml when `openpyxl` requires `lxml>=3.3.4`.

.. code-block:: sh

    pip-compile --output-file requirements/base.pip requirements/base.in

Copy `pre-commit.sh `_ into `.git/hooks/pre-commit`, it ensures staged python flake8 are in acceptable code style and conventions.

.. code-block:: sh

    cp pre-commit.sh .git/hooks/pre-commit
    chmod +x .git/hooks/pre-commit

**Security Acknowledgments**

We would like to thank the following security researchers for responsibly disclosing security issues:

============= ================  ==========  ==============
 Name          Date              Severity    Contribution
============= ================  ==========  ==============
Danish Tariq   1st April 2018     Medium     `Users able to create projects in other user accounts `_
============= ================  ==========  ==============

Code Structure
--------------

* **api** - This app provides the API functionality mostly made up of viewsets

* **logger** - This app serves XForms to and receives submissions from
  ODK Collect and Enketo.

* **viewer** - This app provides a csv and xls export of the data stored in
  logger. This app uses a data dictionary as produced by pyxform. It also
  provides a map and single survey view.

* **main** - This app is the glue that brings logger and viewer
  together.

Localization
------------

To generate a locale from scratch (ex. Spanish)

.. code-block:: sh

    django-admin.py makemessages -l es -e py,html,email,txt ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -l es && cd - ; done

To update PO files

.. code-block:: sh

    django-admin.py makemessages -a ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -a && cd - ; done

To compile MO files and update live translations

.. code-block:: sh

    django-admin.py compilemessages ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py compilemessages && cd - ; done

Api Documentation
-----------------

Generate the API documentation and serve via Django using:

.. code-block:: sh

    cd docs
    make html
    python manage.py collectstatic

Generate sphinx docs for new code using
`autodoc `_.

Run sphinx in autobuild mode using:

.. code-block:: sh

    sphinx-autobuild docs docs/_build/html

Requires sphinx-autobuild, install with ``pip install sphinx-autobuild``.


Django Debug Toolbar
--------------------

* `$ pip install django-debug-toolbar`
* Use/see `onadata/settings/debug_toolbar_settings/py`
* Access api endpoint on the browser and use `.debug` as the format extension e.g `/api/v1/projects.debug`

Upgrading existing installation to django 1.9+
----------------------------------------------

**Requirements**

* Postgres 9.4 or higher
* xcode-select version 2343 or higher

**Upgrading from a pervious Ona setup**
Ensure you upgrade all your pip requirements using the following command:

.. code-block:: sh

    pip install -r requirements/base.pip

Fake initial migration of `guardian`, `django_digest`, `registration`. Migrate `contenttypes` app first.

.. code-block:: sh

    python manage.py migrate contenttypes
    python manage.py migrate --fake-initial django_digest
    python manage.py migrate --fake-initial guardian
    python manage.py migrate --fake-initial registration
    python manage.py migrate


**Major django changes affecting Ona**
* The DATABASES settings key depricates the use of the *autocommit* setting in the *OPTIONS* dictionary.
",,2024-05-03T07:32:40Z,30,181,55,"('ukanga', 2937), ('pld', 1700), ('larryweya', 1250), ('dorey', 1011), ('DavisRayM', 854), ('amarder', 816), ('KipSigei', 426), ('ivermac', 367), ('mejymejy', 281), ('denniswambua', 234), ('lincmba', 230), ('moshthepitt', 160), ('rgaudin', 151), ('prabhasp', 140), ('WinnyTroy', 132), ('mberg', 115), ('modilabs-bumblebee', 108), ('prajjwol', 101), ('FrankApiyo', 96), ('Wambere', 94), ('TomCoder', 84), ('royrutto', 77), ('mrmoje', 50), ('bmarika', 50), ('geoffreymuchai', 50), ('antonatem', 49), ('katembu', 46), ('modilabs-starscream', 44), ('kelvin-muchiri', 42), ('urbanslug', 40)","[4, 'Quality Education']"
opendatateam/udata,Customizable and skinnable social platform dedicated to open data.,"

udata
=====

Customizable and skinnable social platform dedicated to (open)data.

The [full documentation][readthedocs-url] is hosted on Read the Docs.

udata is maintained by [Etalab](https://www.etalab.gouv.fr/), the
french public agency in charge of open data.  Etalab is responsible
for publishing udata's roadmap and for building consensus around it.

It is collectively taken care of by members of the
[OpenDataTeam](https://github.com/opendatateam).

[readthedocs-url]: https://udata.readthedocs.io/en/latest/
","'flask', 'flask-restplus', 'opendata', 'portal', 'python', 'vue', 'vuejs'",2024-05-02T14:52:39Z,30,229,14,"('noirbizarre', 3719), ('davidbgk', 579), ('abulte', 366), ('crowdin-opendatateam', 334), ('pyup-bot', 326), ('maudetes', 220), ('quaxsze', 134), ('jphnoel', 51), ('taniki', 48), ('ThibaudDauce', 43), ('vinyll', 33), ('jdesboeufs', 22), ('micael-grilo', 20), ('bzg', 18), ('l-vincent-l', 16), ('nicolaskempf57', 11), ('pblayo', 10), ('udata-bot', 10), ('grischard', 7), ('AntoineAugusti', 6), ('kojicz983', 5), ('tboye', 5), ('yohanboniface', 4), ('lepture', 4), ('JulienParis', 3), ('petzlux', 3), ('teleboas', 2), ('geoffreyaldebert', 2), ('eraviart', 2), ('ThomasG77', 2)","[17, 'Partnerships for the Goals']"
drivendataorg/zamba,"A Python package for identifying 42 kinds of animals, training custom models, and estimating distance from camera trap videos","# Zamba

[![Docs Status](https://img.shields.io/badge/docs-stable-informational)](https://zamba.drivendata.org/docs/)
[![tests](https://github.com/drivendataorg/zamba/workflows/tests/badge.svg?branch=master)](https://github.com/drivendataorg/zamba/actions?query=workflow%3Atests+branch%3Amaster)
[![codecov](https://codecov.io/gh/drivendataorg/zamba/branch/master/graph/badge.svg)](https://codecov.io/gh/drivendataorg/zamba)


https://user-images.githubusercontent.com/46792169/138346340-98ee196a-5ecd-4753-b9df-380528091f9e.mp4

> *Zamba* means ""forest"" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo.

**`zamba` is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap videos.** You can use `zamba` to:

- Identify which species appear in each video
- Filter out blank videos
- Create your own custom models that identify your species in your habitats
- Estimate the distance between animals in the frame and the camera
- And more! 🙈 🙉 🙊

The official models in `zamba` can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species common to Europe. Users can also finetune models using their own labeled videos to then make predictions for new species and/or new ecologies.

`zamba` can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, [Zamba Cloud](https://www.zambacloud.com/).

We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the [Model Zoo Wiki](https://github.com/drivendataorg/zamba/wiki) for others to be able to use!

Visit https://zamba.drivendata.org/docs/ for full documentation and tutorials.

## Installing `zamba`

First, make sure you have the prerequisites installed:

* Python 3.8 or 3.9
* FFmpeg > 4.3

Then run:
```console
pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz
```

See the [Installation](https://zamba.drivendata.org/docs/stable/install/) page of the documentation for details.

## Getting started

Once you have `zamba` installed, some good starting points are:

- The [Quickstart](https://zamba.drivendata.org/docs/stable/quickstart/) page for basic examples of usage
- The user tutorial for either [classifying videos](https://zamba.drivendata.org/docs/stable/predict-tutorial/) or [training a model](https://zamba.drivendata.org/docs/stable/train-tutorial/) depending on what you want to do with `zamba`

## Example usage

Once `zamba` is installed, you can see the basic command options with:
```console
$ zamba --help

 Usage: zamba [OPTIONS] COMMAND [ARGS]...

 Zamba is a tool built in Python to automatically identify the species seen in camera trap
 videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more
 in-depth documentation.

╭─ Options ─────────────────────────────────────────────────────────────────────────────────╮
│ --version                     Show zamba version and exit.                                │
│ --install-completion          Install completion for the current shell.                   │
│ --show-completion             Show completion for the current shell, to copy it or        │
│                               customize the installation.                                 │
│ --help                        Show this message and exit.                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────╯
╭─ Commands ────────────────────────────────────────────────────────────────────────────────╮
│ densepose      Run densepose algorithm on videos.                                         │
│ depth          Estimate animal distance at each second in the video.                      │
│ predict        Identify species in a video.                                               │
│ train          Train a model on your labeled data.                                        │
╰───────────────────────────────────────────────────────────────────────────────────────────╯
```

`zamba` can be used ""out of the box"" to generate predictions or train a model using your own videos. `zamba` supports the same video formats as FFmpeg, [which are listed here](https://www.ffmpeg.org/general.html#Supported-File-Formats_002c-Codecs-or-Features). Any videos that fail a set of FFmpeg checks will be skipped during inference or training.

### Classifying unlabeled videos

```console
$ zamba predict --data-dir path/to/videos
```

By default, predictions will be saved to `zamba_predictions.csv`. Run `zamba predict --help` to list all possible options to pass to `predict`.

See the [Quickstart](https://zamba.drivendata.org/docs/stable/quickstart/) page or the user tutorial on [classifying videos](https://zamba.drivendata.org/docs/stable/predict-tutorial/) for more details.

### Training a model

```console
$ zamba train --data-dir path/to/videos --labels path_to_labels.csv --save_dir my_trained_model
```

The newly trained model will be saved to the specified save directory. The folder will contain a model checkpoint as well as training configuration, model hyperparameters, and validation and test metrics. Run `zamba train --help` to list all possible options to pass to `train`.

You can use your trained model on new videos by editing the `train_configuration.yaml` that is generated by `zamba`. Add a `predict_config` section to the yaml that points to the checkpoint file that is generated:

```yaml
...
# generated train_config and video_loader_config
...

predict_config:
  checkpoint: PATH_TO_YOUR_CHECKPOINT_FILE

```

Now you can pass this configuration to the command line. See the [Quickstart](https://zamba.drivendata.org/docs/stable/quickstart/) page or the user tutorial on [training a model](https://zamba.drivendata.org/docs/stable/train-tutorial/) for more details.

You can then share your model with others by adding it to the [Model Zoo Wiki](https://github.com/drivendataorg/zamba/wiki).

### Estimating distance between animals and the camera

```console
$ zamba depth --data-dir path/to/videos
```

By default, predictions will be saved to `depth_predictions.csv`. Run `zamba depth --help` to list all possible options to pass to `depth`.

See the [depth estimation page](https://zamba.drivendata.org/docs/stable/models/depth/) for more details.


## Contributing

We would love your contributions of code fixes, new models, additional training data, docs revisions, and anything else you can bring to the project!

See the docs page on [contributing to `zamba`](https://zamba.drivendata.org/docs/stable/contribute/) for details.
","'animals', 'camera-traps', 'chimps', 'cli', 'conservation', 'deep-learning', 'ecology', 'gpu', 'jungle', 'machine-learning', 'neural-network', 'python', 'pytorch', 'pytorch-lightning', 'video-processing', 'videos'",2024-04-26T20:58:03Z,13,94,11,"('caseyfitz', 78), ('pjbull', 60), ('ejm714', 53), ('pdima', 23), ('AllenDowney', 15), ('r-b-g-b', 5), ('klwetstone', 3), ('dependabotbot', 3), ('jayqi', 2), ('wookietreiber', 1), ('Iamshankhadeep', 1), ('tamara-glazer', 1), ('drivendata', 1)","[17, 'Partnerships for the Goals']"
wri/global-power-plant-database,"A comprehensive, global, open source database of power plants","# Global Power Plant Database

**This project is not currently maintained by WRI. There are no planned updates as of this time (early 2022). The last version of this database is version 1.3.0. If we learn of active forks or maintained versions of the code and database we will attempt to provide links in the future.**



This project aims to build [an open database of all the power plants in the world](http://www.wri.org/publication/global-power-plant-database). It is the result of a large collaboration involving many partners, coordinated by the [World Resources Institute](https://www.wri.org/) and [Google Earth Outreach](https://www.google.com/earth/outreach/index.html). If you would like to get involved, please [email the team](mailto:powerexplorer@wri.org) or fork the repo and code! To learn more about how to contribute to this repository, read the [`CONTRIBUTING`](https://github.com/wri/global-power-plant-database/blob/master/.github/CONTRIBUTING.md) document.

The latest database release (v1.3.0) is available in CSV format [here](http://datasets.wri.org/dataset/globalpowerplantdatabase) under a [Creative Commons-Attribution 4.0 (CC BY 4.0) license](https://creativecommons.org/licenses/by/4.0/). A bleeding-edge version is in the [`output_database`](https://github.com/wri/global-power-plant-database/blob/master/output_database) directory of this repo.

All Python source code is available under a [MIT license](https://opensource.org/licenses/MIT).

This work is made possible and supported by [Google](https://environment.google/), among other organizations.

## Database description

The Global Power Plant Database is built in several steps.

* The first step involves gathering and processing country-level data. In some cases, these data are read automatically from offical government websites; the code to implement this is in the `build_databases` directory.
* In other cases we gather country-level data manually. These data are saved in `raw_source_files/WRI` and processed with the `build_database_WRI.py` script in the `build_database` directory. 
* The second step is to integrate data from different sources, particularly for geolocation of power plants and annual total electricity generation. Some of these different sources are multi-national databases. For this step, we rely on offline work to match records; the concordance table mapping record IDs across databases is saved in resources/master_plant_concordance.csv.

Throughout the processing, we represent power plants as instances of the `PowerPlant` class, defined in `powerplant_database.py`. The final database is in a flat-file CSV format.

## Key attributes of the database

The database includes the following indicators:

* Plant name
* Fuel type(s)
* Generation capacity
* Country
* Ownership
* Latitude/longitude of plant
* Data source & URL
* Data source year
* Annual generation

We will expand this list in the future as we extend the database.

### Fuel Type Aggregation

We define the ""Fuel Type"" attribute of our database based on common fuel categories. In order to parse the different fuel types used in our various data sources, we map fuel name synonyms to our fuel categories [here](https://github.com/wri/global-power-plant-database/blob/master/resources/fuel_type_thesaurus). We plan to expand the database in the future to report more disaggregated fuel types.

## Combining Multiple Data Sources

A major challenge for this project is that data come from a variety of sources, including government ministries, utility companies, equipment manufacturers, crowd-sourced databases, financial reports, and more. The reliability of the data varies, and in many cases there are conflicting values for the same attribute of the same power plant from different data sources. To handle this, we match and de-duplicate records and then develop rules for which data sources to report for each indicator. We provide a clear [data lineage](https://en.wikipedia.org/wiki/Data_lineage) for each datum in the database. We plan to ultimately allow users to choose alternative rules for which data sources to draw on.

To the maximum extent possible, we read data automatically from trusted sources, and integrate it into the database. Our current strategy involves these steps:

* Automate data collection from machine-readable national data sources where possible. 
* For countries where machine-readable data are not available, gather and curate power plant data by hand, and then match these power plants to plants in other databases, including GEO and CARMA (see below) to determine their geolocation.
* For a limited number of countries with small total power-generation capacity, use data directly from Global Energy Observatory (GEO). 

A table describing the data source(s) for each country is listed below.

Finally, we are examining ways to automatically incorporate data from the following supra-national data sources:

* [Clean Development Mechanism](https://cdm.unfccc.int/Projects/projsearch.html)
* [ENTSO-E](https://www.entsoe.eu/Pages/default.aspx)
* [E-PRTR](http://prtr.ec.europa.eu/)
* [CARMA](http://carma.org/)
* [Arab Union of Electricity](http://www.auptde.org/Default.aspx?lang=en)
* [IAEA PRIS](https://www.iaea.org/pris/)
* [Industry About](http://www.industryabout.com/energy)
* [Think Geo Energy](http://www.thinkgeoenergy.com/map/)
* [WEC Global Hydropower Database](https://www.worldenergy.org/data/resources/resource/hydropower/)

## ID numbers

We assign a unique ID to each line of data that we read from each source. In some cases, these represent plant-level data, while in other cases they represent unit-level data. In the case of unit-level data, we commonly perform an aggregation step and assign a new, unique plant-level ID to the result. For plants drawn from machine-readable national data sources, the reference ID is formed by a three-letter country code [ISO 3166-1 alpha-3](http://unstats.un.org/unsd/tradekb/Knowledgebase/Country-Code) and a seven-digit number. For plants drawn from other database (including the manually-maintained dataset by WRI), the reference ID is formed by a variable-size prefix code and a seven-digit number.

## Power plant matching

In many cases our data sources do not include power plant geolocation information. To address this, we attempt to match these plants with the GEO and CARMA databases, in order to use that geolocation data. We use an [elastic search matching technique](https://github.com/cbdavis/enipedia-search) developed by Enipedia to perform the matching based on plant name, country, capacity, location, with confirmed matches stored in a concordance file. This matching procedure is complex and the algorithm we employ can sometimes wrongly match two power plants or fail to match two entries for the same power plant. We are investigating using the Duke framework for matching, which allows us to do the matching offline.


## Build Instructions
The build system is as follows

- Create a virtual environment with Python 2.7 and the third-party packages in `requirements.txt`
- `cd` into `build_databases/`
- run each `build_database_*.py` file for each data source or processing method that changed (when making a database update)
- run `build_global_power_plant_database.py` which reads from the pickled store/sub-databases.
- `cd` into `../utils`
- run `database_country_summary.py` to produce summary table
- `cd` into `../output_database`
- copy `global_power_plant_database.csv` to the [`gppd-ai4earth-api`](https://github.com/wri/gppd-ai4earth-api) repository. Look a the `Makefile` in that repo to understand where it should be located
- build new generation estimations as needed based on plant changes and updates compared to the stored and calculated values - this is not automatic, but there are some helper scripts for making the estimates
- run the `make_gppd.py` script in `gppd-ai4earth-api` to construct a new version of the database with the full estimation data
- copy the new merged dataset back to this repo, increment the `DATABASE_VERSION` file, commit, etc...

 
## Related repos

* [Open Power Systems Data](https://github.com/Open-Power-System-Data/)
* [Public Utility Data Liberation Project](https://github.com/catalyst-cooperative/pudl)
* [Global Energy Observatory](https://github.com/hariharshankar/pygeo)
* [GeoNuclearData](https://github.com/cristianst85/GeoNuclearData)
* [Duke](https://github.com/larsga/Duke)
","'climate', 'climate-data', 'energy', 'energy-data', 'free-datasets', 'open-data', 'open-datasets'",2022-01-26T16:48:43Z,2,312,48,"('loganbyers', 39), ('colinmccormick', 12)","[13, 'Climate Action']"
developmentseed/geolambda,Create and deploy Geospatial AWS Lambda functions,"# GeoLambda: geospatial AWS Lambda Layer

The GeoLambda project provides public Docker images and AWS Lambda Layers containing common geospatial native libraries. GeoLambda contains the libraries for GDAL, Proj, GEOS, GeoTIFF, HDF4/5, SZIP, NetCDF, OpenJPEG, WEBP, ZSTD, and others. For some applications you may wish to minimize the size of the libraries by excluding unused libraries, or you may wish to add other libraries. In this case this repository can be used as a template to create your own Docker image or Lambda Layer following the instructions in this README.

This repository also contains additional images and layers for specific runtimes. Using them as a Layer assumes the use of the base GeoLambda layer.

- [Python](python/README.md)

## Usage

While GeoLambda was initially intended for AWS Lambda it is also useful as a base geospatial Docker image. For detailed info on what is included in each image, see the Dockerfile for that version or the [CHANGELOG](CHANGELOG.md). A version summary is provided here:

| geolambda | GDAL  | Notes |
| --------- | ----  | ----- |
| 1.0.0     | 2.3.1 | |
| 1.1.0     | 2.4.1 | |
| 1.2.0     | 2.4.2 | Separate Python (3.7.4) image and Lambda Layer added |
| 2.0.0		| 3.0.1 | libgeotiff 1.5.1, proj 6.2.0 |
| 2.1.0		| 3.2.1 | libgeotiff 1.6.0, proj 7.2.1, openjpeg 2.4.0, Python layer 3.7.9 |

#### Environment variables

When using GeoLambda some environment variables need to be set. These are set in the Docker image, but if using the Lambda Layer they will need to be set:

- GDAL_DATA=/opt/share/gdal
- PROJ_LIB=/opt/share/proj   (only needed for GeoLambda 2.0.0+)

### Lambda Layers

If you just wish to use the publicly available Lambda layers you will need the ARN for the layer in the same region as your Lambda function. Currently, the latest GeoLambda layers are deployed in `us-east-1`, `us-west-2`, `eu-central-1`, `eu-west-2`, and `eu-north-1`. If you want to use it in another region please file an issue or you can also create your own layer using this repository (see instructions below on 'Create a new version').

#### v2.1.0

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda:4 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda:4 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda:4 |
| eu-west-2 | |
| eu-north-1 | | 

#### v2.1.0-python

See the [GeoLambda Python README](python/README.md). The Python Lambda Layer includes the libraries `numpy`, `rasterio`, `GDAL`, `pyproj`, and `shapely`, plus everything in the standard GeoLambda layer. Note this is a change from v2.0.0 where both Layers needed to be included in a Lambda.

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda-python:3 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda-python:3 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda-python:3 |
| eu-west-2 | |
| eu-north-1 | | 


#### v2.0.0

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda:4 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda:4 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda:4 |

#### v2.0.0-python

See the [GeoLambda Python README](python/README.md). The Python Lambda Layer includes the libraries `numpy`, `rasterio`, `GDAL`, `pyproj`, and `shapely`. This is an addition to the standard GeoLambda layer; if you wish to use GeoLambda-Python, both layers must be included.

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda-python:3 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda-python:3 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda-python:3 |

#### v1.2.0

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda:2 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda:2 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda:2 |

#### v1.2.0-python

See the [GeoLambda Python README](python/README.md). The Python Lambda Layer includes the libraries `numpy`, `rasterio`, `GDAL`, `pyproj`, and `shapely`. This is an addition to the standard GeoLambda layer; if you wish to use GeoLambda-Python, both layers must be included.

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda-python:1 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda-python:1 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda-python:1 |

#### v1.1.0

| Region | ARN |
| ------ | --- |
| us-east-1 | arn:aws:lambda:us-east-1:552188055668:layer:geolambda:1 |
| us-west-2 | arn:aws:lambda:us-west-2:552188055668:layer:geolambda:1 |
| eu-central-1 | arn:aws:lambda:eu-central-1:552188055668:layer:geolambda:1 |


### Docker images

The Docker images used to create the Lambda layer are also published to Docker Hub, and thus are also suitable for general use as a base image for geospatial applications. 

The developmentseed/geolambda image in Docker Hub is tagged by version.

	$ docker pull developmentseed/geolambda:

Or just include it in your own Dockerfile as the base image.

```
FROM developmentseed/geolambda:
```

The GeoLambda image does not have an entrypoint defined, so a command must be provided when you run it. This example will mount the current directory to /work and run the container interactively.

	$ docker run --rm -v $PWD:/home/geolambda -it developmentseed/geolambda:latest /bin/bash

All of the GDAL CLI tools are installed so could be run on images in the current directory.


## Development

Contributions to the geolambda project are encouraged. The goal is to provide a turnkey method for developing and deploying geospatial applications to AWS. The 'master' branch in this repository contains the current state as deployed to the Docker Hub images `developmentseed/geolambda:latest` and `devlopmentseed/geolambda-python:latest`, along with a tag of the version. The 'develop' branch is the development version and is not deployed to Docker Hub.

When making a merge to the `master` branch be sure to increment the `VERSION` file. Circle will push the new version as a tag to GitHub and build and push the image to Docker Hub. If a GitHub tag already exists with that version the process will fail.

### Create a new version

Use the Dockerfile here as a template for your own version of GeoLambda. Simply edit it to remove or add additional libraries, then build and tag with your own name. The steps below are used to create a new official version of GeoLambda, replace `developmentseed/geolambda` with your own name.

To create a new version of GeoLambda follow these steps. Note that this is the manual process of what is currently done in CircleCI, so it is not necessary to perform them but they are useful as an example for deploying your own versions.

1. update the version in the `VERSION` file

The version in the VERSION file will be used to tag the Docker images and create a GitHub tag.

2. build the image:
  
```
$ VERSION=$(cat VERSION)
$ docker build . -t developmentseed/geolambda:${VERSION}
```

3. Push the image to Docker Hub:

```
$ docker push developmentseed/geolambda:${VERSION}
```

4. Create deployment package using the built-in [packaging script](bin/package.sh)

```
$ docker run --rm -v $PWD:/home/geolambda \
	-it developmentseed/geolambda:${VERSION} package.sh
```

This will create a lambda/ directory containing the native libraries and related files, along with a `lambda-deploy.zip` file that can be deployed as a Lambda layer.

5. Push as Lambda layer (if layer already exists a new version will be created)

```
$ aws lambda publish-layer-version \
	--layer-name geolambda \
	--license-info ""MIT"" \
	--description ""Native geospatial libaries for all runtimes"" \
	--zip-file fileb://lambda-deploy.zip
```

6. Make layer public (needs to be done each time a new version is published)

```
$ aws lambda add-layer-version-permission --layer-name geolambda \
	--statement-id public --version-number 1 --principal '*' \
	--action lambda:GetLayerVersion
```
",,2022-07-07T10:04:49Z,6,301,50,"('matthewhanson', 317), ('robertd', 11), ('vincentsarago', 2), ('bluetyson', 1), ('kylebarron', 1), ('seilerman', 1)","[17, 'Partnerships for the Goals']"
nordic-institute/X-Road,Source code of the X-Road® data exchange layer software,"# X-Road® Data Exchange Layer

[![Go to X-Road Community Slack](https://img.shields.io/badge/Go%20to%20Community%20Slack-grey.svg)](https://jointxroad.slack.com/)
[![Get invited](https://img.shields.io/badge/No%20Slack-Get%20invited-green.svg)](https://x-road.global/community)
[![Build status](https://github.com/nordic-institute/X-Road/actions/workflows/build.yaml/badge.svg)](https://github.com/nordic-institute/X-Road/actions?query=branch%3Adevelop++)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=nordic-institute_X-Road&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=nordic-institute_X-Road)

![X-Road logo](xroad_logo_small.png)

## About the repository 

This repository contains information about the [X-Road](https://x-road.global), its source code, development, installation and documentation.

## X-Road source code

The [source code](https://github.com/nordic-institute/X-Road/tree/develop/src) of X-Road is published under the MIT open source licence. X-Road is free of charge and can be implemented by any organisation.

**What is X-Road?**

X-Road® is open-source software and ecosystem solution that provides unified and secure data exchange between organisations.

![X-Road overview](X-Road_overview.png)

X-Road Data Exchange Layer is a standardised, cohesive, collaborative, interoperable and secure data exchange layer that gives service providers a completely new kind of opportunity of making themselves visible in services directed at citizens, businesses and civil servants. Creating entities that combine many different services and data sources is easy and cost efficient.

* Improves the quality of existing services and products
* Enables new types of service innovations
* Savings in infrastructure, archiving and other costs
* Standardised data security and privacy protection
* Easy implementation, data access via interfaces – after connecting all included services are available

See [X-Road product website](https://x-road.global) for more information about X-Road.

## Development of X-Road

X-Road development model and all the related documentation is published and
maintained in the [X-Road Development](https://github.com/nordic-institute/X-Road-development/)
GitHub repository.

**How to contribute?**

Guidelines for the X-Road contributors are available [here](CONTRIBUTING.md).

## X-Road installation

**How to build X-Road?**

See instructions in [src/BUILD.md](src/BUILD.md)

**Local installation**

X-Road central servers, configuration proxies and security servers can be automatically installed with the Ansible scripts found in ansible subdirectory. See instructions in [ansible/README.md](ansible/README.md)

## X-Road technical documentation

* [Documentation](https://docs.x-road.global)
* [Knowledge base](https://x-road.global/kb)
* [Release notes](https://x-road.global/release-notes)

## Further information about X-Road

[X-Road developer resources](https://x-road.global/resources)

## Support disclaimer

The following activities, among others, are undertaken by the
[Nordic Institute for Interoperability Solutions (NIIS)](https://www.niis.org/)
with regard to the X-Road core software:

* management, development, verification, and audit of the source code
* administration of documentation
* administration of business and technical requirements
* conducting development
* developing and implementing principles of licensing and distribution
* providing second-line support for the NIIS members
* international cooperation.

[X-Road Technology Partners](https://x-road.global/xroad-technology-partners) are enterprises providing X-Road support and consultation services, e.g. deploying independent X-Road instances, developing X-Road extensions and X-Road-compatible services, integrating informations systems with X-Road etc.

[X-Road Community](https://x-road.global) is the global community of X-Road users and enthusisasts. The community is for anyone interested in X-Road. It's about learning from others and sharing the skills and experiences of how to create better digital services both technically and business-wise.

[X-Road Academy](https://x-road.thinkific.com) provides online training for developers, users, operators, consultants, service providers and for anyone willing to learn more about X-Road.
","'data-exchange', 'hacktoberfest', 'integration', 'open-source', 'x-road'",2024-05-03T07:47:32Z,30,581,42,"('Riippi', 1844), ('iluwatar', 1218), ('jansu76', 1100), ('jhyoty', 1036), ('carohauta', 1029), ('TJaakkola', 628), ('petkivim', 622), ('bertofl90', 595), ('ricardas-buc', 453), ('ovidijusnortal', 426), ('teemukin65', 277), ('raits', 242), ('justasnortal', 209), ('dependabotbot', 208), ('andresrosenthal', 181), ('VPaliliunas', 173), ('enelir', 125), ('ToomasMolder', 108), ('tmkrepo', 96), ('turkkaholmqvist', 92), ('mloitm', 86), ('olliru', 59), ('raulmartinez-leadin', 49), ('liutkute', 46), ('martensoo', 37), ('AnttiLuoma', 36), ('mikkbachmann', 29), ('kristoheero', 25), ('guycalledguy', 22), ('lakoutan', 16)","[17, 'Partnerships for the Goals']"
sugarlabs/musicblocks,Music Blocks -- A musical microworld,"
    



    


# Music Blocks

“_All musicians are subconsciously mathematicians._” — Monk

“_Music is a hidden arithmetic exercise of the soul, which does not
know that it is counting._” — Leibniz

Music Blocks is a _Visual Programming Language_ and collection of
_manipulative tools_ for exploring musical and mathematical concepts
in an integrative and fun way.

## Getting Started

Music Blocks is an interactive Web Application &mdash; the interaction
is done via basic mouse events like _click_, _right click_, _click and
drag_, etc. and keyboard events like _hotkey press_.  The application
is audio-visual; it produces graphics artwork and music. Here are a
couple of screenshots to give you an idea of how the application looks
like:

![alt tag](./screenshots/Screenshot-1.png)

![alt tag](./screenshots/Screenshot-2.png)

Visit the Music Blocks website for a hands on experience:
[https://musicblocks.sugarlabs.org](https://musicblocks.sugarlabs.org).

Or get Music Blocks from the [Google Play Store](https://play.google.com/store/apps/details?id=my.musicblock.sugarlab)

Some background on why we combine music and programming can be found
[here](./WhyMusicBlocks.md).

**Refer to the following sections to get familiar with this application:**

- [Running Music Blocks](#RUNNING-MUSIC-BLOCKS)
- [How to set up a local server](#HOW-TO-SET-UP-A-LOCAL-SERVER)
- [Using Music Blocks](#USING-MUSIC-BLOCKS)

If you are a developer (beginner, experienced, or pro), you are very
welcome to participate in the evolution of Music Blocks.

**Refer to the following sections to get an idea:**

- [Code of Conduct](#CODE-OF-CONDUCT)
- [Contributing](#CONTRIBUTING)
- [Modifying Music Blocks](#MODIFYING-MUSIC-BLOCKS)
- [Reporting Bugs](#REPORTING-BUGS)

**Refer to the following for more information regarding the evolution
  of this project:**

- [Credits](#CREDITS)
- [Music Blocks in Japan](#MUSIC-BLOCKS-IN-JAPAN)

## Running Music Blocks

Music Blocks is available under the _GNU Affero General Public License
(AGPL) v3.0_, a free, copyleft license.

Music Blocks is designed to run in a web browser. The ideal way to run
Music Blocks is to visit the URL
[_musicblocks.sugarlabs.org_](https://musicblocks.sugarlabs.org) in
your browser — _Google Chrome_ (or _Chromium_), _Microsoft Edge_
(_Chromium-based_), _Mozilla Firefox_, and _Opera_ work best.

To run from the latest master branch (experimental), visit
[_sugarlabs.github.io/musicblocks_](https://sugarlabs.github.io/musicblocks).

## How to set up a _local server_

Music Blocks is written using native browser technologies. The bulk of
the functionality is in vanilla _JavaScript_. This means that most of
the functionality can be accessed by launching the
[index.html](./index.html) file in the browser using
`file:///absolute/path/to/index.html`.

However, using so, some functionality will not be available. On top of
that, some web browsers (e.g., Firefox v68) have restrictions that
prevent Music Blocks from running using `file:///`.  Therefore, it is
best to launch a _local web server_ from the directory of Music
Blocks.

1. [Download](https://github.com/sugarlabs/musicblocks/archive/master.zip)
Music Blocks, or clone (`https://github.com/sugarlabs/musicblocks.git`
for _HTTPS_, or `gh repo clone sugarlabs/musicblocks` for _GitHub
CLI_), on your local machine.

2. In a terminal, `cd` to the directory where you downloaded/cloned
Music Blocks, using `cd path/to/musicblocks/`.

3. If you do not have [_Python_](https://www.python.org) installed,
you'll need to install it.  You can test for Python in a terminal
using `python`. Type `exit()` to exit Python. (Note that on some older
Linux systems, the `python3` command is not bound to python. You may
need to perform a `sudo apt install python-is-python3` on Debian-like
distros, or equivalent on others.)

4. After cloning the musicblocks repository, run

    for _Linux_ and _macOS_:

    ```bash
    python -c ""import os, sys; os.system('python -m SimpleHTTPServer 3000 --bind 127.0.0.1') if sys.version_info.major==2 else os.system('python -m http.server 3000 --bind 127.0.0.1')""
    ```

    for _Windows_:

    ```bash
    python -c ""import os, sys; os.system('python -m SimpleHTTPServer 3000') if sys.version_info.major==2 else os.system('python -m http.server 3000 --bind 127.0.0.1')""
    ```

    If you have `npm` installed, simply run `npm run serve` for Linux
    and macOS, and `npm run winserve` for Windows.

    **NOTE:** _Make sure you can run either `python` or `py` from your
    terminal, to launch the Python prompt._

5. You should see a message `Serving HTTP on 127.0.0.1 port 3000
(http://127.0.0.1:3000/) ...` since the HTTP Server is set to start
listening on port 3000.

6. Open your favorite browser and visit `localhost:3000` or `127.0.0.1:3000`.

**NOTE:** _Use `ctrl + c` or `cmd + c` to quit the HTTP Server to avoid
`socket.error:[Errno 48]`_.



## Local Setup with Docker

## Prerequisites

Before you begin, ensure you have Docker installed on your machine. You can download and install Docker from the [official Docker website](https://www.docker.com/get-started).

## Installation

1. Clone the Music Blocks repository to your local machine:

   ```bash
   git clone https://github.com/sugarlabs/musicblocks.git
   ```

2. Navigate to the cloned repository:

   ```bash
   cd musicblocks
   ```

3. Build the Docker image using the provided Dockerfile:

   ```bash
   docker build -t musicblocks .
   ```
## Running Music Blocks

1. Run the Docker container using the built image:

   ```bash
   docker run -p 3000:3000 musicblocks
   ```

   This command will start a Docker container running Music Blocks and expose it on port 3000.

2. Access Music Blocks in your web browser by navigating to `http://localhost:3000`.

## Stopping the Docker container

To stop the Docker container, use `Ctrl + C` in your terminal. This will stop the container and free up the port it was using.

## Additional Notes

- Make sure to replace `musicblocks` with the appropriate image name if you have tagged the Docker image differently.
- You can customize the port mapping (`-p`) if you prefer to use a different port for accessing Music Blocks.

---

This documentation provides a basic setup for running Music Blocks locally using Docker. Feel free to customize it further based on your specific requirements and environment.
## Using Music Blocks

Once Music Blocks is running, you'll want suggestions on how to use
it. Follow [Using Music Blocks](./documentation/README.md) and [Music
Blocks Guide](./guide/README.md).

For Scratch and Snap users, you may want to look at [Music Blocks for
Snap Users](./Music_Blocks_for_Snap_Users.md).

Looking for a block? Find it in the
[Palette Tables](./guide/README.md#6-appendix).

## Code of Conduct

The Music Blocks project adheres to the [Sugar Labs Code of
Conduct](https://github.com/sugarlabs/sugar-docs/blob/master/src/CODE_OF_CONDUCT.md)

## Contributing

Please consider contributing to the project, with your ideas, your
music, your lesson plans, your artwork, and your code.

### Special Notes

Music Blocks is being built from the ground-up, to address several
architectural problems with this run. Since Music Blocks is a fork of
Turtle Blocks JS, musical functionality was added on top of it.
However, music is fundamental to Music Blocks. Besides, the Turtle
Blocks JS started initially with handful of features and was written
without a complex architecture. As Music Blocks got built on top of
that, it got incrementally complex, but the architecture remained
simple, thus resulting in a monolith. Also, the functionality is
tightly coupled with the interface and native client API (Web API).

Keeping these problems in mind, we have considered a foundational
rebuild that will address all these issues, whilst adding buffers for
future additions. We'll also be using a more elegant tech-stack to
develop and maintain this project given its scale. After the core is
built, we'll be porting features from this application to it.

Refer to the repository
[**sugarlabs/musicblocks-2**](https://github.com/sugarlabs/musicblocks-2)
for more information about the new project &mdash; _Music Blocks 2.0_.

### Tech Stack

Music Blocks is a Web Application and is written using browser
technologies &mdash; `HTML`, `CSS` (`SCSS`), `JavaScript`, `SVG`, etc.

If you're just getting started with development, you may refer to the
following resources:

- [HTML tutorial - w3schools.com](https://www.w3schools.com/html/default.asp)
- [HTML reference - MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML)
- [CSS tutorial - w3schools.com](https://www.w3schools.com/css/default.asp)
- [CSS reference - MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/CSS)
- [JavaScript tutorial - w3schools.com](https://www.w3schools.com/js/default.asp)
- [JavaScript reference - MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/JavaScript)

Programmers, please follow these general [guidelines for
contributions](https://github.com/sugarlabs/sugar-docs/blob/master/src/contributing.md).

### New Contributors

Use the
[discussions](https://github.com/sugarlabs/musicblocks/discussions)
tab at the top of the repository to:

- Ask questions you’re wondering about.
- Share ideas.
- Engage with other community members.

Feel free. But, please don't spam :p.

### Keep in Mind

1. Your contributions need not necessarily have to address any
discovered issue. If you encounter any, feel free to add a fix through
a PR, or create a new issue ticket.

2. Use [labels](https://github.com/sugarlabs/musicblocks/labels) on
your issues and PRs.

3. Do not spam with lots of PRs with little changes.

4. If you are addressing a bulk change, divide your commits across
multiple PRs, and send them one at a time. The fewer the number of
files addressed per PR, the better.

5. Communicate effectively. Go straight to the point. You don't need
to address anyone using '_sir_'. Don't write unnecessary comments;
don't be over-apologetic. There is no superiority hierarchy. Every
single contribution is welcome, as long as it doesn't spam or distract
the flow.

6. Write useful, brief commit messages. Add commit descriptions if
necessary. PR name should speak about what it is addressing and not
the issue. In case a PR fixes an issue, use `fixes #ticketno` or
`closes #ticketno` in the PR's comment. Briefly explain what your PR
is doing.

7. Always test your changes extensively before creating a PR. There's
no sense in merging broken code. If a PR is a _work in progress
(WIP)_, convert it to draft. It'll let the maintainers know it isn't
ready for merging.

8. Read and revise the concepts about programming constructs you're
dealing with. You must be clear about the behavior of the language or
compiler/transpiler. See [JavaScript
docs](https://developer.mozilla.org/en-US/docs/Web/JavaScript).

9. If you have a question, do a _web search_ first. If you don't find
any satisfactory answer, then ask it in a comment. If it is a general
question about Music Blocks, please use the new
[discussions](https://github.com/sugarlabs/musicblocks/discussions)
tab on top the the repository, or the _Sugar-dev Devel
_
mailing list. Don't ask silly questions (unless you don't know it is
silly ;p) before searching it on the web.

10. Work on things that matter. Follow three milestones: `Port Ready`,
`Migration`, and `Future`.  Those tagged `Port Ready` are
priority. Those tagged with `Migration` will be taken care of during
or after the foundation rebuild. Feel free to participate in the
conversation, adding valuable comments. Those tagged with `Future`
need not be addressed presently.

_Please note there is no need to ask permission to work on an
issue. You should check for pull requests linked to an issue you are
addressing; if there are none, then assume nobody has done
anything. Begin to fix the problem, test, make your commits, push your
commits, then make a pull request. Mention an issue number in the pull
request, but not the commit message. These practices allow the
competition of ideas (Sugar Labs is a meritocracy)._

## Modifying Music Blocks

The core functionality for Music Blocks resides in the [`js/`
directory](./js/). Individual modules are described in more detail in
[js/README.md](./js/README.md).

**NOTE:** As for any changes, please make a local copy by cloning this
[repository](https://github.com/sugarlabs/musicblocks.git). Make your
changes, test them, and only then make a pull request.

[Contributing
Code](https://github.com/sugarlabs/sugar-docs/blob/master/src/contributing.md)
provides a general overview of Sugar Lab's guidelines. See
[Contributing](#CONTRIBUTING) section for specific details about this
repository.

## Reporting Bugs

Bugs can be reported in the [issues
tab](https://github.com/sugarlabs/musicblocks/issues) of this
repository.

If possible, please include the browser _console log output_, and
_steps to reproduce_, when reporting bugs. To access the console, type
`Ctrl-Shift-J`/`F12` on most browsers. Alternately, _right click_ and
select `Inspect`. You may need to set the `Default levels` for the
console to `Verbose` in order to see all of the output, however, in
most cases that won't be required. In fact, it'll only clutter the
list, so select it only when required.

## Credits

Music Blocks is a fork of [Turtle Blocks
JS](https://github.com/sugarlabs/turtleblocksjs) created by _Walter
Bender ([@walterbender](https://github.com/walterbender))_.

[_Devin Ulibarri_](http://www.devinulibarri.com/) has contributed
functional and user-interface designs. Many of his contributions were
inspired by the music education ideas, representations and practices
(e.g. aspects of matrix, musical cups) developed and published by
[_Larry Scripp_](http://www.larryscripp.net/) with whom _Devin_
studied at New England Conservatory and for whom he worked at Affron
Scripp & Associates, LLC, [Center for Music and the Arts in Education
(CMAIE)](http://centerformie.org/), and [Music in
Education](http://music-in-education.org/). Some of the initial
graphics were contributed by [_Chie
Yasuda_](http://www.chieyasuda.com).

Much of the initial coding specific to Music Blocks was done by _Yash
Khandelwal ([@khandelwalYash](https://github.com/khandelwalYash))_ as
part of Google Summer of Code (GSoC) 2015. _Hemant Kasat
([@hemantkasat](https://github.com/hemantkasat))_ contributed to
additional widgets as part of GSoC 2016. Additional contributions were
made by _Tayba Wasim ([@Tabs16](https://github.com/Tabs16))_, _Dinuka
Tharangi Jayaweera ([@Tharangi](https://github.com/Tharangi))_,
_Prachi Agrawal
([@prachiagrawal269](https://github.com/prachiagrawal269))_, _Cristina
Del Puerto ([@cristinadp](https://github.com/cristinadp))_, and
_Hrishi Patel ([@Hrishi1999](https://github.com/Hrishi1999))_ as part
of GSoC 2017. During GSoC 2018, _Riya Lohia
([@riyalohia](https://github.com/riyalohia))_ developed a Temperament
widget.  _Ritwik Abhishek ([@a-ritwik](https://github.com/a-ritwik))_
added a keyboard widget and a pitch-tracking widget. During GSoC 2019,
_Favor Kelvin ([@fakela](https://github.com/fakela))_ refactored much
of the code to use promises. During GSoC 2020, _Anindya Kundu
([@meganindya](https://github.com/meganindya))_ did a major
refactoring of the code base to support JavaScript export. _Aviral
Gangwar ([@aviral243](https://github.com/aviral243))_ enhanced the
internal representation of mode and key.  _Saksham Mrig
([@sksum](https://github.com/sksum))_ fixed 70+ bugs and added support
for pitch tracking and MIDI import.

Many students contributed to the project as part of Google Code-in
(2015&ndash;2019).  _Sam Parkinson
([@samdroid-apps](https://github.com/samdroid-apps))_ built the Planet
during GCI.  _Emily Ong ([@EmilyOng](https://github.com/EmilyOng))_
designed our mouse icon and _Euan Ong
([@eohomegrownapps](https://github.com/eohomegrownapps))_ redesigned
the Planet code as a series of GCI tasks.  _Austin George
([@aust-n](https://github.com/aust-n))_ refactored the toolbars as a
series of GCI tasks. _Bottersnike
([@Bottersnike](https://github.com/Bottersnike))_ redesigned the
widgets and the Block API, _Andrea Gonzales
([@AndreaGon](https://github.com/AndreaGon))_ made the widgets
responsive, _Marcus Chong ([@pidddgy](https://github.com/pidddgy))_
refactored the update code, resulting in an order-of-magnitude
improvement in CPU usage, and _Samyok Nepal
([@nepaltechguy2](https://github.com/nepaltechguy2))_ updated the
local storage mechanism to use localForage.

A full list of
[contributors](https://github.com/sugarlabs/musicblocks/graphs/contributors)
is available.

## Music Blocks in Japan

[Gakken STEAM](https://gakken-steam.jp/music_blocks/)

## License

Music Blocks is licensed under the
[AGPL](https://www.gnu.org/licenses/agpl-3.0.en.html), which means it
will always be free to copy, modify, and hopefully improve. We respect
your privacy: while Music Blocks stores your session information in
your browser's local storage, it does not and will never access these
data for purposes other than to restore your session. Music Blocks will
never share these data with any third parties.

There is a Planet where you can share your projects with others and
remix projects created by other Music Blocks users. Use of the Planet
is anonymous and not required in order to enjoy Music Blocks.

Have fun, play hard, and learn.
","'education', 'music-blocks'",2024-05-02T13:08:45Z,30,521,35,"('walterbender', 5844), ('meganindya', 370), ('hemantkasat', 157), ('aviral243', 143), ('sksum', 142), ('riyalohia', 138), ('pikurasa', 120), ('Tabs16', 87), ('ksraj123', 86), ('euanong', 85), ('sparsh0204', 74), ('aust-n', 73), ('AndreaGon', 70), ('daksh4469', 70), ('ricknjacky', 67), ('EmilyOng', 63), ('fakela', 60), ('khandelwalYash', 57), ('pidddgy', 55), ('pipix51', 48), ('Ashnidh', 37), ('Bottersnike', 36), ('a-ritwik', 35), ('Ishakikani9117', 34), ('Kachachan', 33), ('b18050', 30), ('Traitor000', 25), ('tradzik', 24), ('kushal-khare-official', 23), ('dependabotbot', 23)","[4, 'Quality Education']"
conative-labs/Platform-API,,"# Platform-API

The API uses a GraphQL server to parse the requests.

API endpoint: https://platformapi.nilebot.com/v2
",,2020-01-28T11:47:30Z,1,0,2,"('OmarKSH', 3)","[6, 'Clean Water and Sanitation']"
odoo/odoo,Odoo. Open Source Apps To Grow Your Business.,"[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/documentation/17.0)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo
----

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an Open Source CRM,
Website Builder,
eCommerce,
Warehouse Management,
Project Management,
Billing &amp; Accounting,
Point of Sale,
Human Resources,
Marketing,
Manufacturing,
...

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured Open Source ERP when you install several Apps.

Getting started with Odoo
-------------------------

For a standard installation please follow the Setup instructions
from the documentation.

To learn the software, we recommend the Odoo eLearning, or Scale-up, the business game. Developers can start with the developer tutorials
","'apps', 'business', 'erp', 'management', 'odoo', 'odoo-apps', 'python'",2024-05-03T15:32:00Z,30,34660,1502,"('tde-banana-odoo', 7340), ('fpodoo', 5965), ('xmo-odoo', 5208), ('KangOl', 4908), ('odony', 3554), ('mart-e', 3358), ('Gorash', 2985), ('rco-odoo', 2951), ('amigrave', 2725), ('nicolas-van', 2538), ('beledouxdenis', 2251), ('nim-odoo', 2196), ('qsm-odoo', 2078), ('antonylesuisse', 1919), ('tivisse', 1735), ('JKE-be', 1667), ('aab-odoo', 1664), ('hmo-odoo', 1586), ('simongoffin', 1381), ('jco-odoo', 1306), ('tpa-odoo', 1178), ('fvdsn', 1100), ('alexkuhn', 965), ('ged-odoo', 963), ('nle-odoo', 944), ('sle-odoo', 932), ('Feyensv', 904), ('jam-odoo', 886), ('robodoo', 865), ('xrg', 841)","[17, 'Partnerships for the Goals']"
openMF/android-client,An android client for the MifosX platform,"![cover](https://user-images.githubusercontent.com/94394661/278370464-4248f4eb-3601-42da-af40-0cc1531661ff.png)





# Android Client for MifosX

This is an Android Application built on top of the [MifosX](https://mifosforge.jira.com/wiki/spaces/MIFOSX/overview) platform and written in Kotlin. It is based on Mifos X - a robust core banking platform that is developed for field officers using which they process transactions, keep track of their client’s data, center records, group details, different types of accounts (loan, savings and recurring) of the client, run reports of clients, etc. Its sole purpose is to make field operations easier and effortless. This application also provides an offline feature that allows officers to connect with clients and provide them financial support in remote areas as well.

### Features

- Search of Clients, Client Details Viewing.
- Creating new Clients, Groups, and Centers.
- Savings Accounts and Loan Accounts Viewing.
- Savings Account Transactions (Withdrawal & Deposit).
- Loan Accounts (Approval, Disbursal, Repayments etc).
- Identifiers and Documents (Creation, Upload, Download and View).
- Collection Sheet Access (Online), Datatables (View, Add/Remove Entries).
- Offline Sync (for Clients, Centers, and Groups) and Offline Dashboard.
- Checker Inbox.
- Path Tracker.

### Status







## Notice

:warning: We are fully committed to implementing [Jetpack Compose](https://developer.android.com/jetpack/compose) and moving ourselves to support `Kotlin multi-platform`. **If you are sending any PR regarding `XML changes` we will `not` consider at this moment but converting XML to Jetpack Compose is most welcome**. We would be pleased to receive any PR you may have regarding logical changes to an Activity/Fragment.

## Join Us on Slack

Mifos boasts an active and vibrant contributor community, Please join us on [slack](https://join.slack.com/t/mifos/shared_invite/zt-2f4nr6tk3-ZJlHMi1lc0R19FFEHxdvng). Once you've joined the mifos slack community, please join the `#android-client` channel to engage with android-client development.

## Demo credentials
Fineract Instance: gsoc.mifos.community

Username: `mifos`

Password: `password`

## How to Contribute

This is an OpenSource project and we like to see new contributors contibuting to the project. The issues should be raised via the GitHub issue tracker.

1. Fork the Project
2. Create Feature Branch 

    ```sh
    git checkout -b fix_#issue_no
    ```
3. Commit your Changes 

    ```sh
    git commit -m ""feat/design:Add some message""
    ```

4. Push to the Branch 

    ```sh
    git push --set-upstream origin fix_#issue_no
    ```

5. Open a Pull Request

### Guidelines
- [Issue Tracker](https://github.com/openMF/android-client/blob/master/.github/ISSUE_TEMPLATE.md)
- [Commit Style](https://github.com/openMF/android-client/wiki/Commit-Style-Guide)
- [Pull Request](https://github.com/openMF/android-client/blob/master/.github/PULL_REQUEST_TEMPLATE.md)

## Development Setup

To start, ensure that you've successfully downloaded and properly configured the Android Studio SDK. You can refer to a guide detailing the setup process [here](http://developer.android.com/sdk/installing/index.html?pkg=studio).

## Building the Code

1. Clone the repository

    ```sh
    git clone https://github.com/openMF/android-client.git
    ```
2. Open Android Studio.

3. Click on 'Open an existing Android Studio project'

4. Browse to the directory where you cloned the android-client repo and click OK.

5. Let Android Studio import the project.

6. Build the application in your device by clicking run button.

## Travis CI
Travis CI is a hosted continuous integration service used to build and test software projects hosted at GitHub. We use Travis CI for continous integration and clean maintainence of code. All your pull requests must pass the CI build only then, it will be allowed to merge. Sometimes,when the build doesn't pass you can use these commands in your local terminal and check for the errors,

For Mac OS and Linux based, you can use the following commands:

* `./gradlew checkstyle` quality checks on your project’s code using Checkstyle and generates reports from these checks.
* `./gradlew pmd` an check and apply formatting to any plain-text file.
* `./gradlew findbugs`  a program which uses static analysis to look for bugs in Java code.
* `./gradlew build`  provides a command line to execute build script.


For Windows, you can use the following commands:

* `gradlew checkstyle` quality checks on your project’s code using Checkstyle and generates reports from these checks.
* `gradlew pmd` an check and apply formatting to any plain-text file.
* `gradlew findbugs`  a program which uses static analysis to look for bugs in Java code.
* `gradlew build`  provides a command line to execute build script.

## Contributors

Special thanks to the incredible code contributors who continue to drive this project forward.


  


## Wiki

View [Wiki](https://github.com/openMF/android-client/wiki)

## License

This project is licensed under the open source [MPL V2](https://github.com/openMF/android-client/blob/master/LICENSE.md).
","'android-application', 'fineract-sdk', 'hilt', 'integration-testing', 'kotlin', 'mifosx', 'mifosx-platform', 'mobile-banking', 'mvvm', 'navigation-component', 'retrofit-2', 'unit-testing', 'viewbinding'",2024-04-20T20:35:46Z,30,191,19,"('therajanmaurya', 232), ('droidchef', 121), ('Aditya-gupta99', 85), ('iamsh4shank', 81), ('AbhilashG97', 56), ('tarun0', 35), ('Grandolf49', 29), ('AkshGautam', 22), ('danishjamal104', 20), ('mayank-kgp', 20), ('vishwasbabu', 15), ('fomenkoo', 15), ('ahmed-fathy-aly', 13), ('avivijay19', 11), ('moksh-mahajan', 11), ('mdzyuba', 9), ('PatelVatsalB21', 8), ('RajaVamsi11', 8), ('asadovsky', 7), ('vjs3', 6), ('xchen07', 6), ('dikshabhatia', 6), ('laxyapahuja', 6), ('k4wel', 6), ('nellyk', 5), ('chhavip', 4), ('mnafees', 4), ('CloudyPadmal', 4), ('aj019', 4), ('xiprox', 3)","[1, 'No Poverty']"
consuldemocracy/consuldemocracy,Consul Democracy - Open Government and E-Participation Web Software,"<!--
  Title: CONSUL DEMOCRACY
  Description: Citizen Participation and Open Government Application
  Keywords: democracy, citizen participation, eparticipation, debates, proposals, voting, consultations, crowdlaw, participatory budgeting.
-->

![CONSUL DEMOCRACY logo](https://raw.githubusercontent.com/consuldemocracy/consuldemocracy/master/public/consul_logo.png)

# CONSUL DEMOCRACY

Citizen Participation and Open Government Application

[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](http://www.gnu.org/licenses/agpl-3.0)
[![Accessibility conformance](https://img.shields.io/badge/accessibility-WAI:AA-green.svg)](https://www.w3.org/WAI/eval/Overview)

![Build status](https://github.com/consuldemocracy/consuldemocracy/workflows/tests/badge.svg)
[![Code Climate](https://codeclimate.com/github/consuldemocracy/consuldemocracy/badges/gpa.svg)](https://codeclimate.com/github/consuldemocracy/consuldemocracy)
[![Coverage Status](https://coveralls.io/repos/github/consuldemocracy/consuldemocracy/badge.svg)](https://coveralls.io/github/consuldemocracy/consuldemocracy?branch=master)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/consul/localized.svg)](https://translate.consuldemocracy.org/)
[![Knapsack Pro Parallel CI builds for RSpec tests](https://img.shields.io/badge/Knapsack%20Pro-Parallel%20/%20RSpec%20tests-%230074ff)](https://knapsackpro.com/dashboard/organizations/176/projects/202/test_suites/318/builds?utm_campaign=organization-id-176&utm_content=test-suite-id-318&utm_medium=readme&utm_source=knapsack-pro-badge&utm_term=project-id-202)

[![Help wanted](https://img.shields.io/badge/help-wanted-brightgreen.svg?style=flat-square)](https://github.com/consuldemocracy/consuldemocracy/issues?q=is%3Aopen+label%3A""help+wanted"")

This is the opensource code repository of the eParticipation website CONSUL DEMOCRACY, originally developed for the Madrid City government eParticipation website, and currently maintained by the open source software community in collaboration with the CONSUL DEMOCRACY Foundation.

## Documentation

Check the [ongoing documentation](https://docs.consuldemocracy.org/index) to learn more about how to start your own CONSUL DEMOCRACY fork, install it, customize it and learn to use it as an administrator/maintainer.

## CONSUL DEMOCRACY Foundation and project website

You can access the main website of the project at [http://consuldemocracy.org](http://consuldemocracy.org) where you can find information about the use of the platform, the CONSUL DEMOCRACY Foundation, the global community of users and local partners, news, and ways to get more support or get in touch.

## Configuration for development and test environments

**NOTE**: For more detailed instructions check the [docs](https://docs.consuldemocracy.org)

Prerequisites: install git, Ruby 3.2.3, CMake, pkg-config, shared-mime-info, Node.js 18.18.2 and PostgreSQL (>=9.5).

```bash
git clone https://github.com/consuldemocracy/consuldemocracy.git
cd consuldemocracy
bin/setup
bin/rake db:dev_seed
```

Run the app locally:

```bash
bin/rails s
```

Run the tests with:

```bash
bin/rspec
```

You can use the default admin user from the seeds file:

 **user:** admin@consul.dev
 **pass:** 12345678

But for some actions like voting, you will need a verified user, the seeds file also includes one:

 **user:** verified@consul.dev
 **pass:** 12345678

## Configuration for production environments

See [installer](https://github.com/consuldemocracy/installer)

## Current state

Development started on [2015 July 15th](https://github.com/consuldemocracy/consuldemocracy/commit/8db36308379accd44b5de4f680a54c41a0cc6fc6). Code was deployed to production on 2015 september 7th to [decide.madrid.es](https://decide.madrid.es). Since then new features are added often. You can take a look at the current features at the [project's website](http://consuldemocracy.org/) and future features at the [Roadmap](https://github.com/orgs/consuldemocracy/projects/1) and [open issues list](https://github.com/consuldemocracy/consuldemocracy/issues).

## License

Code published under AFFERO GPL v3 (see [LICENSE-AGPLv3.txt](LICENSE-AGPLv3.txt))

## Contributions

See [CONTRIBUTING.md](CONTRIBUTING.md)
","'agplv3', 'consuldemocracy', 'democracy', 'e-government', 'hacktoberfest', 'open-government', 'rails', 'ruby'",2024-05-01T22:32:51Z,30,1404,76,"('javierm', 4329), ('decabeza', 2325), ('voodoorai2000', 2226), ('xuanxu', 1591), ('bertocq', 1330), ('kikito', 1184), ('Senen', 599), ('taitus', 564), ('microweb10', 346), ('MariaCheca', 329), ('dependabotbot', 226), ('aitbw', 222), ('amaia', 181), ('amiedes', 174), ('jsperezg', 160), ('dependabot-previewbot', 109), ('dgilperez', 75), ('agileontheweb', 74), ('mlovic', 68), ('martgnz', 51), ('depfubot', 48), ('raul-fuentes', 42), ('entantoencuanto', 30), ('dependabot-support', 27), ('Ana06', 25), ('LumaRay', 23), ('abelardogilm', 21), ('NahiaSolutions', 20), ('matisnape', 20), ('juandefelix', 19)","[16, 'Peace, Justice and Strong Institutions']"
Ideasis/VivoosVR,Virtual treatment tool to assist in exposure therapy,"# VivoosVR

## License

[**BSD 3-Clause**](https://opensource.org/licenses/BSD-3-Clause)

See [LICENSE](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/LICENSE)

## About

VivoosVR is a virtual treatment tool to assist in exposure therapy where participants are placed in a computer-generated 3D virtual world and guided through the selected environments, situations and conditions. 

Physiological values of the participants are constantly monitored via GSR and pulse sensors. The system is designed to be used by qualified therapists and our target participants are young people suffering from phobias and social anxiety. 

Simulation worlds have been created using Unity game engine and we support popular VR HMDs such as Oculus Rift and HTC Vive. 
We also have plans to optimize the system for mobile VR solutions in the near future. 

VivoosVR is being developed in cooperation with a team of psychology / psychiatry experts.  

## Prerequisites

* [SQL Server 2014 (or above) Express](https://download.microsoft.com/download/5/E/9/5E9B18CC-8FD5-467E-B5BF-BADE39C51F73/SQLServer2017-SSEI-Expr.exe)
    * If you have problems while installing, this [link](http://help.dugeo.com/m/Insight4-0/l/438911-downloading-and-installing-sql-server) can guide you through installation

* [SQL Server Management Studio 2017 (or above)](https://go.microsoft.com/fwlink/?linkid=2043154)

* [Neulog API](https://neulog.com/Downloads/neulog_api_ver_002b.exe)

* Acquire and install either or both of the following products:
    * [HTC Vive](https://support.steampowered.com/steamvr/HTC_Vive/)
    * [Oculus Rift](https://www.oculus.com/download_app/?id=1582076955407037)

* [Visual Studio](https://visualstudio.microsoft.com/tr/downloads/?rr=https%3A%2F%2Fwww.google.com%2F)     

## Git initialisation 

### GitHub Desktop

1. Download and install [GitHub Desktop](https://central.github.com/deployments/desktop/desktop/latest/win32)

2. Run GitHub Desktop Application and click on File -> Clone Repository:
    ![Screenshot of Clone Repository](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/GitHubDesktop%20_Installation_SS/SS1_CloneRepository.png)

3. Click on ""URL"" tab and copy the following link of the VivoosVR Repo and paste it on the URL part:
    ![Screenshot of PasteRepoLink](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/GitHubDesktop%20_Installation_SS/SS2_PasteRepoLink.png)

4. Set local path of the repo

5. Click on ""Clone"" to complete connecting to VivoosVR Repo using GitHub Desktop.

## Installation

### SQL Server Management Studio

1. Locate ""Database"" folder in your local repo path and copy ""Vivoos.bak"" file in this folder to ""C:\Program Files\Microsoft SQL Server\MSSQL14.SQLEXPRESS\MSSQL\Backup""

2. Open the SQL Server Management Studio and login. In the Object Explorer tab, right click on ""Databases"" and select ""Restore Database"".

    ![2](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/2.JPG)
    
3. Select Device part, click add and find ""Vivoos.bak"" file
    
    ![3](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/3.JPG)
    
    ![4](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/4.JPG)

    ![5](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/5.JPG)

4. 	Close the SQL Server Management Studio Application.

### Scenario Installation 

If you wish to use the scenarios below in VivoosVR, you need to execute following steps:

1. Click on the link which belongs to the scenario you would like to add.
2. Download the zip in the link
3. Extract the zip into the VivoosVR Scenario folder (Default Path: C:\\Scenarios) (If the folder does not exist, you need to create the folder manually)

Links to the latest version of the scenarios can be found below:

1. [Fear of Dogs - Park Scenario - version 0.9.2](https://drive.google.com/file/d/1l6EVR3hdOhX_jTwCIMZ_PaZLa7a19EgP/view?usp=sharing) 
2. [Fear of Height - Balcony Scenario - version 0.9.6](https://drive.google.com/file/d/16pH0Jrwi6cRGaQGeg-6p9VP1RW7mS4xL/view?usp=sharing)
3. [Fear of Flight - Plane Scenario - version 0.7.1](https://drive.google.com/file/d/1Dm_ebqxB4qzLzNcvMDs-AXPujnVPB9_W/view?usp=sharing)
4. [Social Anxiety - Presantation - version 0.9.6](https://drive.google.com/file/d/13W7iOjsk4p02b_BuoIIaXhvofY9f-ybv/view?usp=sharing)
5. [Fear of Height - Elevator Scenario - version 0.7.5](https://drive.google.com/file/d/1yOCcqogmIBrqa-3yo7Mx9CQNuB7BKmbt/view?usp=sharing)
6. [Fear of Height - Elevator Scenario Transparent - version 0.7.5](https://drive.google.com/file/d/1koNHEGikhHk1Jctas8fRnx2f8ON21-Tz/view?usp=sharing)
7. [Fear of Public Speaking - version 0.1.0](https://drive.google.com/file/d/1B5t33d_pU4rXb2KUe-73DqMDUXLTsVU7/view?usp=sharing)
8. [Fear of Spiders - version 0.9.2](https://drive.google.com/file/d/1jgT7JQxAb4og9eqbO10R5AaC2Pl0NW3Q/view?usp=sharing)
9. [Fear of Neeedle - version 0.2.0](https://drive.google.com/file/d/1eWLBHOQWErkD80sMayAf9kyPMT14Zkm7/view?usp=sharing)
10. [Fear of Cats - version 0.2.2](https://drive.google.com/file/d/1ELPEW2Fa_uegQUR_CPTUuPEzrPLCBpPl/view?usp=sharing)
11. [Metro Scene - Standing setup - version 0.2.1](https://drive.google.com/file/d/10KNko3fTzOBjhFcLCCLpmQyghDyXPouH/view?usp=sharing)
12. [Metro Scene - Sitting setup - version 0.2.1](https://drive.google.com/file/d/11Mv7LLoPwJSNzauzJ214CZ_mwdANAfi0/view?usp=sharing)
13. [Attention Deficit Hyperactivity Disorder (ADHD) - Classrom - version 0.1.9](https://drive.google.com/file/d/19fIkHjaAKDH8aqTNJpuCYjsIxNRuYH1M/view?usp=sharing)
13. [Germofobia - version 0.2.0](https://drive.google.com/file/d/1IYI1P-9gD02ioW8yO7naJArgxR5X-ykE/view?usp=sharing)

WebVR related links can be found below:

1. Fear of Height - Balcony Scenario - WebVR:
	1. Build: [Fear of Height - Balcony Scenario - WebVR version 0.8.0](https://www.dropbox.com/sh/lkpk96ujhm44h6e/AABFP2dSAdalOsYrVswlkaTfa?dl=0) 
	1. Try: [Fear of Height - Balcony Scenario](http://www.ideasis.com.tr/Content/Balkon/index.html)
	
Link to the 360VR videos of some scenarios can be found below:
1. [Fear of Height - Elevator Scenario - 360 video](https://drive.google.com/file/d/1tAup-756FMQzsCHRPr5PvEo81OJgsqKO/view?usp=sharing)
2. [Fear of Height - Balcony Scenario - 360 video](https://drive.google.com/file/d/1fIuxQpQ6ZYPeaNgpX41Y7VNq-Z7V6zdx/view?usp=sharing)
3. [Germofobia - Toilet Scenario - 360 video](https://drive.google.com/file/d/1Ps3aoVkz8hdqy_ryp2TC9WOboUczJRFg/view?usp=sharing)

## How to Use 

### VivoosVR

1.  If you click the exe of the program (VivoosVR\VivoosVR\bin\debug), login form welcomes you. 
    Default user: Username: user Password:123 
    Default admin: Username: admin Password:123
    You can add new users in admin page.
    
![6](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/6.JPG)

2.	For using the scenarios, you can download them in the github. Default path is C:/Scenarios. You can create a folder named Scenarios and copy the scenarios in this folder. If you want to copy another path, please login with the admin password and change the scenarios path. (Login -> Find the scenario and click edit -> Change the path)  

![7](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/7.JPG)

![8](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/8.JPG)

3.	If you login with the default user password, the first screen you will see is the patient page. You can see a list of your patients in this page. You can also add new patients or you can edit existing patient information. 

![9](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/9.JPG)

4.	You can click the “Session List” of one of your patients. In this page, you can see the sessions of the selected patient. You can also download the data (GSR and Pulse data due to the time) of the session or delete the session. 

![10](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/10.JPG)

5.	By clicking the “New Session” button in the “Sessions Page”, you can choose and begin any of the existing exposure scenarios. 

![11](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/11.JPG)

6.	In the “New Session Controls” page, you can trigger and control scenario events. Moreover, you can track the anxiety level of the patient through GSR and pulse graphs. 

![13](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/13.JPG)

![12](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Guideline_SS/12.JPG)

### Neulog

Currently VivoosVR communicates with the following modules and sensors of the Neulog: 

* [Heart Rate & Pulse Logger Sensor](https://neulog.com/heart-rate-pulse)

* [GSR Logger Sensor](https://neulog.com/gsr)

You need to perform following tasks to make Neulog modules and sensors work correctly with VivoosVR:

1.	Plug USB module, Heart Rate & Pulse Logger Sensor and GSR Logger Sensor to each other and plug them to your computer via USB Port (Order of the modules are not important!)

![Neulog_PlugModules](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Neulog_SS/Neulog_PlugModules_SS.jpeg)

2.	Attach handle of the heart rate and pulse sensor to little finger of your right hand in a way that glassy side of the handle touches the bottom side of your finger

![Neulog_HeartRate](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Neulog_SS/Neulog_HeartRate_SS.jpg)

3.	Attach the handles of the gsr module to ring finger and index finger of your right hand in a way that metal part of the handles touches the bottom side of your finger

![Neulog_GSR](https://github.com/Oguzhankoksal/VivoosVR_Private/blob/master/Readme%20Images/Neulog_SS/Neulog_GSR_SS.jpg)

","'phobia', 'psychology', 'virtual-reality'",2022-12-25T14:00:49Z,7,22,2,"('Mertgozcu', 31), ('IlkerDurubal', 21), ('Mert-Ideasis', 14), ('barispunar', 13), ('Oguzhankoksal', 13), ('barisisikpunar', 3), ('codingapevs', 2)","[3, 'Good Health and Well-Being']"
tidepool-org/uploader,An Electron app for uploading diabetes device data to Tidepool's backend,"# Tidepool Uploader


[![CircleCI](https://circleci.com/gh/tidepool-org/uploader/tree/master.svg?style=shield)](https://circleci.com/gh/tidepool-org/uploader/tree/master)


This is an [Electron App](https://electron.atom.io/) that acts as an uploader client for Tidepool. It is intended to allow you to plug diabetes devices into your computer's USB port, read the data stored on them, and upload a standardized version of the data to the Tidepool cloud.

This README is focused on just the details of getting the uploader running locally. For more detailed information aimed at those working on the development of the uploader, please see the [developer guide](docs/StartHere.md).

#### Table of contents

- [Set Up](#how-to-set-it-up)
- [Config](#config)
- [Tests](#tests)
- [Linting & Code Style](#linting--code-style)
- [Docs](#docs)
- [Publishing](#publishing)
- [Use of LGPL libraries](#use-of-lgpl-libraries)

* * * * *

## How to set it up

1. Clone this repository.
1. Make sure you have node v16.x installed. If you are managing node installations with [`nvm`](https://github.com/creationix/nvm 'GitHub: nvm'), which we **highly recommend**, you can just do `nvm use` when navigating to this repository to switch to the correct version of node. (In this repository, the correct version of node will always be the version of node packaged by the version of Electron that we are using and specified in the `.nvmrc` file.)
1. Run `npm install` or, preferably, `yarn`
1. Set the config for the environment you want to target (see [Config](#config) below)
1. Run the following command:
```bash
$ npm run dev
```
or
```bash
$ yarn dev
```
(This will bundle the application with webpack and watch for changes. When it stops printing output you can continue to the next step.)

**NB:** React components and CSS will hot load after changes (this can be confirmed by watching the JavaScript console), but changes to device drivers and other code outside of the React components will not - use 'Reload' to reload after such changes. If the compilation/hot reload of a component fails for any reason (e.g. from a syntax error) you may need to reinitialize the hot loader by reloading the application.

### Docker for Linux

If you are running Linux you probably need to be using an Ubuntu distribution or derivative. To get around this for other distrubutions you can try to build a local docker image which is based on Ubuntu 18.04 and use the yarn/npm commands interactively.

**NOTE:** You need to add udev rules to your host for uploads to actually work. You can find the udev rules [here](resources/linux/51-tidepool-uploader.rules). The file should be placed in `/etc/udev/rules.d/` and the host should be rebooted.


1. Build the image
    `docker-compose build`

2. Run it
    `docker-compose up -d`

3. Work with it interactively.

    Even if you kill the Tidepool Uploader GUI the container will continue to run. You can work with the yarn commands like you would locally by using docker exec.

    **Examples**

    Interactively select the yarn target: `docker exec -it uploader bash -c ""yarn run""`

    Rebuild: `docker exec -it uploader bash -c ""yarn build""`

    Start the Dev GUI: `docker exec -it uploader bash -c ""yarn dev""`

## Config

Configuration values (for example the URL of the Tidepool Platform) are set via environment variables. If you need to add a config value, modify the `.config.js` file. If you need to read a config value inside the app, use `var config = require('./lib/config')`. To set config values (do this before building the app), you can use Shell scripts that export environment variables (see config/local.sh for an example that exports the appropriate variables when [running the whole Tidepool platform locally using runservers](http://developer.tidepool.org/starting-up-services/)), for example:

```bash
$ source config/local.sh
$ yarn start
```

### Debug Mode(s)

For ease of development we have several debug features that developers can turn on and off at will (and to suit various development use cases, such as working on a new device driver versus working on the app's UI). Each of these debug features is set with an environment variable, but rather than being loaded through `.config.js` (as we do for production configuration variables, see above), we load these through the webpack `DefinePlugin` (see [Pete Hunt's webpack-howto](https://github.com/petehunt/webpack-howto#6-feature-flags) for an example, although note Hunt uses the term 'feature flag').

#### `DEBUG_ERROR`

The environment variable `DEBUG_ERROR` (boolean) controls whether or not errors sourced in device drivers are caught and an error message displayed in the UI (the production setting) or whether they are thrown in the console (much more useful for local development because then the file name and line number of the error are easily accessible, along with a stack trace). `DEBUG_ERROR` mode is turned on by default in `config/device-debug.sh`.

This can also be toggled internally in the running Electron app via a right-click context menu available on the login screen, much like the menu for switching environments.

### Local Development w/o Debug Mode(s)

All debug options are turned *off* by default in `config/local.sh`.

## Tests

To run the tests in this repository as they are run on CircleCI use:

```bash
$ yarn test
```
or
```bash
$ yarn test
```

## Linting & Code Style

We use [ESLint](http://eslint.org/) to lint our JavaScript code. We try to use the same linting options across all our client apps, but there are a few exceptions in this application, noted with comments in the `.eslintrc` configuration file.

To run the linter (which also runs on CircleCI with every push, along with `npm test`), use:

```
$ npm run lint
```

Aside from the (fairly minimal) JavaScript code style options we *enforce* through the linter, we ask that internal developers and external contributors try to match the style of the code in each module being modified. New modules should look to similar modules for style guidance. In React component code, use existing ES6/ES2015 components (not legacy ES5 components) as the style model for new components.

**NB: Please keep ES5 and ES6/ES2015 code distinct. Do *NOT* use ES6/ES2105 features in ES5 modules (most easily recognizable by the use of `require` rather than `import`).**

## Docs

Docs reside in several places in this repository, such as `docs/` and `lib/drivers/docs`. They are built as a static site with [GitBook](https://www.gitbook.com/ 'GitBook') and served at [developer.tidepool.org](http://developer.tidepool.org/) via [GitHub Pages](https://pages.github.com/ 'GitHub Pages').

See [this guidance on our use of GitBook at Tidepool](http://developer.tidepool.org/docs/).

## Publishing

This section is Tidepool-specific. Release management and application updates are handled via the Github provider in the `electron-builder` project. The recommended workflow for a new production release is as follows:

1. When you're working on what might become a new release, increment the version number in `package.json` and `app/package.json` and commit/push (on the branch)
1. The CI server will create a draft release in Github with the title of the version from the `package.json` file and will automatically attach the distribution artifacts to that draft (drafts are not publicly visible)
1. When your pull request is approved and merged to `master`, go to the draft release and type in the version for the tag name, ensure that you're targeting the `master` branch, fill out the release notes and publish the release. This will create the tag for you.

For a non-production release (alpha, dev, etc.)

1. Increment the version number in `package.json` and `app/package.json` and ensure that you have included the channel information after the version patch number (i.e. `v0.304.0-alpha` or `v0.304.0-beta.2`). The hyphen separated version semantic is important.
1. The CI server(s) will create a draft release in Github with the title of the version from the `package.json` file and will automatically attach the distribution artifacts to that draft (drafts are not publicly visible)
1. When you want to publish your non-production release, go to your draft and type in the version for the tag name, ensure that you're targeting the branch that you're currently releasing from, mark the release as a `pre-release`, fill out the release notes and publish the release. This will create the tag for you on the branch that you want.

The Uploader has a self-update mechanism that will look at the latest release and compare versions, downloading and prompting the user to update if a newer version is available. For production releases, only official releases will be considered. For non-production releases (`-alpha`, `-beta.2`, etc.) releases marked as `pre-release` will also be checked, matching against the string portion of the post-hyphen version segment. For more detail about this behavior see [the electron-builder docs concerning auto-update options]( https://github.com/electron-userland/electron-builder/wiki/Auto-Update#appupdater--internaleventemitter)

### CI server environment variables

We use the following environment variables on the CI server:

| Variable | OS Image | Use |
|----------|-----------|-------|
| APPLEID                  | MacOS    | Notarization |
| APPLEIDPASS              | MacOS    | Notarization |
| TEAMID                   | MacOS    | Notarization |
| AWS_ACCESS_KEY_ID        | Both     | S3 builds and AV e-mails |
| AWS_SECRET_ACESS_KEY     | Both     | S3 builds and AV e-mails |
| CSC_FOR_PULL_REQUEST     | Both     | `true`, code signing for PR |
| CSC_KEY_PASSWORD         | MacOS    | Certificate password |
| CSC_LINK                 | MacOS    | Code signing certificate |
| WIN_CSC_KEY_PASSWORD     | Windows  | Certificate password |
| WIN_CSC_LINK             | Windows  | Code signing certificate |
| DEBUG                    | MacOS    | Set to `electron-builder` |
| GH_TOKEN                 | Both     | For GitHub builds |
| PUBLISH_FOR_PULL_REQUEST | Both     | `true`, build artefact for PR |
| ROLLBAR_POST_TOKEN       | Both     | Rollbar logging |
| FTP_AV_PASSWORD_TIDEPOOL | Windows  | AV submission |

## Editor Configuration
**Atom**
```bash
apm install editorconfig es6-javascript javascript-snippets linter linter-eslint language-babel autocomplete-modules file-icons
```

**Sublime**
* [Editorconfig Integration](https://github.com/sindresorhus/editorconfig-sublime#readme)
* [Linting](https://github.com/SublimeLinter/SublimeLinter3)
* [ESLint Integration](https://github.com/roadhump/SublimeLinter-eslint)
* [Syntax Highlighting](https://github.com/babel/babel-sublime)
* [Autocompletion](https://github.com/ternjs/tern_for_sublime)
* [Node Snippets](https://packagecontrol.io/packages/JavaScript%20%26%20NodeJS%20Snippets)
* [ES6 Snippets](https://packagecontrol.io/packages/ES6-Toolkit)

**Others**
* [Editorconfig](http://editorconfig.org/#download)
* [ESLint](http://eslint.org/docs/user-guide/integrations#editors)
* Babel Syntax Plugin

## DevTools

#### Toggle Chrome DevTools

- OS X: Cmd Alt I or F12
- Linux: Ctrl Shift I or F12
- Windows: Ctrl Shift I or F12

*See [electron-debug](https://github.com/sindresorhus/electron-debug) for more information.*

#### DevTools extension

This project includes the following DevTools extensions:

* [Devtron](https://github.com/electron/devtron) - Install via [electron-debug](https://github.com/sindresorhus/electron-debug).
* [React Developer Tools](https://github.com/facebook/react-devtools) - Install via [electron-devtools-installer](https://github.com/GPMDP/electron-devtools-installer).
* [Redux DevTools](https://github.com/zalmoxisus/redux-devtools-extension) - Install via [electron-devtools-installer](https://github.com/GPMDP/electron-devtools-installer).

You can find the tabs on Chrome DevTools.

If you want to update extensions version, please set `UPGRADE_EXTENSIONS` env, just run:

```bash
$ UPGRADE_EXTENSIONS=1 npm run dev

# For Windows
$ set UPGRADE_EXTENSIONS=1 && npm run dev
```

## CSS Modules

All `.module.less` files will be use css-modules.

## Packaging

To package apps for the local platform:

```bash
$ npm run package
```
```bash
$ yarn package
```

To package apps with options:

```bash
$ npm run package -- --[option]
```

To package the app on your local machine, you need to set the `ROLLBAR_POST_TOKEN` environment variable to send telemetry data to Rollbar. You can get one for free from https://rollbar.com

macOS: To notarize the app so that it will run on macOS Mojave, you need to set the environment variables `APPLEID`, `APPLEIDPASS`, and `TEAMID`. Note that you need to set an app-specific password in https://appleid.apple.com for this to work.

Note that you'll need to build Windows builds on a Windows machine, and MacOS builds on a Mac.

## Further commands

To run the application without packaging run

```bash
$ npm run build
$ npm start
```

To run End-to-End Test

```bash
$ npm run build
$ npm run test-e2e
```

#### Options

See [electron-builder CLI Usage](https://github.com/electron-userland/electron-builder#cli-usage)

#### Module Structure

This project uses a [two package.json structure](https://github.com/electron-userland/electron-builder/wiki/Two-package.json-Structure).

1. If the module is native to a platform or otherwise should be included with the published package (i.e. bcrypt, openbci), it should be listed under `dependencies` in `./app/package.json`.
2. If a module is `import`ed by another module, include it in `dependencies` in `./package.json`.   See [this ESLint rule](https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/no-extraneous-dependencies.md).
3. Otherwise, modules used for building, testing and debugging should be included in `devDependencies` in `./package.json`.

## Use of LGPL libraries

Tidepool Uploader makes use of the following LGPL-licensed libraries:

- libmtp (http://libmtp.sourceforge.net/)
- LZO implementation in libavutil, which is part of FFmpeg (https://github.com/FFmpeg/FFmpeg/tree/master/libavutil)

These libraries are used in the following Node.js modules created by Tidepool and are dependencies of the Tidepool Uploader:

- https://github.com/tidepool-org/node-mtp (libmtp)
- https://github.com/tidepool-org/lzo-decompress (libavutil)

The LGPL is intended to allow use of libraries in applications that don’t necessarily distribute the source of the application. The LGPL has two requirements:

- users must have access to the source code of the library
- users can make use of modified versions of the library

To satisfy (1) we provide links to the relevant code repositories. To satisfy (2) we dynamically link to the library, so that it’s possible to swap it out for another version of the library.

### Impact on Tidepool

Compile FFmpeg ourselves to ensure that we’re using the LGPL version and only include the minimal set of libraries
Use dynamic linking (e.g. on Windows this means using a .dll, and on MacOS a .dylib) when linking to these libraries
Mention that the software uses libraries from the FFmpeg project and libmtp under the LGPLv3, e.g. `This software uses code of FFmpeg and libmtp licensed under the LGPLv3 and its source can be downloaded here and here`


### Impact on 3rd parties

If your EULA claims ownership over the code, you have to explicitly mention that you do not own FFmpeg or libmtp, and where the relevant owners can be found.


### References

- [LGPL v3 License Text](https://www.gnu.org/licenses/lgpl.html) (on gnu.org)
- [LGPL on Wikipedia](https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License)
",,2024-05-02T20:19:02Z,30,114,42,"('gniezen', 6055), ('krystophv', 1045), ('jebeck', 913), ('greenkeeperbot', 543), ('jh-bate', 280), ('kentquirk', 248), ('greenkeeperio-bot', 152), ('DorianScholz', 133), ('snyk-bot', 117), ('nicolashery', 98), ('pazaan', 66), ('ianjorgensen', 38), ('mgranberry', 36), ('pauloved', 36), ('cheddar', 25), ('hntrdglss', 23), ('darinkrauss', 20), ('jehernandezrodriguez', 17), ('JhymerMartinez', 17), ('TheDukeDK', 15), ('dependabotbot', 14), ('GordyD', 13), ('ursooperduper', 11), ('fabricioflores', 10), ('Sjlmejia', 9), ('jorgeeduardohernandez', 8), ('kpouget', 8), ('ginnyyadav', 5), ('mrinnetmaki', 5), ('gralmurdok', 3)","[3, 'Good Health and Well-Being']"
developmentseed/label-maker,Data Preparation for Satellite Machine Learning,"# Label Maker
## Data Preparation for Satellite Machine Learning

Label Maker downloads [OpenStreetMap QA Tile]((https://osmlab.github.io/osm-qa-tiles/)) information and satellite imagery tiles and saves them as an [`.npz` file](https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html) for use in machine learning training.

![example classification image overlaid over satellite imagery](examples/images/classification.png)
_satellite imagery from [Mapbox](https://www.mapbox.com/) and [Digital Globe](https://www.digitalglobe.com/)_

## Requirements
- [Python 3.6](https://www.python.org/)
- [tippecanoe](https://github.com/mapbox/tippecanoe)

## Installation

```bash
pip install label-maker
```

Note that running this library this requires `tippecanoe` as a ""peer-dependency"" and that command should be available from your command-line before running this.

## Documentation

Full documentation is available here: http://devseed.com/label-maker/

## Acknowledgements

This library builds on the concepts of [skynet-data](https://github.com/developmentseed/skynet-data). It wouldn't be possible without the excellent data from OpenStreetMap and Mapbox under the following licenses:
- OSM QA tile data [copyright OpenStreetMap contributors](http://www.openstreetmap.org/copyright) and licensed under [ODbL](http://opendatacommons.org/licenses/odbl/)
- Mapbox Satellite data can be [traced for noncommercial purposes](https://www.mapbox.com/tos/#[YmtMIywt]).

It also relies heavily on Marc Farra's [tilepie](https://github.com/kamicut/tilepie) to asynchronously process vector tiles
","'computer-vision', 'data-preparation', 'deep-learning', 'keras', 'remote-sensing', 'satellite-imagery'",2023-10-03T21:56:39Z,12,454,53,"('drewbo', 193), ('martham93', 60), ('wronk', 56), ('Geoyi', 54), ('jreiberkyle', 12), ('giswqs', 5), ('palacima', 2), ('vincentsarago', 2), ('achyutjoshi', 1), ('PallawiSinghal', 1), ('McCulloughRT', 1), ('betatim', 1)","[17, 'Partnerships for the Goals']"
otsimo-archive/aac,Otsimo AAC is an application that ables kids with autism who can not speak to compose phrases by tapping on the words via using picture exchange communication system.,"# Otsimo AAC PECS App

An app that helps kids with autism to compose phrases.

Otsimo AAC is an application that ables kids with autism who can not speak to compose phrases by tapping on the words. Otsimo AAC uses picture exchange communication system.

This project was tested with the latest version of nodeJS and npm, please make sure you have atleast node.js 5+ and NPM 3+ installed.

## Usage & Develop

- Clone or fork this repository
- run `npm install` to install dependencies
- run `npm run start` to fire up dev server
- open browser to [`http://localhost:3001`](http://localhost:3001)

## Build

to create a ready production distribution package of the project please run:

```
npm run build
```

after running build the generated files will be available at `/dist`

## Testing

This seed is has protractor and karma for end to end testing and unit testing respectively.

### Unit Testing

make sure your tests are named with a `-test.js` suffix then. to run karma simply run:

```
npm test
```

### End to end Testing

to start protractor tests please run:

```
npm run protractor
```

### Update Symbols

to update the symbols:

```
npm run update-symbols
```
","'aac', 'autism', 'symbols'",2021-09-01T07:37:19Z,3,18,4,"('btk', 386), ('sercand', 43), ('utkayd', 6)","[3, 'Good Health and Well-Being']"
cboard-org/cboard,Augmentative and Alternative Communication (AAC) system with text-to-speech for the browser,"[![Digital public good](https://github.com/cboard-org/cboard/blob/master/public/images/dpg-badge.png)](https://digitalpublicgoods.net/registry/)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/cboard/localized.svg)](https://crowdin.com/project/cboard)
[![Backers on Open Collective](https://opencollective.com/cboard/backers/badge.svg)](#backers)
[![Sponsors on Open Collective](https://opencollective.com/cboard/sponsors/badge.svg)](#sponsors)
[![cboard-org](https://circleci.com/gh/cboard-org/cboard.svg?style=shield)](https://app.circleci.com/pipelines/github/cboard-org/cboard)
# Cboard - AAC Communication Board for browsers

[Cboard](https://app.cboard.io) is an augmentative and alternative communication (AAC) web application, allowing users with speech and language impairments (autism, cerebral palsy) to communicate with symbols and text-to-speech.

![Cboard GIF demo](public/videos/demo.gif)

The app uses the browser's Speech Synthesis API to generate speech when a symbol is clicked. There are thousands of symbols from the most popular AAC symbol libraries to choose from when creating a board. Cboard is available in 40 languages (support varies by platform - Android, iOS, Windows).

**We're using Discord to collaborate, join us at: https://discord.gg/TEH8uxh**

## How does it work?

This video shows Srna. She is one of the children who have received the Cboard Communicator thanks to UNICEF’s [""For every child, a voice""](https://www.unicef.org/innovation/stories/giving-every-child-voice-aac-technology) project.



## Translations

The app supports 40 languages.
Languages were machine translated and require proofreading: if you want to help proofread, please use our translation management platform: https://crowdin.com/project/cboard

**You do not need to be a programmer!**

Translations play a major role in this project and they contribute a lot for the inclusion of children, specially in non developed countries. Please consider collaborating with us!

### Translations for developers

In order to pull the latest translations from CrowdIn into the codebase, you can run `yarn translations:pull`. This will update all language files such as `en.json` as well as the central `cboard.json` file. Please note that this requires the CrowdIn API key to be available in the `.private` config file. Refer to [Secrets Management](#secrets-management). After the script completes, changes to the translation files will need to be committed to the repo by the usual process.

## Getting Started

### `yarn start`

Runs the app in development mode.
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.
You will see the build errors and lint warnings in the console.

### `yarn test`

Runs the test watcher in an interactive mode.
By default, runs tests related to files changed since the last commit.

[Read more about testing.](https://github.com/facebookincubator/create-react-app/blob/master/packages/react-scripts/template/README.md#running-tests)

### `yarn build`

Builds the app for production to the `build` folder.
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.
By default, it also [includes a service worker](https://github.com/facebookincubator/create-react-app/blob/master/packages/react-scripts/template/README.md#making-a-progressive-web-app) so that Cboard loads from local cache on future visits.

Cboard is ready to be deployed.

### `yarn build-cordova-debug`

Use this to produce non-minified build for use in debugging within Cordova. It uses `react-app-rewired` & `config-overrides.js` to  customize webpack operation without ejecting react.

See [CBoard](https://github.com/nous-/cboard) repo for packaging this CBoard application within Cordova.

## Docker getting started

### `make image`

Creates a Docker image with cboard built for production. The image is tagged as cboard:latest.

### `make run`

Runs the cboard:latest Docker image on port 5000.

## Secrets Management

Some external services have APIs we need to access, and these require API keys. To prevent open disclosure of these keys in the public repository, while still tracking them with the code, we encrypt some secrets into a GPG file. These files are `env/local-private.gpg` and `env/prod-private.gpg`.

In order to access the secrets, you must request the `ENCRYPTION_KEY` from @shaycojo and then run the decrypt script: `ENCRYPTION_KEY={key-goes-here} yarn decrypt:local` (or `prod`), which will create the file `.private/local.js` with the secrets in plain text where the scripts can access them. **The files in `.private` should never be committed to the repository.**

If you need to add or change a secret, make the change to the `.private/local.js` file, and then run the encryption script: `ENCRYPTION_KEY={key-goes-here} yarn encrypt:local` (or `prod`).

_Note: These keys/secrets are *not* required to run or develop Cboard._ They are used with scripts by some team members.

## About the Cboard community

[![Cauldron dashboard and metrics for the Cboard project community](https://cauldron.io/project/1683/stats.svg)](https://cauldron.io/project/1683 ""Cauldron dashboard and metrics for the Cboard project community"")

## Thanks

### Symbols sources

 [Mulberry](https://mulberrysymbols.org/)

 [ARASAAC](http://www.arasaac.org/)

 [Global Symbols](https://globalsymbols.com/)

### Translation

[  Crowdin](https://crowdin.com/) - for providing the localization management platform.

### Testing platform

[  Browserstack](https://www.browserstack.com/) - for providing the automation infrastructure for testing.

### Development

[  CSS Tricks](https://css-tricks.com) - for providing feedback and support from the early stage.

## Contributors

This project exists thanks to all the people who contribute. [[Contribute](CONTRIBUTING.md)].


## Backers

Thank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/cboard#backer)]



## Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/cboard#sponsor)]













## :memo: Legal & licenses

Copyright © 2017-2024, Assistive Technology LLC & Cboard contributors.

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License version 3 as published by the Free Software Foundation.

* Code - [GPLv3](https://github.com/cboard-org/cboard/blob/master/LICENSE.txt)
* Mulberry Symbols - [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
* ARASAAC Symbols - [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)
","'aac', 'accessibility', 'assistive-technology', 'autism', 'cerebral-palsy', 'communication', 'communication-board', 'disabilities', 'javascript', 'progressive-web-app', 'react', 'speech', 'symbols', 'text-to-speech', 'tts'",2024-04-22T03:53:36Z,30,640,23,"('martinbedouret', 1824), ('shayc', 597), ('tomivm', 535), ('RodriSanchez1', 528), ('snyk-bot', 144), ('tinchodipalma', 98), ('dependabotbot', 30), ('katerinadimatou', 25), ('sylvansson', 22), ('hanzhaogang', 11), ('aushaag', 10), ('jquintozamora', 8), ('BrendanFDMoore', 7), ('arthurdenner', 7), ('karenhaag', 6), ('mariaspastef', 6), ('dhruverma', 4), ('kidcompassion', 4), ('falinsky', 4), ('amberleyromo', 3), ('heyphilllie', 3), ('muelletr', 3), ('rakeshkumar1019', 3), ('nathanbaleeta', 2), ('vpicone', 2), ('SteveALee', 2), ('NataliaGon', 2), ('jwflory', 2), ('ioRekz', 2), ('gavinhenderson', 2)","[4, 'Quality Education']"
instedd/planwise,"PlanWise applies algorithms and geospatial optimisation techniques to existing data on population, road networks and health facilities, so health care planners can better understand the unmet health needs of their constituents and better locate future health facilities.","# PlanWise

PlanWise applies algorithms and geospatial optimisation techniques to existing
data on population, road networks and health facilities, so health care planners
can better understand the unmet health needs of their constituents and better
locate future health facilities. Together, InSTEDD and Concern Worldwide are
developing PlanWise, to apply such algorithms to help governments visualize how
potential future facilities will improve access to healthcare in underserved
areas.

## Technology

PlanWise is a single page web application implemented in Clojure and
Clojurescript, backed with PostgreSQL database storing both relational and
spatial information, and GeoTiff raster files for demographics data.

### Tech Stack

Server side:

* [Clojure](http://clojure.org/) application, built on top of
  the [Duct](https://github.com/duct-framework/duct) framework, served using
  a [Jetty](http://www.eclipse.org/jetty/) webserver.
* [PostgreSQL](https://www.postgresql.org/) database
  with [PostGIS](http://postgis.net/) and [pgRouting](http://pgrouting.org/)
  extensions providing spatial and routing capabilities.
* [Mapserver](http://mapserver.org/) with
  a [Mapcache](http://mapserver.org/mapcache/index.html) caching façade for
  serving the demographics raster layers.
* [GDAL](http://gdal.org/) tool suite for manipulating raster and vector
  spatial data files.

Client side:

* [Clojurescript](http://clojurescript.org) application using
  the [re-frame](https://github.com/Day8/re-frame) framework, which is built on
  top of [reagent](https://github.com/reagent-project/reagent)
  and [React](https://facebook.github.io/react/).
* [Leaflet](http://leafletjs.com/) for displaying interactive maps.
* [shadow-cljs](http://shadow-cljs.org/) for compilation.

Deployment:

* The production application is deployed as a set of [Docker](http://docker.io/)
  containers.

### Scalability

The webserver is multi-threaded and there is no lock contention on any
operation, so it can handle multiple concurrent requests. The PostgreSQL
database can also handle multiple concurrent connections. Pre-processing of
facilities is parallelized to make use of multiple cores, and can be easily
extended to multiple hosts.

Beyond that, there are three main dimensions for scaling PlanWise:

* Number of analyzed countries: since operations on countries are
  independent of each other, sharding spatial routing and demographics data and
  parallel processing can be easily implemented.
* Number of facilities: this is the main contention point of the application,
  since the demand calculation algorithm is linear in the number of affected
  facilities. Right now, we have limited the demand computation to regions
  within countries which yields near realtime performance for several hundred
  facilities. For the preprocessing portion of the algorithm, it can be easily
  paralellized.
* Number of concurrent users: can be scaled horizontally by adding more
  application servers to fulfill the requests. Most interesting operations are
  read-only and as such can be easily paralellized. Given the nature of the
  application we don't expect a huge demand on this dimension.

### Data Sources

The production deployment of [PlanWise](http://planwise.instedd.org) uses
demographics datasets from [WorldPop](http://www.worldpop.org.uk/).


## Developing

Instructions for setting up a development environment using Docker.

Build the required Docker images:

```sh
$ docker-compose build
```

Start the Docker compose stack defined in `docker-compose.yml`

```sh
$ docker-compose up
```

This will start the PostgreSQL/PostGIS database, the MapServer/MapCache
containers and a headless nREPL container.

Some scripts might require a bit more than 2gb of memory. Increase the default docker limit
if import-osm is run inside a container.

### Mapserver

The mapserver and mapcache containers for development will use the map data in
the `data` folder.

### Bootstrap the database

Run inside the `app` container:

```sh
$ docker-compose run app bash
app$ scripts/bootstrap-dev.sh
```

### Seed the database and data directory

There is a bit of geographical data needed to have a functional environment.

First, the global friction layer needs to be download as described
[here](./scripts/friction/README.md). This is used to compute walking
and car travel time.

Second, the administrative hierarchy of selected countries needs to be
downloaded. Follow [this procedure](./scripts/geojson/README.md) to populate
the `data/geojson` directory.

Third, download and register country population datasets to use as demand
raster source. While doing this last step the administrative hierarchies
will be registered and friction layer will be sliced per country. If you
don't need demand raster sources you will still need to register the
administrative hierarchies and slice the friction layer. Check
[this procedure](./scripts/population/README.md) to see how these steps are done.

### Configure Guisso credentials

Additionally, the project requires [GUISSO](https://github.com/instedd/guisso)
information (identifier and secret) to establish the OAuth flow with resourcemap.
Register your development host in GUISSO, and set the environment variables in
a `docker-compose.override.yml`:

```
version: '2'

services:
  app:
    environment:
      - GUISSO_CLIENT_ID=YOURID
      - GUISSO_CLIENT_SECRET=YOURSECRET
```

Or you can set these values in the local `dev/resources/local.edn`, which is
more useful if you plan to run the application outside Docker (see below):

```clojure
{:duct.core/include [""dev.edn""]

 :planwise.component/auth
 {:guisso-client-id     ""YOURID""
  :guisso-client-secret ""YOURSECRET""}}
```

## Extra steps for running the application outside Docker

To avoid Docker from starting Leiningen, put in your
`docker-compose.override.yml` the following configuration:

```
version: '2'

services:
  app:
    command: /bin/true
```

You need to have [Leiningen](http://leiningen.org) installed. In Mac OSX using
Homebrew, just run:

```sh
$ brew install leiningen
```

### Environment

The following environment variables are used by the project scripts, and it is
suggested to set them before starting development:

```
export RASTER_ISOCHRONES=false
export CALCULATE_DEMAND=false
export POSTGRES_PASSWORD=""planwise""
export POSTGRES_USER=planwise
export POSTGRES_DB=planwise
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5433
```

Default values are set in the file `env`, so you can simply run:

```sh
$ source ./env
```

### GDAL

The project needs GDAL 2.x with support for PostgreSQL and Java bindings. On Mac
OSX you can use the osgeo4mac Homebrew tap to get it.

```bash
$ brew tap osgeo/osgeo4mac
$ brew install gdal2 --with-swig-java --with-postgresql
```

Since `gdal2` is keg-only, you need to force link it with `brew link
--force gdal2` and add `/usr/local/opt/gdal2/bin` to your PATH environment
variable.

You also need to add the make the JNI libraries discoverable by the JVM. For
development, an easy and non-intrusive way of doing it is adding the
`java.library.path` system property in your `profiles.clj`. Eg.

```clojure
;; Local profile overrides
{:profiles/dev {:jvm-opts [""-Djava.library.path=/usr/local/opt/gdal2/lib""]}}
```

### Binaries

The application has some C++ binaries which are run in the context of the
application. If running the application outside Docker, you'll need to compile
these. Otherwise, these are automatically built by `bootstrap-dev.sh`.

```sh
$ brew install cmake boost
$ scripts/build-binaries
```

### Node modules

NPM dependencies are handled by `npm` and updated via the `package.json` file.

Install NPM dependencies before firing up the REPL or compiling the project:

```sh
$ npm install
```

NB: `npm install` is ran automatically when executing `(go)` from the REPL.


## Development workflow with the REPL

Connect to the running REPL inside the Docker container from your editor/IDE or
from the command line:

```
$ lein repl :connect
```

Or, if running outside Docker, start up the REPL with `lein repl`.

Load the development namespace and start the system:

```clojure
user=> (dev)
:loaded
dev=> (go)
```

By default this creates a web server at . Note that
these 3 commands are ran when invoking `scripts/dev`.

When you make changes to your source files, use `reset` to reload any
modified files and reset the server.

```clojure
dev=> (reset)
:reloading (...)
:resumed
```

If you want to access a ClojureScript REPL, make sure that the server is
running and there is a browser with the application loaded.

```
$ shadow-cljs cljs-repl :app
shadow-cljs - config: /app/client/shadow-cljs.edn  cli version: 2.8.59  node: v9.11.2
shadow-cljs - connected to server
cljs.user=> (js/alert ""hi"")
nil
```

The `shadow-cljs` is installed by `./client/package.json` in
`./client/node_modules/.bin/shadow-cljs`. In a docker development environment
you can execute shadow-cljs from the client service container.

## Further configuration information

### Database

The database engine is PostgreSQL, using PostGIS and pgRouting. It is included
in the default Docker compose file.

Database schema is managed through migrations via
[ragtime](https://github.com/duct-framework/duct-ragtime-component). In
development, they are run automatically on system startup and after every
`(reset)`.

With a running system, there is a `(rollback-1)` function to manually rollback
the last migration, though will be rarely needed since the migrations are
rebased automatically on each reset. The expected workflow is:

 - create a new migration, adding both the up and down SQL scripts
 - run `(reset)` to apply the migration
 - modify the migration
 - run `(reset)` again, to rollback the old version and apply the modified
   migration

The function `(rollback-1)` can be used to manually rollback until a specific
migration. This would be needed if switching the development branch *and*
restarting the REPL at the same time. Otherwise the system should be able to
rebase the migrations when resetting.

Migrations can be executed outside the REPL via the `lein migrate` task.

Migration files are located in `resources/migrations`, and follow the
`NUM-name.(up|down).sql` naming convention, where `NUM` is a 3-digit incremental
ID.

Additionally, SQL functions located in `resources/planwise/plpgsql` are
regenerated on every `lein migrate`, or can be manually loaded from the REPL by
running `(load-sql)`.


### Testing

Running the tests require a separate scratch database.

```bash
$ docker-compose exec db createdb planwise-test -U planwise
```

The connection configuration is located in the environment variable
`TEST_DATABASE_URL`, with the default being as specified in
`test/resources/test.edn`.

Testing is fastest through the REPL, as you avoid environment startup time.

```clojure
dev=> (test)
...
```

But you can also run tests through Leiningen.

```sh
lein test
```

### Importing a new country

Use the Planwise Tools Docker image to manage geographic and base source sets in the database. In development, this can be spawned by running:

```sh
docker-compose run tools
```

Then follow the instructions given in `scripts/tools/README.md`.


### Intercom

Planwise supports Intercom as its CRM platform. To load the Intercom chat widget, simply start Planwise with the env variable `INTERCOM_APP_ID` set to your Intercom app id (https://www.intercom.com/help/faqs-and-troubleshooting/getting-set-up/where-can-i-find-my-workspace-id-app-id).

Planwise will forward any conversation with a logged user identifying them through their email address. Anonymous, unlogged users will also be able to communicate.

If you don't want to use Intercom, you can simply omit `INTERCOM_APP_ID` or set it to `''`.

To test the feature in development, add the `INTERCOM_APP_ID` variable and its value to the corresponding `edn` file.

## Deploying

Sample files for docker cloud and docker compose are provided in the root
folder, which make use of the project's [Docker
image](https://hub.docker.com/r/instedd/planwise/).

After setting up the stack, DB data can be provisioned by running the scripts
described in the _Database_ section of this document.


## Legal

Copyright © 2016 InSTEDD

This software is released under the GPLv3 license. See LICENSE.md.
",,2024-05-01T00:40:07Z,16,39,10,"('ggiraldez', 777), ('spalladino', 341), ('ftarulla', 161), ('bcardiff', 111), ('juanedi', 60), ('juanboca', 44), ('hdf1996', 41), ('pmallol', 33), ('ismaelbej', 25), ('matiasgarciaisaia', 11), ('solsarratea', 9), ('jkicillof', 5), ('fgasperij', 4), ('dependabotbot', 4), ('waj', 2), ('nditada', 1)","[17, 'Partnerships for the Goals']"
w3-engineers/telemesh,TeleMesh is an on the grid/off the grid communication tool built on top of the Blockchain and Mobile Mesh Network (MMN) Technology. Vendors can broadcast or multicast information to distance mesh networks without any SIM or cellular network.,"[strom]:https://www.github.com/w3-engineers/android-framework
[travis]:https://travis-ci.com/w3-engineers/
[coverall]:https://coveralls.io/github/w3-engineers/telemesh
[Apache License 2.0]:https://choosealicense.com/licenses/apache-2.0/


# Telemesh

[![Build Status](https://travis-ci.com/w3-engineers/telemesh.svg?branch=master)](https://travis-ci.com/w3-engineers/telemesh)
[![docs](https://readthedocs.org/projects/telemesh/badge/?version=latest)](https://readthedocs.org/projects/telemesh/)
[![Coverage Status](https://coveralls.io/repos/github/w3-engineers/telemesh/badge.svg?branch=master&service=github)](https://coveralls.io/github/w3-engineers/telemesh?branch=master)
[![Lint tool: TeleMesh](https://img.shields.io/badge/Lint_tool-telemesh-2e99e9.svg?style=flat)](https://w3-engineers.github.io/telemesh/lint_reports/lint-report.html)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE-OF-CONDUCT.md)

A mesh network based off-grid messaging application powered by blockchain technology.

## Mission Statement
The freedom of connectivity to the edge of the world.


## Telemesh Documentation

Please follow this [Technical documentation](https://telemesh.readthedocs.io/en/latest/index.html) to get detail about Telemesh


## Contributing Guideline

Please find here [Contributing Guideline](https://github.com/w3-engineers/telemesh/blob/master/CONTRIBUTING.md)

## Reporting Issues 

If you face any bug or have any particular feature request please go [here](https://github.com/w3-engineers/telemesh/blob/master/CONTRIBUTING.md#reportissue)

    

## Test Coverage
* This repo is configured with [Travis](https://travis-ci.org/) and [coverall](https://docs.travis-ci.com/user/coveralls/). Every merge with *master* produced a test coverage report. Latest coverage report is available [here](https://coveralls.io/github/w3-engineers/telemesh?branch=master). [This](#Telemesh) badge here shows coverage status.

* To generate report locally you should go to project's root directory, then execute below command:
    > gradlew coveralls

**NOTE:** You must have a connected device or emulator as it runs instrumentation tests. You will find the coverage report at *telemesh/app/build/reports/coverage*



## User Interface (UI)

Please find here the complete UI flow [Telemesh UI](https://xd.adobe.com/view/102f0226-2937-4d2e-6ec0-e7e82c164fa1-661e/grid)

![](https://github.com/MimoSaha/app-share/blob/master/images/splash.png)
![](https://github.com/MimoSaha/app-share/blob/master/images/profile.png)
![](https://github.com/MimoSaha/app-share/blob/master/images/discovery.png)
![](https://github.com/MimoSaha/app-share/blob/master/images/wallet.png)

## Notice

Telemesh
Copyright [2018-2020] W3 Engineers Ltd.

This product includes software developed at W3 Engineers Ltd. and licensed under Apache 2.0 Licence. 
The [LICENSE.md](https://github.com/w3-engineers/telemesh/blob/master/LICENSE.md) file contains a copy of the Apache 2.0 Licence.

This software also use other open source libraries and tools. The list of other open source projects with associate license are as follows

1. Android Paging library	Android Native	Android SDK license
2. Circular image	https://github.com/hdodenhof/CircleImageView	Apache 2.0 license
3. Dexter for runtime permission	https://github.com/Karumi/Dexter	Apache 2.0 license
4. RxJava view binding	https://github.com/JakeWharton/RxBinding	Apache 2.0 license
5. Facebook Shimmer effect	https://github.com/facebook/shimmer-android	 BSD license
6. Bottom Navigation View	https://github.com/ittianyu/BottomNavigationViewEx	MIT 
7. Glide for image load	https://github.com/bumptech/glide	Copyright (c) 1994 Anthony Dekker Copyrighted legal terms are meet.
8. Retrofit adapter	https://github.com/square/retrofit/tree/master/retrofit-adapters/rxjava2	Apache 2.0 license
9. Web3J	https://github.com/web3j/web3j	Apache 2.0 license
10. bouncyCastle	https://github.com/bcgit/bc-java	Free
11. qrgenerator	https://github.com/androidmads/QRGenerator	MIT license
12. Parse	https://github.com/parse-community/parse-server	BSD license
13. Android ripple background	https://github.com/skyfishjy/android-ripple-background	MIT license
14. Viper	https://github.com/w3-engineers/viper	GPL-3.0 license
15. Coveralls Grdle Plugin	https://github.com/kt3k/coveralls-gradle-plugin	MIT license

For any kind of assistance, queries, improvement please contact at info@telemesh.net

## Community manager 

If you have any suggestions or feedback, you are always welcome to reach our community manager through [media@telemesh.net] & [info@telemesh.net]


","'blockchain', 'infrastructure', 'mesh-networks', 'refugees', 'smartphone', 'unicef'",2022-12-07T13:04:13Z,15,36,8,"('W3-Tariqul', 635), ('W3-Mimo', 514), ('W3-Anjan', 238), ('W3-Aziz', 76), ('w3-engineers', 70), ('W3-MajorArif', 42), ('sabbir360', 40), ('W3-Azim', 23), ('mimosaha', 12), ('z4zzaman', 11), ('W3-Shaibal', 10), ('W3-Partha', 5), ('azizcse', 3), ('W3-Monir', 3), ('W3-Ekram', 1)","[10, 'Reduced Inequalities']"
radiantearth/stac-browser,A full-fledged UI in Vue for browsing and searching static STAC catalogs and STAC APIs,"# STAC Browser

This is a [Spatio-Temporal Asset Catalog (STAC)](https://github.com/radiantearth/stac-spec) browser for static catalogs.
Minimal support for APIs is implemented, but it not the focus of the Browser and may lead to issues.
It attempts to surface all included data in a user-centric way (an approach
which can inform how data is represented in the evolving spec). It is
implemented as a single page application (SPA) for ease of development and to
limit the overall number of catalog reads necessary when browsing (as catalogs
may be nested and do not necessarily contain references to their parents).

Version: **3.2.0** (supports all STAC versions between 0.6.0 and 1.0.0)

This package has also been published to npm as [`@radiantearth/stac-browser`](https://www.npmjs.com/package/@radiantearth/stac-browser).

It's not officially supported, but you may also be able to use it for
certain _OGC API - Records_ and _OGC API - Features_ compliant servers.

**Please note that STAC Browser is currently without funding for both maintenance, bug fixes and improvements. This means issues and PRs may be addressed very slowly.
If you care about STAC Browser and have some funds to support the future of STAC Browser, please contact matthias@mohr.ws**

**Table of Contents:**

- [Examples](#examples)
- [Get Started](#get-started)
  - [Private query parameters](#private-query-parameters)
  - [Migrate from old versions](#migrate-from-old-versions)
- [Customize](#customize)
  - [Options](#options)
  - [Languages](#languages)
  - [Themes](#themes)
  - [Basemaps](#basemaps)
  - [Actions](#actions)
  - [Additional metadata fields](#additional-metadata-fields)
  - [Customize through root catalog](#customize-through-root-catalog)
  - [Custom extensions](#custom-extensions)
- [Docker](#docker)
- [Contributing](#contributing)
  - [Adding a new language](#adding-a-new-language)
- [Sponsors](#sponsors)

## Examples

A demo instance is running at .

The catalog section of [STAC Index](https://stacindex.org) is also built on top of STAC Browser (currently v2).

## Get Started

First, you need to clone or download this repository.

Then switch into the newly created folder and install all dependencies:

```bash
npm install
```

By default, STAC Browser will let you browse all catalogs on STAC Index.

To browse only your own static STAC catalog or STAC API, set the `catalogUrl` CLI parameter when running the dev server.
In this example we point to EarthSearch (`https://earth-search.aws.element84.com/v1/`):

```bash
npm start -- --open --catalogUrl=""https://earth-search.aws.element84.com/v1/""
```

To open a local file on your system, see the chapter [Using Local Files](docs/local_files.md).

If you'd like to publish the STAC Browser instance use the following command:

```bash
npm run build -- --catalogUrl=""https://earth-search.aws.element84.com/v1/""
```

This will only work on the root path of your domain though. If you'd like to publish in a sub-folder,
you can use the [`pathPrefix`](docs/options.md#pathprefix) option.

After building, `dist/` will contain all assets necessary
host the browser. These can be manually copied to your web host of choice.
**Important:** If `historyMode` is set to `history` (which is the default value), you'll need to add
an additional configuration file for URL rewriting.
Please see the [`historyMode`](docs/options.md#historymode) option for details.

You can customize STAC Browser, too. See the options and theming details below.
If not stated otherwise, all options can either be specified via CLI, ENV variables or in the [config file](config.js).
You can also provide configuration options ""at runtime"" (after the build).

### Private query parameters

**_experimental_**

STAC Browser supports ""private query parameters"", e.g. for passing an API key through. Any query parameter that is starting with a `~` will be stored internally, removed from the URL and be appended to STAC requests. This is useful for token-based authentication via query parameters.

So for example if your API requires to pass a token via the `API_KEY` query parameter, you can request STAC Browser as such:
`https://examples.com/stac-browser/?~API_KEY=123` which will change the URL to `https://examples.com/stac-browser/` and store the token `123` internally. The request then will have the query parameter attached and the Browser will request e.g. `https://examples.com/stac-api/?API_KEY=123`.

Please note: If the server hosting STAC Browser should not get aware of private query parameters and you are having `historyMode` set to `""history""`, you can also append the private query parameters to the hash so that it doesn't get transmitted to the server hosting STAC Browser.
In this case use for example `https://examples.com/stac-browser/#?~API_KEY=123` instead of `https://examples.com/stac-browser/?~API_KEY=123`.

### Migrate from old versions

Please read the [migration documentation](docs/migrate.md) for details.

## Customize

### Options

STAC Browser supports customization through a long list of options that can be set in various ways.

Please read the **[documentation for the options](docs/options.md)**.

### Languages

STAC Browser can be translated into other languages and can localize number formats, date formats etc.

You need to change the [`locale`](docs/options.md#locale) and [`supportedLocales`](docs/options.md#supportedlocales) settings to select the default language and the languages available to users.

The following languages are currently supported:

- de: German (Germany, Switzerland)
- es: Spanish
- en: English (US, UK)
- fr: French (Canada, France, Switzerland)
- it: Italian (Italy, Switzerland)
- ro: Romanian
- ja: Japanese
- pt: Portuguese

We manage the translations in Crowdin, please see  for details.

To add your own language, please follow the guide below: [Adding a new language](#adding-a-new-language)

#### Custom phrases

You can define custom phrases in the `custom.json`.
This is especially useful for phrases that are coming from non-standadized metadata fields (see the chapter ""[Additional metadata fields](#additional-metadata-fields)"").
If you've found metadata labels (e.g. ""Price"" and ""Generation Time"") that are not translated,
you can add it to the `custom.json`. For metadata fields you need to add it to a the object `fields`
as it is the group for the metadata-related phrases.
There you can add as many phrases as you like. For example:

```json
{
  ""fields"": {
    ""Price"": ""Preis"",
    ""Generation Time"": ""Generierungszeit""
  }
}
```

### Themes

You can customize STAC Browser in the `src/theme` folder. It contains Sass files (a CSS preprocessor), which you can change to suit your needs.

The easiest solution is to start with the `variables.scss` file and customize the options given there.
For simplicity we just provide some common options as default, but you can also add and customize any Bootstrap variable,
see  for details.

The file `page.scss` contains some Sass declarations for the main sections of STAC Browser and you can adopt those to suit your needs.

If you need even more flexibility, you need to dig into the Vue files and their dependencies though.

### Basemaps

The file `basemaps.config.js` contains the configuration for the basemaps.
You can update either just the `BASEMAPS` object or you can write a custom function `configureBasemap` that returns the desired options for [vue2-leaflet](https://vue2-leaflet.netlify.app/).
[XYZ](https://vue2-leaflet.netlify.app/components/LTileLayer.html#props) and [WMS](https://vue2-leaflet.netlify.app/components/LWMSTileLayer.html#props) basemaps are supported and have different options that you can set.

### Actions

STAC Browser has a pluggable interface to share or open assets and links with other services, which we call ""actions"".

More information about how to add or implement actions can be found in the **[documentation](docs/actions.md)**.

### Additional metadata fields

The metadata that STAC Browser renders is rendered primarily through the library [`stac-fields`](https://www.npmjs.com/package/@radiantearth/stac-fields).
It contains a lot of rules for rendering [many existing STAC extensions](https://github.com/stac-utils/stac-fields/blob/main/fields.json) nicely.
Nevertheless, if you use custom extensions to the STAC specification you may want to register your own rendering rules for the new fields.
This can be accomplished by customizing the file [`fields.config.js`](./fields.config.js).
It uses the [Registry](https://github.com/stac-utils/stac-fields/blob/main/README.md#registry) defined in stac-fields to add more extensions and fields to stac-fields and STAC Browser.

To add your own fields, please consult the documentation for the [Registry](https://github.com/stac-utils/stac-fields/blob/main/README.md#registry).

#### Example

If you have a custom extension with the title ""Radiant Earth"" that uses the prefix `radiant:` you can add the extension as such:

```js
Registry.addExtension(""radiant"", ""Radiant Earth"");
```

If this extension has a boolean field `radiant:public_access` that describes whether an entity can be accessed publicly or not, this could be described as follows:

```js
Registry.addMetadataField(""radiant:public_access"", {
  label: ""Data Access"",
  formatter: (value) => (value ? ""Public"" : ""Private""),
});
```

This displays the field (with a value of `true`) in STAC Browser as follows: `Data Access: Public`.

The first parameter is the field name, the second parameter describes the field using a [""field specification""](https://github.com/stac-utils/stac-fields/blob/main/README.md#fieldsjson).
Please check the field specification for available options.

#### Translation

STAC Browser supports [multiple languages](#languages).
If you use more than one language, you likely want to also translate the phrases that you've added above (in the example `Data Access`, `Public` and `Private`, assuming that `Radiant Earth` is a name and doesn't need to be translated).
All new phrases should be added to the [active languages](docs/options.md#supportedlocales).
To add the phrases mentioned above you need to go through the folders in `src/locales` and in the folders of the active languages update the file `custom.json` as described in the section that describes [adding custom phrases](#custom-phrases).
All new phrases must be added to the property `fields`.

Below you can find an example of an updated `custom.json` for the German language (folder `de`). It also includes the `authConfig`, which is contained in the file by default for [other purposes](docs/options.md#authconfig).

```json
{
  ""authConfig"": {
    ""description"": """"
  },
  ""fields"": {
    ""Data Access"": ""Zugriff auf die Daten"",
    ""Public"": ""Öffentlich"",
    ""Private"": ""Privat""
  }
}
```

### Customize through root catalog

You can also provide a couple of the config options through the root catalog.
You need to provide a field `stac_browser` and then you can set any of the following options:

- `apiCatalogPriority`
- `authConfig` (except for the `formatter` as function)
- `cardViewMode`
- `cardViewSort`
- `crossOriginMedia`
- `defaultThumbnailSize`
- `displayGeoTiffByDefault`
- `showThumbnailsAsAssets`

### Custom extensions

STAC Browser supports some non-standardized extensions to the STAC specification that you can use to improve the user-experience.

1. [Provider Object](https://github.com/radiantearth/stac-spec/blob/master/collection-spec/collection-spec.md#provider-object):
   Add an `email` (or `mail`) field with an e-mail address and the mail will be shown in the Browser.
2. [Alternative Assets Object](https://github.com/stac-extensions/alternate-assets?tab=readme-ov-file#alternate-asset-object):
   Add a `name` field and it will be used as title in the tab header, the same applies for the core Asset Object.
3. A link with relation type `icon` and a Browser-supported media type in any STAC entity will show an icon in the header and the lists of Catalogs, Collections and Items.

## Docker

### Create a custom image

Building the Dockerfile without changing any build options:

```bash
docker build -t stac-browser:v1 .
```

Run the container for a specific URL:

```bash
docker run -p 8080:8080 -e SB_catalogUrl=""https://earth-search.aws.element84.com/v1/"" stac-browser:v1
```

STAC Browser is now available at `http://localhost:8080`

---

You can pass further options to STAC Browser to customize it to your needs.

The build-only options
[`pathPrefix`](docs/options.md#pathprefix) and [`historyMode`](docs/options.md#historymode)
can be provided as a
[build argument](https://docs.docker.com/engine/reference/commandline/build#set-build-time-variables---build-arg)
when building the Dockerfile.

For example:

```bash
docker build -t stac-browser:v1 --build-arg pathPrefix=""/browser/"" --build-arg historyMode=hash .
```

All other options, except the ones that are explicitly excluded from CLI/ENV usage,
can be passed as environment variables when running the container.
For example, to run the container with a pre-defined
[`catalogUrl`](docs/options.md#catalogurl) and [`catalogTitle`](docs/options.md#catalogtitle):

```bash
docker run -p 8080:8080 -e SB_catalogUrl=""https://earth-search.aws.element84.com/v1/"" -e SB_catalogTitle=""Earth Search"" stac-browser:v1
```

If you want to pass all the other arguments to `npm run build` directly, you can modify to the Dockerfile as needed.

STAC browser is now available at `http://localhost:8080/browser`

### Use an existing image

Since version 3.1.1, you can add an existing image from [Packages](https://github.com/radiantearth/stac-browser/pkgs/container/stac-browser) to your docker-compose.yml:

```
services:
  stac-browser:
    image: ghcr.io/radiantearth/stac-browser:latest
    ports:
      - 8080:8080
    environment:
      SB_catalogUrl: ""https://localhost:7188""
```

## Contributing

We are happy to review and accept Pull Requests.
STAC Browser is following the [STAC code of conduct](https://github.com/radiantearth/stac-spec/blob/master/CODE_OF_CONDUCT.md).

STAC Browser uses [Vue](https://vuejs.org/) and [vue-cli](https://cli.vuejs.org/), so you need a recent version of [NodeJS and npm](https://nodejs.org/en/) installed.

You can run the following commands (see also ""[Get started](#get-started)"" above):

- `npm run install`: Install the dependencies, this is required once at the beginning.
- `npm start`: Start the development server
- `npm run lint`: Lint the source code files
- `npm run build`: Compile the source code into deployable files for the web. The resulting files can be found in the folder `dist` and you can then deploy STAC Browser on a web host. There are two other variants:
  - `npm run build:report`: Same as above, but also generates a bundle size report (see `dist/report.html`), which should not be deployed.
  - `npm run build:minimal`: Same as above, but tries to generate a minimal version without bundle size report and without source maps.
- `npm run i18n:fields`: Generates an updated version of the locales from the stac-fields package.

The [release process is documented separately](docs/release.md).

### Adding a new language

You can translate STAC Browser into other languages.
You can also use one of the existing languages and provide an alternate version for a specifc country, e.g. a Australian English (en-AU) version of the US-English language pack (en).

**Please follow this guide:**

- Copy the `en` folder (or any other language without a country code that you want to base the translation on).
  - Note: If you start with the `en` folder, you have to remove the leading `//` from the line `// { fields: require('./fields.json') }` in the file `default.js`.
- Name the new folder according to [RFC5646](https://www.rfc-editor.org/rfc/rfc5646).
- Add the language to the list of supported locales ([`supportedLocales`](docs/options.md#supportedlocales)) in the `config.js` file.
- Add the language to the [list of languages in this README file](#languages).
- Add yourself to the list of code owners (`.github/CODEOWNERS`) for this language (we'll invite you to this repository after you've opened a PR). **Persons contributing languages are expected to maintain them long-term! If you are not able to maintain the language pack, please indicate so in the PR and we'll release it separately.**
- Translate the `.json` files, most importantly `config.json`, `fields.json` and `texts.json`.
  - Please note that you never need to translate any object keys!
  - If you base your language on another existing language (e.g. create `en-IN` based on `en`) you can delete individual files and import existing files from other languages in `default.js`.
- Adapt the `datepicker.js`, `duration.js` and `validation.js` files to import the existing definitions from their corresponding external packages, but you could also define the specifics yourself.
- Check that your translation works by running the development server (`npm start`) and navigating to the STAC Browser instance in your browser (usually `http://localhost:8080`).
- Once completed, please open a pull request and we'll get back to you as soon as possible.
- After merging the PR for the first time, we'll add you to our translation management tool Crowdin: . Please get in touch to get your invite!

## Sponsors

The following sponsors have provided a substantial amount of funding for STAC Browser in the past:

- [Radiant Earth](https://radiant.earth) (base funding for versions 1, 2 and 3)
- [National Resources Canada](https://natural-resources.canada.ca/home) (multi-language support, maintenance)
- [Matthias Mohr - Softwareentwicklung](https://mohr.ws) (maintenance)
- [Spacebel](https://spacebel.com) (collection search)
- [Planet](https://planet.com) (OpenID Connect authentication, other features, maintenance)
",,2024-04-30T21:58:13Z,30,255,27,"('m-mohr', 597), ('mojodna', 223), ('lossyrob', 33), ('charlesbluca', 13), ('rowanwins', 8), ('p1d1d1', 4), ('constantinius', 2), ('drwelby', 2), ('tschaub', 2), ('mneagul', 2), ('schwehr', 1), ('tmokmss', 1), ('digitaltopo', 1), ('philvarner', 1), ('rnanclares', 1), ('tschumpr', 1), ('trevorlang', 1), ('glmxndr', 1), ('jlaura', 1), ('jqtrde', 1), ('Keenan-Nicholson', 1), ('juliayun23', 1), ('geospatial-jeff', 1), ('zakjan', 1), ('Firefishy', 1), ('fabricebrito', 1), ('uba', 1), ('dereklieu', 1), ('drnextgis', 1), ('waystilos', 1)","[9, 'Industry, Innovation and Infrastructure']"
dymaxionlabs/ap-latam,Detection of slums and informal settlements from satellite imagery,"# AP Latam

[![Build Status](https://travis-ci.org/dymaxionlabs/ap-latam.svg?branch=master)](https://travis-ci.org/dymaxionlabs/ap-latam)
[![codecov](https://codecov.io/gh/dymaxionlabs/ap-latam/branch/master/graph/badge.svg)](https://codecov.io/gh/dymaxionlabs/ap-latam)
[![Join the chat at https://gitter.im/dymaxionlabs/ap-latam](https://badges.gitter.im/dymaxionlabs/ap-latam.svg)](https://gitter.im/dymaxionlabs/ap-latam?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

This is the main repository of AP Latam project.

For more information on the website frontend, see the repository at
[https://github.com/dymaxionlabs/ap-latam-web](https://github.com/dymaxionlabs/ap-latam-web).


## Dependencies

* Python 3+
* GDAL
* Proj4
* libspatialindex
* Dependencies for TensorFlow with GPU support


## Install

### Quick install and usage: Docker image

If you have [Docker](https://www.docker.com/community-edition) installed on
your machine, with NVIDIA CUDA installed and configured, you can simply pull
our image and run the scripts for training and detection.

Otherwise, follow the steps in this
[tutorial](https://medium.com/google-cloud/jupyter-tensorflow-nvidia-gpu-docker-google-compute-engine-4a146f085f17)
to install Docker, CUDA and `nvidia-docker`.  This has been tested on an Ubuntu
16.04 LTS instance on Google Cloud Platform.

For all scripts you will need to mount a data volume so that the scripts can
read the input rasters and vector files, and write the resulting vector file.

It is recommended that you first set an environment variable that points to the
data directory in your host machine, like this:

```
export APLATAM_DATA=$HOME/aplatam-data
```

Then, to use any of the scripts, you would have to run them using
`nvidia-docker` and mounting a volume to `$APLATAM_DATA` like this:

```
nvidia-docker run -ti -v $APLATAM_DATA:/data dymaxionlabs/ap-latam SCRIPT_TO_RUN [ARGS...]
```

where `SCRIPT_TO_RUN` is either `ap_train` or `ap_detect` and `[ARGS...]` the
command line arguments of the specified script. You can run with `--help` to
see all available options on each script.

For example, suppose you have the following files inside the `$APLATAM_DATA`
directory:

* Training rasters on `images/`
* A settlements vector file `settlements.geojson`

To prepare a dataset and train a model you would run:

```
nvidia-docker run -ti -v $APLATAM_DATA:/data dymaxionlabs/ap-latam \
  ap_train /data/images /data/settlements.geojson /data/dataset
```

When using `[nvidia-]docker run` for the first time, it will pull the image
automatically for you, so it is not neccessary to do `[nvidia-]docker pull`
first.

#### `run_with_docker.sh`

You can also use `run_with_docker.sh` to do the same:

```
export APLATAM_DATA=$HOME/data/
./run_with_docker.sh ap_train /data/images /data/settlements.geojson /data/dataset
...
```

## Development

First you will need to install the following packages.  On Debian-based distros
run:

```
sudo apt install libproj-dev gdal-bin build-essential libgdal-dev libspatialindex-dev python3-venv virtualenv
```

Clone the repository and run `python setup.py install` to install the package
with its dependencies.  Add `--extras gpu` to install GPU dependencies
(TensorFlow for GPUs).

Run `make` to run tests and `make cov` to build a code coverage report. You can
run `make` to do both.


## Issue tracker

Please report any bugs and enhancement ideas using the GitHub issue tracker:

  https://github.com/dymaxionlabs/ap-latam/issues

Feel free to also ask questions on our
[Gitter channel](https://gitter.im/dymaxionlabs/ap-latam), or by email.


## Help wanted

Any help in testing, development, documentation and other tasks is highly
appreciated and useful to the project.

For more details, see the file [CONTRIBUTING.md](CONTRIBUTING.md).


## License

Source code is released under a BSD-2 license.  Please refer to
[LICENSE.md](LICENSE.md) for more information.
","'keras', 'machine-learning', 'remote-sensing', 'tensorflow'",2022-06-21T21:24:12Z,1,34,7,"('munshkr', 255)","[11, 'Sustainable Cities and Communities']"
getodk/xlsform-offline,ODK XLSForm Offline is a Python application for converting an XLSForm into an XForm that is valid and compliant with the ODK XForms specification. Contribute and make the world a better place! ✨🌍✨,"## ⚠️ ODK XLSForm Offline is no longer being updated. Please use [ODK XLSForm Online](https://github.com/getodk/xlsform-online) or [pyxform](https://github.com/xlsform/pyxform) instead. ⚠️

We are no longer doing active maintenance on ODK Aggregate. The recommended replacement is [XLSForm Online](https://github.com/getodk/central). Central is fast, actively-developed, and addresses many of the issues that users have had with Aggregate over the years. Fixes and small improvements to Aggregate are welcome, but please discuss in an issue or [on the forum](https://forum.getodk.org/c/development/5) first to make sure that a reviewer will be available.

## Overview
![Platform](https://img.shields.io/badge/platform-Python-blue.svg)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build status](https://api.travis-ci.org/getodk/xlsform-offline.svg?branch=master)](https://travis-ci.org/github/getodk/xlsform-offline)
[![Slack](https://img.shields.io/badge/chat-on%20slack-brightgreen)](https://slack.getodk.org)

ODK XLSForm Offline is a Windows and macOS application for converting an XLSForm into an XForm that is compliant with the [ODK XForms spec](http://getodk.github.io/xforms-spec). Once converted, the application also validates that the XForm will run perfectly with all ODK tools.
   
ODK XLSForm Offline is part of ODK, a free and open-source set of tools which help organizations author, field, and manage mobile data collection solutions. Learn more about the ODK project and its history [here](https://getodk.org/about/) and read about example ODK deployments [here](https://getodk.org/about/deployments/).

* ODK website: [https://getodk.org](https://getodk.org)
* ODK forum: [https://forum.getodk.org](https://forum.getodk.org)
* ODK developer Slack chat: [https://slack.getodk.org](https://slack.getodk.org)

## Prerequisites

1. Install [Python 3.6](https://www.python.org/downloads/)
	* Windows: Use the 32 bit version.
1. Install Python packages: ``pip3 install -r requirements.txt``
	* macOS: Use the default Python. virtualenvs will not work.
1. Install packaging utilities
	* macOS: ``brew install unix2dos upx``
	* Windows: [upx](https://upx.github.io/)

## Run

To run the app, `python src/main.py`

## Automated packaging

[Travis](https://travis-ci.com/) will automatically build all of this repo's branches and place the binaries here for 30 days.

* Mac: https://travis.getodk.org/xlsform-offline/ODK-XLSForm-Offline-macOS-{GIT_HASH}.zip
* Windows: https://travis.getodk.org/xlsform-offline/ODK-XLSForm-Offline-Windows-{GIT_HASH}.zip

`{GIT_HASH}` should be replaced with the output of the command:
```shell 
git describe --tags --dirty --always
```

## Manual packaging

The easiest way to do manual packaging is to use a macOS machine running a Windows 10 virtual machine and a macOS virtual machine. Both VMs should have Python installed natively (no virtualenv, no pyenv) to minimize problems with pyinstaller.

1. In the macOS VM, run `./make-mac.sh` to build the Mac binary.
1. In the Windows VM, run `make-win.bat` to build the Windows binary.
1. Copy the resulting binaries into the `dist/mac` and `dist/win` folders on the host machine.
1. On the host machine, run `./make-dist.sh` to zip up the Mac and Windows binaries.

## Releases

Before releasing a version, be sure to update the version in src/res/about.html
",,2021-04-29T16:42:11Z,4,17,16,"('yanokwa', 58), ('lognaturel', 1), ('estberg', 1), ('shouryaj', 1)","[17, 'Partnerships for the Goals']"
openMF/community-app,This was the former default web application built on top of the Apache Fineract platform. It's now deprecated and replaced by the the Mifos X Web App (https://github.com/openMF/web-app maintained by the Mifos Initiative as a reference solution for financial inclusion. It is a Single-Page App (SPA) written in web standard technologies. ,"# MifosX Community App [![Join the chat at https://gitter.im/openMF/community-app](https://badges.gitter.im/openMF/community-app.svg)](https://gitter.im/openMF/community-app?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Build Status](https://travis-ci.com/openMF/community-app.svg?branch=develop)](https://travis-ci.com/github/openMF/community-app)  [![Docker Hub](https://img.shields.io/docker/pulls/openmf/community-app.svg)](https://hub.docker.com/r/openmf/community-app)  [![Docker Build](https://img.shields.io/docker/cloud/build/openmf/community-app.svg)](https://hub.docker.com/r/openmf/community-app/builds)

This is the default web application built on top of the MifosX platform for the mifos user community. It is a Single-Page App (SPA) written in web standard technologies like JavaScript, CSS and HTML5. It leverages common popular frameworks/libraries such as AngularJS, Bootstrap and Font Awesome.


## Getting started / Online Demo

The latest version of this UI is continuously re-deployed immediately (CI/CD) at openmf.github.io/community-app every time a Pull Request with a new feature or bugfix is merged.  You should always specify the backend via `baseApiUrl` (see details below), so for example to access the https://www.fineract.dev online demo environment, use:

https://openmf.github.io/community-app?baseApiUrl=https://demo.fineract.dev&tenantIdentifier=default

## Building from source

1. Ensure you have

   * ```npm``` installed - goto http://nodejs.org/download/ to download the installer for your OS.
   * ```ruby``` installed - goto https://www.ruby-lang.org/en/documentation/installation/ to download the latest version of ruby.

Note: On Ubuntu Linux you can use `sudo apt-get install npm nodejs-legacy`, which avoids the `/usr/bin/env: node: No such file or directory` problem.

Note that on Linux distributions you'll need to install the Ruby Development package (e.g. `sudo dnf install ruby-devel` on Fedora), and not just `ruby`, otherwise `bundle install` below will fail when it gets to installing `ffi` which uses native extensions.

1. Clone this repository to your local filesystem (default branch is 'develop'):

   ```
    git clone https://github.com/openMF/community-app.git
   ```
   
1. To download the dependencies, and be able to build, first install bower & grunt:

   ```
    npm install -g bower
    npm install -g grunt-cli
   ```

If this fails with `npm WARN checkPermissions Missing write access to /usr/local/lib` and `npm ERR! code EACCES` because you are not running `npm` with `sudo` as `root` (which you rightfully really shouldn't!) then use `npm config set prefix ~` once before doing `npm install`.  Note that in that case `bower` and `grunt` will be installed into `./bin/bower` instead of `/usr/local/bin`, and so you need to prefix it in the usages below.


1. Next pull the runtime and build time dependencies by running `bower`, `npm`, and `gem` commands on the project root folder:

   ```
    bower install
   ```
For Windows PC, before you run `npm install` check in the root folder if any package-lock.json is generated and delete it then you can run `npm install` other wise you will be faced with `Npm ERR! code EPERM  error errno -4048 error { Error: EPERM: operation not permitted, rename ....` a permission error even if you are using administrator user 
   ```
    npm install
   ```
   ```
    gem install bundler
   ```
   ```
    bundle install
   ```
   
   If you used `npm config set prefix ~`, then you have to use `./bin/bower install` instead of `bower install`.


1. To preview the app, run the following command on the project root folder:

   ```
    grunt serve
   ```
   
   If you used `npm config set prefix ~`, then you have to use `./bin/grunt serve` instead of `grunt serve`.

   or open the 'index.html' file in FIREFOX browser

   Note: If you see a warning similar to the one shown below on running `grunt serve` , try increasing the number of open files limit as per the suggestions at http://stackoverflow.com/questions/34588/how-do-i-change-the-number-of-open-files-limit-in-linux/

   ```
    Waiting...Warning: EMFILE, too many open files

   ```
   
1. You can use these credentials to log in:

   ```
    Username: mifos
    Password: password
   ```

You are done.

### Connecting to a MifosX Platform using OAuth 2 authentication:

Edit the value of property ""security"" in configurations.js to ""oauth"".

### Connecting to a MifosX Platform running on a different host:

By default, when the app is running from the local filesystem, it will connect to the platform (fineract-provider REST API) deployed on demo.mifos.io, but that environment is no longer actively updated; we recommend using https://www.fineract.dev instead, as above.

The app connects to the platform running on the same host/port when deployed on a server.

If you want to connect to the Fineract API running elsewhere, then append the `baseApiUrl` and `tenantIdentifier` as query parameters, for example:

* http://localhost:9002/?baseApiUrl=https://localhost:8443&tenantIdentifier=default if you are running the Fineract backend locally; note that because of the default self signed SSL certification, on the first time use (or after you have cleared the cookies from your browser), you will need to first bypass the security warning by accepting the SSL in your browser by going once to https://localhost:8443/fineract-provider/api/v1&tenantIdentifier=default and accepting it.

* http://localhost:9002/?baseApiUrl=https://demo.fineract.dev&tenantIdentifier=default to use https://www.fineract.dev which always automatically runs the very latest Fineract back-end


## Adding dependencies

You can also add more dependencies on bower.json.
You can search for them in http://sindresorhus.com/bower-components/ or even:

```
bower search 
```

## Running grunt tasks

Grunt tasks are used to automate repetitive tasks like minification, compilation, unit testing, linting, production builds, etc

Following are the tasks integrated.

### Validate JS and HTML files

Validate the JS files to detect errors and potential problems in JavaScript code. All errors output will be written to jshint-log.xml file which get created under project base directory.
Checks the markup validity of HTML files. All errors output will be written to console.

```
grunt validate
```

### Build

Build the code for production deployment.

```
grunt prod
```

### Serve

Use this for development.
Start a static server and open the project in the default browser. The application will hit the demo server.

```
grunt serve
```

### Docker

This project publishes a Docker image (since #[3112](https://github.com/openMF/community-app/issues/3112)) available on https://hub.docker.com/r/openmf/community-app/.  Our [Dockerfile](Dockerfile) uses a Ruby and Node.JS base image to build the current repo and deploy the app on Nginx, which is exposed on port 80 within the container.  It can be used like this to access the webapp on http://localhost:9090 in your browser:

    docker run --name community-app -it -p 9090:80 openmf/community-app


To locally build this Docker image from source (after `git clone` this repo), run:
```
docker build -t mifos-community-app .
```
You can then run a Docker Container from the image above like this:
```
docker run --name mifos-ui -it -d -p 80:80 mifos-community-app
```

Access the webapp on http://localhost in your browser.


### Compile sass to css

```
grunt compass:dev
```
## Running the tests

Just open test/SpecRunner.html in the browser.

## Getting Started doc

https://docs.google.com/document/d/1oXQ2mNojyDFkY_x4RBRPaqS-xhpnDE9coQnbmI3Pobw/edit#heading=h.vhgp8hu9moqn

## Contribution guidelines

Please read the contribution guidelines

Note: This application will hit the demo server by default.
",'hacktoberfest',2024-05-02T12:09:56Z,30,307,57,"('vishwasbabu', 713), ('Nayan', 403), ('translatewiki', 352), ('mbj36', 207), ('safiyu', 147), ('nazeer1100126', 145), ('goutham-M', 114), ('chandrikamohith', 81), ('pramodn02', 76), ('gauravsaini03', 67), ('siebrand', 63), ('rajuan', 62), ('gkrishnan724', 54), ('renovatebot', 48), ('mifoscontributer', 41), ('ckomlo', 37), ('vorburger', 36), ('ankit01ojha', 31), ('botraunak', 28), ('alexivanov', 24), ('smontanari', 23), ('abhaychawla', 21), ('mauliksoneji', 21), ('DanCarl857', 19), ('Anh3h', 19), ('ShruthiRajaram', 18), ('hhartono', 17), ('MYKatz', 15), ('sachinkulkarni12', 15), ('muddlebee', 14)","[11, 'Sustainable Cities and Communities']"
hotosm/tasking-manager,Tasking Manager - The tool to team up for mapping in OpenStreetMap,"# Tasking Manager

[![hotosm](https://dl.circleci.com/status-badge/img/gh/hotosm/tasking-manager/tree/develop.svg?style=shield)](https://dl.circleci.com/status-badge/redirect/gh/hotosm/tasking-manager/tree/develop)
[![TM Backend on Quay](https://quay.io/repository/hotosm/tasking-manager/status ""Tasking Manager Backend Build"")](https://quay.io/repository/hotosm/tasking-manager)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=hotosm_tasking-manager&metric=alert_status)](https://sonarcloud.io/dashboard?id=hotosm_tasking-manager)

![tm-landing-page](./docs/images/screenshot.jpg)

The most popular tool for teams to coordinate mapping on OpenStreetMap. With this web application, an area of interest can be defined and divided up into smaller tasks that can be completed rapidly. It shows which areas need to be mapped and which areas need a review for quality assurance. You can see the tool in action: log into the widely used [HOT Tasking Manager](https://tasks.hotosm.org/) and start mapping.

This is Free and Open Source Software. You are welcome to use the code and set up your own instance. The Tasking Manager has been initially designed and built by and for the [Humanitarian OpenStreetMap Team](https://www.hotosm.org/), and is nowadays used by many communities and organizations.

## Get involved!

* Start by reading our [Code of conduct](docs/developers/code_of_conduct.md)
* Get familiar with our [contributor guidelines](docs/developers/contributing.md) explaining the different ways in which you can support this project! We need your help!
* Join the Tasking Manager Collective Meet up - an opportunity to meet other Tasking Manager contributors. The meet ups take place on the second Wednesday of the month at 9:00 or 15:00UTC! Register to receive a calendar invite: https://bit.ly/3s6ntmV or join directly via this link: https://meet.jit.si/TaskingManagerCollectiveMeetUp
* Read the monthly update blogs on [OSM Discourse](https://community.openstreetmap.org/c/general/38/all).

## Product Roadmap
We have included below a [high level roadmap/plan](https://github.com/orgs/hotosm/projects/28/) [subject to change] that can be used as an overview.


## Developers

* [Understand the code](./docs/developers/understanding-the-code.md)
* [Setup the TM for development](./docs/developers/development-setup.md)
* [Learn about versions and releases](./docs/developers/versions-and-releases.md)
* Help us and submit [pull requests](https://github.com/hotosm/tasking-manager/pulls)

## Instances
* [HOT Tasking Manager (production)](https://tasks.hotosm.org)
* [HOT Tasking Manager (staging)](https://tasks-stage.hotosm.org)
* [TeachOSM](https://tasks.teachosm.org/)
* [OpenStreetMap Indonesia](https://tasks-indonesia.hotosm.org/)
* [OpenStreetMap US](https://tasks.openstreetmap.us/)
* [Map My Kerala](https://mapmykerala.in/)
* [OpenHistoricalMap](https://tasks.openhistoricalmap.org)
* [Oceania Tasking Manager](https://tasks.smartcitiestransport.com/)
","'flask', 'javascript', 'openstreetmap', 'postgis', 'postgresql', 'python', 'reactjs'",2024-05-03T09:53:34Z,30,491,37,"('willemarcel', 761), ('HelNershingThapa', 438), ('Aadesh-Baral', 356), ('dependabot-previewbot', 355), ('ramyaragupathy', 340), ('hunt3ri', 299), ('dakotabenjamin', 200), ('LindaAlblas', 199), ('eternaltyro', 194), ('ethan-nelson', 187), ('JorgeMartinezG', 161), ('xamanu', 159), ('dependabotbot', 116), ('feenster', 96), ('d-rita', 85), ('tsmock', 84), ('smit1678', 64), ('bgirardot', 58), ('varun2948', 53), ('bgirardot-np', 51), ('NoemiNahomy', 50), ('royallsilwallz', 43), ('spwoodcock', 36), ('nrotstan', 35), ('fitoria', 34), ('robsavoye', 33), ('david-hotosm', 33), ('sunidhiraheja', 27), ('mahesh-naxa', 26), ('kshitijrajsharma', 23)","[17, 'Partnerships for the Goals']"
iHRIS/iHRIS,iHRIS V,"# iHRIS ![docs](https://github.com/iHRIS/iHRIS/workflows/docs/badge.svg) ![docker](https://github.com/iHRIS/iHRIS/workflows/docker/badge.svg) ![docker-fsh](https://github.com/iHRIS/iHRIS/workflows/docker-fsh/badge.svg)

Technical documentation has been started and is hosted at: [https://iHRIS.github.io/iHRIS](https://iHRIS.github.io/iHRIS)

## Installation
Follow instructions in here to install dependents
See the notes/ directory for additional installation instructions for development.  
Installation instructions for supporting software is [here](docs/admin/install.md).
A docker version for simpler installation will be created soon.

iHRIS is built with a VueJS frontend and a NodeJS backend, both of which are included 
in this repository.

If you are done installing supporting softwares as in [here](docs/admin/install.md), then follow below steps to setup iHRIS

### Clone Repository
It is recommended to install iHRIS under /var/lib for linux/mac users just for standardization purpose, but iHRIS can be installed on any directory.
```bash
cd /var/lib
git clone https://github.com/iHRIS/iHRIS.git
```

### Install dependencies
```bash
cd /var/lib/iHRIS/ihris-backend
sudo npm install
cd /var/lib/iHRIS/ihris-frontend
sudo npm install
```

### Create your backend site
```bash
cd /var/lib/iHRIS/ihris-backend
sudo cp -r ihris-backend-site your_site_name
```
Change **your_site_name** to the name of your preference, it is recommended your_site_name to be on the directory /var/lib/iHRIS/ihris-backend so that you can easily reuse iHRIS npm packages. This site will be used to run the iHRIS backend and customizing iHRIS backend based with your needs. Customization is covered on a separate section.
If you already have your_site_name on github, clone it instead of copying ihris-backend-site

### Create configuration file
iHRIS comes with default configuration file that you can copy and customize based on your needs, run below commands to copy the default configuration.
```bash
cd your_site_name
cp config/baseConfig.json.example config/baseConfig.json
```
Now you may open config/baseConfig.json and change configurations based on your setup, espeially the hapi fhir server base url (fhir:base), path to your iHRIS site (app:site:path) and path to your iHRIS Core (app:core:path)

### Create your frontend site
This is only needed if you intend to customize the frontend by adding new components
```bash
cd /var/lib/iHRIS/ihris-frontend
sudo cp -r src/ihris-frontend-site site
```
For now the name of the frontend site is **must** be **site** and **must** be located inside the directory /var/lib/iHRIS/ihris-frontend

The backend component relies on the presence of a fhir compliant server. 
The login credentials can be placed inside the config/baseConfig.json file for 
the backend component.

## Running iHRIS
iHRIS is divided into two components, the ui frontend and a backend. To build the 
ui frontend, go to /var/lib/iHRIS/ihris-frontend and run `npm run build` then copy built files into /var/lib/iHRIS/ihris-backend/your_site_name/public.
The backend can be started as below
```bash
cd /var/lib/iHRIS/ihris-backend/your_site_name
npm run start
```

",,2024-05-02T13:17:42Z,12,36,8,"('lokisapocalypse', 671), ('ashaban', 372), ('lukeaduncan', 290), ('nobertmn', 246), ('MulukenMegersa', 97), ('gerard-bisama', 57), ('kwirica', 47), ('citizenrich', 46), ('sovello', 38), ('pierrekasongo', 15), ('cloehr651', 4), ('dependabotbot', 4)","[3, 'Good Health and Well-Being']"
developmentseed/landsat-util,"A utility to search, download and process Landsat 8 satellite imagery","Landsat-util
===============

.. image:: https://travis-ci.org/developmentseed/landsat-util.svg?branch=master
    :target: https://travis-ci.org/developmentseed/landsat-util

.. image:: https://badge.fury.io/py/landsat-util.svg
    :target: http://badge.fury.io/py/landsat-util

.. image:: https://img.shields.io/pypi/dm/landsat-util.svg
    :target: https://pypi.python.org/pypi/landsat-util/
    :alt: Downloads

.. image:: https://img.shields.io/pypi/l/landsat-util.svg
    :target: https://pypi.python.org/pypi/landsat-util/
    :alt: License


Landsat-util is a command line utility that makes it easy to search, download, and process Landsat imagery.

Docs
+++++

For full documentation visit: https://pythonhosted.org/landsat-util/

To run the documentation locally::

    $ pip install -r requirements/dev.txt
    $ cd docs
    $ make html


Recently Added Features
+++++++++++++++++++++++

- Improved pansharpening
- Use BQA bands for cloud/snow coverage and use in color correction
- Add support for different NDVI color maps (three included)
- Add support for image clipping using the new `--clip` flag

Change Log
++++++++++

See `CHANGES.txt `_.
",,2022-04-14T18:05:53Z,19,689,127,"('drewbo', 25), ('scisco', 17), ('julesair', 15), ('matthewhanson', 13), ('kamicut', 6), ('njwilson23', 6), ('astrojuanlu', 5), ('mileswwatkins', 5), ('lpinner', 2), ('angevineMiller', 2), ('bradh', 1), ('RoboDonut', 1), ('gmaclennan', 1), ('jflasher', 1), ('robintw', 1), ('sgillies', 1), ('wknowles', 1), ('YKCzoli', 1), ('cayetanobv', 1)","[17, 'Partnerships for the Goals']"
GFDRR/open-risk-data-dashboard,"Repository for the Open Data for Resilience Index, a website to track and improve the state of Open Data for Resilience worldwide.","The Open Data for Resilience Index assesses the state of open data for disaster risk management across the globe. It provides a list of key datasets that are needed for reducing vulnerability and building resilience to natural hazards, and that should be made available as open data for any country. Anyone can contribute to the index by submitting information on a dataset.

The Open Data for Resilience Index is managed by the Open Data for Resilience Initiative (OpenDRI). Together with other tools and initiatives such as ThinkHazard!, Geonode, Inasafe and the Understanding Risk community, it aims at improving risk information through better access to data.

## OpenDRI

In 2011, Global Facility for Disaster Reduction and Recovery (GFDRR) launched the Open Data for Resilience Initiative to apply the concepts of the global open data movement to the challenges of reducing vulnerability to natural hazards and the impacts of climate change. OpenDRI supports World Bank Regional Disaster Risk Management Teams to build capacity and long-term ownership of open data projects with client countries that are tailored to meet specific needs and goals of stakeholders.

## Acknowledgements

The Open Data for Resilience Index has been developed and is being maintained in partnership with CIMA Foundation, Global Earthquake Model and Deltares.
The Open Data for Resilience Index is inspired by the Open Data Index of Open Knowledge and the  Open Data Barometer of the Web Foundation. It applies the idea of tracking and assessing open data to the Disaster Risk Management sector.

## Licenses and terms of use

The Open Data for Resilience Index is available under the GNU General Public License, Version 3, 29 June 2007 and the source code is available on Github.
Unless specified, data and texts of the website are licensed under a Creative Commons Attribution Share Alike 4.0 License (CC-BY-SA 4.0).
Homepage picture: Kofu, Japan, by Joseph Chan available on Unsplash under an open license.

## Disclaimer

Information available on the Open Data for Resilience Index is collected through a crowdsourced effort by registered users and then reviewed by a team of Disaster Risk Management and Open Data specialists. Although it aims to provide accurate and up to date information, the Open Data for Resilience Index cannot guarantee the accuracy and freshness of all information related to key datasets available on the website.
","'disaster-risk-management', 'natural-hazards', 'opendata', 'opendri', 'resilience', 'risk', 'worldbank'",2022-12-08T07:35:45Z,10,23,12,"('nastasi-oq', 460), ('CIMAManuel', 234), ('dedandy', 113), ('pzwsk', 88), ('daniviga', 82), ('newick', 26), ('dario85-dev', 10), ('dependabotbot', 6), ('cgiovando', 1), ('gracedoherty', 1)","[13, 'Climate Action']"
GlobalDigitalLibraryio/book-api,API for accessing books from The Global Digital Library,"# book-api
[![Build Status](https://travis-ci.org/GlobalDigitalLibraryio/book-api.svg?branch=master)](https://travis-ci.org/GlobalDigitalLibraryio/book-api)

API for fetching books from the Global Digital Library
",,2020-10-02T09:13:54Z,7,2,6,"('gunnarvelle', 158), ('kstigen', 148), ('runarfu', 56), ('Erlendgje', 21), ('Mkohm', 4), ('kievu', 1), ('vikre', 1)","[4, 'Quality Education']"
Veativetech/VeativeWebVR,"WebVR Module for PC, Cardboard, DayDream, Oculus Go and GearVR devices","[![Build Status](http://ec2-174-129-91-236.compute-1.amazonaws.com:8080/buildStatus/icon?job=VativeWebVR)](http://ec2-174-129-91-236.compute-1.amazonaws.com:8080/job/VativeWebVR/lastBuild/)
# VeativeWebVR

Gaining real-life experience in certain professions can be difficult to achieve, dangerous, or just plain expensive. However, WebVR can connect users with those experiences, from the most specialized skill-set training, to simple lab experiments performed by school students. VeativeWebVR is a package of 6 Interactive WebVR Modules covering topics from STEM for Cardboard, DayDream, Oculus Go and GearVR devices using [A-Frame](https://aframe.io/).

![VeativeWebVR](https://media.giphy.com/media/VbKU5KRHWFu9Mh4r1o/giphy.gif)

## Technology Used

Framework : A-Frame
IDE       :	Atom
Scripting : JavaScript

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

## Demos

* [Line and Plane of Symmetry](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ms300035/)
* [Structure of Phenol](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ss200049/)
* [Introduction to Complex Numbers](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs300012/)
* [Reproductive Parts of a Flower](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ms100027/)
* [Opaque Translucent and Transparent](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs400052/)
* [Series and Parallel Circuits](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs400034/)
* [Galvanometer](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs400034/)
* [Rutherford’s Atomic model](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs200040/)
* [Lewis dot structures](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/hs200069/)
* [Human brain](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ms100057/)
* [Lines](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ms300045/)
* [Dominant and Recessive Alleles](http://ec2-52-5-117-32.compute-1.amazonaws.com/WebVR/Aframe/ms100176/)

## Working with VeativeWebVR

### How to set up Atom

Install Atom : Download the file from https://atom.io/ and Install  AtomSetup-x64 in your pc.

Install html-preview package: To show Output on editor we will have to add html-preview package to Atom editor:

Step 1. Open Atom editor.

Step 2. Go to Packages Tab >> Setting View >> Install Packages/Themes

Step 3. Search for atom-html-preview Package

Step 4. Install first one from showing list.

Step 5. After installation we can show output of web page

Use html-preview package: To use html preview Go to Packages Tab >> Preview HTML >> Enable Preview. By enabling preview you can show the output of your project.


### How to create new Project

Step1: Create folder structure same as below

Module Id (Root Directory)

○images

○models

○scripts

○sounds

○loginform

○index.html (welcome page)

○login.html (login form)

○manifest.json

○Appdata.js

Step2: Create index.html text file under the root directory, it will be Game Play page.

Step3: Copy login.html, manifest.json, appdata.js file and loginform folder to your project

Step4: Open Atom editor and Select File tab then select Add Project Folder and select project directory.


### Uses and Description of Downloaded Project

Step1: Open Atom editor and Select File tab then select Add Project Folder and select downloaded project directory

Login page (login.html)

If login cookie does not find in browser, Application shows Login form
  
Fill below detail to the form
User name, user age , avatar and gender

After submitting the form user will be redirected to module. User will login for 24 hour with login name.
 

### Creating Game play (Using Index.html)


### Linking Manifest


Linking manifest file to add below line

```

```

### Add Module Title


Add Title to page to add below line

```
MS300035
```

### Load scripts

Load Script using `` tag

```












```

### Create Scene

 
Create Scene by adding a-scene Tag like below line

```
  code here     >
```


### Add Camera

Add following code:
```

	
		<a-cursor 
			fuse=""false"" 
			fuseTimeout='60000' 
			rayOrigin= ""mouse"" 
			geometry=""primitive: ring"" 
			material=""color: white;  shader: flat"">
		
	

```

### Load sound in scene

Write code to load sound inside a-scene Tag.
Load sound with a-sound 

Note: This is recommended if clip name is ‘click.wav’ then sound id sound be ‘s_click’ 
(s_NameOfClip).

```

	 
	 
	 
	 

```

### Load Assets

Write code to load assets inside a-scene Tag.
Load assets with a-sound 

Note: This is recommended if loading file name is ‘dog.png’ then id should be ‘dog’ .

```


   // Load texture
    // Load 360 Image
  //Load audio
  // Load 3D model (.gltf)


```

### Add Background music

Add Background music with a-sound tag in a-scene tag.

Note: To disable autoplay change property autoplay=""false""

```



```

### Add Skybox	

Add Skybox in with a-sky tag inside a-scene tag.

Note: Change rotation with rotation property.

```



```

### Add Level

Add level inside a-scene tag with a-entity tag.
Note: To active/deactive level as default use property setactive=""value:true/false""

```

 Code for main menu level  
 Code for LO level  
 Code for L1 level  
 Code for AS level  

```

### Add MainMenu

```



<a-entity  position=""0 2.5 -3""
  geometry=""primitive: plane; width: 3; height: 0.35;""
  material=""color: black; opacity:0.53""
  text=""value: Line and Plane of Symmetry; align:center; width: 5 "">


<a-image id=""btn_lo"" position=""-1.3 1.5 -3""
  geometry=""primitive: plane; width: 1.2; height: 0.65""
  material=""color: white; opacity:1 ; src: #tex_menu_off ;""
  text=""value: View learning \n objectives; align:center; width: 3""
  click_sound cardboard_input vo_hover color_hover onclick=""OnClickObjective();"">


 -->

<a-image id=""btn_l1"" position=""0 1.5 -3""
  geometry=""primitive: plane; width: 1.2; height: 0.65""
  material=""color: white; opacity:1 ; src: #tex_menu_off ;""
  text=""value: Learn about lines\n and planes\n of symmetry; align:center; width: 3""
  click_sound cardboard_input vo_hover color_hover onclick=""OnClickL1();"">


<a-image id=""btn_as"" position=""1.3 1.5 -3""
  geometry=""primitive: plane; width: 1.2; height: 0.65""
  material=""color: white; opacity:1 ; src: #tex_menu_off ;""
  text=""value: Assess your \n knowledge; align:center; width: 3""
  click_sound  cardboard_input vo_hover color_hover  onclick=""OnClickAS();"">



```

### Add Panel With OK Button

Note: Method execute on ok click should name PanelIdName_click().
Eg. if panel id is ‘p_comp_as’ method should be declared ‘p_comp_as_click()’

```
<a-entity id=""p_comp_as"" position=""0 2 -3""
    setactive=""value:false""
    geometry=""primitive: plane; width: 2.4; height: 1.1""
    material=""color: black; opacity:0.53"">

    <a-text value=""You have completed the assessment. 
    Here is your score:"" position=""0 0.3 0"" align=""center""  
     width=""2.8"">

    <a-image id=""btn_ok"" position=""0 -0.56 0.01""
   src=""#tex_menu_off""
  geometry=""primitive: plane; width: 0.6; height: 0.3""
  text=""value: OK; align:center; width: 3; color: white""
  click_sound cardboard_input color_hover onclick=""p_comp_as_click();"">
  

```

### Add Panel In Camera

Add panel inside  tag.

Note: Panel in camera should have scale ‘0.1’ for xyz.

```
<a-entity id=""i_nextToProceed"" 
  setactive=""value:false ; scale: 0.1""
  vo_enable
  position=""0 -0.13 -0.3""
  scale=""0.1 0.1 0.1""
  geometry=""primitive: plane; width: 1.7; height: 0.27""
  material=""color: black; opacity:0.53""
  text=""value:Select NEXT to proceed.; align:center; width: 2.8;"">

```

### Add Controller For GearVR, Oculus and Daydream

```

    <a-entity setactive=""value:false"" gearvr-controls laser-controls=""hand: right""  
           id=""gvrc"" line=""color: #00ffff; opacity: 0.75"">
    <a-entity setactive=""value:false"" daydream-controls laser-controls=""hand: right"" 
id=""ddc"" line=""color: #00ffff; opacity: 0.75"">
    <a-entity setactive=""value:false"" oculus-go-controls laser-controls=""hand: right"" 
id=""oc"" line=""color: #00ffff; opacity: 0.75"">
 
```



### Controller setting for different platforms (Cardboard,DayDream,GearVR and OculusGo)

Add below code in index.html file at last

```
	
	    var assetsId = '#assets';
	    var entity = document.querySelector(assetsId);
	    var controllerId = null;

    if (entity != null) {
        entity.addEventListener(""loaded"", function() {
            console.log(""All Assets loaded"");
            setTimeout(ShowMainMenu, 2000);
            setInterval(ActivateController, 1000);
            login_isUserAuthenticated();

        });

        entity.addEventListener(""timeout"", function() {
            console.log(""Time up to load assets"");
            setTimeout(location.reload.bind(location), 500);
        });

    } else {
        console.log(""Exception : Id not found ID: "", assetsId);
    }


    function ShowMainMenu() {
        SetActive('#MainMenu', true);
        if (!login_isReferred()) {
            SetActive('#btn_close', false);
        }
    }


    function ActivateController() {
        let cId = """";

        if (AFRAME.utils.device.isGearVR()) {
            cId = '#gvrc';
            app_device_platform = ""GearVR"";
        } else if (isDayDream()) {
            cId = '#ddc';
            var gvrc = document.querySelector('#gvrc');
            var oc = document.querySelector('#oc');
            gvrc.parentNode.removeChild(gvrc);
            oc.parentNode.removeChild(oc);
            SetActive('#cursor', false);
            app_device_platform = ""Daydream"";
        } else if (AFRAME.utils.device.isOculusGo()) {
            cId = '#oc';
            app_device_platform = ""OculusGo"";
        } else {
            cId = '#cursor';

            let isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

            if (isMobile) {
                app_device_platform = ""Cardboard"";
                let scene = document.querySelector('a-scene');
                //scene.setAttribute('vr-mode-ui','enabled',true);
            } else {
                app_device_platform = ""Desktop"";
                let scene = document.querySelector('a-scene');
                //scene.setAttribute('vr-mode-ui','enabled',false);
            }

        }

        if (cId != controllerId) {
            SetActive(controllerId, false);
            controllerId = cId;
            SetActive(controllerId, true);
            console.log(""controller id is "", controllerId);
        }
    }

    function isDayDream() {
        let isDayDream = false;
        let gamepad = navigator.getGamepads()[0];
        if (gamepad != undefined) {
            let gamepadId = gamepad.id;
            isDayDream = gamepadId.includes(""Daydream"") || gamepadId.includes(""daydream"");
        }
        return isDayDream;
    }


    window.onload = function() {

        login_referred_set();

        if (!login_isUserAuthenticated()) {
            window.location.href = ""login.html"";
        } else {
            console.log(""User Name : "", login_getCookie('unicef_name'));
        }
    };
	
```


### Sending user activity on Impact analysis platform

URL: http://ec2-52-5-117-32.compute-1.amazonaws.com/unicef/public

API: /api/app-activity


#### Used parameter 

```
function questiondata() {
    this.USER_OID = ""mango"";
    this.GL_MODULE_ID = ""SSD"";
    this.GL_MODULE_NAME = ""Disability Statistics"";
    this.GL_LEVEL_ID = 1;
    this.GL_LEVEL_NAME = ""L1"";
    this.GL_LEVEL_KNOWLEDGE_DOMAIN = ""Seeing"";
    this.GL_LEVEL_COGNITIVE_DOMAIN = ""CFM-7"";
    this.GL_LEVEL_TYPE = ""T1"";
    this.GL_LEVEL_INTERACTIVITY = ""GL_l1"";
    this.GL_QUESTION_ID = ""1"";
    this.GL_QUESTION_COGNITIVE = ""dsgdsgsdg"";
    this.GL_QUESTION_ACTION_VERB = ""sdgdgdfg"";
    this.LL_QUESTION_TYPE = ""QT1"";
    this.TR_USER_SCORE = ""1"";
    this.HOST_IP = """";
    this.DEVICE_BROWSER_VERSION = """";
    this.DEVICE_MODEL = ""NA"";
    this.DEVICE_KERNEL_VERSION = ""NA"";
    this.DEVICE_SERIAL_NUMBER = ""NA"";
    this.DEVICE_PLATFORM = """";
    this.ATTEMPTED_ON = """";
}
```


#### Save data to local DB in Json format
```
function saveScoreToLocalDB() {
    let dataInJson = JSON.stringify(data);
    data = new datamodel();
    localStorage.setItem(""UserScoreData"", """");
    localStorage.setItem(""UserScoreData"", dataInJson);

    if (app_unicef_user_id != null) {
        setTimeout(sendDataToServer, 1000);
    }
}
```
#### Sending data to server
```
function sendDataToServer() {
    xhr.open(method, url.concat(saveActivity), true);
    xhr.setRequestHeader(""Authorization"", ""Basic "" + btoa(userNamePassword));
    xhr.setRequestHeader(""Content-type"", ""application/json"");
    let scoreDate = localStorage.getItem(""UserScoreData"");
    xhr.send(scoreDate);
}
```


### Cookie parameters

app_unicef_user_id
",,2020-03-30T07:42:24Z,2,0,3,"('Veativetech', 71), ('sanjeev059', 1)","[4, 'Quality Education']"
GFDRR/thinkhazard,ThinkHazard!,"# ThinkHazard

A natural hazard screening tool for disaster risk management project planning. ThinkHazard! is maintained by the Global Facility for Disaster Reduction and Recovery (GFDRR). Provides classified hazard level (very low to high) for any location in the world, and advice on managing disaster risk, plus useful reports and contacts, for 11 natural hazards. 

API instructions can be found here: https://github.com/GFDRR/thinkhazard/blob/master/API.md 



[![Build Status](https://travis-ci.org/GFDRR/thinkhazard.svg?branch=master)](https://travis-ci.org/GFDRR/thinkhazard)

## Getting Started

The following commands assume that the system is Debian/Ubuntu. Commands may need to be adapted when working on a different system.

Build docker images:

    $ make build

Run the composition:

    $ docker-compose up -d
    $ make initdb

Now point your browser to .

Run checks and automated tests:

    $ make check test

## Initialize a fresh database

Install postgres `unaccent` extension database engine :

    $ sudo apt install postgresql-contrib

Edit `/etc/postgresql/9.5/main/postgresql.conf`, and set `max_prepared_transactions` to 10

Create a database:

    $ sudo -u postgres createdb -O www-data thinkhazard_admin
    $ sudo -u postgres psql -d thinkhazard_admin -c 'CREATE EXTENSION postgis;'
    $ sudo -u postgres psql -d thinkhazard_admin -c 'CREATE EXTENSION unaccent;'
    $ sudo -u postgres createdb -O www-data thinkhazard
    $ sudo -u postgres psql -d thinkhazard -c 'CREATE EXTENSION postgis;'
    $ sudo -u postgres psql -d thinkhazard -c 'CREATE EXTENSION unaccent;'

If you want to use a different user or different database name, you’ll have to provide your own configuration file. See “Use local.ini” section below.

Create the required schema and tables and populate the enumeration tables:

    $ make populatedb

Note: this may take a while. If you don’t want to import all the world administrative divisions, you can import only a subset:

    $ make populatedb DATA=turkey

or:

    $ make populatedb DATA=indonesia

In order to harvest geonode instance with full access, you need to create and
configure an API key.
On geonode side:

* create a superuser with following command:

    $ python manage.py createsuperuser

* Then, create api keys for all users with:

    $ python manage.py backfill_api_keys

* Finally, you can display all api keys with:

```sql
SELECT people_profile.id, username, key
FROM people_profile
LEFT JOIN tastypie_apikey ON (tastypie_apikey.user_id = people_profile.id)
```

On Thinkhazard side:

* Change username and api_key value according to previous setup in
`thinkhazard_processing.yaml` file.

You’re now ready to harvest, download and process the data:

    $ make harvest
    $ make download
    $ make complete
    $ make process
    $ make decisiontree

For more options, see:

    $ make help

## Processing tasks

Administrator can also launch the different processing tasks with more options.

`.build/venv/bin/harvest [--force] [--dry-run]`

Harvest metadata from GeoNode, create HazardSet and Layer records.

`.build/venv/bin/download [--title] [--force] [--dry-run]`

Download raster files in data folder.

`.build/venv/bin/complete [--force] [--dry-run]`

Identify hazardsets whose layers have been fully downloaded, infer several fields and mark these hazardsets complete.

`.build/venv/bin/process [--hazardset_id ...] [--force] [--dry-run]`

Calculate output from hazardsets and administrative divisions.

`.build/venv/bin/decision_tree [--force] [--dry-run]`

Apply the decision tree followed by upscaling on process outputs to get the final relations between administrative divisions and hazard categories.

## Publication of admin database on public site

Publication consist in overwriting the public database with the admin one. This can be done using :

`make publish`

And this will execute as follow :
 * Lock the public site in maintenance mode.
 * Store a publication date in the admin database.
 * Backup the admin database in archives folder.
 * Create a new fresh public database.
 * Restore the admin backup into public database.
 * Unlock the public site from maintenance mode.

### Configure admin username/password

By default, the admin interface authentification file is `/var/www/vhosts/wb-thinkhazard/conf/.htpasswd`. To change the location you can set `AUTHUSERFILE` on the `make modwsgi` command line.

To create a authentification file `.htpasswd` with `admin` as the initial user :

    $ htpasswd -c .htpasswd admin

It will prompt for the passwd.

Add or modify `username2` in the password file `.htpasswd`:

    $ htpasswd .htpasswd username2

### Analytics

If you want to get some analytics on the website usage (via Google analytics), you can add the tracking code using a analytics variable:

    analytics = UA-75301865-1

### Feedback

The `feedback_form_url` can be configured in the `local.ini` file.

### Configuration of processing parameters

The configuration of the threshold, return periods and units for the different hazard types can be done via the thinkhazard\_processing.yaml.

After any modification to this file, next harvesting will delete all layers, hazardsets and processing outputs. This means that next processing task will have to treat all hazardsets and may take a while (close to one hour).

## hazard\_types

Harvesting and processing configuration for each hazard type. One entry for each hazard type mnemonic.

Possible subkeys include the following:

-   `hazard_type`: Corresponding hazard\_type value in geonode.
-   `return_periods`: One entry per hazard level mnemonic with corresponding return periods. Each return period can be a value or a list with minimum and maximum values, example:

    ```yaml
    return_periods:
      HIG: [10, 25]
      MED: 50
      LOW: [100, 1000]
    ```

-   `thresholds`: Flexible threshold configuration.

    This can be a simple and global value per hazardtype. Example:

    ```yaml
    thresholds: 1700
    ```

    But it can also contain one or many sublevels for complex configurations:

    1.  `global` and `local` entries for corresponding hazardsets.
    2.  One entry per hazard level mnemonic.
    3.  One entry per hazard unit from geonode.

    Example:

    ```yaml
    thresholds:
      global:
        HIG:
          unit1: value1
          unit2: value2
        MED:
          unit1: value1
          unit2: value2
        LOW:
          unit1: value1
          unit2: value2
        MASK:
          unit1: value1
          unit2: value2
      local:
        unit1: value1
        unit2: value2
    ```

-   `values`: One entry per hazard level, with list of corresponding values in preprocessed layer. If present, the layer is considered as preprocessed, and the above `thresholds` and `return_periods` are not taken into account. Example:

    ```yaml
    values:
      HIG: [103]
      MED: [102]
      LOW: [101]
      VLO: [100, 0]
    ```

## Translations

ThinkHazard! is translated using `Transifex`.

### Workflow ###

We use lingua to extract translation string from `jinja2` templates.

Use the following command to update the gettext template (`.pot`):

    make extract_messages

Note: this should be done from the production instance ONLY in order to have
the up-to-date database strings extracted!
You will have to make sure that the `~/.transifexrc` is valid and the
credentials correspond to the correct rights.

Then you can push the translation sources to transifex.

    make transifex-push

Once the translations are OK on Transifex it's possible to pull the translations:

    make transifex-pull

Don't forget to compile the catalog (ie. convert .po to .mo):

    make compile_catalog

### Development

There are 3 different ways to translate strings in the templates:

 - `translate` filter

   This should be used for strings corresponding to enumeration tables in
   database.

```
{{ hazard.title | translate }}
```

 - `gettext` method

   To be used for any UI string.

```
{{gettext('Download PDF')}}
```

 - model class method

   Some model classes have specific method to retrive the value from a field
   specific to chosen language.

```
{{ division.translated_name(request.locale_name)}}
```
",,2024-04-17T17:37:18Z,11,30,17,"('arnaud-morvan', 362), ('tonio', 236), ('elemoine', 121), ('fvanderbiest', 62), ('tkohr', 54), ('stufraser1', 37), ('marionb', 37), ('Vampouille', 10), ('pyjavo', 5), ('matamadio', 5), ('ingenieroariel', 3)","[13, 'Climate Action']"
arkhn/fhir-pipe,Populate FHIR-compliant objects using SQL databases and processing rules,"
# FHIR pipeline: a smart ETL to standardize health data

[![Arkhn](https://img.shields.io/badge/ARKHN-CORE-000000.svg?style=for-the-badge&logo=data:image/png;base64,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)](https://arkhn.org/)
[![GitHub license](https://img.shields.io/badge/LICENSE-APACHE2-blue.svg?style=for-the-badge)](https://github.com/arkhn/fhir-pipe/blob/master/LICENSE)
[![Build Status](https://img.shields.io/travis/arkhn/fhir-pipe.svg?style=for-the-badge&)](https://travis-ci.com/arkhn/fhir-pipe)


Fhir-Pipe is an ETL tool backed by IA solutions, which makes standardisation of health data easy. It extracts and converts data from SQL databases into the standardized health format [FHIR](https://www.hl7.org/fhir/), using mapping rules build from the [pyrog interface](https://github.com/arkhn/pyrog).

## Mapping rules

The mapping rules are provided through the [pyrog](https://github.com/arkhn/pyrog) graphql API. To each FHIR Resource (e.g. `Patient`) corresponds a set of rules, and each attribute of the ressource (like `patient.name.firstname`) has a mapping instruction which details which `DATABASE/TABLE/COLUMN` to select and which processing scripts to apply, as the data might need to be cleaned. The list of generic and health care specific scripts is available at [arkhn/scripts](https://github.com/arkhn/cleaning-scripts).

## Goal of `fhir-pipe`

Fhir-Pipe is an ETL which is able to parse complex mapping rules which may describe intricated relations between tables. It is agnostic of the type of SQL databases used as input for its extraction, and uses Spark to perform highly scalable processing of data. The FHIR objects created are stored in a distributed file system to meet strict performance requirements.

## Get started

[Read our guide](./GET_STARTED.md) to start standardizing health data!

## Contribute

We have reported several issues with the label `Good first issue` which can be a good way to start! You can also join our [Slack](https://join.slack.com/t/arkhn/shared_invite/enQtNTc1NDE5MDIxMDU3LWZmMzUwYWIwN2U0NGI1ZjM2MjcwNTAyZDZhNzcyMWFiYjJhNTIxNWQ1MWY4YmRiM2VhMDY4MDkzNGU5MTQ4ZWM) to contact us if you have trouble or questions :)

If you're enthusiastic about our project, :star: it to show your support! :heart:

* * *

## License

[Apache License 2.0](./LICENSE)
",'fhir',2020-05-04T12:35:16Z,7,25,11,"('LaRiffle', 107), ('Jasopaum', 22), ('simonvadee', 21), ('alexisthual', 7), ('Nabellaleen', 4), ('achauve', 1), ('elsiehoffet-94', 1)","[3, 'Good Health and Well-Being']"
MuckRock/muckrock,"MuckRock's source code - Please report bugs, issues and feature requests to info@muckrock.com","# MuckRock
**MuckRock** &middot; [Squarelet][squarelet] &middot; [DocumentCloud][documentcloud] &middot; [DocumentCloud-Frontend][documentcloudfrontend]

[![Codeship Status for MuckRock/muckrock][codeship-img]][codeship]
[![codecov.io][codecov-img]][codecov]

MuckRock is a non-profit collaborative news site that gives you the tools to keep our government transparent and accountable.

## Prerequisites 
MuckRock depends on Squarelet for user authentication. As the services need to communivate directly, the development environment for MuckRock depends on the development environment for Squarelet - the MuckRock docker containers will join Squarelet's docker network. [Please install Squarelet and set up its development environment first][squarelet].

## Install

### Software required

1. [docker][docker-install]
3. [python][python-install]
4. [invoke][invoke-install]
5. [git][git-install]

### Installation Steps

1. Check out the git repository - `git clone git@github.com:MuckRock/muckrock.git`
2. Enter the directory - `cd muckrock`
3. Run the dotenv initialization script - `python initialize_dotenvs.py`
This will create files with the environment variables needed to run the development environment.
4. Set an environment variable that directs `docker-compose` to use the `local.yml` file - `export COMPOSE_FILE=local.yml`
5. Set up the javascript run `inv npm ""install""` and `inv npm ""run build""`
6. Start the docker images - `inv up`
This will build and start all of the docker images using docker-compose.
7. Set `dev.muckrock.com` to point to localhost - `echo ""127.0.0.1   dev.muckrock.com"" | sudo tee -a /etc/hosts`
8. Enter `dev.muckrock.com` into your browser - you should see the MuckRock home page.
9. In  `.envs/.local/.django` set the following environment variables:

-   `SQUARELET_KEY`  to the value of Client ID from the Squarelet Client
-   `SQUARELET_SECRET`  to the value of Client SECRET from the Squarelet Client
10. You must restart the Docker Compose session (via the command `docker-compose down` followed by `docker-compose up`) each time you change a `.django` file for it to take effect.

You should now be able to log in to MuckRock using your Squarelet account.

## Docker info

The development environment is managed via [docker][docker] and [docker compose][docker compose].  Please read up on them if you are unfmiliar with them.  The docker compose file is `local.yml`.  If you would like to run `docker compose` commands directly, please run `export COMPOSE_FILE=local.yml` so you don't need to specify it in every command.

The containers which are run include the following:

* Django
This is the [Django][django] application

* PostgreSQL
[PostgreSQL][postgres] is the relational database used to store the data for the Django application

* Redis
[Redis][redis] is an in-memory datastore, used as a message broker for Celery as well as a cache backend for Django.

* Celery Worker
[Celery][celery] is a distrubuted task queue for Python, used to run background tasks from Django.  The worker is responsible for running the tasks.

* Celery Beat
The celery beat image is responsible for queueing up periodic celery tasks.

All systems can be brought up using `inv up`.  You can rebuild all images using `inv build`.  There are various other invoke commands for common tasks interacting with docker, which you can view in the `tasks.py` file.

### Networking Setup

The MuckRock development environment will join Squarelet's environments docker network, so that the services can coexist.  Please see the README file from Squarelet for more information.

### Environment Variables

The application is configured with environment variables in order to make it easy to customize behavior in different environments (dev, testing, staging, production, etc).  Some of this environment variables may be sensitive information, such as passwords or API tokens to various services.  For this reason, they are not to be checked in to version control.  In order to assist with the setup of a new development environment, a script called `initialize_dotenvs.py` is provided which will create the files in the expected places, with the variables included.  Those which require external accounts will generally be left blank, and you may sign up for an account to use for development and add your own credentials in.  You may also add extra configuration here as necessary for your setup.

## Invoke info

Invoke is a task execution library.  It is used to allow easy access to common commands used during development.  You may look through the file to see the commands being run.  I will go through some of the more important ones here.

### Release
`inv prod` will merge your dev branch into master, and push to GitHub, which will trigger [CodeShip][codeship] to release it to Heroku, as long as all code checks pass.  The production site is currently hosted at [https://www.muckrock.com/](https://www.muckrock.com/).
`inv staging` will push the staging branch to GitHub, which will trigger CodeShip to release it to Heroku, as long as all code checks pass.  The staging site is currently hosted at [https://muckrock-staging.herokuapp.com/](https://muckrock-staging.herokuapp.com/).

### Test
`inv test` will run the test suite.  To reuse the database, pass it the `-r=1` option.
`inv coverage` will run the test suite and generate a coverage report at `htmlcov/index.html`.

The test suite will be run on CodeShip prior to releasing new code.  Please ensure your code passes all tests before trying to release it.  Also please add new tests if you develop new code.

### Code Quality
`inv pylint` will run [pylint][pylint].  It is possible to silence checks, but should only be done in instances where pylint is misinterpreting the code.
`inv format` will format the code using the [yapf][yapf] code formatter.

Both linting and formatting are checked on CodeShip.  Please ensure your code is linted and formatted correctly before attempting to release changes.

### Run
`inv up` will start all containers in the background.
`inv runserver` will run the Django server in the foreground.  Be careful to not have multiple Django servers running at once.  Running the server in the foreground is mainly useful for situations where you would like to use an interactive debugger within your application code.
`inv shell` will run an interactive python shell within the Django environment.
`inv sh` will run a bash shell within the Django docker comtainer.
`inv dbshell` will run a postgresql shell.
`inv manage` will allow you to easily run Django manage.py commands.
- `inv manage migrate` to migrate database
`inv npm` will allow you to run NPM commands.  `inv npm ""run build""` should be run to rebuild assets if any javascript or CSS is changed. If you will be editing a lot of javascript or CSS, you can run `inv npm ""run watch""`.
`inv heroku` will open a python shell on Heroku.

## Pip Tools

Python dependencies are managed via [pip-tools][pip-tools].  This allows us to keep all of the python dependencies (including underling dependencies) pinned, to allow for consistent execution across development and production environments.

The corresponding files are kept in the `pip` folder.  There are `requirements` and `dev-requirements` files.  `requirements` will be installed in all environments, while `dev-requirements` will only be installed for local development environments.  It can be used for code only needed during develpoment, such as testing.  For each environment there is an `.in` file and a `.txt` file.  The `.in` file is the input file - you list your direct dependencies here.  You may specify version constraints here, but do not have to.

Running `inv pip-compile` will compile the `.in` files to the corresponding `.txt` files.  This will pin all of the dependencies, and their dependencies, to the latest versions that meet any constraints that have been put on them.  You should run this command if you need to add any new dependencies to an `.in` files.  Please keep the `.in` files sorted.  After running `inv pip-compile`, you will need to run `inv build` to rebuild the docker images with the new dependencies included.

## FOIAMachine

FOIAMachine is our free FOIA filing tool, that allows you to track your requests while requiring you to manually handle all of the message sending and receiving.  It is run off of the same code base as MuckRock.  To access it, set `dev.foiamachine.org` to point to localhost - `sudo echo ""127.0.0.1   dev.foiamachine.org"" >> /etc/hosts`.  Then pointing your browser to `dev.foiamachine.org` will take you to FOIAMachine - the correst page is shown depending on the domain host.

## Update search index

MuckRock uses [watson][watson] for search.  The index should stay updated. If a new model is registered with watson, then build the index (`fab manage:buildwatson`). This command should be run on any staging or production servers when pushing code that updates the registration.


[docker]: https://docs.docker.com/
[django]: https://www.djangoproject.com/
[postgres]: https://www.postgresql.org/
[redis]: https://redis.io/
[celery]: https://docs.celeryproject.org/en/latest/
[invoke]: http://www.pyinvoke.org/
[docker-install]: https://docs.docker.com/install/
[invoke-install]: http://www.pyinvoke.org/installing.html
[python-install]: https://www.python.org/downloads/
[git-install]: https://git-scm.com/downloads
[codeship]: https://app.codeship.com/projects/296009
[pylint]:  https://www.pylint.org/
[pip-tools]: https://github.com/jazzband/pip-tools
[codeship]: https://codeship.com/projects/52228
[codeship-img]: https://codeship.com/projects/c14392c0-630c-0132-1e4c-4ad47cf4b99f/status?branch=master
[codecov]: https://codecov.io/github/MuckRock/muckrock?branch=master
[codecov-img]:https://codecov.io/github/MuckRock/muckrock/coverage.svg?token=SBg37XM3j1&branch=master
[squarelet]: https://github.com/muckrock/squarelet/
[yapf]: https://github.com/google/yapf
[watson]: https://github.com/etianen/django-watson
[documentcloudfrontend]: https://github.com/MuckRock/documentcloud-frontend
[documentcloud]: https://github.com/MuckRock/documentcloud
","'django', 'foia', 'muckrock', 'python'",2024-04-25T13:35:47Z,18,113,17,"('mitchelljkotler', 7075), ('allanlasser', 4528), ('simongle', 99), ('morisy', 51), ('jugglinmike', 5), ('aleenaloves', 5), ('duckduckgrayduck', 4), ('erikreyna', 4), ('CallaJune', 2), ('emilyliu7321', 2), ('adityajain15', 1), ('advayDev1', 1), ('anthonyjpesce', 1), ('benlk', 1), ('BenjaminDoran', 1), ('ericliuche', 1), ('titanous', 1), ('pjsier', 1)","[16, 'Peace, Justice and Strong Institutions']"
AsylumConnect/asylum-connect-catalog,AsylumConnect is a volunteer initiative that seeks to provide LGBTQ asylum seekers in the U.S. with lifesaving online informational resources.,"# AsylumConnect

AsylumConnect is a fiscally sponsored nonprofit creating the first online, centralized resource database for LGBTQ asylum seekers in the U.S.
  - [AsylumConnect website](http://www.asylumconnect.org/)
  - [AsylumConnect catalog](http://asylumconnectcatalog.org/)

## What's included?

* Blueprints
* User and permissions management
* Flask-SQLAlchemy for databases
* Flask-WTF for forms
* Flask-Assets for asset management and SCSS compilation
* Flask-Mail for sending emails
* gzip compression
* gulp autoreload for quick static page debugging

## Extensions

Other branches include even more features

* `admin-edit-static-pages`: allow administrators to edit static pages using the [ckeditor](http://ckeditor.com/) WYSIWYG editor 
* `gulp-static-watcher`: quick and easy webpage refresh as static pages and assets are edited. 

## Setting up

##### Clone the repo

```
$ git clone git@github.com:AsylumConnect/asylum-connect-catalog.git
$ cd asylum-connect-catalog
```

##### Initialize a virtualenv

```
$ pip install virtualenv
$ virtualenv env
$ source env/bin/activate
```

##### (If you're on a mac) Make sure xcode tools are installed

```
$ xcode-select --install
```

##### Add Environment Variables 

Create a file called `.env` that contains environment variables in the following syntax: `ENVIRONMENT_VARIABLE=value`. For example,
the mailing environment variables can be set as the following
```
MAIL_USERNAME=example@domain.com
MAIL_PASSWORD=SuperSecretPassword
SECRET_KEY=SuperRandomStringToBeUsedForEncryption
```
**Note: do not include the `.env` file in any commits. This should remain private.**

##### Install the dependencies

```
$ pip install -r requirements/common.txt
$ pip install -r requirements/dev.txt
```

##### Other dependencies for running locally

You need to install [Foreman](https://ddollar.github.io/foreman/) and [Redis](http://redis.io/). Chances are, these commands will work:

```
$ gem install foreman
```

Mac (using [homebrew](http://brew.sh/)):

```
$ brew install redis
```

Linux:

```
$ sudo apt-get install redis-server
```

##### Create the database

```
$ python manage.py recreate_db
```

##### Other setup (e.g. creating roles in database)

```
$ python manage.py setup_dev
```

##### [Optional] Add Seattle data and fake data to the database

```
$ python manage.py add_seattle_data
$ python manage.py add_fake_data
```

##### [Optional. Only valid on `gulp-static-watcher` branch] Use gulp to live compile your files

* Install the Live Reload browser plugin from [here](http://livereload.com/)
* Run `npm install`
* Run `gulp`

## Running the app

```
$ source env/bin/activate
$ foreman start -f Local
```

## Formatting code

Before you submit changes to flask-base, you may want to auto format your code with `python manage.py format`.

## Project Structure


```
├── Procfile
├── README.md
├── app
│   ├── __init__.py
│   ├── account
│   │   ├── __init__.py
│   │   ├── forms.py
│   │   └── views.py
│   ├── admin
│   │   ├── __init__.py
│   │   ├── forms.py
│   │   └── views.py
│   ├── assets
│   │   ├── scripts
│   │   │   ├── app.js
│   │   │   └── vendor
│   │   │       ├── jquery.min.js
│   │   │       ├── semantic.min.js
│   │   │       └── tablesort.min.js
│   │   └── styles
│   │       ├── app.scss
│   │       └── vendor
│   │           └── semantic.min.css
│   ├── assets.py
│   ├── decorators.py
│   ├── email.py
│   ├── main
│   │   ├── __init__.py
│   │   ├── errors.py
│   │   ├── forms.py
│   │   └── views.py
│   ├── models.py
│   ├── static
│   │   ├── fonts
│   │   │   └── vendor
│   │   ├── images
│   │   └── styles
│   │       └── app.css
│   ├── templates
│   │   ├── account
│   │   │   ├── email
│   │   │   ├── login.html
│   │   │   ├── manage.html
│   │   │   ├── register.html
│   │   │   ├── reset_password.html
│   │   │   └── unconfirmed.html
│   │   ├── admin
│   │   │   ├── index.html
│   │   │   ├── manage_user.html
│   │   │   ├── new_user.html
│   │   │   └── registered_users.html
│   │   ├── errors
│   │   ├── layouts
│   │   │   └── base.html
│   │   ├── macros
│   │   │   ├── form_macros.html
│   │   │   └── nav_macros.html
│   │   ├── main
│   │   │   └── index.html
│   │   └── partials
│   │       ├── _flashes.html
│   │       └── _head.html
│   └── utils.py
├── config.py
├── manage.py
├── requirements
│   ├── common.txt
│   └── dev.txt
└── tests
    ├── test_basics.py
    └── test_user_model.py
```

## Contributing

Contributions are welcome! Check out our [Waffle board](https://waffle.io/hack4impact/flask-base) which automatically syncs with this project's GitHub issues. Please refer to our [Code of Conduct](./CONDUCT.md) for more information.

## License
[MIT License](LICENSE.md)
",,2017-06-27T14:14:07Z,20,13,5,"('yoninachmany', 340), ('sandlerben', 142), ('anniemeng', 122), ('riyer15', 68), ('stephanieyshi', 57), ('dzhang55', 50), ('abhisuri97', 47), ('OBrandon', 41), ('sanjayss34', 38), ('ehamp', 37), ('kyle-rosenbluth', 32), ('nnarang7', 29), ('Huntrr', 29), ('armant', 25), ('ilakumar', 25), ('Weronica', 11), ('sen-lu', 6), ('aharelick', 5), ('HanaPearlman', 1), ('waffle-iron', 1)","[10, 'Reduced Inequalities']"
muzima/muzima-android,Android app for muzima,"# mUzima for Health Providers
An adaptable, open source, android-based mHealth platform whose current version interoperates seamlessly with the OpenMRS &copy;
Electronic Health Record System.
",,2024-04-29T18:36:47Z,26,7,20,"('mssavai', 735), ('benamonya', 665), ('ztthibaut', 241), ('prasann', 238), ('samuelowino', 226), ('sthaiya', 174), ('zhangruimin', 149), ('shashanktomar', 130), ('rezita', 111), ('ShwethaThammaiah', 58), ('nribeka', 58), ('akanim', 55), ('ialuj', 43), ('voloide', 33), ('vikaspandeysahaj', 18), ('patryllus', 15), ('Reagan', 11), ('kirang20', 7), ('mpmysight', 7), ('yoctopus', 5), ('mbjoan', 3), ('Kibnelson', 3), ('corneliouzbett', 3), ('adakyeung', 1), ('WanjiruCate', 1), ('vijayakumarn', 1)","[5, 'Gender Equality']"
PecanProject/pecan,The Predictive Ecosystem Analyzer (PEcAn) is an integrated ecological bioinformatics toolbox.,"[![GitHub Actions CI](https://github.com/PecanProject/pecan/workflows/CI/badge.svg)](https://github.com/PecanProject/pecan/actions)
[![Slack](https://img.shields.io/badge/slack-login-green.svg)](https://pecanproject.slack.com/)
[![Slack](https://img.shields.io/badge/slack-join_chat-green.svg)](https://join.slack.com/t/pecanproject/shared_invite/enQtMzkyODUyMjQyNTgzLWEzOTM1ZjhmYWUxNzYwYzkxMWVlODAyZWQwYjliYzA0MDA0MjE4YmMyOTFhMjYyMjYzN2FjODE4N2Y4YWFhZmQ)
[![DOI](https://zenodo.org/badge/4469/PecanProject/pecan.svg)](https://zenodo.org/badge/latestdoi/4469/PecanProject/pecan)

## Our Vision

#### Ecosystem science, policy, and management informed by the best available data and models

## Our Mission

#### Develop and promote accessible tools for reproducible ecosystem modeling and forecasting

## What is PEcAn?

The Predictive Ecosystem Analyzer (PEcAn) (see [pecanproject.org](http://pecanproject.org)) is an integrated ecological bioinformatics toolbox (Dietze et al 2013, LeBauer et al, 2013) that consists of: 1) a scientific workflow system to manage the immense amounts of publicly-available environmental data and 2) a Bayesian data assimilation system to synthesize this information within state-of-the-art ecosystems models. This project is motivated by the fact that many of the most pressing questions about global change are not necessarily limited by the need to collect new data as much as by our ability to synthesize existing data. This project seeks to improve this ability by developing a accessibe framework for integrating multiple data sources in a sensible manner.

The PEcAn workflow system allows ecosystem modeling to be more reproducible, automated, and transparent in terms of operations applied to data, and thus ultimately more comprehensible to both peers and the public. It reduces the redundancy of effort among modeling groups, facilitate collaboration, and makes models more accessible the rest of the research community.

PEcAn is not itself an ecosystem model, and it can be used to with a variety of different ecosystem models; integrating a model involves writing a wrapper to convert inputs and outputs to and from the standards used by PEcAn. Currently, PEcAn supports over a dozen ecosystem models, with more being added all the time (see the _models_ folder for the most up-to-date list)

## Documentation

Consult documentation of the PEcAn Project; either the [latest stable development](https://pecanproject.github.io/pecan-documentation/develop/) branch, the latest [release](https://pecanproject.github.io/pecan-documentation/master/). Documentation from [earlier releases is here](https://pecanproject.github.io/documentation.html).

## Getting Started

See our [""Tutorials Page""](https://pecanproject.github.io/tutorials.html) that provides self-guided tutorials, links to vignettes, and an overview presentation.

### Installation

Complete instructions on how to install PEcAn can be found in the [documentation here](https://pecanproject.github.io/pecan-documentation/develop/pecan-manual-setup.html). To get PEcAn up and running you can use one of three methods:

1. Run a [Virtual Machine](https://pecanproject.github.io/pecan-documentation/develop/install-vm.html#install-vm). This is recommended for students and new users, and provides a consistent, tested environment for each release.

2. Use [Docker](https://pecanproject.github.io/pecan-documentation/develop/install-docker.html#install-docker). This is recommended, especially for development and production deployment.

3. Install all of the PEcAn R packages on your own Linux or MacOS computer or server. This can be done by [installing from r-universe](https://pecanproject.github.io/pecan-documentation/develop/r-universe.html):

```R
# Enable repository from pecanproject
options(repos = c(
  pecanproject = 'https://pecanproject.r-universe.dev',
  CRAN = 'https://cloud.r-project.org'))
# Download and install PEcAn.all in R
install.packages('PEcAn.all')
```

This, however, may have limited functionality without also installing other components of PEcAn, in particular [BETYdb](https://pecanproject.github.io/pecan-documentation/develop/osinstall.html#install-bety).

### Website

Visit our [webpage](https://pecanproject.github.io) to keep up with latest news, version, and information about the PEcAn Project

#### Web Interface demo

The fastest way to begin modeling ecosystems is through the PEcAn web interface.  
We have a [demo website](http://pecan.ncsa.illinois.edu/pecan/01-introduction.php) that runs the current version of PEcAn. Using this instance you can perform a run using either ED or SIPNET at any of the predefined sites.

The demo instance only allows for runs at pecan.ncsa.illinois.edu. Once you have set up the run it will execute on our server; depending on the number of people executing a model and the model selected this can take between a few seconds and a few minutes to finish. Once it's finished, you see the results of the execution and can plot the outputs of the model. Complete examples of a few executions can be found in our online [tutorials](http://pecanproject.github.io/tutorials.html).

## Publications

* LeBauer, D.S., D. Wang, K. Richter, C. Davidson, and M.C. Dietze (2013). Facilitating feedbacks between field measurements and ecosystem models. Ecological Monographs. [doi:10.1890/12-0137.1](https://doi.org/10.1890/12-0137.1)
* Wang, D, D.S. LeBauer, and M.C. Dietze (2013). Predicting yields of short-rotation hybrid poplar (Populus spp.) for the contiguous US through model-data synthesis. Ecological Applications [doi:10.1890/12-0854.1](https://doi.org/10.1890/12-0854.1)
* Dietze, M.C., D.S LeBauer, and R. Kooper (2013). On improving the communication between models and data. Plant, Cell, & Environment [doi:10.1111/pce.12043](https://doi.org/10.1111/pce.12043)
* Dietze, Michael C., Shawn P. Serbin, Carl Davidson, Ankur R. Desai, Xiaohui Feng, Ryan Kelly, Rob Kooper et al. ""A quantitative assessment of a terrestrial biosphere model's data needs across North American biomes."" Journal of Geophysical Research: Biogeosciences 119, no. 3 (2014): 286-300.
* Viskari, Toni, Brady Hardiman, Ankur R. Desai, and Michael C. Dietze. ""Model-data assimilation of multiple phenological observations to constrain and predict leaf area index."" (2015) [doi:10.1890/14-0497.1](https://doi.org/10.1890/14-0497.1)
* Shiklomanov. A, MC Dietze, T Viskari, PA Townsend, SP Serbin. 2016 ""Quantifying the influences of spectral resolution on uncertainty in leaf trait estimates through a Bayesian approach to RTM inversion"" Remote Sensing of the Environment 183: 226-238
* LeBauer, David, Rob Kooper, Patrick Mulrooney, Scott Rohde, Dan Wang, Stephen P. Long, and Michael C. Dietze. ""BETYdb: a yield, trait, and ecosystem service database applied to second‐generation bioenergy feedstock production."" GCB Bioenergy (2017).

A extensive list of publications that apply PEcAn or are informed by our work on [Google Scholar](https://scholar.google.com/citations?hl=en&user=HWhxBY4AAAAJ).

## Acknowledgements

The PEcAn project is supported by the National Science Foundation (ABI #1062547, ABI #1458021, DIBBS #1261582, ARC #1023477, EF #1318164, EF #1241894, EF #1241891), NASA Terrestrial Ecosystems, the Energy Biosciences Institute, Department of Energy (ARPA-E awards #DE-AR0000594 and DE-AR0000598), and an Amazon AWS in Education Grant.

Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, NASA, or other federal agencies. PEcAn is a collaboration among research groups at the Department of Earth And Environment at Boston University, the Carl Woese Institute for Genomic Biology at the University of Illinois, the Image Spatial Data Analysis group at the National Center for Supercomputing Applications, the Department of Atmospheric & Oceanic Sciences at the University Wisconsin-Madison, and the Terrestrial Ecosystem Science & Technology group at Brookhaven National Lab.

BETYdb is a product of the Energy Biosciences Institute at the University of Illinois at Urbana-Champaign. We gratefully acknowledge the great effort of other researchers who generously made their own data available for further study.

## License

University of Illinois/NCSA Open Source License

Copyright (c) 2012, University of Illinois, NCSA.  All rights reserved.

PEcAn project


Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the  ""Software""), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.
* Neither the names of University of Illinois, NCSA, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON INFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF  CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.
","'bayesian', 'cyberinfrastructure', 'data-assimilation', 'data-science', 'ecosystem-model', 'ecosystem-science', 'forecasting', 'meta-analysis', 'national-science-foundation', 'pecan', 'plants', 'r'",2024-04-29T18:40:25Z,30,198,38,"('mdietze', 3036), ('dlebauer', 2455), ('robkooper', 1559), ('ashiklom', 1497), ('infotroph', 1426), ('istfer', 1375), ('tonygardella', 1019), ('bcow', 712), ('para2x', 694), ('Aariq', 443), ('LiamBurke24', 343), ('araiho', 342), ('meetagrawal09', 295), ('DongchenZ', 283), ('bpbond', 279), ('moki1202', 260), ('ayushprd', 238), ('annethomas', 228), ('Luke-Dramko', 226), ('kzarada', 225), ('nanu1605', 218), ('ankurdesai', 174), ('jam2767', 106), ('crollinson', 99), ('Amanskywalker', 99), ('tezansahu', 96), ('MukulMaheshwari', 79), ('apourmok', 77), ('tobeycarman', 67), ('rahul799', 63)","[13, 'Climate Action']"
RefugeRestrooms/refugerestrooms-ios,"[DECOMMISSIONED] iOS app for Refuge Restrooms, crowdsourcing safe restrooms for the trans community","Refuge Restrooms for iOS
========================

[![License](https://img.shields.io/badge/license-AGPL-lightgrey.svg)](https://raw.githubusercontent.com/RefugeRestrooms/refugerestrooms-ios/master/LICENSE)
![iOS](https://img.shields.io/badge/platform-ios-lightgrey.svg)

The Refuge iOS app was decommissioned as of 2020. See [Issue#156](https://github.com/RefugeRestrooms/refugerestrooms-ios/issues/156)

To view and submit restrooms use [refugerestrooms.org](https://refugerestrooms.org)

## Credits

Refuge Restrooms was written by a group of awesome [contributors](https://github.com/RefugeRestrooms/refugerestrooms-ios/contributors).

### API

Refuge Restrooms uses the [Refuge Restrooms API](http://www.refugerestrooms.org/api/docs/).

### Cocoapods

[ADClusterMapView](https://github.com/applidium/ADClusterMapView)

[AFNetworking](https://github.com/AFNetworking/AFNetworking)

[EAIntroView](https://github.com/ealeksandrov/EAIntroView)

[HNKGooglePlacesAutocomplete](https://github.com/hkellaway/HNKGooglePlacesAutocomplete)

[Mantle](https://github.com/Mantle/Mantle)

[Reachability](https://github.com/tonymillion/Reachability)

## License

Refuge Restrooms for iOS is licenced under the GNU Affero General Public License. See the [LICENSE](https://raw.githubusercontent.com/RefugeRestrooms/refugerestrooms-ios/master/LICENSE) file for more info.
","'ios-app', 'lgbt', 'objective-c', 'refuge', 'refuge-restrooms', 'safe-restrooms', 'trans', 'transgender'",2023-05-09T17:11:02Z,6,21,12,"('k-giardchase', 55), ('hkellaway', 50), ('nataliegroman', 4), ('tkwidmer', 3), ('benreyn', 1), ('mi-wood', 1)","[5, 'Gender Equality']"
codepath/android_guides,Extensive Open-Source Guides for Android Developers,"# CodePath Android Cliffnotes

Welcome to the open-source [Codepath](http://codepath.com) Android Cliffnotes! Our goal is to become the **central crowdsourced resource** for complete and up-to-date Android content and tutorials. [Just take me to the notes](https://github.com/codepath/android_guides/wiki#getting-started)!

[![CodePath](http://i.imgur.com/XgxWfyF.png)](http://codepath.com)

We have guides for everyone whether you are **beginner, intermediate or advanced**. Want to learn how to [use the ActionBar](https://github.com/codepath/android_guides/wiki/Defining-The-ActionBar) or the [ins and outs of fragments](https://github.com/codepath/android_guides/wiki/Creating-and-Using-Fragments)? We got that. Want to learn [about testing](https://github.com/codepath/android_guides/wiki/Android-Testing-Options) or how to [build flexible user interfaces for multiple devices](https://github.com/codepath/android_guides/wiki/Flexible-User-Interfaces)? We got you covered. We don't waste time with the ""theoretical approach"" from a book. We cover **exactly the things we use every day** as we are developing apps for contracts.

**Need Help?** Please join the [google groups](https://groups.google.com/forum/#!forum/codepath-android-guides) for these guides where you can post related questions.

## Motivation

Ever been **frustrated finding information on outdated one-off blog posts and tutorials** that has since become irrelevant? How many times have you been googling only to find your answer on a **2 year old Stack Overflow post**? We believe there's got to be a better way. Why not have a community to work together in creating useful and detailed documentation for every aspect of Android development(or any platform)? 

Read about our [mission to change the way engineers learn new technologies](https://github.com/codepath/android_guides/wiki/The-CodePath-Goal) and we would love for you to [get involved](https://github.com/codepath/android_guides/wiki/The-CodePath-Goal#how-do-i-help)! In addition, we are a fledgling startup so if you like this guide and what we are trying to do, please consider following us on twitter [@codepath](https://twitter.com/codepath)! 

## Live in San Francisco?

Located in the San Francisco Bay Area and interested in learning with others in a more structured program? Check out our local [Android](http://www.meetup.com/Learning-Android-Development) or [iOS](http://www.meetup.com/Learning-iOS-Development-SF/) meetup events. We have free evening events and at-cost 1-day workshops to make learning social and connect you with others passionate about mobile.

If you are an experienced engineer (2+ years of professional experience in software development) and serious about learning Android, check out our [free evening 8-week Android bootcamp](http://codepath.com/androidbootcamp). Learn how to build mobile apps while collaborating with other engineers and designers. Work on solving important problems for non-profits with our free 8-week accelerated evening mobile boot camp. [Learn more and apply here](http://courses.codepath.com/snippets/intro_to_android/about_bootcamp).

## We Need Your Help!

We need your help making the guides even better. In particular, here are the easiest ways to contribute:

1. **Update Guides.** Review existing guides and update outdated content, add tips or add/update images
2. **Fill Out Guides.** Find guides that are [simply stubs](https://github.com/codepath/android_guides/issues/2) and fill them out with content.
3. **Create New Guides.** Review the [missing topics list](https://github.com/codepath/android_guides/issues/2) and create new topic guides.

We [maintain a master missing topics list](https://github.com/codepath/android_guides/issues/2) that contains the most important missing topics. Also, look for items in the cliff notes with the **Needs Attention** mark which indicates the guide needs some love. If you see a topic you'd like added, please check the [issues](https://github.com/codepath/android_guides/issues) for this repository to let us know.

If you are interested in contributing to our guides, please check out our [contribution guidelines](https://github.com/codepath/android_guides/wiki/Contributing-Guidelines) first.

## Contributors

These guides were originally created and adapted by [Nathan Esquenazi](https://github.com/nesquena) 
as a part of our [CodePath](http://codepath.com) training and bootcamps. We have also had contributions from many
community members including:

 * [Nidhi Shah](https://github.com/nidhi1608) (CodePath Alumni and Staff)
 * [Roger Hu](https://github.com/rogerhu) (CodePath Alumni and Instructor)
 * [Nick Aiwazian](https://github.com/nickai) (CodePath Alumni and Instructor)
 * [Kevin Leong](https://github.com/kgleong) (CodePath Alumni and Instructor)
 * [Michael Alan Huff](https://github.com/koalahamlet) (CodePath Alumni and Mentor)
 * [Vibhor Bharadwaj](https://github.com/vibhorB) (CodePath Alumni and Mentor)
 * [Ari Lacenski](https://github.com/tensory) (CodePath Alumni)
 * [Chunyan Song](https://github.com/chunyan) (CodePath Alumni)
 * [Vishal Kapoor](https://github.com/kapoor) (CodePath Alumni)
 * [Trevor Elkins](https://github.com/trevor-e)
 * [Adrian Romero](https://github.com/romeroadrian)
 * [Aaron Fleshner](https://github.com/adfleshner)
 * [Steven Dobek](https://github.com/sdobek)
","'android', 'codepath', 'development', 'guides', 'tutorials'",2023-05-16T18:30:41Z,10,28255,2106,"('nesquena', 45), ('CPZackParker', 2), ('kgleong', 2), ('ABHISHEK-AMRUTE', 1), ('H3RSKO', 1), ('eltociear', 1), ('jc4p', 1), ('rogerhu', 1), ('flekken', 1), ('Sammug', 1)","[4, 'Quality Education']"
nubianvr-source/BasicElectronics,,"# BasicElectronics
Setting up the Basic Electronics Demo
This Demo is a demonstration of an implementation of the Basic Electronics in VR made with the Unity Game Engine and created for the Oculus Go Device. Along with the document is an APK that is going to be transferred to the Oculus Go.

Game Engine:
The version of Unity used for this project is Unity 2018.3.6f1

Windows Users
Steps to Transferring the APK to the Oculus Go
Enable Developer Mode
First Go to Oculus Dashboard website and create an organization. The name of the Organisation can be anything https://dashboard.oculus.com/organization/create/ if you are not logged in, login with your facebook account linked to the Oculus Device.

Go the Oculus App on your phone, Make sure your Oculus Go device is turned on though. 
Head to Settings on the Oculus App on the phone. 
Click on the Oculus Device to confirm its connected to the device.
Click on more settings and choose Developer mode
Click on the switch to enable the developer mode



Android Device Bridge
Download the content in the dropbox. This contains the files need to the Android Device Bridge. https://www.dropbox.com/s/pk4e0lmwim5vgi0/Adb.rar?dl=0
Unzip and Paste the platform-tools folder you have in a location Downloads

Connect your Oculus Device(...press Enter after you type.)
Go to your command prompt
Type: cd Downloads 
Type: adb devices
If you see a device under List devices attached, it means it has detected your Oculus Device.
Now wear the Headset and use your Oculus Controller to authorize connection to the PC you are using..
Type: adb devices again and you will realize you will see the serial number of the VR headset connected.
Copy the *Basic Electronics.apk* to the platform-tools folder. The folder that has the adb file.
Type adb install Basic_Electronics.apk
When you see success, you are done
In your VR headset, go to Library, and then Unknown Sources to find your app there and then run it.

Connecting to Vysor.
Download and install Vysor to your PC. (https://www.vysor.io/download/)
Install the application downloaded.
Run it and you will see this Dialog box. below
With the Head set connect click on Find Devices and it should find it in a sec.
Click on the View button to see a mirrored view of the content in the headset.


Mac Users (https://headjack.io/tutorial/sideload-install-app-apk-oculus-go-quest/)
Open Terminal
Paste ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" to install homebrew .. this takes a while to install so be patient
After that, Paste brew cask install android-platform-tools
When this is done, plug in your Headset and type, adb devices. If it shows up as unauthorize, check your headset and authorize the device.
Now copy the apk to the downloads folder your mac and then from your terminal, type cd Downloads 
Type adb install Basic_Electronics.apk
success!






https://www.youtube.com/watch?v=Eiz5WKObDeA - This Youtube provides a guide to complete the same task for windows and Mac
",,2019-11-23T22:57:51Z,1,0,1,"('sabetech', 41)","[4, 'Quality Education']"
OpenSourcePolitics/decidim,Fork of Decidim,"

The participatory democracy framework.

> Free Open-Source participatory democracy, citizen participation and open government for cities and organizations

[Decidim](https://decidim.org) is a participatory democracy framework, written in Ruby on Rails, originally developed for the Barcelona City government online and offline participation website. Installing these libraries will provide you a generator and gems to help you develop web applications like the ones found on [example applications](#example-applications) or like [our demo application](http://staging.decidim.codegram.com).

All members of the Decidim community agree with [Decidim Social Contract or Code of Democratic Guarantees](http://www.decidim.org/contract/).

---

[![Gem](https://img.shields.io/gem/v/decidim.svg)](https://rubygems.org/gems/decidim)
[![Gem](https://img.shields.io/gem/dt/decidim.svg)](https://rubygems.org/gems/decidim)
[![GitHub contributors](https://img.shields.io/github/contributors/decidim/decidim.svg)](https://github.com/decidim/decidim/graphs/contributors)
[![Yard Docs](http://img.shields.io/badge/yard-docs-blue.svg)](http://rubydoc.info/github/decidim/decidim/master)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/decidim/decidim)

Code quality

[![codecov](https://codecov.io/gh/decidim/decidim/branch/develop/graph/badge.svg)](https://codecov.io/gh/decidim/decidim)
[![Maintainability](https://api.codeclimate.com/v1/badges/ad8fa445086e491486b6/maintainability)](https://codeclimate.com/github/decidim/decidim/maintainability)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/decidim/localized.svg)](https://crowdin.com/project/decidim)
[![Inline docs](http://inch-ci.org/github/decidim/decidim.svg?branch=master)](http://inch-ci.org/github/decidim/decidim)
[![Accessibility issues](https://rocketvalidator.com/badges/a11y_issues.svg?url=http://staging.decidim.codegram.com/)](https://rocketvalidator.com/badges/link?url=http://staging.decidim.codegram.com/&report=a11y)
[![HTML issues](https://rocketvalidator.com/badges/html_issues.svg?url=http://staging.decidim.codegram.com/)](https://rocketvalidator.com/badges/link?url=http://staging.decidim.codegram.com/&report=html)

Test suite


  See all

[![Accountability](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Accountability/develop.svg?label=%5BCI%5D%20Accountability)](https://github.com/decidim/decidim/actions)
[![Admin](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Admin/develop.svg?label=%5BCI%5D%20Admin)](https://github.com/decidim/decidim/actions)
[![Api](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Api/develop.svg?label=%5BCI%5D%20Api)](https://github.com/decidim/decidim/actions)
[![Assemblies](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Assemblies/develop.svg?label=%5BCI%5D%20Assemblies)](https://github.com/decidim/decidim/actions)
[![Blogs](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Blogs/develop.svg?label=%5BCI%5D%20Blogs)](https://github.com/decidim/decidim/actions)
[![Budgets](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Budgets/develop.svg?label=%5BCI%5D%20Budgets)](https://github.com/decidim/decidim/actions)
[![Comments](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Comments/develop.svg?label=%5BCI%5D%20Comments)](https://github.com/decidim/decidim/actions)
[![Conferences](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Conferences/develop.svg?label=%5BCI%5D%20Conferences)](https://github.com/decidim/decidim/actions)
[![Consultations](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Consultations/develop.svg?label=%5BCI%5D%20Consultations)](https://github.com/decidim/decidim/actions)
[![Core](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Core/develop.svg?label=%5BCI%5D%20Core)](https://github.com/decidim/decidim/actions)
[![Debates](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Debates/develop.svg?label=%5BCI%5D%20Debates)](https://github.com/decidim/decidim/actions)
[![Forms](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Forms/develop.svg?label=%5BCI%5D%20Forms)](https://github.com/decidim/decidim/actions)
[![Generators](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Generators/develop.svg?label=%5BCI%5D%20Generators)](https://github.com/decidim/decidim/actions)
[![Initiatives](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Initiatives/develop.svg?label=%5BCI%5D%20Initiatives)](https://github.com/decidim/decidim/actions)
[![Main](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Main%20folder/develop.svg?label=%5BCI%5D%20Main)](https://github.com/decidim/decidim/actions)
[![Meetings](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Meetings/develop.svg?label=%5BCI%5D%20Meetings)](https://github.com/decidim/decidim/actions)
[![Pages](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Pages/develop.svg?label=%5BCI%5D%20Pages)](https://github.com/decidim/decidim/actions)
[![Participatory processes](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Participatory%20processes/develop.svg?label=%5BCI%5D%20Participatory%20processes)](https://github.com/decidim/decidim/actions)
[![Proposals (system admin)]()](https://github.com/decidim/decidim/actions)
[![Proposals (system public)]()](https://github.com/decidim/decidim/actions)
[![Proposals (unit tests)]()](https://github.com/decidim/decidim/actions)
[![Sortitions](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Sortitions/develop.svg?label=%5BCI%5D%20Sortitions)](https://github.com/decidim/decidim/actions)
[![Surveys](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Surveys/develop.svg?label=%5BCI%5D%20Surveys)](https://github.com/decidim/decidim/actions)
[![System](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20System/develop.svg?label=%5BCI%5D%20System)](https://github.com/decidim/decidim/actions)
[![Verifications](https://img.shields.io/github/workflow/status/decidim/decidim/%5BCI%5D%20Verifications/develop.svg?label=%5BCI%5D%20Verifications)](https://github.com/decidim/decidim/actions)



---

# What do you need to do?

* [Getting started with Decidim](#getting-started-with-decidim)
* [Contribute to the project](#how-to-contribute)
* [Modules](#modules)
* [Create & browse development app](#browse-decidim)

---

## Getting started with Decidim

TLDR: install gem, generate a Ruby on Rails app, enjoy.

```console
gem install decidim
decidim decidim_application
```

We've set up a guide on how to install, set up and upgrade Decidim. See the [Getting started guide](https://github.com/decidim/decidim/blob/master/docs/getting_started.md).

## How to contribute

See [Contributing](CONTRIBUTING.md).

### Browse Decidim

After you create a development app (`bundle exec rake development_app`), you
have to switch to it and boot the rails server with `cd development_app &&
bundle exec rails s`.

After that, you can:

* Browse the main interface at `http://localhost:3000`, and log in as: user@example.org | decidim123456
* Browse the admin interface at `http://localhost:3000/admin`, and log in as: admin@example.org | decidim123456
* Browse the system interface at `http://localhost:3000/system`, and log in as: system@example.org | decidim123456

Also, if you want to verify yourself against the default authorization handler use a document number ended with ""X"".

## Modules

If you need to have some features that we don't have yet, we recommend that you make a module. This is a Ruby on Rails engine with some APIs specific to Decidim (for registering with the menus, integration with spaces like Participatory Processes or Assemblies, with /admin or /api, etc).

As a base you can use these modules, although check first that the version is compatible with your current Decidim version. Also you should know that until v1.0.0 We're under development and these internal APIs can change. We recommend that you extensively test your module.

See [Modules page on Decidim.org](https://decidim.org/modules).

### Authorizations Strategies

One specific thing regarding these kind of applications is the [authorization/verification](decidim-verifications/README.md) logic. Here are some examples:

* [Barcelona (City)](https://github.com/AjuntamentdeBarcelona/decidim-barcelona/blob/master/app/services/census_authorization_handler.rb)
* [Calafell](https://github.com/AjuntamentdeCalafell/decidim-calafell/blob/master/app/services/census_authorization_handler.rb)
* [DIBA (Barcelona Province)](https://github.com/diputacioBCN/decidim-diba/blob/master/decidim-diba_census_api/app/services/diba_census_api_authorization_handler.rb)
* [Gavà](https://github.com/AjuntamentDeGava/decidim-gava/blob/master/app/services/census_authorization_handler.rb)
* [Hospitalet de Llobregat](https://github.com/HospitaletDeLlobregat/decidim-hospitalet/blob/master/app/services/census_authorization_handler.rb)
* [Malgrat de Mar](https://github.com/AjMalgrat/decidim-malgrat/blob/master/app/services/carpetaciutada_handler.rb)
* [Mataró](https://github.com/AjuntamentDeMataro/decidimmataro.cat/blob/master/app/services/census_authorization_handler.rb)
* [Pamplona](https://github.com/ErabakiPamplona/erabaki/blob/master/app/services/census_authorization_handler.rb)
* [Reus](https://github.com/AjuntamentdeReus/decidim/blob/master/app/services/census_authorization_handler.rb)
* [Sabadell](https://github.com/AjuntamentDeSabadell/decidim-sabadell/blob/master/app/services/census_authorization_handler.rb)
* [Sant Cugat](https://github.com/AjuntamentdeSantCugat/decidim-sant_cugat/blob/master/app/services/census_authorization_handler.rb)
* [Terrassa](https://github.com/AjuntamentDeTerrassa/decidim-terrassa/blob/master/app/services/census_authorization_handler.rb)
* [Vilanova i la Geltrú](https://github.com/vilanovailageltru/decidim-vilanova/blob/master/app/services/vilanova_authorization_handler.rb)

Other special verifications:

* [Podemos](https://github.com/podemos-info/participa2/tree/master/decidim-module-census_connector)
* [FundAction](https://github.com/ElectricThings/fund_action/blob/master/app/services/anybody_authorization_handler.rb)
* [CSV emails](https://github.com/CodiTramuntana/decidim-verifications-csv_emails)
* [Access Requests](https://github.com/mainio/decidim-module-access_requests)

## Following our license

If you plan to release your application you'll need to publish it using the same license: GPL Affero 3. We recommend doing that on GitHub before publishing, you can read more on ""[Being Open Source From Day One is Especially Important for Government Projects](http://producingoss.com/en/governments-and-open-source.html#starting-open-for-govs)"". If you have any trouble you can contact us on [Gitter](https://gitter.im/decidim/decidim).

## Example applications

Since Decidim is a ruby gem, you can check out the [dependent repositories](https://github.com/decidim/decidim/network/dependents?type=application) to see how many applications are on the wild or tests that other developers have made. Here's a partial list with some of the projects that have used Decidim:

* [Demo](http://staging.decidim.codegram.com)
* [Decidim Barcelona](https://decidim.barcelona) - [View code](https://github.com/AjuntamentdeBarcelona/decidim-barcelona)
* [L'H ON Participa](https://www.lhon-participa.cat) - [View code](https://github.com/HospitaletDeLlobregat/decidim-hospitalet)
* [Decidim Terrassa](https://participa.terrassa.cat) - [View code](https://github.com/AjuntamentDeTerrassa/decidim-terrassa)
* [Decidim Sabadell](https://decidim.sabadell.cat) - [View code](https://github.com/AjuntamentDeSabadell/decidim-sabadell)
* [Decidim Gavà](https://participa.gavaciutat.cat) - [View code](https://github.com/AjuntamentDeGava/decidim-gava)
* [Decidim Sant Cugat](https://decidim.santcugat.cat/) - [View code](https://github.com/AjuntamentdeSantCugat/decidim-sant_cugat)
* [Vilanova Participa](http://participa.vilanova.cat) - [View code](https://github.com/vilanovailageltru/decidim-vilanova)
* [Erabaki Pamplona](https://erabaki.pamplona.es) - [View code](https://github.com/ErabakiPamplona/erabaki)
* [Decidim Mataró](https://www.decidimmataro.cat) - [View code](https://github.com/AjuntamentDeMataro/decidim-mataro)
* [MetaDecidim](https://meta.decidim.barcelona/) - [View Code](https://github.com/decidim/metadecidim)

## Security

Security is very important to us. If you have any issue regarding security, please disclose the information responsibly by sending an email to security [at] decidim [dot] org and not by creating a github/metadecidim issue. We appreciate your effort to make Decidim more secure. See [full security policy](SECURITY.md).
",'hacktoberfest',2024-04-25T15:09:03Z,30,20,13,"('mrcasals', 651), ('josepjaume', 527), ('deivid-rodriguez', 382), ('oriolgual', 307), ('decidim-bot', 270), ('beagleknight', 188), ('tramuntanal', 148), ('lastpotion', 70), ('leio10', 68), ('agustibr', 67), ('andreslucena', 66), ('Crashillo', 63), ('microstudi', 61), ('ahukkanen', 50), ('rbngzlv', 45), ('aitorlb', 37), ('armandfardeau', 35), ('ItsGenis', 33), ('entantoencuanto', 22), ('MarcReniu', 17), ('mijailr', 14), ('jsperezg', 13), ('Leusev', 13), ('Digharatta', 11), ('jkraemer', 10), ('davidbeig', 10), ('jesusdb', 8), ('jmnzdz', 8), ('PierreMesure', 7), ('imgbotbot', 7)","[16, 'Peace, Justice and Strong Institutions']"
drivendataorg/deon,A command line tool to easily add an ethics checklist to your data science projects.,"

[![tests](https://github.com/drivendataorg/deon/workflows/tests/badge.svg?branch=main)](https://github.com/drivendataorg/deon/actions?query=workflow%3A%22tests%22+branch%3Amain) [![codecov](https://codecov.io/gh/drivendataorg/deon/branch/main/graph/badge.svg)](https://codecov.io/gh/drivendataorg/deon) [![PyPI](https://img.shields.io/pypi/v/deon.svg)](https://pypi.org/project/deon/) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/deon.svg)](https://anaconda.org/conda-forge/deon)

 > [Read more about `deon` on the project homepage](http://deon.drivendata.org/)

------


An ethics checklist for data scientists


`deon` is a command line tool that allows you to easily add an ethics checklist to your data science projects. We support creating a new, standalone checklist file or appending a checklist to an existing analysis in [many common formats](#supported-file-types).

To help get started, `deon ` includes a default [Data Science Ethics Checklist](#data-science-ethics-checklist) along with a list of [real-world examples](http://deon.drivendata.org/examples/) connected with each item. Users can draw on the default list or develop their own.

---

**δέον** • (déon) [n.] (_Ancient Greek_) wikitionary
 > Duty; that which is binding, needful, right, proper.

--------

The conversation about ethics in data science, machine learning, and AI is increasingly important. The goal of `deon` is to push that conversation forward and provide concrete, actionable reminders to the developers that have influence over how data science gets done.

# Quickstart

You only need two lines of code to get started!

First, install deon:

```
$ pip install deon
```

Then, write out the [default checklist](#default-checklist) to a markdown file called `ETHICS.md`:

```
$ deon -o ETHICS.md
```

Dig into the checklist questions to identify and navigate the ethical considerations in your data science project.

For more configuration details, see the sections on [command line options](#command-line-options), [supported output file types](#supported-file-types), and [custom checklists](#custom-checklists).

# Background and perspective

We have a particular perspective with this package that we will use to make decisions about contributions, issues, PRs, and other maintenance and support activities.

First and foremost, our goal is not to be arbitrators of what ethical concerns merit inclusion. We have a [process for changing the default checklist](#changing-the-checklist), but we believe that many domain-specific concerns are not included and teams will benefit from developing [custom checklists](#custom-checklists). Not every checklist item will be relevant. We encourage teams to remove items, sections, or mark items as `N/A` as the concerns of their projects dictate.

Second, we built our initial list from a set of proposed items on [multiple checklists that we referenced](#checklist-citations). This checklist was heavily inspired by an article written by Mike Loukides, Hilary Mason, and DJ Patil and published by O'Reilly: [""Of Oaths and Checklists""](https://www.oreilly.com/ideas/of-oaths-and-checklists). We owe a great debt to the thinking that proceeded this, and we look forward to thoughtful engagement with the ongoing discussion about checklists for data science ethics.

Third, we believe in the power of examples to bring the principles of data ethics to bear on human experience. This repository includes a [list of real-world examples](http://deon.drivendata.org/examples/) connected with each item in the default checklist. We encourage you to contribute relevant use cases that you believe can benefit the community by their example. In addition, if you have a topic, idea, or comment that doesn't seem right for the documentation, please add it to the [wiki page](https://github.com/drivendataorg/deon/wiki) for this project!

Fourth, it's not up to data scientists alone to decide what the ethical course of action is. This has always been a responsibility of organizations that are part of civil society. This checklist is designed to provoke conversations around issues where data scientists have particular responsibility and perspective. This conversation should be part of a larger organizational commitment to doing what is right.

Fifth, we believe the primary benefit of a checklist is ensuring that we don't overlook important work. Sometimes it is difficult with pressing deadlines and a demand to multitask to make sure we do the hard work to think about the big picture. This package is meant to help ensure that those discussions happen, even in fast-moving environments. Ethics is hard, and we expect some of the conversations that arise from this checklist may also be hard.

Sixth, we are working at a level of abstraction that cannot concretely recommend a specific action (e.g., ""remove variable X from your model""). Nearly all of the items on the checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Because of this, most of the items are framed as prompts to discuss or consider. Teams will want to document these discussions and decisions for posterity.

Seventh, we can't define exhaustively every term that appears in the checklist. Some of these terms are open to interpretation or mean different things in different contexts. We recommend that when relevant, users create their own glossary for reference.

Eighth, we want to avoid any items that strictly fall into the realm of statistical best practices. Instead, we want to highlight the areas where we need to pay particular attention above and beyond best practices.

Ninth, we want all the checklist items to be as simple as possible (but no simpler), and to be actionable.

# Using this tool



## Prerequisites

 - Python >3.6: Your project need not be Python 3, but you need Python 3 to execute this tool.

## Installation

```
$ pip install deon
```

or

```
$ conda install deon -c conda-forge
```

## Simple usage

We recommend adding a checklist as the first step in your data science project. After creating your project folder, you could run:

```
$ deon -o ETHICS.md
```

This will create a markdown file called `ETHICS.md` that you can add directly to your project.

For simple one-off analyses, you can append the checklist to a Jupyter notebook or RMarkdown file using the `-o` flag to indicate the output file. `deon` will automatically append if that file already exists.

```
$ jupyter notebook my-analysis.ipynb

...

$ deon -o my-analysis.ipynb  # append cells to existing output file
```

This checklist can be used by individuals or teams to ensure that reviewing the ethical implications of their work is part of every project. The checklist is meant as a jumping-off point, and it should spark deeper and more thourough discussions rather than replace those discussions.

## Proudly display your Deon badge
You can add a Deon badge to your project documentation, such as the README, to encourage wider adoption of these ethical practices in the data science community.

### HTML badge
```html

    

```

### Markdown badge

```
[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)
```

# Supported file types

Here are the currently supported file types. We will accept pull requests with new file types if there is a strong case for widespread use of that filetype.


- `.txt`: ascii
- `.html`: html
- `.ipynb`: jupyter
- `.md`: markdown
- `.rmd`: rmarkdown
- `.rst`: rst

# Command line options

```
Usage: deon [OPTIONS]

  Easily create an ethics checklist for your data science project.

  The checklist will be printed to standard output by default. Use the --output
  option to write to a file instead.

Options:
  -l, --checklist PATH  Override default checklist file with a path to a custom
                        checklist.yml file.
  -f, --format TEXT     Output format. Default is ""markdown"". Can be one of
                        [ascii, html, jupyter, markdown, rmarkdown, rst].
                        Ignored and file extension used if --output is passed.
  -o, --output PATH     Output file path. Extension can be one of [.txt, .html,
                        .ipynb, .md, .rmd, .rst]. The checklist is appended if
                        the file exists.
  -w, --overwrite       Overwrite output file if it exists. Default is False,
                        which will append to existing file.
  -m, --multicell       For use with Jupyter format only. Write checklist with
                        multiple cells, one item per cell. Default is False,
                        which will write the checklist in a single cell.
  --help                Show this message and exit.

```

# Default checklist



# Data Science Ethics Checklist

[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)

## A. Data Collection
 - [ ] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?
 - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?
 - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?
 - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?

## B. Data Storage
 - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?
 - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?
 - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?

## C. Analysis
 - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?
 - [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?
 - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?
 - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?
 - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?

## D. Modeling
 - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?
 - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?
 - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?
 - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?
 - [ ] **D.5 Communicate bias**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?

## E. Deployment
 - [ ] **E.1 Monitoring and evaluation**: How are we planning to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?
 - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?
 - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?
 - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?

*Data Science Ethics Checklist generated with [deon](http://deon.drivendata.org).*



# Custom checklists

This is not meant to be the only ethical checklist, but instead we try to capture reasonable defaults that are general enough to be widely useful. For your own projects with particular concerns, we recommend your own `checklist.yml` file that is maintained by your team and passed to this tool with the `-l` flag.

Custom checklists must follow the same schema as `checklist.yml`. There must be a top-level `title` which is a string, and `sections` which is a list. Each section in the list `sections` must have a `title`, a `section_id`, and then a list of `lines`. Each line must have a `line_id`, a `line_summary` which is a 1-3 word shorthand, and a `line` string which is the content. The format is as follows:

```
title: TITLE
sections:
  - title: SECTION TITLE
    section_id: SECTION NUMBER
    lines:
        - line_id: LINE NUMBER
          line_summary: LINE SUMMARY
          line: LINE CONTENT
```

# Changing the checklist

Please see [the framing](#background-and-perspective) for an understanding of our perspective. Given this perspective, we will consider changes to the default checklist that fit with that perspective and follow this process.

Our goal is to have checklist items that are actionable as part of a review of data science work or as part of a plan. Please avoid suggesting items that are too vague (e.g., ""do no harm"") or too specific (e.g., ""remove social security numbers from data"").

**Note: This process is an experiment and is subject to change based on how well it works.**

 A pull request to add an item should change:

  - [`deon/assets/checklist.yml`](https://github.com/drivendataorg/deon/blob/main/deon/assets/checklist.yml): contains the default checklist items
  - [`deon/assets/examples_of_ethical_issues.yml`](https://github.com/drivendataorg/deon/blob/main/deon/assets/examples_of_ethical_issues.yml): contains example of harms caused when the item was not considered

  The description in the pull request must include:

  - A justification for the change
  - A consideration of related items that already exist, and why this change is different from what exists
  - A published example (academic or press article) of where neglecting the principle has lead to concrete harm (articles that discuss potential or hypothetical harm will not be considered sufficient)

See detailed [contributing instructions here](https://github.com/drivendataorg/deon/blob/main/CONTRIBUTING.md).

# Discussion and commentary

In addition to this documentation, the [wiki pages for the GitHub repository](https://github.com/drivendataorg/deon/wiki) are enabled. This is a good place for sharing of links and discussion of how the checklsits are used in practice.

If you have a topic, idea, or comment that doesn't seem right for the documentation, please add it to the wiki!

# References, reading, and more

 A robust discussion of data ethics is important for the profession. The goal of this tool is to make it easier to implement ethics review within technical projects. There are lots of great resources if you want to think about data ethics, and we encourage you to do so!

## Checklist citations

We're excited to see so many articles popping up on data ethics! The short list below includes articles that directly informed the checklist content as well as a few case studies and thought-provoking pieces on the big picture.

- [Of oaths and checklists](https://www.oreilly.com/ideas/of-oaths-and-checklists)
- How to build ethics into AI ([Part I](https://medium.com/salesforce-ux/how-to-build-ethics-into-ai-part-i-bf35494cce9) and [Part II](https://medium.com/salesforce-ux/how-to-build-ethics-into-ai-part-ii-a563f3372447))
- [An ethical checklist for data science](https://dssg.uchicago.edu/2015/09/18/an-ethical-checklist-for-data-science/)
- [How to recognize exclusion in AI](https://medium.com/microsoft-design/how-to-recognize-exclusion-in-ai-ec2d6d89f850)
- [Case studies in data ethics](https://www.oreilly.com/ideas/case-studies-in-data-ethics)
- [Technology is biased too. How do we fix it?](https://fivethirtyeight.com/features/technology-is-biased-too-how-do-we-fix-it/)
- [The dark secret at the heart of AI](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)

## Where things have gone wrong

To make the ideas contained in the checklist more concrete, we've compiled [examples](http://deon.drivendata.org/examples/) of times when things have gone wrong. They're paired with the checklist questions to help illuminate where in the process ethics discussions may have helped provide a course correction.

We welcome contributions! Follow [these instructions](https://github.com/drivendataorg/deon/blob/main/CONTRIBUTING.md) to add an example.

## Related tools

There are other groups working on data ethics and thinking about how tools can help in this space. Here are a few we've seen so far:

- [Aequitas](https://dsapp.uchicago.edu/aequitas/) ([github](https://github.com/dssg/aequitas))
- [Ethical OS Toolkit](https://ethicalos.org/)
- [Ethics & Algorithms Toolkit: A risk management framework for governments](http://ethicstoolkit.ai/)
- Ethics and Data Science ([free ebook](https://www.amazon.com/dp/B07GTC8ZN7/ref=cm_sw_r_cp_ep_dp_klAOBb4Z72B4G)) and ([write-up](https://medium.com/@sjgadler/care-about-ai-ethics-what-you-can-do-starting-today-882a0e63d828))


-------

`deon` was created and is maintained by the team at [DrivenData](https://www.drivendata.org/). Our mission is to bring the power of data science to social impact organizations.","'data-ethics', 'data-science', 'ethics', 'machine-learning'",2024-04-29T21:20:59Z,13,273,15,"('ejm714', 36), ('pjbull', 25), ('jayqi', 20), ('manelsen', 5), ('glipstein', 4), ('ivergara', 2), ('dependabotbot', 2), ('andrewnc', 1), ('brylie', 1), ('caseyfitz', 1), ('IshaShah27', 1), ('mjschlauch', 1), ('koaning', 1)","[17, 'Partnerships for the Goals']"
MSH/eTBManager3,e-TB Manager is a web or desktop-based tool for managing all the information needed by national TB control programs. This tool was created by the USAID-funded Systems for Improved Access to Pharmaceutical and Services (SIAPS) Program implemented by Management Sciences for Health. See the README below for more info.," &nbsp;&nbsp; 

# e-TB Manager 3

The SIAPS Program is funded by the U.S. Agency for International Development (USAID) under cooperative agreement AID-OAA-A-11-00021 and implemented by Management Sciences for Health. The information provided on this web site is not official U.S. Government information and does not represent the views or positions of the U.S. Agency for International Development or the U.S. Government. 

Disclaimer of warranties and limitation of liability

The e-TB Manager software, documentation and other products, information, materials and services provided by Management Sciences for Health (MSH or Licensor) are provided “as is.” Licensor hereby disclaims all warranties, whether express, implied, statutory or other (including all warranties arising from course of dealing, usage or trade practice), and specifically disclaims all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement. Without limiting the foregoing, licensor makes no warranty of any kind that this software or documentation, or any other licensor or third-party goods, services, technologies or materials (including any software or hardware), or any products or results of the use of any of them, will meet the users’ or other persons' requirements, operate without interruption, achieve any intended result, be compatible or work with any other goods, services, technologies or materials (including any software, hardware, system or network), or be secure, accurate, complete, free of harmful code or error-free. Licensor is not responsible for further development or any future versions of e-TB Manager.

Copyright Management Sciences for Health.

## Goal

This repository contains the source code of e-TB Manager 3. Please note that this version has not been thoroughly tested yet.

e-TB Manager is a web-based tool for managing all the information needed by national TB control programs. It integrates data across all aspects of TB control, including information on suspects, patients, medicines, laboratory testing, diagnosis, treatment, and outcome.

## Getting started

In order to build e-TB Manager from the source code, you will need the following programs installed:

* Java JDK 1.8+
* Maven 3+

In order to work with client side development, it is highly recommended that you also install the following programs:

* Node 5.0+
* Gulp 3.8+

Development tools
* A development environment of your choice - Recommended: `IntelliJ` for server development, and `Sublime 3` for client development
* eslint, for client linting


Installation instructions can be found in the site of each one of these programs.

## Frameworks and libraries

Below is the list of frameworks and libraries used in e-TB Manager.

### Server side
* Spring boot (currently 1.2.6);
* Spring JPA, using Hibernate;
* Freemarker, as template engine;
* Liquibase, for database initialization;
* Dozer, for object to object mapping;
* Google Guava;

### Client side
* Twitter Bootstrap 3;
* React framework;
* React-bootstrap, to link react and bootstrap;
* Superagent, for Server requests;
* Underscore;
* Font-awesome 4;

All these frameworks are ***automatically*** loaded by the respective building tools (Maven and NPM), so the list is just a reference.

## Directories & files

Below is a list of the folders in the root directory of the source code:

* `src` - The server source code (used by Maven);
* `client` - The client source code (used by NodeJS);
* `resource` - Any file that is not part of the source, but it is important to keep them as templates, configuration examples, etc.

These are folders generated by the build tools (not to be included in the git repository:

* `target` - Folder where the final artifacts are generated (for instance, etbmanager.jar);
* `node` - Local version of NodeJS installed by Maven and used during the build process;
* `node_modules` - NodeJS modules installed by NPM and used in the client folder;
* `.idea` - Created by IntelliJ IDE, if used for development;

These are the main files in the root folder:

* `pom.xml` - Maven project file;
* `gulpfile.js` - Gulp project file, responsible for development, testing and build of the client code;
* `package.json` - List of dependencies used in the client side (required by NPM tool);
* `webpack.config.js` - Used by webpack tool for UI building and development;
* `.jshintrc` - Used by jshint for UI validation;
* `.gitignore` - List of files and folders to be ignored by git;


## Development environment

You need the following tools in order to start development:

* Git
* Java 1.8
* Maven 3+
* Node 5+
* Gulp 3.8+
* A development environment of your choice (IntelliJ recommended)

### Downloading the source code

The source code is stored in a Git repository, and the current Git URL is:

https://github.com/MSH/eTBmanager-3

To download the code, issue the git command

    git clone https://github.com/MSH/eTBmanager-3

You will need a user name and password for that.

Inside the repository, there are two main branches: `master` and `devel`. The master contains the stable version, and should be merged with stable versions achieved in the development branch. So, if you want to change the code, move to the devel branch:

    git checkout devel

When you finish your changes, perform the following git sequence (as described in the git documentation):

1. Add all changed files to be committed

    git add .

2. Commit the changes providing a good description

    git commit -m ""description of the changes""

3. Upload the changes to the remote server

    git push origin development



### Building from the source code

In order to generate a new version of e-TB Manager from the source code, you must issue the following Maven command:

    mvn clean package

This will install all necessary dependencies and generate a new version in `target/etbmanager-x.x.x.jar`, where x.x.x is the version number.

### Configuration file

Before running e-TB Manager, it is necessary to create a text-file called `etbmanager.properties` containing information about e-TB Manager  configuration.

This file is a text-based property file in the format value=key and must be in the same directory of the `ebmanager-xxx.jar` (or in the working directory, if set).

An example of this file can be found in the `resources` folder. These are the main properties available:

    db.url= # jdbc connection string
    db.user= # database user name
    db.password= # database password
    web.port = # the web server port to use

### Supported databases

For now, only two databases are supported:

* MySQL 5.5+ - Recommended when installing in a server computer;
* HSQLDB - Recommended when installing in a desktop computer for off-line and local usage;

Below are examples of connection strings:

#### MySQL

    db.url = jdbc:mysql://localhost/etbmanager

#### HSQLDB

    db.url = jdbc:hsqldb:file:database/etbmanager;default_schema=true

## Running e-TB Manager

In order to run e-TB Manager, just run it as any other java application:

    java -jar etbmanager-xxx.jar

or using Maven

    mvn clean package exec:exec

You must provide a configuration file etbmanager.properties, as described in the previous section.

When initialization finishes, open the URL below:

    http://localhost:8080/index.html

## Server side development

There is no restriction on the IDE in use, but the recommended one is IDEA IntelliJ.


### CheckStyle

The build tool uses [CheckStyle](http://checkstyle.sourceforge.net/) to guarantee that Java code follows code standards. CheckStyle rules are located in the `checkstyle.xml` file. To check if your code adheres to the rules, you may use maven for that:

    mvn checkstyle:check

**IDEA CheckStyle plugin** - There is a [CheckStyle plugin](https://plugins.jetbrains.com/plugin/1065) available, which makes it easier to display the CheckStyle issues.

Once installed, go to Preferences -> Other Setting -> CheckStyle, and include the file `checkstyle.xml` located in the project root folder.

## Client side development

e-TB Manager client side (browser code) uses `npm` and `gulp` as the building tool system. `npm` is used for dependency management, while `gulp` is used for running several tasks like the development server, testing the code, system building and much more.

Although not required, a group of tasks are available in `gulp` to make client side development easier. They are implemented in the `gulpfile.js` . The main command lines are:

**`npm run run`** - Prepare and run the client side on a proxy web server using gulp (the same as running `gulp run`). This proxy web-server has the following features:

* Provides client files (js, html, css, etc);
* Automatically update browser window when a file is changed;
* Proxy requests to the server-side (the server side must be running);
* Cache JS files in order to make quick rebuild on file changes;

**`gulp build`** - Prepare and build all client files and copy them to the folder `src\main\resources\static`.

**`gulp test`** - Run the UI tests (TO BE DONE);

### Sublime plugins

Sublime is a lightweight text editor widely used for Nodes and Javascript development. One of the strenght in Sublime is its plugin system. Although not required, it is recommended to use the following plugins:

* SublimeLinter
* SublimeLinter-contrib-eslint
* Babel
* ColorPicker
* DocBlockr
* LESS
* MarkdownEditing

`SublimeLinter` is a plugin for Sublime Text 3 that provides a framework for linting code, and `SublimeLinter-contrib-eslint` provides integration to the eslint tool, in order to provide JavaScript linting. Check how to install them and use them in Sublime.

 Inside the source code structure, `SublimeLinter-contrib-eslint` recognizes files called `.eslintrc` as the eslint configuration.

It is also necessary to install the following programs, in order to have the plugins working properly:

* [eslint](http://eslint.org/)
* [eslint-plugin-react](https://github.com/yannickcr/eslint-plugin-react)

### Atom plugins

Atom editor is another IDE that you can use to edit the client code. Please go to http://atom.io and follow instructions on how to install it.


## Testing

### JUnit tests

Server side tests are implemented using JUnit and Spring Boot under the `src/test/java` folder.

### Test configuration

When running test suites, the application will be started. When started in test mode, the application will search for the `etbmanager.properties` file in the `target/test` folder.

When executing tests using maven, it will automatically copy a configuration file from `resources/test/mysql/etbmanager.properties`.


### Executing the test
e-TB Manager follows the maven standard way of executing tests:

    mvn test

The source will be compiled and all tests will be executed.

### Other tests (TO BE DONE)

JUnit tests are not the only one available - The test suite also contains API call tests and UI tests.
",,2018-04-23T23:30:22Z,3,3,4,"('rmemoria', 696), ('mauricio-santos-deel', 572), ('JulieFrye', 17)","[17, 'Partnerships for the Goals']"
llaske/sugarizer,Sugarizer is a web implementation of the Sugar platform to run on any device or browser,"![](images/sugarizer_logo_with_text.svg)

# What is Sugarizer?

Sugarizer is a free/libre learning platform. The Sugarizer UI uses ergonomic principles from The [Sugar platform](https://sugarlabs.org/), developed for the One Laptop per Child project and used by more than 2 million children around the world.

Sugarizer runs on every device: laptops, desktops, tiny computers, tablets or smartphones.

Sugarizer includes a large set of pedagogic activities thought for children, see [here](https://sugarizer.org/activities.html) for a full list.

Sugarizer is available as:

* Application: an installable app for every operating system
* Web Application: a web application that runs in modern web browsers


# Sugarizer Application

Sugarizer Application is a cross-platform application for installation on any GNU+Linux, Windows, Mac OS, Android or iOS device.

To run **Sugarizer Application on Android**, download it on [Google Play](https://play.google.com/store/apps/details?id=org.olpc_france.sugarizer), [Amazon Store](http://www.amazon.com/gp/product/B00NKK7PZA) or [F-Droid](https://f-droid.org/repository/browse/?fdid=org.olpc_france.sugarizer).

Sugarizer on Android is also available as a launcher to replace the current launcher of your device so you can launch native Android applications from Sugarizer. You can download this Sugarizer version on [Google Play](https://play.google.com/store/apps/details?id=org.olpc_france.sugarizeros).

You could also build yourself the Sugarizer Application APK using the instructions below.







To run **Sugarizer Application on iOS**, download it on [Apple Store](https://itunes.apple.com/us/app/sugarizer/id978495303) or build yourself the Sugarizer Application IPA using the instructions below.



To run **Sugarizer Application on GNU Linux/Mac OS/Windows**, download it [here](https://sugarizer.org#desktop).
The Sugarizer desktop application has four possible arguments:

* `--window` to open Sugarizer in a window (instead of fullscreen)
* `--sdebug` to open Sugarizer with the debug console
* `--logoff` to logoff the previous user if one is connected (unsynchronized content will be lost)
* `--init` to remove all existing Journal and settings (all will be lost)

If you're a developer you could also launch Sugarizer desktop application using [electron](https://github.com/electron/electron). First, install Node.js and npm on your computer. See [here](http://nodejs.org/) for more information. Then install electron and specific modules for Sugarizer by running:

	npm install

Then launch Sugarizer for GNU Linux with:

	npm start > /dev/null

Or, for Mac OS/Windows, just:

	npm start

You could use Sugarizer desktop arguments using ""--"" after start. For example:

	npm start -- --window

To run locally **Sugarizer Application into the Web Browser** (GNU Linux/Mac OS/Windows), you should launch it with a special option to enable access to local files.

For **Chrome**, close ALL running instances of Chrome and re-launch it using the command line:

 	chrome --allow-file-access-from-files file:\\\\sugarizer\index.html

In the path above, `` is where you have stored/cloned your sugarizer repo into.

In Windows, \ is the way path is written and / is the way for Linux/MacOS backward and forward slashes differ here, hence path for Linux/MacOS will be:

	chrome --allow-file-access-from-files file:////sugarizer/index.html


On Windows, you should launch:

	""C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"" --allow-file-access-from-files 

On Mac OS, you should launch:

	open -n /Applications/Google\ Chrome.app --args --allow-file-access-from-files 

On Linux, you should launch:

	google-chrome-stable --allow-file-access-from-files 

> Note: `google-chrome-stable` is the name of Chrome in Ubuntu but it could be different on other distribution, you can get the package-name for Chrome by running `sudo dpkg -l | grep chrome`

For **Firefox**, type in the address bar:

    about:config

Search for the `security.fileuri.strict_origin_policy` parameter and
set it to `false`.

For **Safari** go to the `Safari/Preferences...` menu, under Advanced panel check the *Show develop menu in menu bar* box. Then from the `Develop` menu, select *Disable local file restrictions*.



# Sugarizer Web Application

[Try it now! (try.sugarizer.org)](http://try.sugarizer.org/)

Sugarizer Web App is a web application that runs on any device with a recent version of Chrome, Firefox or Safari browser.

As a web application, it does not run offline and requires a permanent network connection to a **Sugarizer Server**.

Sugarizer Server allows deployment of Sugarizer on a local server, for example on a school server, so exposes locally Web Application (without Internet access). Sugarizer Server can also be used to provide collaboration features for Sugarizer Application on the network.

To install your own Sugarizer Server, follow the instructions on
[Sugarizer Server repository](https://github.com/llaske/sugarizer-server)



# Architecture

If you're a developer and you want to learn more about Sugarizer architecture, see the dedicated page [here](docs/architecture.md).



# Activities distribution

All activities can be found in the [activities](activities) directory. Each activity has its own subdirectory. So for example, the *Abecedarium* activity is located in [activities/Abecedarium.activity](activities/Abecedarium.activity)

You could distribute Sugarizer with whatever activities you want.
To do that, you first need to adapt the content of the [activities](activities) directory to match your wish: removing activities you don't want to distribute and adding in this directory new activities you want to include.

Then you need to update the [activities.json](activities.json) file to reflect your choice.
Here is an example of this file:

	[
		{""id"": ""org.sugarlabs.GearsActivity"", ""name"": ""Gears"", ""version"": 6, ""directory"": ""activities/Gears.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.MazeWebActivity"", ""name"": ""Maze Web"", ""version"": 2, ""directory"": ""activities/MazeWeb.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.PaintActivity"", ""name"": ""Paint"", ""version"": 1, ""directory"": ""activities/Paint.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.TamTamMicro"", ""name"": ""TamTam Micro"", ""version"": 1, ""directory"": ""activities/TamTamMicro.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.MemorizeActivity"", ""name"": ""Memorize"", ""version"": 1, ""directory"": ""activities/Memorize.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpg-france.physicsjs"", ""name"": ""Physics JS"", ""version"": 1, ""directory"": ""activities/PhysicsJS.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.CalculateActivity"", ""name"": ""Calculate"", ""version"": 1, ""directory"": ""activities/Calculate.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.TurtleBlocksJS"", ""name"": ""Turtle Blocks JS"", ""version"": 1, ""directory"": ""activities/TurtleBlocksJS.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.Clock"", ""name"": ""Clock Web"", ""version"": 1, ""directory"": ""activities/Clock.activity"", ""icon"": ""activity/activity-clock.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.RecordActivity"", ""name"": ""Record"", ""version"": 1, ""directory"": ""activities/Record.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.Abecedarium"", ""name"": ""Abecedarium"", ""version"": 5, ""directory"": ""activities/Abecedarium.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.KAView"", ""name"": ""KA View"", ""version"": 1, ""directory"": ""activities/KAView.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.FoodChain"", ""name"": ""FoodChain"", ""version"": 4, ""directory"": ""activities/FoodChain.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpc-france.labyrinthjs"", ""name"": ""Labyrinth JS"", ""version"": 1, ""directory"": ""activities/LabyrinthJS.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.TankOp"", ""name"": ""Tank Operation"", ""version"": 1, ""directory"": ""activities/TankOp.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.ChatPrototype"", ""name"": ""ChatPrototype"", ""version"": 1, ""directory"": ""activities/ChatPrototype.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpcfrance.Gridpaint"", ""name"": ""Grid Paint"", ""version"": 2, ""directory"": ""activities/Gridpaint.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.olpc-france.LOLActivity"", ""name"": ""Last One Loses Activity"", ""version"": 1, ""directory"": ""activities/LastOneLoses.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.StopwatchActivity"", ""name"": ""Stopwatch"", ""version"": 1, ""directory"": ""activities/Stopwatch.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.Markdown"", ""name"": ""Markdown"", ""version"": 3, ""directory"": ""activities/Markdown.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.sugarlabs.GTDActivity"", ""name"": ""Get Things Done"", ""version"": 1, ""directory"": ""activities/GetThingsDone.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.laptop.WelcomeWebActivity"", ""name"": ""WelcomeWeb"", ""version"": 1, ""directory"": ""activities/WelcomeWeb.activity"", ""icon"": ""activity/welcome-activity.svg"", ""favorite"": true, ""activityId"": null},
		{""id"": ""org.vpri.EtoysActivity"", ""name"": ""Etoys"", ""version"": 1, ""directory"": ""activities/Etoys.activity"", ""icon"": ""activity/activity-etoys.svg"", ""favorite"": false, ""activityId"": null},
		{""id"": ""io.cordova.all_in_one_plugin_sample"", ""name"": ""Cordova"", ""version"": 1, ""directory"": ""activities/Cordova.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": false, ""activityId"": null},
		{""id"": ""org.olpcfrance.MediaViewerActivity"", ""name"": ""MediaViewer"", ""version"": 1, ""directory"": ""activities/MediaViewer.activity"", ""icon"": ""activity/activity-icon.svg"", ""favorite"": false, ""activityId"": null}
  	]

Each line in this file is one activity. Here is the description of each field:

* **id**: Activity unique ID
* **name**: Display name of the activity
* **version**: Activity version number
* **directory**: Location directory of the activity in Sugarizer
* **icon**: Location of the icon in the activity directory
* **favorite**: true means that the activity is in the favorite view
* **activityId** Reserved for internal use

Remove in this file rows for activities that you want to remove. Add in this file a line for each activity you want to add.

Note than:

1. The [activities/ActivityTemplate](activities/ActivityTemplate) directory does not contain a real activity. It's just a template that you could use to create your own activity.
2. The [activities.json](activities.json) is used only by Sugarizer Application, the Web Application relies on the */api/activities* API that dynamically browse the [activities](activities) directory. By the way, it's a good practice to match the content of the activities.json file and the content of the activities directory.

# Create your own activity

With Sugarizer, it's easy to create an activity with a bunch of HTML and JavaScript.

![](images/tutorial_teaser.png)

If you're interested in creating your own activity, a full tutorial will guide you between all the development steps:

* **Step 1**: create the activity from a template
* **Step 2**: customize icon and content
* **Step 3**: add a toolbar icon
* **Step 4**: handle journal and datastore
* **Step 5**: localize the activity
* **Step 6**: handle multi-user with presence
* **Step 7**: use journal chooser dialog
* **Step 8**: create your own palette
* **Step 9**: integrate a tutorial

Let's start [here](docs/tutorial.md).


# Unit testing

To run unit tests for Sugarizer Application, run ""file:///PathToYourSugarizerRepo/test/index.html"" in your browser.


# Build Application for Android and iOS

Sugarizer Application could be packaged as an Android or iOS application using [Cordova](http://cordova.apache.org/).

For Android:

A dedicated tool named [Sugarizer APK Builder](https://github.com/llaske/sugarizer-apkbuilder) allow you to create the Android packaging without any Android knowledge.

If you want to build it yourself, you could adapt the [source code](https://github.com/llaske/sugarizer-apkbuilder/blob/master/src/make_android.sh) of this tool.

For iOS:

Refer [this](docs/ios/ios_doc.md) documentation for building sugarizer for iOS.

# Reduce package size

The current size of Sugarizer is more than 400 Mb. This huge size is related to media content and resources included in three activities:

* **Abecedarium activity**: about 150 Mb
* **Etoys activity**: about 100 Mb
* **Scratch activity**: about 50 Mb

By the way, these activities are able to retrieve the content remotely if it's not deployed locally. So, if you want to reduce the Sugarizer package size (specifically for deployment on mobile) you could either remove completely those three activities or just remove the media content of these activities.

To remove activities, just remove these activities directory and update [activities.json](activities.json) file as explained above.

To remove media content for **Abecedarium**, remove directories:

* [activities/Abecedarium.activity/audio/en](activities/Abecedarium.activity/audio/en)
* [activities/Abecedarium.activity/audio/fr](activities/Abecedarium.activity/audio/fr)
* [activities/Abecedarium.activity/audio/es](activities/Abecedarium.activity/audio/es)
* [activities/Abecedarium.activity/images/database](activities/Abecedarium.activity/images/database)

The activity will look for media content on the server referenced in [activities/Abecedarium.activity/database/db_url.json](activities/Abecedarium.activity/database/db_url.json), by default `http://server.sugarizer.org/activities/Abecedarium.activity/`.

To remove resources for **Etoys**, remove directory [activities/Etoys.activity/resources](activities/Etoys.activity/resources) and replace the value `resources/etoys.image` in [activities/Etoys.activity/index.html](activities/Etoys.activity/index.html) by the remote location of the resources, for example `http://server.sugarizer.org/activities/Etoys.activity/resources/etoys.image`.

To remove resources for **Scratch**, remove directory [activities/Scratch.activity/static/internal-assets](activities/Scratch.activity/static/internal-assets) and remove the value `class=""offlinemode""` in [activities/Scratch.activity/index.html](activities/Scratch.activity/index.html).

# Optimize performance

If you want to optimize JavaScript performance, you could generate an optimized version of Sugarizer with [Grunt](http://gruntjs.com). This optimized version will minimize and reduce the size of all JavaScript files.

First, ensure that Node.js and npm are installed on your machine. See [here](http://nodejs.org/) for more information.

The [Gruntfile.js](Gruntfile.js) contains tasks settings to build an optimized version of Sugarizer. To do that, ensure first that grunt is installed:

	npm install -g grunt-cli

Then install the specific component for Sugarizer by running:

	npm install --only=dev

Finally, launch:

	grunt -v

At the end of the process, all JavaScript files in all directories have been replaced by an optimized version.


# Localization

If you're not a developer and you want to translate Sugarizer into your own language, please go to the [Sugarizer translation platform](http://translate.sugarizer.org) where you will be able to do that. If you're a developer, the following paragraphs will explain to you how the Sugarizer localization system works.

Sugarizer use [i18next](https://www.i18next.com/) localization system.

All strings are localized in JSON files in the [locales](locales) directory at the root of the repository.
If you want to add a new translation, copy the `en.json` files in a new one and:

* Replace ""en"" in the new file name by the [ISO 639-1 code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) of your language. For example, ""fr.json"" for French,
* Substitute the right side of the "":"" character on each line of the file by the string localized in your language. For example:

		""StartNew"": ""Commencer un nouveau"",
		""NameActivity"": ""Activité {{name}}"",
		""RemoveFavorite"": ""Retirer le favori"",

Warning: Note that text inside {{}} must not be localized. So here, **{{name}}** is not translated.

Sugarizer automatically detects the navigator language. To enable this detection, you need to update the settings.init function in the [lib/settings.js](lib/settings.js) file. Add a test on your language code. For example in French:

	else if (navigatorLanguage.indexOf(""fr"") != -1)
		this.language = ""fr"";

Sugarizer settings display a list of all available languages. You need to add your language in this dialog. For this you have to:

* Add a new string in [locales/en.json](locales/en.json) with the name of your language in English. For example:

		""French"": ""French"",

* Add the same line in all other language files. If you're able to do that, translate the right side of the "":"" character with the localized string for the name of your language. If you don't know how to translate it, just use the English word. For example:

		""French"": ""Français"",

* Add your string in the [js/dialog.js](js/dialog.js) file in the create function of the Enyo class Sugar.DialogLanguage. You should give the ISO 639-1 language code and the new string for your language name. For example:

		{code: ""fr"", icon: null, name: l10n.get(""French"")},

That's all. Test the result in your browser.

Note that this translation is for Sugarizer only. Each activity could provide its own localization feature.


# How to contribute

As all Open Source software, contributions to this software are welcome.

See [here](docs/credits.md) the current list of Sugarizer contributors.

Read [CONTRIBUTING](CONTRIBUTING.md) to learn more about how to contribute to Sugarizer.



# License

Sugarizer is licensed under the **Apache-2.0** license. See [LICENSE](LICENSE) for full license text.  Most Sugarizer activities use this license too but some use a different license, see [here](docs/licenses.md) for details.

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
",,2024-05-02T10:12:40Z,30,195,20,"('AndreaGon', 277), ('dhruvmisra', 272), ('llaske', 214), ('sarthak-g', 196), ('prakashujjwal1010', 180), ('sanatankc', 89), ('saurabhhere', 88), ('sdziuda', 79), ('pauloslomp', 74), ('pidddgy', 72), ('ashish0910', 72), ('UtkarshSiddhpura', 70), ('ksraj123', 63), ('weblate', 62), ('Vishalk91-4', 58), ('s-mag', 55), ('EstyxTranslations', 50), ('matiasmartineeez', 44), ('VishnuVardhanBR', 43), ('S-kus', 42), ('Ayeshashaik759', 32), ('AnandChourasia007', 30), ('manki11', 29), ('Scar26', 29), ('VedantSharma11', 23), ('FreddieN', 21), ('NischayGoyal1', 21), ('vinayaknayar', 20), ('ricknjacky', 19), ('codebloded', 18)","[4, 'Quality Education']"
WattTime/watttime-python-client,"A software development kit for the WattTime API, showing basic examples of usage. ","# About
This SDK is meant to help users with basic queries to WattTime’s API (version 3), and to get data returned in specific formats (e.g., JSON, pandas, csv).

Users may register for access to the WattTime API through this client, however the basic user scoping given will only allow newly registered users to access data for the `CAISO_NORTH` region. Additionally, data may not be available for all signal types for newly registered users.

Full documentation of WattTime's API, along with response samples and information about [available endpoints is also available](https://docs.watttime.org/).

# Configuration
The SDK can be installed as a python package from the PyPi repository, we recommend using an environment manager such as [miniconda](https://docs.conda.io/projects/miniconda/en/latest/) or [venv](https://docs.python.org/3/library/venv.html).
```
pip install watttime
```

If you are not registered for the WattTime API, you can do so using the SDK:
```
from watttime import WattTimeMyAccess

wt = WattTimeMyAccess(username=, password=)
wt.register(email=, organization=)

```

If you are already registered for the WattTime API, you may set your credentials as environment variables to avoid passing these during class initialization:
```
# linux or mac
export WATTTIME_USER=
export WATTTIME_PASSWORD=
```

Once you have set your credentials as environment variables, you can omit passing `username` and `password` when instantiating sdk objects. For instance, in the example below, you could replace the second line with

```python
wt_myaccess = WattTimeMyAccess()
```

# Using the SDK
Users may first want to query the `/v3/my-access` endpoint using the `WattTimeMyAccess` class to get a dataframe of regions and signal types available to them:

```python
from watttime import WattTimeMyAccess

wt_myaccess = WattTimeMyAccess(username, password)

# return a nested json describing signals and regions you have access to
wt_myaccess.get_access_json()

# return a pandas dataframe describing signals and regions you have access to
wt_myaccess.get_access_pandas()
```

### Accessing Historical Data

Once you confirm your access, you may wish to request data for a particular region:

```python
from watttime import WattTimeHistorical

wt_hist = WattTimeHistorical(username, password)

# get data as a pandas dataframe
moers = wt_hist.get_historical_pandas(
    start = '2022-01-01 00:00Z', # ISO 8601 format, UTC
    end = '2023-01-01 00:00Z', # ISO 8601 format, UTC
    region = 'CAISO_NORTH',
    signal_type = 'co2_moer' # ['co2_moer', 'co2_aoer', 'health_damage', etc.]
)

# save data as a csv -> ~/watttime_historical_csvs/___.csv
wt_hist.get_historical_csv(
    start = '2022-01-01 00:00Z', # ISO 8601 format, UTC
    end = '2023-01-01 00:00Z', # ISO 8601 format, UTC
    region = 'CAISO_NORTH',
    signal_type = 'co2_moer' # ['co2_moer', 'co2_aoer', 'health_damage', etc.]
)
```

You could also combine these classes to iterate through all regions where you have access to data:

```python
from watttime import WattTimeMyAccess, WattTimeHistorical
import pandas as pd

wt_myaccess = WattTimeMyAccess(username, password)
wt_hist = WattTimeHistorical(username, password)

access_df = wt_myaccess.get_access_pandas()

moers = pd.DataFrame()
moer_regions = access_df.loc[access_df['signal_type'] == 'co2_moer', 'region'].unique()
for region in moer_regions:
    region_df = wt_hist.get_historical_pandas(
        start = '2022-01-01 00:00Z',
        end = '2023-01-01 00:00Z',
        region = region,
        signal_type = 'co2_moer'
    )
    moers = pd.concat([moers, region_df], axis='rows')
```

### Accessing Real-Time and Historical Forecasts
You can also use the SDK to request a current forecast for some signal types, such as co2_moer and health_damage:

```python
from watttime import WattTimeForecast

wt_forecast = WattTimeForecast(username, password)
forecast = wt_forecast.get_forecast_json(
    region = 'CAISO_NORTH',
    signal_type = 'health_damage'
)

```
We recommend using the `WattTimeForecast` class to access data for real-time optimization. The first item of the response from this call is always guaranteed to be an estimate of the signal_type for the current five minute period, and forecasts extend at least 24 hours at a five minute granularity, which is useful for scheduling utilization during optimal times.

Methods also exist to request historical forecasts, however these responses may be slower as the volume of data can be significant:
```python
hist_forecasts = wt_forecast.get_historical_forecast_json(
    start = '2022-12-01 00:00+00:00',
    end = '2022-12-31 23:59+00:00',
    region = 'CAISO_NORTH',
    signal_type = 'health_damage'
)
```

### Accessing Location Data
We provide two methods to access location data:

1) The `region_from_loc()` method allows users to provide a latitude and longitude coordinates in order to receive the valid region for a given signal type.

2) the `WattTimeMaps` class provides a `get_maps_json()` method which returns a [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) object with complete boundaries for all regions available for a given signal type. Note that access to this endpoint is only available for Pro and Analyst subscribers. 

```python
from watttime import WattTimeMaps

wt = WattTimeMaps()

# get BA region for a given location
wt.region_from_loc(
    latitude=39.7522,
    longitude=-105.0,
    signal_type='co2_moer'
)

# get shape files for all regions of a signal type
wt.get_maps_json('co2_moer')
```",,2024-03-28T15:17:15Z,5,3,3,"('sam-watttime', 20), ('nsteins', 18), ('skoeb', 10), ('geoffhancock', 2), ('xginn8', 2)","[7, 'Affordable and Clean Energy']"
liberapay/liberapay.com,Source code of the recurrent donations platform Liberapay,"# Liberapay

[![Weblate](https://hosted.weblate.org/widgets/liberapay/-/shields-badge.svg)](https://hosted.weblate.org/engage/liberapay/?utm_source=widget)
[![Open Source Helpers](https://www.codetriage.com/liberapay/liberapay.com/badges/users.svg)](https://www.codetriage.com/liberapay/liberapay.com)
[![Gitter](https://badges.gitter.im/liberapay/salon.svg)](https://gitter.im/liberapay/salon?utm_source=badge)
[![Income](https://img.shields.io/liberapay/receives/Liberapay.svg)](https://liberapay.com/Liberapay)
[![Donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/liberapay/donate)

[Liberapay](http://liberapay.com) is a recurrent donations platform. We help you fund the creators and projects you appreciate.

Note: This webapp is not self-hostable.

## Table of Contents

- [Contact](#contact)
- [Contributing to the translations](#contributing-to-the-translations)
- [Contributing to the code](#contributing-to-the-code)
  - [Introduction](#introduction)
  - [Installation](#installation)
  - [Configuration](#configuration)
  - [Running](#running)
    - [Payday](#payday)
  - [SQL](#sql)
  - [CSS and JavaScript](#css-and-javascript)
  - [Testing](#testing)
    - [Updating test fixtures](#updating-test-fixtures)
    - [Speeding up the tests](#speeding-up-the-tests)
  - [Tinkering with payments](#tinkering-with-payments)
  - [Modifying python dependencies](#modifying-python-dependencies)
  - [Processing personal data](#processing-personal-data)
  - [Deploying the app](#deploying-the-app)
- [License](#license)

## Contact

Want to chat? [Join us on Gitter](https://gitter.im/liberapay/salon).

Alternatively you can post a message in [our GitHub salon](https://github.com/liberapay/salon).


## Contributing to the translations

You can help translate Liberapay [via Weblate](https://hosted.weblate.org/engage/liberapay/). Current status:

[![global translation status](https://hosted.weblate.org/widgets/liberapay/-/core/287x66-white.png)](https://hosted.weblate.org/engage/liberapay/?utm_source=widget)

[![translation status by language](https://hosted.weblate.org/widgets/liberapay/-/core/multi-auto.svg)](https://hosted.weblate.org/projects/liberapay/core/?utm_source=widget)

If you have questions about translating Liberapay, you can ask them [in the salon](https://github.com/liberapay/salon/labels/i18n).


## Contributing to the code

### Introduction

Liberapay was originally forked from [Gratipay](https://github.com/gratipay/gratipay.com) and inherited its web micro-framework [Pando](https://github.com/AspenWeb/pando.py) (*né* Aspen), which is based on filesystem routing and [simplates](http://simplates.org/). Don't worry, it's quite simple. For example to make Liberapay return a `Hello $user, your id is $userid` message for requests to the URL `/$user/hello`, you only need to create the file `www/%username/hello.spt` with this inside:

```
from liberapay.utils import get_participant
[---]
participant = get_participant(state)
[---] text/html
{{ _(""Hello {0}, your id is {1}"", request.path['username'], participant.id) }}
```

As illustrated by the last line our default template engine is [Jinja](http://jinja.pocoo.org/).

The `_` function attempts to translate the message into the user's language and escapes the variables properly (it knows that it's generating a message for an HTML page).

The python code inside simplates is only for request-specific logic, common backend code is in the `liberapay/` directory.

### Installation

Make sure you have the following dependencies installed first:

- python ≥ 3.11
  - including the C headers of python and libffi, which are packaged separately in many Linux distributions
- postgresql 16 (see [the official download & install docs](https://www.postgresql.org/download/))
- make

Then run:

    make env

Now you need to give yourself superuser postgres powers (if it hasn't been done already), and create two databases:

    su postgres -c ""createuser --superuser $(whoami)""

    createdb liberapay
    createdb liberapay_tests

If you need a deeper understanding take a look at the [Database Roles](https://www.postgresql.org/docs/9.4/static/user-manag.html) and [Managing Databases](https://www.postgresql.org/docs/9.4/static/managing-databases.html) sections of PostgreSQL's documentation.

Then you can set up the DB:

    make schema

### Configuration

Environment variables are used for configuration, the default values are in
`defaults.env` and `tests/test.env`. You can override them in
`local.env` and `tests/local.env` respectively.

### Running

Once you've installed everything and set up the database, you can run the app:

    make run

It should now be accessible at [http://localhost:8339/](http://localhost:8339/).

There are no users provided by default. You can create accounts as you would on the real website, and if you want you can also create a bunch of fake users (but they're not great):

    make data

To grant admin permissions to an account, modify the database like so:

    psql liberapay -c ""update participants set privileges = 1 where username = 'account-username'""

#### Payday

To run a local payday open [http://localhost:8339/admin/payday](http://localhost:8339/admin/payday) and click the ""Run payday"" button. You can add `OVERRIDE_PAYDAY_CHECKS=yes` in the `local.env` file to disable the safety checks that prevent running payday at the wrong time.

### SQL

The python code interacts with the database by sending raw SQL queries through
the [postgres.py](https://postgres-py.readthedocs.org/en/latest/) library.

The [official PostgreSQL documentation](https://www.postgresql.org/docs/9.6/static/index.html) is your friend when dealing with SQL, especially the sections ""[The SQL Language](https://www.postgresql.org/docs/9.6/static/sql.html)"" and ""[SQL Commands](https://www.postgresql.org/docs/9.6/static/sql-commands.html)"".

The DB schema is in `sql/schema.sql`, but don't modify that file directly,
instead put the changes in `sql/branch.sql`. During deployment that script will
be run on the production DB and the changes will be merged into `sql/schema.sql`.
That process is semi-automated by `release.sh`.

### CSS and JavaScript

For our styles we use [SASS](http://sass-lang.com/) and [Bootstrap 3](https://getbootstrap.com/). Stylesheets are in the `style/` directory and our JavaScript code is in `js/`. Our policy for both is to include as little as possible of them: the website should be almost entirely usable without JS, and our CSS should leverage Bootstrap as much as possible instead of containing lots of custom rules that would become a burden to maintain.

We compile Bootstrap ourselves from the SASS source in the `style/bootstrap/`
directory. We do that to be able to easily customize it by changing values in
`style/variables.scss`. Modifying the files in `style/bootstrap/` is probably
a bad idea.

### Icons

For user interface icons we use [Bootstrap Icons](https://icons.getbootstrap.com/). An icon can be included in a page by calling the `icon` macro from `templates/macros/icons.html`, e.g. `{{ icon('liberapay') }}`. The icons are stored in the `www/assets/icons.svg` file. To add a new icon in that file, the root `` element of the icon being added must be turned into a `` element, preserving only its `viewBox` attribute and adding an `id` attribute.

If you don't find any icon in Bootstrap Icons that fits your use case, you can try to search online catalogs like [Flaticon](https://www.flaticon.com/search?type=uicon), [Icons8](https://icons8.com/icons), [Pictogrammers](https://pictogrammers.com/), [SVG Repo](https://www.svgrepo.com/) and [The Noun Project](https://thenounproject.com/). For brand icons, [Simple Icons](https://simpleicons.org/) is a good resource.

### Testing

The easiest way to run the test suite is:

    make test

This recreates the test DB's schema and runs all the tests. To speed things up
you can also use the following commands:

- `make pytest` only runs the python tests without recreating the test DB
- `make pytest-re` only runs the tests that failed previously

#### Updating test fixtures

Some of our tests include interactions with external services. In order to speed up those tests we record the requests and responses automatically using [vcr](https://pypi.python.org/pypi/vcrpy). The records are in the `tests/py/fixtures` directory, one per test class.

If you add or modify interactions with external services, then the tests will fail, because VCR will not find the new or modified request in the records, and will refuse to record the new request by default (see [Record Modes](https://vcrpy.readthedocs.io/en/latest/usage.html#record-modes) for more information). When that happens you can either add `VCR=new_episodes` to your test command (e.g. `make pytest VCR=new_episodes`) or delete the obsolete fixture files (e.g. `rm tests/py/fixtures/TestPayinsStripe.yml`).

If you're testing an API which uses idempotency keys (for example Stripe's API), then some requests will fail if they're no longer exactly identical. In that case, increase the value of the test class' `offset` attribute so that different idempotency keys will be used.

#### Speeding up the tests

PostgreSQL is designed to prevent data loss, so it does a lot of synchronous disk writes by default. To reduce the number of those blocking writes, our `recreate-schema.sh` script automatically switches the `synchronous_commit` option to `off` for the test database, however this doesn't completely disable syncing. If your PostgreSQL instance only contains data that you can afford to lose, then you can speed things up further by setting `fsync` to `off`, `wal_level` to `minimal` and `max_wal_senders` to `0` in the server's configuration file (`postgresql.conf`).

### Tinkering with payments

Liberapay currently supports two payment processors: [Stripe](https://stripe.com/docs) and [PayPal](https://developer.paypal.com/).

#### Testing Stripe webhooks

You can forward Stripe's callbacks to your local Liberapay instance by running `make stripe-bridge`. The [stripe-cli](https://github.com/stripe/stripe-cli) program has to be installed for this to work.

### Modifying python dependencies

All new dependencies need to be audited to check that they don't contain malicious code or security vulnerabilities.

We use [pip's Hash-Checking Mode](https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode) to protect ourselves from dependency tampering. Thus, when adding or upgrading a dependency the new hashes need to be computed and put in the requirements file. For that you can use [hashin](https://github.com/peterbe/hashin):

    pip install hashin
    hashin package==x.y -r requirements_base.txt

If for some reason you need to rehash all requirements, run `make rehash-requirements`.

To upgrade all the dependencies in the requirements file, run `hashin -u -r requirements_base.txt`. You may have to run extra `hashin` commands if new subdependencies are missing.

The testing dependencies in `requirements_tests.txt` don't follow these rules because they're not installed in production. It's up to you to isolate your development environment from the rest of your system in order to protect it from possible vulnerabilities in the testing dependencies.

### Processing personal data

When writing code that handles personal information, keep in mind the principles enshrined in the [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation).

### Deploying the app

Note: Liberapay cannot be self-hosted, this section is only meant to document how we deploy new versions.

Liberapay is currently hosted on [AWS](https://aws.amazon.com/) (Ireland).

To deploy the app simply run `release.sh`, it'll guide you through it. Of course you need to be given access first.

## License

[CC0 Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/) (See [this discussion](https://github.com/liberapay/liberapay.com/issues/564) for details.)
","'crowdfunding', 'donation', 'money', 'python', 'web'",2024-05-03T12:17:26Z,30,1575,55,"('Changaco', 8101), ('chadwhitacre', 5013), ('jorgesumle', 402), ('seanlinsley', 299), ('tuxayo', 297), ('jelv', 234), ('zwn', 223), ('rohitpaulk', 214), ('bisqwit', 178), ('clone1018', 167), ('trebmuh', 157), ('comradekingu', 152), ('mnh48', 147), ('rummik', 129), ('jmontane', 129), ('eugenia-russell', 95), ('IhorHordiichuk', 85), ('ryonakano', 83), ('hippi777', 81), ('Katharsisdrill', 77), ('eldelacajita', 74), ('wyze', 74), ('Skybladev2', 70), ('SWSAmor', 69), ('joonas', 65), ('oakes', 64), ('nijel', 60), ('techtonik', 59), ('ESWAT', 58), ('weblate', 55)","[17, 'Partnerships for the Goals']"
ushahidi/platform-client,"Ushahidi Platform Client, version 3+","Ushahidi Platform Web Client
============================

[![Greenkeeper badge](https://badges.greenkeeper.io/ushahidi/platform-client.svg)](https://greenkeeper.io/)

[![Build Status](https://travis-ci.org/ushahidi/platform-client.svg?branch=master)](https://travis-ci.org/ushahidi/platform-client)
[![Coverage Status](https://coveralls.io/repos/github/ushahidi/platform-client/badge.svg?branch=master)](https://coveralls.io/github/ushahidi/platform-client?branch=master)
[![Dependency Status](https://david-dm.org/ushahidi/platform-client/dev-status.svg?style=flat)](https://david-dm.org/ushahidi/platform-client#info=devDependencies)

___
## Report and find Ushahidi Platform V3 issues

All our issues live in https://github.com/ushahidi/platform/issues . 

## Try it out on Heroku

[![Deploy](https://www.herokucdn.com/deploy/button.png)](https://heroku.com/deploy?template=https://github.com/ushahidi/platform-client/tree/master)

You need to deploy the [Platform API](http://github.com/ushahidi/platform) first

# Platform Client installation

### What is the platform client?

The web client is the component that end users interact with when opening the Platform website with a web browser. The client interacts with the API in order to perform operations on the system \(i.e. submit posts, query posts\).

### Installation steps 

**Pre-requisite: Install the platform API by following one of the API setup guides**


**Pre-requisite: Install Node V6.x \(you might want to use NVM for this\) before continuing.**

#### **Getting the platform-client code**

Clone the repository \(this will create a directory named _platform-client\)_

```bash
git clone https://github.com/ushahidi/platform-client.git
```

Go into the platform directory

```bash
cd platform-client
```

Switch to the _develop_ branch

```bash
git checkout develop
```

**If you haven't used git before or need help with git specific issues, make sure to check out their docs here** [https://git-scm.com/doc](https://git-scm.com/doc)

#### Install the platform-client dependencies.

```text
npm install
```

The client needs to point to the hostname where the backend expects to receive HTTP requests. This has to be set before building the client.

**In order to set up all that, create a file at the location /var/www/platform-client/.env . Use the following contents as an example:**

```text
BACKEND_URL=http://192.168.33.110/
PORT=8000
APP_LANGUAGES=en
OAUTH_CLIENT_ID=ushahidiui
OAUTH_CLIENT_SECRET=35e7f0bca957836d05ca0492211b0ac707671261
```


To make it easy to call \`gulp\` when building and developing in the app, add **node\_modules/.bin** to your PATH in ~/_.bashrc_. Example PATH \(relevant part in bold\):

export PATH=$HOME/bin:/usr/local/bin:**node\_modules/.bin**:$PATH

```
gulp
```

alternatively, if you haven't setup node\_modules in your PATH, run:

### Running a local development server

Run:

```
node_modules/gulp/bin/gulp.js
```

This will start the watcher for local development, and any changes you make to the code will be reflected in the application.

### Building for production deployments

Run:

```
gulp build
```

alternatively, if you haven't setup node\_modules in your PATH, run:

```
node_modules/gulp/bin/gulp.js build
```

This will start the process of generating the static site. Once the files are generated, you can host the **server/www** directory and load the site.

In the **server** directory you will also find an example nginx and an example apache2 file to get you started on hosting the client.


#### Running unit tests

To run unit tests once, run:
```
gulp test
```

For test driven development we have a gulp task `gulp tdd`. This watches for JS changes and re-runs the unit tests.


### Native Server (Apache or Nginx)

If you are running the client with a native web server like Apache or nginx, you will need to use URL rewriting to point all non-existant files to `index.html`. There is a sample `.htaccess` file, which can be used with Apache:

```
% cp server/rewrite.htaccess server/www/.htaccess
```

Nginx users will have to manually configure rewriting in the site configuration file.

### I'm a developer, should I contribute to Ushahidi 3.x?

Yes! Development moves pretty quickly but the tech stack is getting more and more stable. If you're keen to help build something awesome, [jump on board](https://docs.ushahidi.com/platform-developer-documentation/). 

[Code of Conduct](https://docs.ushahidi.com/platform-developer-documentation/code-of-conduct)


",'hacktoberfest',2023-10-27T23:45:40Z,30,86,31,"('rjmackay', 1199), ('Angamanga', 948), ('willdoran', 909), ('rowasc', 656), ('tuxpiper', 223), ('Ifycode', 183), ('AmTryingMyBest', 180), ('jasonmule', 97), ('crcommons', 90), ('ryanchristo', 47), ('spaudanjo', 43), ('rohit645', 25), ('kinstella', 22), ('greenkeeperio-bot', 20), ('renujain31', 20), ('kinstelli', 19), ('brandonrosage', 18), ('vbhv', 18), ('himil-vasava', 17), ('hollycorbett', 15), ('bimbolabuari', 13), ('Somaru-chan', 12), ('greenkeeperbot', 10), ('evansims', 9), ('andrew21-mch', 9), ('noone0212', 9), ('individual-it', 7), ('aMoniker', 7), ('webong', 7), ('GabrielFalcom', 7)","[10, 'Reduced Inequalities']"
sharmalab/Datascope,Interactive linked visual query system for large datasets,"# DataScope [![Build Status](https://travis-ci.org/sharmalab/Datascope.svg?branch=dev)](https://travis-ci.org/sharmalab/Datascope) [![DOI](https://zenodo.org/badge/70261830.svg)](https://zenodo.org/badge/latestdoi/70261830) #

We propose an environment for visualizing and exploring multidimensional data. We propose methods to create a new search interface to the data as an alternate way to explore data, create dynamic dashboards that can be extended to support data exploration using Javascript libraries like crossfilter and dc.js. This method is extendible to support data from other remote archives.

### Quickstart guide ###

(requires docker)
* Enter the datascope directory (this directory)
* `docker build -t datascope .`
* `docker run -p 3001:3001 datascope`

### Running Without Containers ###

##### Prerequisites

* Install [Node.js](https://nodejs.org/en/download/) and [NPM](https://www.npmjs.com/get-npm)
* `sudo npm install -g webpack`
* `sudo npm install -g forever` ((Optional) recommended for production deployements)
* `sudo npm install -g apidoc`


##### Installation

* Clone the repository
* Enter the datascope directory (this directory)
* Get dependencies with ```npm install``
* Run ```npm run-script build```

##### Running
* Copy an example config and data folders to this directory from ```examples```
* Modify the files present in ```config``` to fit your needs:
    * dataSource.json
    * dataDescription.json
    * interactiveFilters.json
    * visualization.json
    * dashboard.json (For dashboard settings)

* Run ```node app.js```
* Goto ```http://localhost:3000``` from your favorite browser.

Read the [User Guide](https://github.com/sharmalab/Datascope/wiki)  for more details

##### Recommended production deployement
We recommend deploying Datascope with forever.js.

* Install forever.js `npm install forever -g`
* `forever start app.js`
* `forever ps` gives a list of current instances running. You can get uptime, log details etc.


## Developers

* Use ```webpack --watch``` to rebuild automatically after edits.
* Use ```nodemon```(https://github.com/remy/nodemon) to restart the server automatically after edits.

### API Documentation
Head over to [API Doc](https://sharmalab.github.io/Datascope/apidoc/) for documentation about Datascope's REST API.
","'nci-qin', 'tcia-dac'",2022-05-25T23:42:45Z,7,16,3,"('lastlegion', 2501), ('birm', 90), ('srflorea', 46), ('ashishof77', 20), ('sharmaashish', 7), ('dependabotbot', 2), ('loghijiaha', 1)","[3, 'Good Health and Well-Being']"
HTBox/MobileKidsIdApp,Application for enabling storage and retrieval of important information on missing children.,"

[![Build status](https://ci.appveyor.com/api/projects/status/0y0b8ctdaesjbomq/branch/master?svg=true)](https://ci.appveyor.com/project/HTBox/mobilekidsidapp-i1cep)

Kids ID Kit App
===============
The intent of the app is to help parents keep up to date information on their kids, and to provide parents with relevant information to keep their kids safe and what to do if they can’t find their child.

This app is created in collaboration with [Missing Children Minnesota](http://missingchildrenmn.com/).

The majority of missing children are runaways (many of whom are trafficked), followed by parental abductions, with the high profile stranger abductions being least common. There’s another category of “lost, injured, otherwise missing” where the cause of the child being missing is unknown.

I would think that having this information would also be relevant in terms of disaster preparedness, where I’m sure there’s always a scramble for parents to find and reconnect with their kids in the chaos.

So the app has three key aspects.

1. Allow a parent to enter and maintain information on their kids – measurements, pictures, relevant legal documents
 1. Also help the parent create and maintain a physical file of relevant information and documentation for offline use
 1. Instructions on how to collect a DNA sample from the child
1. Allow a parent to authorize authorities (police, court workers, missing children orgs like MCM) to access the info about a missing child to help in locating/recovering the child
1. Provide the parent with information about how to keep their kids safe and what to do if they can’t find their child

![](https://raw.github.com/htbox/mobileKidsIdApp/master/resources/MCM-logo-teal%20small.jpg)

Contributing
============
We accept pull requests, and appreciate assistance in this important effort.

Anyone participating in this repo or project must follow our [code of conduct](https://github.com/HTBox/MobileKidsIdApp/blob/master/code_of_conduct.md).

We have documentation on [setting up a dev environment](https://github.com/HTBox/MobileKidsIdApp/blob/master/docs/Developer%20setup.md).
",,2023-09-21T17:27:23Z,27,45,38,"('rockfordlhotka', 415), ('jacob-maristany', 56), ('brian-haiintcom', 41), ('JScearcy', 32), ('CoderNate', 30), ('estenrye', 23), ('Chuxel', 21), ('BillWagner', 19), ('binaryjanitor', 19), ('bseebacher', 18), ('dnordquist', 11), ('brunck', 10), ('devfables', 10), ('millermatt', 9), ('jwendl', 6), ('DocHammoc', 5), ('snickler', 3), ('zenzugr', 3), ('teneresa', 2), ('Bowman74', 1), ('krdmllr', 1), ('LuceCarter', 1), ('mjmilan', 1), ('natescode', 1), ('stmcallister', 1), ('dependabotbot', 1), ('left-on-red', 1)","[4, 'Quality Education']"
openmrs/openmrs-core,OpenMRS API and web application code,"

[![Build Status](https://travis-ci.org/openmrs/openmrs-core.svg?branch=master)](https://travis-ci.org/openmrs/openmrs-core) [![Coverage Status](https://coveralls.io/repos/github/openmrs/openmrs-core/badge.svg?branch=master)](https://coveralls.io/github/openmrs/openmrs-core?branch=master) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/a51303ee46c34775a7c31c8d6016da6b)](https://www.codacy.com/app/openmrs/openmrs-core?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=openmrs/openmrs-core&amp;utm_campaign=Badge_Grade)

api: [![API](https://snyk.io/test/github/openmrs/openmrs-core/badge.svg?targetFile=api%2Fpom.xml)](https://snyk.io/test/github/openmrs/openmrs-core?targetFile=api%2Fpom.xml)
test: [![test](https://snyk.io/test/github/openmrs/openmrs-core/badge.svg?targetFile=test%2Fpom.xml)](https://snyk.io/test/github/openmrs/openmrs-core?targetFile=test%2Fpom.xml)
tools: [![tools](https://snyk.io/test/github/openmrs/openmrs-core/badge.svg?targetFile=tools%2Fpom.xml)](https://snyk.io/test/github/openmrs/openmrs-core?targetFile=tools%2Fpom.xml)
web: [![web](https://snyk.io/test/github/openmrs/openmrs-core/badge.svg?targetFile=web%2Fpom.xml)](https://snyk.io/test/github/openmrs/openmrs-core?targetFile=web%2Fpom.xml)
webapp: [![webapp](https://snyk.io/test/github/openmrs/openmrs-core/badge.svg?targetFile=webapp%2Fpom.xml)](https://snyk.io/test/github/openmrs/openmrs-core?targetFile=webapp%2Fpom.xml)

OpenMRS is a patient-based medical record system focusing on giving providers a free customizable electronic medical record system (EMR).

The mission of OpenMRS is to improve health care delivery in resource-constrained environments by coordinating a global community that creates a robust, scalable, user-driven, open source medical record system platform.

#### Table of Contents

1. [Build](#build)
   1. [Prerequisites](#prerequisites)
   2. [Build Command](#build-command)
   3. [Deploy](#deploy)
2. [Docker build](#docker-build)
3. [Navigating the repository](#navigating-the-repository)
4. [Software Development Kit](#software-development-kit)
5. [Extending OpenMRS with Modules](#extending-openmrs-with-modules)
6. [Documentation](#documentation)
   1. [Developer guides](#developer-guides)
   2. [Wiki](#wiki)
   3. [Website](#website)
7. [Contributing](#contributing)
   1. [Code](#code)
   2. [Code Reviews](#code-reviews)
   3. [Translation](#translation)
8. [Issues](#issues)
9. [Community](#community)
10. [Support](#support)
11. [License](#license)

## Build

### Prerequisites

#### Java

OpenMRS is a Java application which is why you need to install a Java JDK.

If you want to build the master branch you will need a Java JDK of minimum version 8.

#### Maven

Install the build tool [Maven](https://maven.apache.org/).

You need to ensure that Maven uses the Java JDK needed for the branch you want to build.

To do so execute

```bash
mvn -version
```

which will tell you what version Maven is using. Refer to the [Maven docs](https://maven.apache.org/configure.html) if you need to configure Maven.

#### Git

Install the version control tool [git](https://git-scm.com/) and clone this repository with

```bash
git clone https://github.com/openmrs/openmrs-core.git
```

### Build Command

After you have taken care of the [Prerequisites](#prerequisites)

Execute the following

```bash
cd openmrs-core
mvn clean package
```

This will generate the OpenMRS application in `webapp/target/openmrs.war` which you will have to deploy into an application server like for example [tomcat](https://tomcat.apache.org/) or [jetty](http://www.eclipse.org/jetty/).

### Deploy

For development purposes you can simply deploy the `openmrs.war` into the application server jetty via

```bash
cd openmrs-core/webapp
mvn jetty:run
```

If all goes well (check the console output) you can access the OpenMRS application at `localhost:8080/openmrs`.

Refer to [Getting Started as a Developer - Maven](https://wiki.openmrs.org/display/docs/Maven) for some more information
on useful Maven commands and build options.

## Docker build

Docker builds are still work in progress. We appreciate any feedback and improvements to the process.

The only prerequisite needed is Docker. 

In order to build a development version run:
```bash 
docker-compose build
```
It calls `mvn install` by default. If you would like to customize mvn build arguments you can do so by running:
```bash
docker-compose build --build-arg MVN_ARGS='install -DskipTests'
```
It is also possible to use the built dev image to run jetty:
```bash
docker-compose up
```

In order to build a production version run:
```bash
docker-compose -f docker-compose.yml build
```
It first builds the dev image and then an image with Tomcat and openmrs.war. 
It has no dev dependencies.

The production version can be run with:
```bash
docker-compose -f docker-compose.yml up
```
If you want to debug, you need to run a development version and connect your debugger to port 8000, which is exposed by default.

Unfortunately, at this point any code changes require full restart and rebuild of the docker container. To speed up the process,
please use:
```bash
docker-compose build --build-arg MVN_ARGS='install -DskipTests'
docker-compose up
```
We are working towards providing support for Spring Boot auto-reload feature, which will be documented here once ready.

It is also possible to deploy an image built by our CI, which is published at 
https://hub.docker.com/r/openmrs/openmrs-core

You can run any tag available with:
```bash
TAG=nightly docker-compose -f docker-compose.yml up
```
It is also possible to run a development version of an image with:
```bash
TAG=dev docker-compose up
```
All development versions contain dev suffix. The cache suffix is for use by our CI.

## Navigating the repository

The project tree is set up as follows:


 
  api/
  Java and resource files for building the java api jar file.
 
 
  tools/
  Meta code used during compiling and testing. Does not go into any released binary (like doclets).
 
 
  web/
  Java and resource files that are used in the webapp/war file.
 
 
  webapp/
  files used in building the war file (contains JSP files on older versions).
 
 
  pom.xml
  The main maven file used to build and package OpenMRS.
   


## Software Development Kit

For rapid development of modules and the OpenMRS Platform code check out the
awesome SDK at

https://wiki.openmrs.org/display/docs/OpenMRS+SDK

## Extending OpenMRS with Modules

OpenMRS has a modular architecture that allows developers to extend the OpenMRS core functionality by creating modules that can easily be added or removed to meet the needs of a specific implementation.

Before creating your own module go to the [OpenMRS Module Repository](https://addons.openmrs.org/) and see if there is already a module for your specific use case. If so deploy and try it and if a functionality is missing join the developers of the module to add a feature.

If you haven't found what you were looking for refer to the [Module - wiki](https://wiki.openmrs.org/display/docs/Modules) to learn how you can create a new module.

## Documentation

### Developer guides

If you want to contribute please refer to these resources

* [Getting Started as a Developer](https://wiki.openmrs.org/display/docs/Get+Started+as+a+Developer)
* [How To Configure Your IDE](https://wiki.openmrs.org/display/docs/How-To+Setup+And+Use+Your+IDE)
* [How To Make a Pull Request](https://wiki.openmrs.org/display/docs/Pull+Request+Tips)

### Wiki

If you are looking for detailed guides on how to install, configure, contribute and
extend OpenMRS visit

http://wiki.openmrs.org

### Website

If you are looking for more information regarding OpenMRS as an organization
check

http://openmrs.org

## Contributing

Contributions are very welcome, we can definitely use your help!

OpenMRS organizes the privileges of its contributors in developer stages which
are documented [here](https://wiki.openmrs.org/display/RES/OpenMRS+Developer+Stages).

Read the following sections to find out where you could help.

### Code

Check out our [contributing guidelines](CONTRIBUTING.md), read through the [Developer guides](#developer-guides).

After you've read up :eyeglasses: [grab an introductory issue](https://wiki.openmrs.org/display/docs/Contribute+as+a+Developer#ContributeasaDeveloper-Workonanissue) that is `Ready For Work`.

### Code Reviews

You might not have the time to develop yourself but enough experience with
OpenMRS and/or reviewing code, your help on code reviews will be much
appreciated!

Read

https://wiki.openmrs.org/display/docs/Code+Review

and get started with re-:eyes: pull requests!

### Translation

We use

https://www.transifex.com/openmrs/OpenMRS/

to manage our translations.

The `messages.properties` file in this repository is our single source of
truth. It contains key, value pairs for the English language which is the
default.

Transifex fetches updates to this file every night which can then be translated
by you and me on transifex website itself. At any time we can pull new translations from transifex
back into this repository. Other languages like for ex. Spanish will then be in
the `messages_es.properties` file.

If you would like to know how to help with translations see

http://openmrs.org/join-the-community/translate/

## Issues

If you want help fix existing issues or you found a bug and want to tell us please go to

https://issues.openmrs.org

## Community

[![OpenMRS Talk](https://omrs-shields.psbrandt.io/custom/openmrs/talk/F26522?logo=openmrs)](http://talk.openmrs.org)
[![OpenMRS IRC](https://img.shields.io/badge/openmrs-irc-EEA616.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MTIiIGhlaWdodD0iNjEyIiB2aWV3Qm94PSIwIDAgNjEyIDYxMiI%2BPHBhdGggZD0iTTE1MyAyMjkuNWMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzEzMS44NjcgMzA2IDE1MyAzMDZjMjEuMTE0IDAgMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzE3NC4xMzMgMjI5LjUgMTUzIDIyOS41em0xNTMgMGMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzI4NC44NjcgMzA2IDMwNiAzMDZjMjEuMTE0IDAgMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzMyNy4xMzMgMjI5LjUgMzA2IDIyOS41em0xNTMgMGMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzQzNy44NjcgMzA2IDQ1OSAzMDZzMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzQ4MC4xMzMgMjI5LjUgNDU5IDIyOS41ek0zMDYgMEMxMzcuMDEyIDAgMCAxMTkuODc1IDAgMjY3Ljc1YzAgODQuNTE0IDQ0Ljg0OCAxNTkuNzUgMTE0Ljc1IDIwOC44MjZWNjEybDEzNC4wNDctODEuMzRjMTguNTUyIDMuMDYyIDM3LjYzOCA0Ljg0IDU3LjIwMyA0Ljg0IDE2OS4wMDggMCAzMDYtMTE5Ljg3NSAzMDYtMjY3Ljc1UzQ3NS4wMDggMCAzMDYgMHptMCA0OTcuMjVjLTIyLjMzOCAwLTQzLjkxLTIuNi02NC42NDMtNy4wMmwtOTAuMDQgNTQuMTI0IDEuMjA0LTg4LjdDODMuNSA0MTQuMTMzIDM4LjI1IDM0NS41MTMgMzguMjUgMjY3Ljc1YzAtMTI2Ljc0IDExOS44NzUtMjI5LjUgMjY3Ljc1LTIyOS41czI2Ny43NSAxMDIuNzYgMjY3Ljc1IDIyOS41UzQ1My44NzUgNDk3LjI1IDMwNiA0OTcuMjV6IiBmaWxsPSIjZmZmIi8%2BPC9zdmc%2B)](http://irc.openmrs.org)
[![OpenMRS Telegram](https://img.shields.io/badge/openmrs-telegram-009384.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNDAgMjQwIj48ZGVmcz48bGluZWFyR3JhZGllbnQgaWQ9ImEiIHgxPSIuNjY3IiB5MT0iLjE2NyIgeDI9Ii40MTciIHkyPSIuNzUiPjxzdG9wIHN0b3AtY29sb3I9IiMzN2FlZTIiIG9mZnNldD0iMCIvPjxzdG9wIHN0b3AtY29sb3I9IiMxZTk2YzgiIG9mZnNldD0iMSIvPjwvbGluZWFyR3JhZGllbnQ%2BPGxpbmVhckdyYWRpZW50IGlkPSJiIiB4MT0iLjY2IiB5MT0iLjQzNyIgeDI9Ii44NTEiIHkyPSIuODAyIj48c3RvcCBzdG9wLWNvbG9yPSIjZWZmN2ZjIiBvZmZzZXQ9IjAiLz48c3RvcCBzdG9wLWNvbG9yPSIjZmZmIiBvZmZzZXQ9IjEiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48Y2lyY2xlIGN4PSIxMjAiIGN5PSIxMjAiIHI9IjEyMCIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGZpbGw9IiNjOGRhZWEiIGQ9Ik05OCAxNzVjLTMuODg4IDAtMy4yMjctMS40NjgtNC41NjgtNS4xN0w4MiAxMzIuMjA3IDE3MCA4MCIvPjxwYXRoIGZpbGw9IiNhOWM5ZGQiIGQ9Ik05OCAxNzVjMyAwIDQuMzI1LTEuMzcyIDYtM2wxNi0xNS41NTgtMTkuOTU4LTEyLjAzNSIvPjxwYXRoIGZpbGw9InVybCgjYikiIGQ9Ik0xMDAuMDQgMTQ0LjQxbDQ4LjM2IDM1LjczYzUuNTIgMy4wNDQgOS41IDEuNDY3IDEwLjg3Ni01LjEyNGwxOS42ODUtOTIuNzYzYzIuMDE2LTguMDgtMy4wOC0xMS43NDYtOC4zNTgtOS4zNWwtMTE1LjU5IDQ0LjU3MmMtNy44OSAzLjE2NS03Ljg0NCA3LjU2Ny0xLjQ0IDkuNTI4bDI5LjY2NCA5LjI2IDY4LjY3My00My4zMjZjMy4yNC0xLjk2NiA2LjIxNy0uOTEgMy43NzUgMS4yNTgiLz48L3N2Zz4%3D)](https://telegram.me/openmrs)
[![OpenMRS Wiki](https://img.shields.io/badge/openmrs-wiki-5B57A6.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNjAiIGhlaWdodD0iMTQyIiB2aWV3Qm94PSIwIDAgMTYwIDE0MiI%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%3D%3D)](https://wiki.openmrs.org)

## Support

Talk to us on [OpenMRS Talk](https://talk.openmrs.org/)

## License

[MPL 2.0 w/ HD](http://openmrs.org/license/) © [OpenMRS Inc.](http://www.openmrs.org/)

","'ehr', 'emr', 'health', 'healthcare', 'hospital', 'java', 'medical-records', 'openmrs', 'openmrs-community'",2024-05-03T07:37:13Z,30,1333,149,"('dkayiwa', 2015), ('wluyima', 1521), ('rkorytkowski', 897), ('djazayeri', 802), ('dependabotbot', 482), ('teleivo', 462), ('mseaton', 236), ('mogoodrich', 224), ('sunbiz', 165), ('jlkeiper', 125), ('jmiranda', 121), ('lluismf', 110), ('k-joseph', 98), ('ibacher', 94), ('BartlomiejRasztabiga', 86), ('pihdave', 82), ('kishoreyekkanti', 77), ('syhaas', 59), ('umupfumu', 52), ('bmamlin', 47), ('jkondrat', 40), ('harsha89', 37), ('k4pran', 36), ('rpuzdrowski', 35), ('bwolfe', 33), ('mblanchette', 33), ('dkithmal', 28), ('achilep', 25), ('tomaszmueller', 24), ('mvorobey', 22)","[3, 'Good Health and Well-Being']"
insight-lane/crash-model,Build a crash prediction modeling application that leverages multiple data sources to generate  a set of dynamic predictions we can use to identify potential trouble spots and direct timely safety interventions.,"Crash Modeling
===================

Outline:
-----------------------
 - Project Overview
 - Data Sources and Modelling
 - Setting up
 - Contributing
 - Connect with us
 - Project Organization

Project Overview
-----------------------

**Motivation**

This project was originally begun as a collaboration between Data4Democracy and the City of Boston.

On Jan 25th, 2017, [9 pedestrians were hit in Boston by vehicles](http://www.bostonherald.com/news/local_coverage/2017/01/battle_for_safer_streets_nine_pedestrians_hit_in_boston_in_1_day). While this was a particularly dangerous day, there were 21 fatalities and over 4000 severe injuries due to crashes in 2016 alone, representing a public health issue for all those who live, work, or travel in Boston. The City of Boston would like to partner with Data For Democracy to help develop a dynamic prediction system that they can use to identify potential trouble spots to help make Boston a safer place for its citizens by targeting timely interventions to prevent crashes before they happen.

This is part of the City's long-term [Vision Zero initiative](http://www.visionzeroboston.org/), which is committed to the goal of zero fatal and serious traffic crashes in the city by 2030. The Vision Zero concept was first conceived in Sweden in 1997 and has been widely credited with a significant reduction in fatal and serious crashes on Sweden’s roads in the decades since then. Cities across the United States are adopting bold Vision Zero initiatives that share these common principles.

> Children growing up today deserve...freedom and mobility. Our seniors should be able to safely get around the communities they helped build and have access to the world around them. Driving, walking, or riding a bike on Boston’s streets should not be a test of courage.
>
> — Mayor Martin J. Walsh


**What is the goal of the project?**

The goal of the project is to promote the development of safer roads by identifying areas of high risk in a city's road network. It seeks to support the decision-making of transportation departments in 3 ways:

1. Identify high risk locations - which roads in the network represent the greatest risk of crashes?

2. Explain the contributing factors of risk - what are the features, patterns and trends that result in a location having elevated risk?

3. Assess the impact of intervention - what is the effect of a past or planned intervention on the risk of crashes?

**Who are the intended users of the project?**

Though originally a collaboration between Data4Democracy and the City of Boston, the project is now being developed to work for any city that wishes to use it. The intended users include city transportation departments, those responsible for managing risk on road networks and individuals interested in crash risk.


**How does the project achieve its goal?**

The project uses machine learning to generate predictions of risk by combining various types of data. Right now it makes use of:

- road segment data to build a map of a city's road network, presently being sourced from [OpenStreetMap](https://www.openstreetmap.org/)

- historical crash data to determine which locations have proved high risk in the past, provided by participating cities through their open data portals

- safety concerns data to understand where citizens believe their roads are unsafe and the nature of their concerns, also provided by participating cities by way of their respective VisionZero programs or [SeeClickFix](https://seeclickfix.com/)

Future versions of the project are likely to make use of:

- traffic volume data to understand which roads experience the highest traffic and how changing trends of usage might affect risk

- more detailed road features including speed limits, signals, bike lanes, crosswalks, parking etc.

- road construction data

Predictions are generated on a per road-segment basis can be explored with an interactive visualization.

**Who are the intended users?**
Though originally a collaboration between Data4Democracy and the City of Boston, the project is now being developed to work for any city that wishes to use it. The intended users include city transportation departments, those responsible for managing risk on road networks and individuals interested in crash risk.

**What are the requirements for use?**

Any city that wishes to can make use of the project. At a minimum, geo-coded historical crash data is required. Beyond this, cities that can supply safety concerns data (VisionZero or otherwise) will be able to generate more advanced predictions of risk.

**What is the release schedule?**

The intended roadmap of development for the project can be found at [https://github.com/Data4Democracy/crash-model/projects](https://github.com/Data4Democracy/crash-model/projects).

**How can I access the project?**

This repo can be downloaded and run in its entirety using Docker, or you can see a current deployment of the project at [https://insightlane.org](https://insightlane.org).


Data Sources and Modelling
-----------------------
### Data Sources
-	[Open street maps network and features](https://wiki.openstreetmap.org/wiki/Map_Features)
-	Crash data must be provided (see data standards)
-	Pipeline can incorporate other networks and features (see using custom data sources)
-	All our processed data is in a private repository in data.world -- ping a project lead or maintainer on Slack to get access. More detailed documentation is contained there.


### Data Model 
-	The [data dictionary](https://github.com/Data4Democracy/crash-model/blob/master/docs/model_data_dictionary.md) contains information about the default features included in the model 
-	As of V2.0, the models tests [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) vs [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier) and picks the best performing (based on [ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html))

Setting up
-----------------------
**I want to set up a local development environment and run the pipeline**
- Clone the repo
- Insight Lane with Conda
- Install [anaconda](https://www.anaconda.com/) or [miniconda](https://docs.conda.io/en/latest/miniconda.html)
    - Use the Python 3.7 version
- Navigate to the repo directory
- Create an environment for the project using the command `conda env create -f environment_.yml ( will be Linux, Mac or PC)`
- Activate the environment using `source activate crash-model`

**I want to set up a Docker development environment**
- A basic (Docker)[https://www.docker.com/] image has been created to run the project in a container, using the ContinuumIO miniconda3 base image (Python 3.6)
- Download or build the image
    - Download from Docker Hub: `$ docker pull insightlane/crash-model:latest`
    - Build from the repo: `$ docker build --tag insightlane/crash-model:[tag] .`
- To run the image: `$ docker run -d -p 8080:8080 --name bcm.local -v /local/path/to/project_repo:/app insightlane/crash-model:[tag]`
- Once the image is running, you can get a bash prompt to run pipeline commands/etc by running the following: `$ docker exec -it bcm.local /bin/bash`

**I want to add a new city**
- Step 1: Obtain crash data for your city
    - Try looking for your city’s open data portal or contacting someone from your local transportation department
    - Format should be CSV
- Step 1a: My crash data has addresses instead of latitude and longitude
    - See our [geocoding section](https://github.com/Data4Democracy/crash-model/tree/master/src#geocoding) for how to process this into latitude and longitude
- Step 2: Set up your environment (See above)
- Step 3: Generate a configuration file
[Detailed walkthrough](https://github.com/Data4Democracy/crash-model/tree/master/src#initializing-a-city)
    - Run `python initialize_city.py -city  -f  -crash  --supplemental ,`
        - City name: e.g. ""Cambridge, MA, USA"".
        - Folder name: Name for city folder
        - Crash file: The location of the crash data
         - Supplemental files: Any other files that contain additional features
    - Edit generated configuration file to specify columns in crash data containing id, latitude and longitude
- Step 4: Run the pipeline
    - Navigate to the src directory
    - Run python pipeline.py -c 
- Step 5: Check results
    - There should be a number of files in the data//processed directory

**I want to run the interactive visualization (showcase)** 
- [Obtain a Mapbox token](https://docs.mapbox.com/help/how-mapbox-works/access-tokens/) 
- Export an environment variable called MAPBOX_TOKEN
- Export an environment variable `CONFIG_FILE=config_.yml`

Contributing
-----------------------
""First-timers"" are welcome! Whether you're trying to learn data science, hone your coding skills, or get started collaborating over the web, we're happy to help. If you have any questions feel free to pose them on our [Slack channel](https://join.slack.com/t/insightlane/shared_invite/zt-ewlvaic7-ymYlps33v2M2~RhC4DFRGg), or reach out to one of the team leads. 

**I want to know what’s going on and pick up a task I like**

Open tasks are available [here](https://github.com/Data4Democracy/crash-model/issues)
Issues pertaining towards upcoming releases are available [here](https://github.com/Data4Democracy/crash-model/projects)


**I want to add a new city to the online showcase**

Once you’ve successfully run the pipeline on a city, get in touch with the Insight Lane team for details how to add to the showcase

Connect with us
-----------------------
Join our [Slack channel](https://join.slack.com/t/insightlane/shared_invite/zt-ewlvaic7-ymYlps33v2M2~RhC4DFRGg).

Leads:
 - @bpben
 - @j-t-t
 - @terryf82
 - @andhint
 - @alicefeng
 


Project Organization
-----------------------

    ├── LICENSE
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py

Project structure based on the cookiecutter data science project template. #cookiecutterdatascience
",,2024-04-11T03:34:02Z,19,111,27,"('j-t-t', 1060), ('bpben', 252), ('alicefeng', 194), ('christophercahill', 33), ('shreyapandit', 24), ('andhint', 8), ('anooparoor', 7), ('Joshuakim1011', 7), ('nsteins', 7), ('errcHuang', 6), ('therriault', 3), ('alchenist', 3), ('azkajavaid', 2), ('learningsomethingnew', 2), ('piyushm08', 2), ('catmurd0ck', 1), ('CooperData', 1), ('spbail', 1), ('jeffrimko', 1)","[16, 'Peace, Justice and Strong Institutions']"
TheTorProject/gettor,GetTor - a Tor Browser distribution system,"GetTor Revamp
=============

GetTor Revamp done during the Google Summer of Code 2014 for the Tor Project.
This repository continues to being used for improvements and further
development.

What is GetTor?
===============

GetTor was created as a program for serving Tor and related files over SMTP,
thus avoiding direct and indirect _censorship_ of Tor's software, in particular,
the Tor Browser Bundle (TBB). Users interacted with GetTor by sending emails
to a specific email address. After the user specified his OS and language,
GetTor would send him an email with an attachment containing the requested
package. This worked well for a while, but the bundles started to get too
large for being sent as attachments in most email providers. In order to fix
this, GetTor started to send (Dropbox) links instead of attachments.

What are the goals of the new GetTor?
=====================================

Here is a list of the main goals the new GetTor should accomplish:

 * Safe. Remember we are serving people under _heavy censorship_.
 * Easy to use. The fewer user interactions, the better.
 * Clean code. It should be clear to other developers/contributors how GetTor
 works and how it can be improved.
 * Automated. We should try to automate things as much as possible.
 * Language and provider friendly. It should be easy to support new languages
 and to add new providers for storing packages and generate links.


How does the new GetTor works?
==============================

Below are some specifications and core concepts on how the new GetTor works.

*Links files*: Currently links are saved in files with the '.links' extension,
using the ConfigParser format (RFC 882). A sample link file should look like
this:

--- BEGIN FILE ---

  [provider]
  name = CoolCloudProvider

  [key]
  fingerprint = AAAA BBBB CCCC DDDD EEEE FFFF GGGG HHHH IIII JJJJ

  [linux]
  en = Package (64-bit): https://cool.cloud.link64
	ASC signature (64-bit): https://cool.cloud.link64.asc
	Package SHA256 checksum (64-bit): superhash64,
	Package (32-bit): https://cool.cloud.link32
	ASC signature (32-bit): https://cool.cloud.link32.asc
	Package SHA256 checksum (32-bit): superhash32

  [windows]
  ...

  [osx]
  ...

--- END FILE ---

You can also check providers/dropbox.links for a better example.

*Core*: the heart of GetTor. Receives requests for links for a certain OS and
language and respond accordingly. It also presents an easy way for scripts
to create links file.

*SMTP*: Receives requests via email, process them, contact the core module if
necessary and respond to the user in the specified language. People can send
blank or dummy emails to it to receive a help message describing how to ask
for links. Email forwarding is used to redirect the emails to GetTor.

*XMPP*: Same as above, but via XMPP (account needed). It has been tested with
dukgo.com, jabber.ccc.de, riseup.net. It doesn't seem to be able to interact
with gtalk users.

*Twitter*: Receive requests via Twitter direct messages, contact the core module
if necessary and respond to the user in the specified language. Unfinished.

*DB*: Store anonymous info about the people that interact with GetTor in order
to keep count of the number of requests per person and avoid malicious users
that try to collapse the service. It also keeps count of how many requests
GetTor has received during its lifetime. A lot of other data was being saved
in the original gsoc project, but it was changed to save the minimum. 

*Blacklist*: Provide a mechanism to avoid flood and deny interaction to
malicious users.

*Providers scripts*: every supported provider should have a script to
automatically upload packages to 'the cloud' and create the corresponding
links files. The script should consider the following steps:

 * Upload the packages.
 * Get the sha256 checksum of the files uploaded.
 * Get the PGP key fingerprint that signed the files.
 * Check for .asc file for every package uploaded.
 * Put all together in a '.link' file (using the core module).


What is the current status of the new GetTor?
=============================================

Deployed and working.


How can I help?
================

If you have ideas to improve GetTor and/or add new providers, please tell us!
I'm currently the lead developer on this, so if you have any comments/doubts/
ideas you can send me an e-mail to ilv _at_ riseup _dot_ net or ping me (ilv),
or sukhe or mrphs at #tor-dev in the OFTC IRC network. For openning tickets you
should use the trac[0] and select the GetTor component. Some neat ideas we
could use are the following:

 * Report bugs!
 * Create script for new providers, namely: Google Drive, Github. Check 
providers.txt
 * Create a new module for distributing links. Check distribution_methods.txt
 * Finish the Twitter module.
 * Propose code/behaviour improvements.
 * Update the specs.


References
===========

[0] https://trac.torproject.org/projects/tor/query?status=accepted&status=assigned&status=needs_information&status=needs_review&status=needs_revision&status=new&status=reopened&component=GetTor&col=id&col=summary&col=component&col=status&col=type&col=priority&col=milestone&order=priority

",,2021-01-27T12:11:44Z,10,100,26,"('kaner', 182), ('ilv', 160), ('ioerror', 96), ('azadi', 8), ('runasand', 6), ('aagbsn', 5), ('agharbeia', 4), ('DonnchaC', 3), ('0xPoly', 3), ('arlolra', 1)","[16, 'Peace, Justice and Strong Institutions']"
opengovernment/opengovernment,OpenGovernment -- a project of the Participatory Politics Foundation,"OpenGovernment is a Ruby on Rails application for aggregating and presenting open government data.

# Overview

This project powers [OpenGovernment.org](http://opengovernment.org) and was started by the [Participatory Politics Foundation](http://ppolitics.org).

We hope you'll get involved! Read our [Contributors' Guide](https://github.com/opengovernment/opengovernment/blob/master/CONTRIBUTING.md) for details.

  * Mailing list: Join our [developer list](http://groups.google.com/group/opengovernment/).
  * IRC: Find us in chat.freenode.net channel [#opengovernment](irc://chat.freenode.net/opengovernment).
  * Development Roadmap: [December 2012](https://github.com/opengovernment/opengovernment/wiki/Dec.-2012---Development-Roadmap).
  * Project management & bug tracker: to come, will be updated Dec. 2012. Previously on Pivotal Tracker & Lighthouse. 

## Visit our [Wiki](https://github.com/opengovernment/opengovernment/wiki) for full installation instructions.
",,2023-01-03T16:20:02Z,8,295,31,"('morganknutson', 285), ('saki', 174), ('davidmooreppf', 90), ('eostrom', 25), ('huynguyen', 17), ('jeff-r', 12), ('benmoss', 3), ('opencongress', 2)","[16, 'Peace, Justice and Strong Institutions']"
mojaloop/mojaloop,Starting point for on-boarding and contribution documentation for mojaloop ,"
  
    
  


Mojaloop aims at creating payment platforms that are interoperable, connecting digital financial service providers and customers by providing specifications, standards and open-source software.

[![Git Releases](https://img.shields.io/github/v/release/mojaloop/helm?label=helm%20version)](https://github.com/mojaloop/helm/releases)
[![License](https://img.shields.io/badge/Licence-Apache%202.0-orange.svg)](./LICENSE.md)

## Documentation
- [Official Mojaloop Documentation](https://docs.mojaloop.io/documentation)
- [Mojaloop Specification](https://github.com/mojaloop/mojaloop-specification)
- [Developer Onboarding Guide](./onboarding.md)
- [Deploy Mojaloop On Kubernetes](https://docs.mojaloop.io/documentation/deployment-guide/)
- [Mojaloop Business Documentation](https://docs.mojaloop.io/mojaloop-business-docs/)

## Contact

[Slack](https://mojaloop-slack.herokuapp.com/) is the best way to keep in touch with the Mojaloop Community. Sign up for your Mojaloop Slack account [here](https://mojaloop-slack.herokuapp.com/).

__Channels:__
- `#announcements` - Announcements for new Releases and QA Status
- `#design-authority`- Questions + Discussion around Mojaloop Design
- `#general` - General discussion about Mojaloop
- `#help-mojaloop` - Ask for help with installing or running Mojaloop
- `#ml-oss-bug-triage` - Discussion and triage for new bugs and issues

## Contributing

Refer to the [Contributors guide](https://docs.mojaloop.io/documentation/contributors-guide/) to get started making contributions.

You may also want to read:
- [Mojaloop Github Project](https://github.com/mojaloop/project)
  > _note: You must install the [ZenHub extension](https://www.zenhub.com/extension) in order to see the Project Kanban Board_
- [Developer Onboarding Guide](./onboarding.md)

If you have any trouble getting started, or want to know where to start, reach out to us on the `#general` channel in [Slack](https://mojaloop-slack.herokuapp.com/)!

### Mojaloop Repositories

Refer to the [Mojaloop Repository Overview](https://docs.mojaloop.io/documentation/repositories/) for a detailed breakdown of each Mojaloop Repository in development.

As a quick reference, some of the most important repositories for contributors are:

- [central-ledger](https://github.com/mojaloop/central-ledger) - Core Service in charge of managing transfers between DFSPs
- [ml-api-adapter](https://github.com/mojaloop/ml-api-adapter) - Translation layer to convert to/from Mojaloop API to an internal format that is used in Central Services Stack.
- [account-lookup-service](https://github.com/mojaloop/account-lookup-service) - A service for managing party accounts across a Mojaloop environment
- [helm](https://github.com/mojaloop/helm) - The Helm chart development for deploying Mojaloop on Kubernetes
- [postman](https://github.com/mojaloop/postman) - A set of Postman scripts for testing and managing a Mojaloop environment
- [documentation](https://github.com/mojaloop/documentation) - The mojaloop documentation, published [here](https://docs.mojaloop.io/documentation)

### Creating New Issues

We use the [mojaloop/project](https://github.com/mojaloop/project) repository as an issue tracker for all repositories across the Mojaloop Project.

When creating a new issue, select from the issue templates provided [here](https://github.com/mojaloop/project/issues/new/choose)


## Support from the wider community

Special thanks to all the organizations and individuals who have supported this Open-Source effort.

>_This list (in alphabetical order) is by no means exhaustive and complete. Please raise a Pull Request if you believe your Organization should be included in this list._

- [Bill & Melinda Gates Foundation](https://www.gatesfoundation.org/)
- [Coil](https://coil.com/)
- [Crosslake](https://crosslaketech.com/)
- [Fintech Inversiones](http://www.fintechinversiones.com.py)
- [Google LLC](https://opensource.google/)
- [Modusbox](http://modusbox.com/)
- [![pullreminders](https://pullreminders.com/badge.svg)](https://pullreminders.com?ref=badge)
","'africa', 'core-related', 'digital', 'financial-products', 'financial-providers', 'financial-services', 'fintech', 'interoperable-payments-platforms', 'mobile', 'payment', 'payment-service', 'payments', 'poor', 'ussd'",2023-09-07T14:16:08Z,15,306,87,"('kjw000', 170), ('HenkKodde', 4), ('lewisdaly', 4), ('BillHodghead', 2), ('millerabel', 2), ('simeonoriko', 2), ('adrianhopebailie', 1), ('tbm', 1), ('vorburger', 1), ('mdebarros', 1), ('rmothilal', 1), ('rdonkin', 1), ('gitter-badger', 1), ('eoln', 1), ('hlry', 1)","[17, 'Partnerships for the Goals']"
openeemeter/caltrack,Shared repository for documentation and testing of CalTRACK methods,"CalTRACK Technical Documentation
================================

CalTRACK methods are developed in an open and transparent stakeholder process that uses empirical testing to define replicable methods for calculating normalized metered energy consumption using either monthly or interval data from an existing conditions baseline.

This repository contains the CalTRACK 2.0 methods, and the CalTRACK Technical Appendix, which explains how many of the methods were developed.

Future improvements are catalogued as ""Issues"" and are found in the Projects tab. These issues are considered ""closed"" until they are formally re-opened by the Working Group. 

Formal changes in methods will follow processes established under the JDF charter:

Deliverable Development Process

Working Groups.  The Project may have multiple Working Groups, and each Working Group will operate as set forth in this Section and its Working Group Charter.

Working Group Chair.  Each Working Group will designate a chair for that Working Group.  A Working Group may select a new chair upon Approval of the Working Group Participants.

Working Group Requirements.  Each Working Group must be comprised of at least 2 Working Group Participants.  No Working Group Participant will be permitted to participate in a Working Group without first Joining the Working Group.

Conditions for Contributions.  A Steering Member, Associate, or Contributor may not make any Contribution unless that Steering Member, Associate or Contributor is the exclusive copyright owner of the Contribution or has sufficient copyright rights from the copyright owners to make the Contribution under the terms of this Project Charter and applicable Working Group Charter.  The Steering Member, Associate, or Contributor must disclose the identities of all known copyright owners in the Contribution.

**Deliverable Development Process**

Pre-Draft.  Any Working Group Participant or Contributor may submit a proposed initial draft document as a candidate Draft Deliverable of that Working Group.  The Working Group chair will designate each submission as a “Pre-Draft” document. This [quick-start video](https://www.dropbox.com/s/n5r3ihq6eanyl7l/em2_github_issues.mp4?dl=0) shows how to submit an issue for consideration.

Draft.  Each Pre-Draft document of a Working Group must first be Approved by the Working Group Participants of that Working Group to become a Draft Deliverable.  Once the Working Group approves a document as a Draft Deliverable, the Draft Deliverable becomes the basis for all going forward work on that deliverable.

Working Group Approval.  Once a Working Group believes it has achieved the objectives for its deliverable as described in the Scope, it will progress its Draft Deliverable to “Working Group Approved” status. 

Final Approval.  Upon a Draft Deliverable reaching Working Group Approved status, the Executive Director or his/her designee will present that Working Group Approved Draft Deliverable to all Steering Members for Approval.  Upon Approval by the Steering Members, that Draft Deliverable will be designated an “Approved Deliverable.”

Publication and Submission.  Upon the designation of a Draft Deliverable as an Approved Deliverable, the Executive Director will publish the Approved Deliverable in a manner agreed upon by the Working Group Participants (i.e., Project Participant only location, publicly available location, Project maintained website, Project member website, etc.).  The publication of an Approved Deliverable in a publicly accessible manner must include the terms under which the Approved Deliverable and/or source code is being made available under, as set forth in the applicable Working Group Charter.

Submissions to Standards Bodies.  No Draft Deliverable or Approved Deliverable may be submitted to another standards development organization without Approval by the Steering Members.  Upon Approval by the Steering Members, the Executive Director will coordinate the submission of the applicable Draft Deliverable or Approved Deliverable to another standards development organization with Joint Development Foundation Projects, LLC.    Working Group Participants that developed that Draft Deliverable or Approved Deliverable agree to grant the copyright rights necessary to make those submissions.
",,2023-05-18T21:58:12Z,16,54,44,"('matthewgee', 81), ('mcgeeyoung', 53), ('tplagge', 36), ('hshaban', 30), ('goldenmatt', 23), ('marcpare', 11), ('blakehough', 11), ('dyeager-recurve', 9), ('houghb', 7), ('kevin-gries', 5), ('philngo-recurve', 5), ('peterbolson', 4), ('marcrecurve', 3), ('CBestbadger', 2), ('philngo', 2), ('ssuffian', 1)","[7, 'Affordable and Clean Energy']"
SFDigitalServices/sf-dahlia-web,DAHLIA is the affordable housing portal for the City and County of San Francisco.,"# Dahlia

[![CircleCi Builds](https://app.circleci.com/pipelines/github/SFDigitalServices/sf-dahlia-web)](https://app.circleci.com/pipelines/github/SFDigitalServices/sf-dahlia-web)

Cross-browser testing done with 

## Purpose

DAHLIA is the affordable housing portal for the City and County of San Francisco. It was created by the Mayor's Office of Housing and Community Development (MOHCD). This application streamlines the process of searching and applying for affordable housing, making it easier to rent, buy and stay in our City.

## Technical Architecture

### In-progress technical migration

We are currently in the process of migrating our app from AngularJS to React/TS.

The new, React codebase lives under app/javascript. Pages are routed via rails to load either react or angular versions of each page.

React pages will be released behind feature flags, see the **Rewrite feature flags** section of the readme for more information. On any page that has an in-progress react version, you can override the rewrite feature flag behavior by adding `?react=true` or `?react=false` to the url to force React or Angular rendering, respectively.

### Pre-migration

This repository contains the source code for [housing.sfgov.org](https://housing.sfgov.org), which is the user-facing web application of the DAHLIA platform. It is a [Ruby on Rails](http://rubyonrails.org/) application that serves up a single page [AngularJS](https://angularjs.org/) app. The web application connects to a Salesforce backend (you can find the source code for that [here](https://github.com/Exygy/sf-dahlia-salesforce)), which is where the listings are actually created and administered. The primary purpose of the PostgreSQL database on the web application is to serve as user authentication (using [Devise](https://github.com/plataformatec/devise) + [Devise Token Auth](https://github.com/lynndylanhurley/devise_token_auth)), with every user in the database getting a `salesforce_contact_id` which corresponds to their record in the Salesforce database.

## Dependencies

Before you install DAHLIA, your system should have the following:

- [Homebrew](http://brew.sh)
- [Ruby](https://www.ruby-lang.org/en/documentation/installation/) 3.1.3 (Use [RVM](https://rvm.io/rvm/install) or [rbenv](https://github.com/rbenv/rbenv))
  - For issues installing on an Apple Silicon mac, go [here](https://zwbetz.com/install-ruby-version-manager-on-mac/)
- [Bundler](https://github.com/bundler/bundler) `gem install bundler`
- [PostgreSQL](https://postgresapp.com/)
- [Node.js](https://nodejs.org/en/) 18.12.1
  - Installing node with nvm is recommended. See [installing NVM and node.js on MacOS](https://stackoverflow.com/a/28025834/260495).
- [Yarn](https://classic.yarnpkg.com/en/docs/install/#mac-stable)
  - After node is installed, you can install yarn with `npm install --global yarn`

## Getting started

More information about getting started can be found on the team confluence.

1. Make sure your PostgreSQL server is running (e.g. using [Postgres.app](https://postgresapp.com/) listed above)
1. Open a terminal window
1. `git clone https://github.com/SFDigitalServices/sf-dahlia-web.git` to create the project directory
   - Using gh is recommended. This can be installed with either [Brew](https://brew.sh/) or downloading directly from [Github](https://cli.github.com/)
1. `cd sf-dahlia-web` to open the directory
1. Using NVM, install 18.12.1 (or whatever version we are on) with `nvm install 18.12.1`
1. Using RVM, install 3.1.3 (or whatever version we are on) with `rvm instal 3.1.3`
1. `bundle install` to download all necessary gems
   - See [here](https://stackoverflow.com/a/19850273/260495) if you have issues installing `pg` gem with Postgres.app, you may need to use: `gem install pg -v  -- --with-pg-config=/Applications/Postgres.app/Contents/Versions/latest/bin/pg_config`
   - If you need to run this command make sure you run bundle install again following the success of the Postgres installation to install the remaining gems
1. `yarn install` to install bower, grunt and other dependencies (which will also automatically `bower install` to load front-end JS libraries)
1. `overcommit --install` to install git hooks into the repo
1. Download PostgreSQL. You only need to turn it on, the next step will set it up for you.
1. `rake db:create && rake db:migrate` to create the dev database and migrate the DB tables
1. copy `.env.sample` into a file called `.env`, and copy correct Salesforce environment credentials (not shared publicly in this repo)
1. Start Servers
   - `yarn client` to start the webpack dev server alone
   - `yarn server` to start rails server alone, which will now be running at http://localhost:3000 by default
   - `yarn start` to start both servers with a single command
1. Alternatively you can start the servers using the webpack and rails command directly
   - `NODE_OPTIONS=--openssl-legacy-provider ./bin/shakapacker-dev-server` to start webpack
     - This command might fail with `Command ""webpack-dev-server"" not found.`. In that case, you'll need to reinstall webpacker with `bundle exec rails:shakapacker:install`. During the install it will ask if you want to overwrite a few config files, do not overwrite them.
   - In another terminal tab, run `rails s` to start the rails server

## How to migrate a page from AngularJS to React

See [docs/migrating-to-react](docs/migrating-to-react.md) for a step-by-step guide.

## Running Tests

To run ruby tests:

- `rake spec`
  - you may need to install [imagemagick](https://formulae.brew.sh/formula/imagemagick) due to a dependency on the [minimagick gem](https://github.com/minimagick/minimagick)

To run Angular unit tests:

- `rake jasmine:ci` to run in terminal
- `rake jasmine` to then run tests interactively at http://localhost:8888/

To run React unit tests:

- to run the entire suite run `yarn test`
- to run a single file run `jest path/to/folder/.test.ts`

To run Legacy E2E (Angular) tests:

- Installation (needs to be run once): `./node_modules/protractor/bin/webdriver-manager update --versions.chrome 2.41 --versions.standalone 3.141.59` to get the selenium webdriver installed
- On one tab have your Rails server running: `rails s`
- On another tab, run `yarn protractor` to run the selenium webdriver and protractor tests. A Chrome browser will pop up and you will see it step through each of the tests.
- If you get errors starting selenium, make sure you have [java](https://java.com/en/download/) installed

To run E2E (React) tests:

- In one terminal start the application by running `yarn start`
- In another terminal
  - To run the full suite of tests run `yarn test:e2e`
  - To run a specific file run `cypress run --spec 'path/to/folder/.e2e.ts'`

Note: These tests will run on [CircleCi](https://app.circleci.com/pipelines/github/SFDigitalServices/sf-dahlia-web) as well for every review app and QA deploy.

Note: If you want to output logs to the terminal locally and in CircleCI, replace the `yarn test:e2e` command with `ELECTRON_ENABLE_LOGGING=true DEBUG=cypress:electron yarn test:e2e`

## Importing pattern library styles

We currently manually transfer the application's CSS from [our pattern library](https://github.com/SFDigitalServices/sf-dahlia-pattern-library) using Grunt.

If you do not already have grunt installed, run `brew install grunt` to install it before proceeding.

To update this app with the latest PL styles:

1. [Clone the PL repository in the same parent directory as this one.](https://github.com/SFDigitalServices/sf-dahlia-pattern-library)
1. Switch to the PL branch you want to import styles from, either main or a specific branch
1. Run `npm run-script build` in the pattern lib directory to compile the css
1. `cd` to your `sf-dahlia-web` folder
1. Run `grunt`
1. Commit the updates to toolkit.scss with a reference to the commit you're updating from on pattern-lib

We use `grunt-clean` and `grunt-copy` to transfer the CSS, and `grunt-replace` to replace relative background image paths with Rails asset URLs.

## Running with a cache locally

In order to test caching locally,

1. Install memcached locally with `brew install memcached`
1. Start memcached in a new tab by running `memcached`
1. Add an empty file to the repo route named `tmp/caching-dev.txt`
1. Set CACHE_SALESFORCE_REQUESTS='true' in your .env
1. Start up the app

## Running stress testing against Salesforce

To run stress testing against the Salesforce instance, refer to the documentation in the [stress testing folder](load_testing/load_testing.md)

## Releases

Follow the [Webapp release process](https://sfgovdt.jira.com/wiki/spaces/HOUS/pages/2775351453/Frontend+release+process) page on Confluence for the full release guide.

## Environment variable configurations

### DALP Advertising

- ADVERTISE_DALP -> If set to 'true', the Sales directory page will display info about applying to DALP in a ""Help with downpayments"" section. Otherwise it'll show the plain ""Get help"" section
- DALP_PROGRAM_INFO -> If provided, we will override the default DALP text of ""The 2021 Downpayment Assistance Loan Program (DALP) will begin accepting applications on February 26, 2021."" with whatever is in this env var.

### Rewrite feature flags

We have flags for each chunk of the rewrite we release. These will set those pages to default to the React version. This can be overridden with

- HOME_PAGE_REACT='true'
- DIRECTORY_PAGE_REACT='true'
- LISTING_DETAIL_PAGE_REACT='true'
- GET_ASSISTANCE_PAGES_REACT='true'

### React env variables

- TOP_MESSAGE string, turn top message on
- TOP_MESSAGE_TYPE defaults to `alert`, other options: [`primary`, `success`]
- TOP_MESSAGE_INVERTED default to `false`, when set to `true` sets AlertBox prop to inverted

### Other
- COVID_UPDATE -> If set to 'true', shows COVID-19 update info in the apply section and hides pre lottery info

## Contributing changes

Use the engineering workflow and coding style standards established below. :smiley:

### Acceptance/Feature Apps

Temporary ""acceptance"" apps are created upon opening a pull request for a feature branch. After the pull request is closed, the acceptance app is automatically spun down. See [this Heroku article](https://devcenter.heroku.com/articles/github-integration-review-apps) for details.

### Code style and quality

#### Javascript

Javascript code quality is ensured by two npm packages: JsHint and JSCS. They will run automatically as a pre-commit hooks. Follow the [Airbnb JavaScript Style guide](http://nerds.airbnb.com/our-javascript-style-guide/).

#### Ruby

[Rubocop](https://github.com/bbatsov/rubocop) is configured in `.rubocop.yml` to enforce code quality and style standards based on the [Ruby Style Guide](https://github.com/bbatsov/ruby-style-guide) and runs every time you commit using a pre-commit hook. Refer to the [Ruby Style Guide](https://github.com/bbatsov/ruby-style-guide) for all Ruby style questions.
To identify and have Rubocop automatically correct violations when possible, run:

- `rubocop -a [path_to_file]` for individual files
- `rubocop -a` for all Ruby files

### Changing the Style Guide settings

Any changes to Rubocop, JSCS, etc. affect the entire team, so it should be a group decision before commiting any changes. Please don't commit changes without discussing with the team first.

### VS Code Setup

1. Copy `.vscode-default` to `.vscode` like `cp -r .vscode-default .vscode`
   a. We don't commit vscode workspace settings directly to the repo, instead we have a shared settings starting point file. That way you can add workspace specific settings that don't affect your team members (for example [Peacock workspace color settings](https://www.peacockcode.dev/guide/#install))
1. Install recommended extensions (under [.vscode-default/extensions](.vscode-default/extensions)).
1. Double check your user settings aren't overriding the [workspace editor settings](.vscode-default/settings)

### Credits

### License

Copyright (C) 2015 City and County of San Francisco

DAHLIA is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with DAHLIA. If not, see [http://choosealicense.com/licenses/gpl-3.0/](http://choosealicense.com/licenses/gpl-3.0/)
","'civic', 'government', 'housing', 'open-source'",2024-05-03T15:30:55Z,30,29,22,"('kaptron', 626), ('chrisdolendo', 593), ('menslow', 568), ('akegan', 245), ('callaghanc', 97), ('christine-sfg', 96), ('cade-exygy', 88), ('software-project', 78), ('tallulahkay', 69), ('chadbrokaw', 69), ('jimlin-sfgov', 66), ('dependabotbot', 56), ('cliu02', 56), ('kramduckner', 56), ('creativecoder', 48), ('alulabeshue-sfgov', 48), ('james-wills-sf', 46), ('chadbrokaw-exygy', 36), ('emilyjablonski', 30), ('mluedke2', 24), ('CindyConway', 18), ('dstavis', 16), ('jrubenoff', 12), ('exygy-dev', 8), ('LoganArnett', 6), ('shawnbot', 4), ('slowbot', 3), ('aekong', 3), ('ashleymmeyers', 2), ('bk3c', 1)","[11, 'Sustainable Cities and Communities']"
motech/motech,Mobile Technology for Community Health,"MOTECH Platform
---------------

The MOTECH Platform is an open source enterprise software package that connects popular eHealth technologies to unlock new outcomes toward strengthening health systems. MOTECH has been deployed across the globe in numerous health domains including maternal and child health, treatment adherence, frontline worker education and information collection.

MOTECH consists of a core platform and optional modules, each providing use of a technology such as SMS or email, or access to an external system such as CommCare or OpenMRS. Implementers can choose to install one or more modules, and developers can extend MOTECH by writing new modules. This repository contains the code for the core platform, which comes with a few essential modules. Many additional modules may be found in our [Modules repo](http://github.com/motech/modules).

Interested in learning more about MOTECH? Try the following resources:
* [MOTECH Project Website](http://motechproject.org)
* [MOTECH Documentation](http://docs.motechproject.org)
* [Issue Tracker](https://applab.atlassian.net/projects/MOTECH/summary)
* [Mailing List](https://groups.google.com/forum/?fromgroups#!forum/motech-dev)

Installation
------------

### Platform

If you'd like to install and run the latest MOTECH binaries, go [here](http://docs.motechproject.org/en/latest/get_started/installing.html).

If you'd prefer to build MOTECH yourself, try [these instructions](http://docs.motechproject.org/en/latest/development/dev_setup/dev_install.html) instead.

### Modules

The Platform war file contains all modules required for starting and managing MOTECH. To install additional modules, you can either use the Admin UI to install them at runtime or place them in the `~/.motech/`bundles directory and restart MOTECH. Note that doing a `mvn clean install` on any of our modules will place that module in the `~/.motech/bundles` directory automatically. Modules from that directory always override the ones contained in the war if their Bundle-Version and Bundle-SymbolicName are the same.

Contributing
------------

We welcome contributions from the open source community. For instructions on how to get started as a MOTECH contributor, please check out the [Contribute](http://docs.motechproject.org/en/latest/contribute/index.html) section of our documentation.

Disclaimer Text Required By Our Legal Team
------------------------------------------

Third party technology may be necessary for use of MOTECH 2.0. This agreement does not modify or abridge any rights or obligations you have in open source technology under applicable open source licenses.

Open source technology programs that are separate from MOTECH are provided as a courtesy to you and are licensed solely under the relevant open source license. Any distribution by you of code licensed under an open source license, whether alone or with MOTECH, must be under the applicable open source license.
",,2019-02-08T14:56:38Z,30,17,37,"('pgesek', 1429), ('sebbrudzinski', 389), ('wstrzelczyk', 315), ('jslawinski', 236), ('dileepbapat', 212), ('ngraczewski', 175), ('pkornowski', 157), ('mkwiatkowskisoldevelo', 138), ('sanchitbahal', 132), ('igoop', 105), ('mkustusz', 100), ('mkruszynski', 91), ('wlricky', 88), ('balaji', 87), ('sroytw', 80), ('sohamghosh', 68), ('agrzywinski', 68), ('lukasimha2', 54), ('balajin', 54), ('vinkesh', 47), ('devaradhan', 45), ('MichalKus', 40), ('nickdotreid', 34), ('VigneshRE', 33), ('atishbeehyv123', 33), ('shruthidipali', 28), ('sgunadhya', 27), ('geetchandratre', 27), ('shanmukhm', 24), ('sr-nu', 24)","[17, 'Partnerships for the Goals']"
Social-Synergy/inclusion-UKR,Ukrainian inclusion font for printing and web,"# About font “Inclusion UKR”
Thank you for your interest in the new Ukrainian Cyrillic font “Inclusion UKR” developed by UNICEF Ukraine in partnership with NGO “Social Synergy”.
“Inclusion UKR” was specifically designed to be convenient for a wide range of users, including children who have difficulties learning to read due to dyslexia. For reference, dyslexia is a learning disorder caused by differences in the brain that make processing and interpreting of written symbols problematic. Statistics estimates that anywhere between 10% and 20% of global population experience difficulties related to dyslexia. These difficulties present particular challenge for children as they learn to read and write.  

One of the ways to mitigate the issue and facilitate learning is using specialized typefaces/fonts. Some Latin script type fonts have long been developed and, but the need of those, who use Cyrillic script was not met for a long time. Unlike some attempts to adapt existing Latin-script solutions, “Inclusion UKR” is a fundamentally new type font created in Ukraine in close collaboration with scientists. The initial version was tested for ease and comfort of use with an impressive group of 74 children with dyslexia and their parents (cf. similar tests done internationally included groups of some 15-20 adults).  

![Illustration 1](https://github.com/Social-Synergy/inclusion-UKR/blob/master/docs/picture_1.png)

At present, the testing continues in pilot schools both mainstream and specialized and engages a few hundred students with and without learning disorders.

![Illustration 1](https://github.com/Social-Synergy/inclusion-UKR/blob/master/docs/picture_2.png)

The font was designed by the Ukrainian artist, illustrator and type-designer Oleh Petrenko-Zanevsky, well known to young readers and their parents for illustrations in children's books from publishing houses publishing houses [А-БА-БА-ГА-ЛА-МА-ГА](http://ababahalamaha.com.ua/en), [ВСЛ](https://starylev.com.ua/foreign-rights) and many others. The related scientific research studies were facilitated by Romana Ivanus’ and Eliana Danilavichute.  

The original version of the font consisted of letters, numbers and punctuation marks just sufficient for use in Language learning or general (non-technical) literature and offered technical formats for use in printing and web.  

Inclusion UKR will be made available for commercial use by publishers of children's, educational and other literature.  

![Illustration 1](https://github.com/Social-Synergy/inclusion-UKR/blob/master/docs/picture_4.png)


Currently, the font is open for non-commercial use under **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License** (""Public License"") [CC BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).
 
For free download please use files from repository folders above (file types: [.ttf](https://github.com/Social-Synergy/inclusion-UKR/blob/master/OpenType-TT/inclusion.ukr-regular.ttf), [.otf](https://github.com/Social-Synergy/inclusion-UKR/blob/master/OpenType-PS/inclusion.ukr-regular.otf), [.woff](https://github.com/Social-Synergy/inclusion-UKR/blob/master/Web-TT/inclusion.ukr-regular.woff), .[woff2](https://github.com/Social-Synergy/inclusion-UKR/blob/master/Web-PS/inclusion.ukr-regular.woff2), [.eot](https://github.com/Social-Synergy/inclusion-UKR/blob/master/Web-TT/inclusion.ukr-regular.eot)).

For additional support on installing fonts, view instructions for Windows OS [here](https://support.microsoft.com/en-us/help/314960/how-to-install-or-remove-a-font-in-windows) and for OS X (Mac) [here](https://support.apple.com/guide/font-book/install-and-validate-fonts-fntbk1000/mac).
The font can also be used on Linux, iOS and Android devices.

Happy typing!

### Version 2.004
The current version added features Latin script, a set of symbols (including arithmetic) and an extended set of punctuation marks.
Further development of the font (including styles, letters and symbols) is under way.

![Illustration 1](https://github.com/Social-Synergy/inclusion-UKR/blob/master/docs/picture_3.png) 
",,2020-12-04T10:13:17Z,1,0,1,"('Social-Synergy', 88)","[10, 'Reduced Inequalities']"
usetania/tania-core,Tania is a farm management software for the hobbyist and smallholder farmer.,"
    
    The Farmer Journal
    
    
    
    


# Warning

This is the development branch of Tania. Changes can occur nightly. If you need the stable branch you can checkout [the master branch](https://github.com/Tanibox/tania-core/tree/master).

## Roadmap

You can check the roadmap in [Tania's GitHub project](https://github.com/orgs/usetania/projects/6/views/1).

---

**Tania** is a free and open source farm management software. You can manage your farm areas, farm reservoirs, farm tasks, inventories, and the crop growing progress. It is designed for any type of farms.

Download Tania for Windows x64 and Linux x64 on [the release page](https://github.com/Tanibox/tania-core/releases/tag/1.7.1).

![Screenshot](screenshot.PNG)

## Getting Started

This software is built with [Go](https://golang.org) programming language. It means you will get an executable binary to run on your machine. You **don't need** extra software like MAMP, XAMPP, or WAMP to run **Tania**, but you may need MySQL database if you choose to use it instead of SQLite *(the default database.)*

If your OS is not listed on our releases page, you have to build Tania for your OS by yourself. You can follow our instructions to build **Tania**.

### Prerequisites
- [Go](https://golang.org) >= 1.16
- [NodeJS](https://nodejs.org/en/) >= 16

### Building Instructions

**THIS DOCUMENTATION WILL BE UPDATED LATER**

We are in the progress of building the new frontend application.

### Database Engine

Tania uses SQLite as the default database engine. You may use MySQL as your database engine by replacing `sqlite` with `mysql` at `tania_persistence_engine` field in your `backend/conf.json`.

```
{
  ""app_port"": ""8080"",
  ""tania_persistence_engine"": ""sqlite"",
  ""demo_mode"": true,
  ""upload_path_area"": ""uploads/areas"",
  ""upload_path_crop"": ""uploads/crops"",
  ""sqlite_path"": ""db/sqlite/tania.db"",
  ""mysql_host"": ""127.0.0.1"",
  ""mysql_port"": ""3306"",
  ""mysql_dbname"": ""tania"",
  ""mysql_user"": ""root"",
  ""mysql_password"": ""root"",
  ""redirect_uri"": [
      ""http://localhost:8080"",
      ""http://127.0.0.1:8080""
  ],
  ""client_id"": ""f0ece679-3f53-463e-b624-73e83049d6ac""
}
```

### Run The Test

Use `go test ./...` inside the `backend` folder to run all the Go tests.

## REST APIs
**Tania** have REST APIs to easily integrate with any softwares, even you can build a mobile app client for it. You can import the JSON file inside Postman directory to [Postman app](https://www.getpostman.com).

## Contributing to Tania

We welcome contributions, but request you to follow these [guidelines](contributing.md).

### Localisation

You can help us to localise Tania into your language by following these steps:

1. Copy `frontend/languages/template.pot` and paste it to `frontend/languages/locale` directory.
2. Rename it with your language locale code e.g: `en_AU.po`, `de_DE.po`, etc.
3. Fill `msgstr` key with your translation. You can edit the `.po` file by using text editor or PO Edit software.
4. Pull request your translation to the `master` branch.

### Build Tania localisation by yourself

**THIS DOCUMENTATION WILL BE UPDATED LATER**

We are in the progress of building the new frontend application.

Then follow the instruction to [build Tania](#building-instructions).

## Support Us

We will move from OpenCollective to GitHub sponsorship. Thank you for all your donation in OpenCollective.

### Contributors

This project exists thanks to all the people who contribute.


### Backers



## Copyright and License

Copyright to Tania and other contributors under [Apache 2.0](https://github.com/usetania/tania-core/blob/master/LICENSE) open source license.
","'ddd-architecture', 'end-user', 'farm', 'farm-management', 'farming', 'go', 'golang', 'nextjs', 'reactjs', 'tania'",2023-09-21T15:18:40Z,19,744,34,"('adhatama', 458), ('kerwinjorbina', 238), ('bepitulaz', 143), ('gkgranada', 106), ('pablojimpas', 93), ('dependabotbot', 48), ('purwandi', 46), ('yasintze', 26), ('retnoika', 24), ('lynxluna', 23), ('suciptoid', 5), ('harkce', 2), ('kosnick', 2), ('jhonatas-mendes', 1), ('jokosu10', 1), ('deepsource-autofixbot', 1), ('joelschutz', 1), ('trendspotter', 1), ('wotnak', 1)","[9, 'Industry, Innovation and Infrastructure']"
openai/spinningup,An educational resource to help anyone learn deep reinforcement learning.,"**Status:** Maintenance (expect bug fixes and minor updates)

Welcome to Spinning Up in Deep RL! 
==================================

This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).

For the unfamiliar: [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with [deep learning](http://ufldl.stanford.edu/tutorial/).

This module contains a variety of helpful resources, including:

- a short [introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory,
- an [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role,
- a [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important papers organized by topic,
- a well-documented [code repo](https://github.com/openai/spinningup) of short, standalone implementations of key algorithms,
- and a few [exercises](https://spinningup.openai.com/en/latest/spinningup/exercises.html) to serve as warm-ups.

Get started at [spinningup.openai.com](https://spinningup.openai.com)!


Citing Spinning Up
------------------

If you reference or use Spinning Up in your research, please cite:

```
@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}
```",,2024-04-17T06:06:50Z,20,9659,226,"('jachiam', 43), ('bchess', 19), ('rootulp', 5), ('albertwujj', 4), ('mimoralea', 4), ('christopherhesse', 2), ('marko-bast', 2), ('allanbreyes', 1), ('alok', 1), ('maksay', 1), ('abinitio888', 1), ('marcinic', 1), ('colllin', 1), ('joaomacp', 1), ('michalkordyzon', 1), ('Qinusty', 1), ('srstevenson', 1), ('seungjaeryanlee', 1), ('siyavash', 1), ('ibrahiminfinite', 1)","[17, 'Partnerships for the Goals']"
mapswipe/mapswipe,MapSwipe mobile application,"# MapSwipe

**The badges refer to the `dev` branch**

[![Syntax & Linting](https://github.com/mapswipe/mapswipe/actions/workflows/build.yml/badge.svg?branch=dev&event=push)](https://github.com/mapswipe/mapswipe/actions/workflows/build.yml)
[![Android build](https://github.com/mapswipe/mapswipe/actions/workflows/android.yml/badge.svg?branch=dev&event=push)](https://github.com/mapswipe/mapswipe/actions/workflows/android.yml)
[![iOS build](https://github.com/mapswipe/mapswipe/actions/workflows/ios.yml/badge.svg?branch=dev&event=push)](https://github.com/mapswipe/mapswipe/actions/workflows/ios.yml)


Welcome to the MapSwipe app. This is the app that is distributed through [mapswipe.org](http://mapswipe.org) as well as through the Google Play and Apple stores. It was initially developed by Doctors without Borders as part of the Missing Maps project.

## Main Overview

In a nutshell, here is how MapSwipe works:

1. Humanitarian organisations set the parameters for projects through a web-based admin interface.
1. Our backend workers process those projects and place them on Firebase. It imports them into groups that are safe for the user to cache locally on their phone (ideally 200 tiles). This [image](http://i.imgur.com/giQq43i.jpg ""image of grouping"") shows an example of how that grouping algorithm works.
1. The app fetches the projects from the /projects reference in Firebase through the JavaScript SDK (don't use http requests to Firebase) and displays them to the user.
1. The user searches those tiles and classifies them. The results are then synced back to Firebase.
1. When a user chooses to map an area, he or she is distributed groups of the project. On completion, the user then gets badges for the distance they've mapped.

:)

## Project Diagram

The following is an outline of how data typically flows and makes it into the mobile application. Most of the action happens in one of the three areas, namely the **backend scripts**, **Firebase database**, and **clients**. 

![Main overview](http://i.imgur.com/PYT62JF.png)

This application encompasses only the mobile Android & iOS clients. The role of the clients are to retrieve project information (metadata and tile information) so that volunteers can swipe through and tag them. Then, this tagging information is synchronized back to Firebase. The backend scripts (in a [separate repository](https://github.com/mapswipe/python-mapswipe-workers)) are responsible for populating and processing the project information in Firebase.

## Developing, Building, and Contributing to MapSwipe

If you'd like to modify and improve MapSwipe, read through the following to get familiar with the project. Please also read [CONTRIBUTING](CONTRIBUTING.md).

## Technology Used

1. The app is written entirely in [React Native](https://facebook.github.io/react-native/docs/getting-started.html)
1. Firebase provides the backend database. It is protected with security rules so that users and contributors to this open source project can not cause damage.
1. The [workers](https://github.com/mapswipe/python-mapswipe-workers) on the backend are running on Google Cloud and handle pre-processing and post-processing the data.

## State of the project

The app was rebuilt at the end of 2018 on a recent version of react-native, and expanded to support multiple types of tasks, as well as a variety  of languages.
","'android', 'firebase', 'gis', 'gis-data', 'hacktoberfest', 'humanitarian', 'nonprofit', 'ong'",2024-04-25T04:54:08Z,22,145,15,"('laurentS', 1379), ('HHK1', 106), ('transifex-integrationbot', 81), ('samshara', 81), ('Hagellach37', 71), ('frozenhelium', 54), ('PimDeWitte', 27), ('tnagorra', 23), ('ElJocho', 16), ('dependabotbot', 14), ('jhenshall', 8), ('mathcass', 8), ('kmpoppe', 7), ('TahiraU', 6), ('danbjoseph', 3), ('ericboucher', 3), ('Birnbaum2001', 1), ('habi', 1), ('kopitek8', 1), ('oiva', 1), ('Rudloff', 1), ('thenav56', 1)","[17, 'Partnerships for the Goals']"
hotosm/osm-analytics,OSM Analytics lets you interactively analyze how specific OpenStreetMap features are mapped in a specific region.,"osm-analytics: data analysis tool frontend
=========================================

[![Join the chat at https://gitter.im/hotosm/osm-analytics](https://badges.gitter.im/hotosm/osm-analytics.svg)](https://gitter.im/hotosm/osm-analytics?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

OSM-Analytics lets you analyse interactively how specific OpenStreetMap features are mapped in a specific region.

Say, you'd like to know when most of a specific feature type (e.g. buildings) in a specific country or city were added. This tool lets you select the geographical region of interest and shows a graph of the mapping activity in the region. You can even select a specific time interval to get the number of touched features in that period, and the map will highlight the matching features. Alternatively, one can view the distribution of features by their mapper's user experience. The tool also gives a side by side comparison of the map state at different points in time and lets you view which [HOT](https://hotosm.org/) projects may have included the mapping of a region.

Features
--------

* supported feature types: *buildings* (any closed osm way with a building tag), *roads* (any osm way with a highway tag), *rivers* (any osm way with a waterway tag)
* graphs of feature *recency* or *mapper experience*
* highlighting of features by custom date range or user experience interval
* calculated statistics: total number/length of features in selected region and date/experience range, number of contributors
* shows which hot projects influenced the mapping of the selected region
* compare map at different points in time
* data updated daily

Technical Overview & Limitations
--------------------------------

See [documentation/architecture.md](documentation/architecture.md) for background information.

Installation and Usage
----------------------

The frontend is implemented in React/Redux and based on [tj/Frontend Boilerplate](https://github.com/tj/frontend-boilerplate).

Install dependencies:

```
$ npm install
```

Run in development mode:

```
$ npm start
```

Generate static build:

```
$ npm run build
```

The [`deploy.sh`](https://github.com/hotosm/osm-analytics/blob/master/deploy.sh) script can be useful to publish updates on github-pages.

Embedding
---------

This user interface supports a custom UI for embedding on 3rd party websites, using an HTML `iframe`. It allows generating a
time comparison between two points in time for the same region.

![Comparison map](https://github.com/GFDRR/osm-analytics/blob/master/documentation/embed-example-1.png?raw=true ""Comparison map"")


The above visualization can be generated using a specific URL structure:

`https://osm-analytics.org/#/compare//...//embed/`

- __iframe_base_url__ (`http://osm-analytics.org`)
- __region__ the area of interest the embedded map is shown for. Can be a bounding box (`bbox:110.28050,-7.02687,110.48513,-6.94219`), an [encoded polyline](https://www.npmjs.com/package/@mapbox/polyline) of a polygon (e.g. `polygon:ifv%7BDndwkBx%60%40aYwQev%40sHkPuf%40ss%40%7BfA_%40uq%40xdCn%7D%40%5E`)), or a hot project id (e.g. `hot:4053`) or a link to a github gist that contains a `polygon.geojson` file (e.g. `gist:36ea172ef996a44d36a554383d5fb4fa`).
- __start_year__ (`2016`) represents the start year of an OpenDRI project
- __end_year__ (`now`) represents the end year of an OpenDRI project. `now` can also be provided to compare with latest OSM data
- __feature_layer__ (`buildings`) compare `buildings`, `highways` or `waterways`
- __theme_name__ (`default`) use the `default` OSM Analytics visual style, or the `opendri` theme

The *gap detection* view can also be used as an embedded map in a very similar way:

`https://osm-analytics.org/#/gaps//buildings-vs-ghs/embed/`

The *edit recency* and *user experience* views can also be embedded like this:

`https://osm-analytics.org/#/show///embed//recency` or `https://osm-analytics.org/#/show///embed//experience`

Here, one can optionally supply a time or user experience selection, which triggers highlights respective features or regions on the map that fall into the given time period or user experience range. Just append the respective query parameter to the embed URL: `/?timeSelection=,` (timestamps are seconds since epoch) or `/?experienceSelection=,` (experience values as defined in the respective layer's [experience](https://github.com/hotosm/osm-analytics-config/blob/master/analytics-json.md#experience-field) field).

See Also
--------

* [lukasmartinelli/osm-activity](https://github.com/lukasmartinelli/osm-activity) – similar to OSM-Analytics, but with a simpler, more basic user interface
* [OSMatrix](http://wiki.openstreetmap.org/wiki/OSMatrix) by GIScience Heidelberg – ""precursor"" of OSM-Analytics
* [joto/taginfo](https://github.com/joto/taginfo) – aggregate statistics for all OSM tags
* [tyrasd/taghistory](https://github.com/tyrasd/taghistory) – tagging history for all OSM tags
* [Crowdlens](http://stateofthemap.us/2016/learning-about-the-crowd/) by Sterling Quinn – prototype of interactive mapping history visualization
* [mapbox/osm-analysis-collab](http://mapbox.github.io/osm-analysis-collab/osm-quality.html) – misc OSM contributor analyses based on osm-qa-tiles data
","'analytics', 'openstreetmap', 'openstreetmap-data', 'osm-analytics', 'osm-qa-tiles', 'vector-tiles', 'visualization'",2024-04-23T12:12:29Z,15,103,23,"('tyrasd', 275), ('nerik', 25), ('tiagojsag', 18), ('vectorsize', 13), ('dependabotbot', 12), ('dakotabenjamin', 6), ('dodobas', 3), ('cgiovando', 1), ('david-hotosm', 1), ('matkoniecz', 1), ('robert-ancell', 1), ('gitter-badger', 1), ('willemarcel', 1), ('rrequero', 1), ('spwoodcock', 1)","[17, 'Partnerships for the Goals']"
public-accountability/littlesis-rails,LittleSis is a free database of who-knows-who at the heights of business and government,"# LittleSis

## About

[LittleSis](https://littlesis.org) is a free database of who-knows-who at the heights of business and government. It is a public wiki and purpose-built database for researching powerful organizations, tracking conflicts of interest, and visualizing networks of political influence. LittleSis started in 2009 and our database contains over 1.6 million relationships between over 400 thousand people and organizations.

LittleSis is a project of [The Public Accountability Initiative](https://public-accountability.org/), a non-profit public interest research organization focused on corporate and government accountability. Visit [Eyes on the Ties](https://news.littlesis.org) to read our research or follow [@twittlesis](https://twitter.com/twittlesis) on twitter.

This repository [littlesis-rails](https://github.com/public-accountability/littlesis-rails) is our core application. See [public-accountability/oligrapher](https://github.com/public-accountability/oligrapher) for our javascript mapping tool.


## Project history & software

Matthew Skomarovsky ([@lovemedicine](https://github.com/lovemedicine)) co-founded LittleSis and was the lead developer behind the project, with help from co-founder Kevin Connor. LittleSis started in 2009 as a [PHP application](https://github.com/littlesis-org/littlesis). The port to Ruby on Rails began in 2013 and finished in 2017.

Ziggy ([@aepyornis](https://github.com/aepyornis)) joined in 2016 and currently maintains the project.

Along the way, Eddie ([@eddietejeda](https://github.com/eddietejeda)) helped with some of the first data import scripts. Austin ([@aguestuser](https://github.com/aguestuser)) worked on on oligrapher and the rails codebase. Liz ([@lizstarin](https://github.com/lizstarin)) helped port PHP code to rails and developed the chrome extension. Pea ([@misfist](https://github.com/misfist)) coded our wordpress sites. Since 2020, Rob [@robjlucas](https://github.com/robjlucas) has contributed to the rails application.



| Key software |               |
|:------------:|:-------------:|
| Application  | Ruby on Rails |
| Database     | Postgresql    |
| Web Server   | Puma, Nginx   |
| Search       | Manticore     |
| Cache        | Redis         |
| Blog         | Wordpress     |
| OS           | Debian        |


[Developer Instructions](./DEVELOPMENT.md)
",,2024-05-01T13:34:43Z,10,93,7,"('aepyornis', 5903), ('skomputer', 622), ('robjlucas', 423), ('aguestuser', 268), ('lizstarin', 103), ('lovemedicine', 74), ('josephlacey', 36), ('dependabotbot', 8), ('jessib', 2), ('kevinls', 2)","[16, 'Peace, Justice and Strong Institutions']"
betagouv/aides-jeunes,Un simulateur global des prestations sociales françaises pour les jeunes.,"## Cette documentation est technique. Pour plus d'informations sur le [simulateur d'aides pour les jeunes](https://mes-aides.1jeune1solution.beta.gouv.fr), regardez notre [wiki](https://github.com/betagouv/aides-jeunes/wiki).

> L'interface utilisateur (et le serveur principal) du [simulateur d'aides et de prestations sociales pour les jeunes](https://mes-aides.1jeune1solution.beta.gouv.fr). Il est basé sur simulateur socio-fiscal libre [Openfisca](https://www.openfisca.fr/).

# Setup

## Stack

- VueJS
- NodeJS
- MongoDB
- OpenFisca (Python, numpy)
- NetlifyCMS ([config](https://github.com/betagouv/aides-jeunes/blob/main/contribuer/public/admin/config.yml))
  - [website](https://contribuer-aides-jeunes.netlify.app)
- Fabric ([fabfile](https://github.com/betagouv/aides-jeunes-ops/blob/main/fabric.yml))

## 3rd parties

- Github Actions ([config](https://github.com/betagouv/aides-jeunes/blob/main/.github/workflows/))
  - Continuous integration and deployment
- Netlify
  - Deloy previews
- SMTP server
- Matomo ([stats.data.gouv.fr](https://stats.data.gouv.fr/index.php?module=CoreHome&action=index&idSite=165&period=range&date=previous30))
  - [Dedicated site for usage data and impact](https://betagouv.github.io/mes-aides-analytics/) [source](https://github.com/betagouv/mes-aides-analytics)
- Sentry
  - [backend](https://sentry.io/organizations/betagouv-f7/projects/aides-jeunes-node/?project=5709109)
  - [frontend](https://sentry.io/organizations/betagouv-f7/projects/aides-jeunes-front/?project=5709078)

# Front only install

If you want to play with the UI, you can be set up very quickly:

```bash
npm ci
npm run front
```

Cf. `package.json` for more on the underlying commands.

The application should be accessible at `localhost:8080`.

# Full install

## System dependencies

Make sure `node` 16.x is installed on your machine:

### Ubuntu

And also `build-essential`, `mongodb` are installed on your machine:

```sh
sudo apt-get install build-essential
sudo apt-get install mongodb
```

### MacOs

And also `brew` is installed on your machine:

```sh
brew tap mongodb/brew # Download official homebrew formula for MongoDb
brew update # Update Homebrew and all existing formulae
brew install mongodb-community@7.0 # Install MongoDb
```

### For all platforms

The runtime is Node 18.x for the web application, and Python >= 3.9 for Openfisca.

You can for example use [`nvm`](https://github.com/creationix/nvm) to install this specific version.

You will need [`pip`](https://pip.pypa.io/) to install Openfisca.

## Application

Run the following from the root of the project to install the dependencies

```sh
npm ci
```

## Openfisca

There are 2 ways to run Openfisca:

- either by installing its dependencies in a Python virual environment locally on your machine
- or by using Docker to pull and build an image with the required dependencies

### Install Openfisca in a virtual environment

You should [install Python 3 in a virtual environment](https://virtualenv.pypa.io/en/stable/) to prevent yourself from messing with your main python installation. The instructions below rely on the built-in `venv` module so that there are no additional external dependencies:

```bash
python3 -m venv .venv   # create the virtual environment in the .venv folder
source .venv/bin/activate  # activate the virtual environment
pip install pip --upgrade  # make sure we're using the latest pip version
npm run install-openfisca  # install dependencies
```

Then, to start the OpenFisca server, simply run `source .venv/bin/activate` followed by `npm run openfisca`.

OpenFisca dependencies are specified in [openfisca/requirements.txt](https://github.com/betagouv/aides-jeunes/blob/main/openfisca/requirements.txt), a basic [Python requirements file](https://pip.pypa.io/en/stable/reference/pip_install/#example-requirements-file). It is possible to refer to non-production commit hashs but is prefered to use _main-merged_ commits.

### Install and run Openfisca in a docker container

If you want to run Openfisca without having to install a specific version of Python or create a virtual environment you can use the docker file provided to run Openfisca in a container. From the root of the project run the following command to build the docker image:

```bash
docker build -f openfisca/Dockerfile ./openfisca -t openfisca
```

### Development mode

If you are working on `openfisca-france` and want to use your local version:

```
cd (...)/openfisca-france
pip install --editable .
```

## Test in production mode

If you want to test locally the app in production mode:

```sh
npm run build
npm run start
```

## Usage

First, start a Mongo server:

```sh
npm run db
```

Then, in another shell you will need to start openfisca. If you installed it locally activate the virtual environment (run `source .venv/bin/activate`) and start the Openfisca server:

```sh
OPENFISCA_WORKERS=1 npm run openfisca
```

If instead you want to run Openfisca in a docker container run:

```bash
docker run -d -p 2000:2000 openfisca
```

(note that in that case Openfisca will run in the background and you will have to run `docker ps` and `docker stop XXXXX` where XXXXX is the container ID to stop Openfisca)

Finally, in a third shell, start the server:

```sh
npm run serve
```

# Testing

There are several levels of tests:

- Unit tests are executed by [Jest](https://jestjs.io/fr/) and run with `npm test`.
- End-to-end test are executed with [Cypress](https://www.cypress.io/) with `npm run cypress`

You can safely use `npm test && npm run cypress` to drive your developments.

## Email

We use the framework [MJML](https://mjml.io/) to design and integrate the templates. [Tipimail](https://fr.tipimail.com) is our service to send emails.

The development server for emails can be easily start with: `npm run tools:serve-mail`

If you want to verify the email sending process, you can generate a set of the required `SMTP_*` environment variables by running `ts-node tools/create-temp-smtp-server.ts` to generate a test account on `https://ethereal.email`.

## Linting and format

We use [ESLint](https://eslint.org/) as a linter and [Prettier](https://prettier.io/) to format the codebase.
We also utilize some ESLint plugins, such as [vue-eslint](https://eslint.vuejs.org/user-guide/) and [eslint-plugin-cypress](https://github.com/cypress-io/eslint-plugin-cypress), to provide a support for tests and framework.

# Continuous deployment

SSHs keys were generated to [run scripts](http://man.openbsd.org/sshd#command=%22command%22) on the production server.

The `main` and `dev` branches are automatically deployed on the production server when they are updated using a [continuous deployment script](https://github.com/betagouv/aides-jeunes/actions/workflows/cd.yml).

Note that it is also possible to re-trigger a deployment manually by clicking on `Run workflow` button on the [continuous deployment's page](https://github.com/betagouv/aides-jeunes/actions/workflows/cd.yml) and selecting either the `main` or `dev` branch.

To access the applications server it is possible to connect to it with a registered public key using ssh:

```sh
ssh debian@equinoxe.mes-aides.1jeune1solution.beta.gouv.fr
```

# Other tools scripts & tips

In order to use those tools you need to build the server at least once using the command `npm run build:server`.

- `npm run husky` installs git hooks used to facilitate development and reduce the CI running time.

- `npm run tools:check-links-validity` validates links to 3rd parties in benefits files.

- `npm run tools:cleaner` cleans simulations data older than 31 days.

- `npm run tools:evaluate-benefits ` evaluates benefits linked to a simulation id.

- `npm run tools:generate-missing-institutions-aides-velo` generates missing institutions for the package `aides-velo`.

- `npm run tools:geographical-benefits-details` gets the relevant benefits for each commune.

- `npm run tools:get-all-steps` gets all the steps and substeps of a simulation.

- `npm run tools:serve-mail` generates emails which contain the result of a simulation or a survey.

- `npm run tools:test-benefits-geographical-constraint-consistency` validates geographical constraint consistency of benefits.

- `npm run tools:test-definition-periods`validates the periods of openfisca requested variables.

- [Locally](http://localhost:8080/simulation/resultats?debug) or on [production](https://mes-aides.1jeune1solution.beta.gouv.fr/simulation/resultats?debug), it is possible to visualize all the available benefits of the simulator. It is done by adding `debug` as a parameter. It is also possible to set `debug=ppa,rsa` to choose which benefits are listed.

- Adding `debug=parcours` as a parameter, show a debug version of all the steps in the simulator, [locally](https://localhost:8080/simulation/individu/demandeur/date_naissance?debug=parcours) and [production](https://mes-aides.1jeune1solution.beta.gouv.fr/simulation/individu/demandeur/date_naissance?debug=parcours).

- [OpenFisca tracer](https://openfisca.github.io/tracer/) allows you to debug OpenFisca computations. ([source](https://github.com/openfisca/tracer))

# Export simulations data from database

It is possible to generate simulation statistics from the database running the commande `npm run tools:generate-mongo-stats`.

This will generate 3 csv files in the `dist/documents` folder:

- `monthly_activite.csv` that lists the number of simulations per activity for each month
- `monthly_age.csv` that lists the number of simulations per age for each month
- `monthly_geo.csv` that lists the number of simulations per epci, departement and regions for each month

## Decap CMS development

It is possible to locally debug changes in Decap CMS configuration.

- `npm ci` and `npm run dev` should be ran from `contribuer`.
- Decap CMS should now be accessible at `http://localhost:3000/admin/index.html`

If you want changes to be made locally instead of generating pull requests in production:

- First, [contribuer/public/admin/config.yml#L19](https://github.com/betagouv/aides-jeunes/blob/main/contribuer/public/admin/config.yml#L19) ([`local_backend: true`](https://decapcms.org/docs/working-with-a-local-git-repository)) must be uncommented;
- `npx netlify-cms-proxy-server` should be ran from `.` and
","'mes-aides', 'openfisca', 'openfisca-server'",2024-05-03T14:51:27Z,30,76,15,"('guillett', 2238), ('nanocom', 922), ('aides-jeunes-bot', 676), ('MattiSG', 593), ('fpagnoux', 498), ('alexsegura', 483), ('Cugniere', 341), ('Kout95', 281), ('jdesboeufs', 257), ('Shamzic', 180), ('charlottelecuit', 165), ('monbocal', 131), ('Flightan', 130), ('baptou12', 109), ('QuentinMadura', 96), ('yasmine-glitch', 95), ('Valandr', 86), ('dependabotbot', 81), ('ManonLger', 47), ('clembiffaud', 39), ('alizeeeeeee', 34), ('Vanessa-D', 27), ('ClementNumericite', 26), ('LucasCharrier', 25), ('desoindx', 25), ('Allan-CodeWorks', 23), ('ppezziardi', 20), ('Tumulte', 10), ('jenovateurs', 9), ('mes-aides-bot', 7)","[16, 'Peace, Justice and Strong Institutions']"
medic/cht-core,"The CHT Core Framework makes it faster to build responsive, offline-first digital health apps that equip health workers to provide better care in their communities. It is a central resource of the Community Health Toolkit.","# The Core Framework of the Community Health Toolkit (CHT)

This is the repository of the CHT Core Framework, a technical resource of the [Community Health Toolkit (CHT)](https://communityhealthtoolkit.org) contributed by Medic.

Medic is a nonprofit organization on a mission to improve health in the hardest-to-reach communities through open-source software. Medic serves as the technical steward of the Community Health Toolkit.

For the latest changes and release announcements see our [release notes](https://github.com/medic/cht-core/tree/master/release-notes). Our exact support matrix (including older app versions) can be found [in our docs](https://docs.communityhealthtoolkit.org/core/overview/supported-software/).

## Overview

The CHT's Core Framework is a software architecture that makes it faster to build full-featured, scalable digital health apps that equip health workers to provide better care in their communities. To learn more about building an application with the Core Framework, visit our guide for [developing community health apps](https://docs.communityhealthtoolkit.org/apps/).

The Core Framework addresses complexities like health system roles and reporting hierarchies, and its features are flexible enough to support a range of health programs and local care provider workflows.

Mobile and web applications built with the Core Framework support a team-based approach to healthcare delivery and management. Health workers can use SMS messages or mobile applications to submit health data that can then be viewed and exported using a web application. These web applications are fully responsive with a mobile-first design, and support localization using any written language. They can be installed locally or in the cloud by setting up the individual components or as a Docker container.

For more information about Medic's architecture and how the pieces fit together, see [Architecture Overview](https://docs.communityhealthtoolkit.org/core/overview/architecture/).
For more information about the format of docs in the database, see [Database Schema](https://docs.communityhealthtoolkit.org/core/overview/db-schema/).
For more information about the SMS exchange protocol between webapp and gateway, see [Message States](https://docs.communityhealthtoolkit.org/apps/guides/messaging/sms-states/).

## Using the Core Framework

If you are a developer looking to contribute to the Core Framework itself, you should follow the [development setup instructions](./DEVELOPMENT.md).

If you wish to evaluate the Core Framework, _or_ you are a developer looking to create or modify applications built with the Core Framework, you can instead follow the [easy deployment](https://docs.communityhealthtoolkit.org/apps/tutorials/local-setup/) instructions, which will get the latest stable release running locally via Docker.

You will need to also familiarise yourself with [cht-conf](https://github.com/medic/cht-conf), a tool to manage and configure your apps built using the Core Framework. A brief guide for modifying the config is available [alongside the config](./config/default/GUIDE.md). A more detailed guide is available in [cht-docs](https://docs.communityhealthtoolkit.org/apps/).

### Supported Browsers

Currently, the latest versions of Chrome, Chrome for Android and Firefox are functionally supported. We do not support Safari (unreliable implementations of necessary web APIs) and the generic android browser (unreliable implementations in general). Our webapp code, which includes any code written as configuration, is still ES5. Our exact support matrix (including older app versions) can be found [in our docs](https://docs.communityhealthtoolkit.org/core/overview/supported-software/).

## Contributing

The Core Framework of the [Community Health Toolkit](https://communityhealthtoolkit.org) is powered by people like you. We appreciate your contributions, and are dedicated to supporting the developers who improve our tools whenever possible.

To setup a development environment to contribute to the Core Framework follow the [development instructions](./DEVELOPMENT.md).

First time contributor? Issues labeled [help wanted](https://github.com/medic/cht-core/labels/Help%20wanted) are a great place to start.

Looking for other ways to help? You can also:
* Improve documentation. Check out our style guide [here](https://docs.communityhealthtoolkit.org/contribute/docs/style-guide/)
* Review or add a [translation](CONTRIBUTING.md#translations)
* Find and mark duplicate issues
* Try to reproduce issues and help with troubleshooting
* Or share a new idea or question with us!

The easiest ways to get in touch are by raising issues in the [medic Github repo](https://github.com/medic/cht-core/issues) or [joining our Community Forum](https://forum.communityhealthtoolkit.org).

For more information check out our [contributor guidelines](CONTRIBUTING.md).

## Build Status

Builds brought to you courtesy of GitHub Actions.

![Build Status](https://github.com/medic/cht-core/actions/workflows/build.yml/badge.svg)

## Copyright

Copyright 2013-2022 Medic Mobile, Inc. 

## License

The software is provided under AGPL-3.0. Contributions to this project are accepted under the same license.
","'cht', 'couchdb', 'ehealth', 'hacktoberfest', 'health', 'javascript', 'medical', 'mhealth', 'mobile', 'phone', 'pouchdb'",2024-05-03T11:03:45Z,30,434,56,"('garethbowen', 3837), ('mandric', 1262), ('alxndrsn', 1141), ('wombleton', 648), ('SCdF', 536), ('dianabarsan', 494), ('estellecomment', 362), ('abbyad', 245), ('medic-translators', 164), ('latin-panda', 138), ('newtewt', 125), ('endor', 121), ('ngaruko', 88), ('kennsippell', 78), ('tookam', 74), ('Jodge', 67), ('ryanramage', 62), ('mrjones-plip', 60), ('vimemo', 51), ('lorerod', 40), ('jkuester', 36), ('njogz', 30), ('m5r', 30), ('1yuv', 25), ('tatilepizs', 25), ('ochiengpeter', 25), ('mrsarm', 25), ('rmhowe', 23), ('henokgetachew', 22), ('Benmuiruri', 17)","[3, 'Good Health and Well-Being']"
MSH/QuanTB,"QuanTB is an electronic quantification and early warning system designed to improve procurement processes, ordering, and supply planning for TB treatment. This tool was created by the USAID-funded Systems for Improved Access to Pharmaceutical and Services (SIAPS) Program implemented by Management Sciences for Health.  See README below for more info.","﻿ &nbsp;&nbsp; 

# QuanTB
The SIAPS Program is funded by the U.S. Agency for International Development (USAID) under cooperative agreement AID-OAA-A-11-00021 and implemented by Management Sciences for Health. The information provided on this web site is not official U.S. Government information and does not represent the views or positions of the U.S. Agency for International Development or the U.S. Government. 

For more information, contact digital@msh.org

Disclaimer of warranties and limitation of liability

The QuanTB software, documentation and other products, information, materials and services provided by Management Sciences for Health (MSH or Licensor) are provided “as is.” Licensor hereby disclaims all warranties, whether express, implied, statutory or other (including all warranties arising from course of dealing, usage or trade practice), and specifically disclaims all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement. Without limiting the foregoing, licensor makes no warranty of any kind that this software or documentation, or any other licensor or third-party goods, services, technologies or materials (including any software or hardware), or any products or results of the use of any of them, will meet the users’ or other persons' requirements, operate without interruption, achieve any intended result, be compatible or work with any other goods, services, technologies or materials (including any software, hardware, system or network), or be secure, accurate, complete, free of harmful code or error-free. Licensor is not responsible for further development or any future versions of QuanTB.

Copyright Management Sciences for Health.

See the [https://github.com/MSH/QuanTB/blob/devel/QuanTB_Build_Instructions.pdf] file for information on how to build the software.
",,2022-09-02T19:26:54Z,2,0,2,"('JulieFrye', 6), ('khoppenworth', 2)","[17, 'Partnerships for the Goals']"
Datawheel/canon,Reusable React environment and components for creating visualization engines.,"# Canon

A reusable react environment, and components for creating visualization engines.

![](https://github.com/datawheel/canon/raw/master/docs/bang.png)

## Packages

| Folder | Package | Version | Changelog |
| ------ | ------- | ------- | --------- |
| [/packages/core](./packages/core/) | [@datawheel/canon-core](https://www.npmjs.com/package/@datawheel/canon-core) | [![npm version](https://badge.fury.io/js/%40datawheel%2Fcanon-core.svg)](https://badge.fury.io/js/%40datawheel%2Fcanon-core) | [changelog](./packages/core/CHANGELOG.md) |
| [/packages/create-canon](./packages/create-canon/) | [@datawheel/create-canon](https://www.npmjs.com/package/@datawheel/create-canon) | [![npm version](https://badge.fury.io/js/%40datawheel%2Fcreate-canon.svg)](https://badge.fury.io/js/%40datawheel%2Fcreate-canon) | [changelog](./packages/create-canon/CHANGELOG.md) |
| [/packages/cms](./packages/cms/) | [@datawheel/canon-cms](https://www.npmjs.com/package/@datawheel/canon-cms) | [![npm version](https://badge.fury.io/js/%40datawheel%2Fcanon-cms.svg)](https://badge.fury.io/js/%40datawheel%2Fcanon-cms) | [changelog](./packages/cms/CHANGELOG.md) |
| [/packages/logiclayer](./packages/logiclayer/) | [@datawheel/canon-logiclayer](https://www.npmjs.com/package/@datawheel/canon-logiclayer) | [![npm version](https://badge.fury.io/js/%40datawheel%2Fcanon-logiclayer.svg)](https://badge.fury.io/js/%40datawheel%2Fcanon-logiclayer) | [changelog](./packages/logiclayer/CHANGELOG.md) |
| [/packages/vizbuilder](./packages/vizbuilder/) | [@datawheel/canon-vizbuilder](https://www.npmjs.com/package/@datawheel/canon-vizbuilder) | [![npm version](https://badge.fury.io/js/%40datawheel%2Fcanon-vizbuilder.svg)](https://badge.fury.io/js/%40datawheel%2Fcanon-vizbuilder) | [changelog](./packages/vizbuilder/CHANGELOG.md) |

## Summaries

- [Canon Core](./packages/core/) - Reusable React environment and components for creating visualization engines.
- [Create Canon](./packages/create-canon/) - Package for create the boilerplate code for a new Canon app.
- [CMS](./packages/cms/) - Content Management System for Canon sites
- [Canon Logic Layer](./packages/canon-logiclayer/) - a REST API that simplifies queries to a mondrian cube using shorthand and introspective logic.
- [VizBuilder](/packages/vizbuilder) - a dynamic chart builder engine component.


## License

&copy; 2020 Datawheel, LLC. Licensed under GPLv3.
",,2024-04-26T18:35:46Z,14,26,7,"('davelandry', 1768), ('jhmullen', 1673), ('perpetualgrimace', 593), ('frabarz', 486), ('cnavarreteliz', 134), ('scespinoza', 107), ('palamago', 62), ('jspeis', 44), ('rhauckdot', 9), ('alexandersimoes', 6), ('greenrhyno', 4), ('jazzido', 3), ('MarcioPorto', 2), ('ffigueroal', 1)","[11, 'Sustainable Cities and Communities']"
oppia/oppia-ml,,"# Oppia-ml

Oppia-ml is a supplementary component which is used with Oppia for training machine learning models on a separate VM instance. Oppia puts job requests for training a classifier in database. Oppia-ml picks this job requests one by one, trains classifier for these requests and stores the result of training back into database. Oppia uses this stored results to predict outcome for answers.
 
Oppia-ml is written in Python and uses various machine learning libraries for creating classifiers.

## Installation

### Installing on your development machine

1. Clone this repo in folder oppia-ml.

2. Open terminal and navigate to oppia-ml folder and run:
  ```
    git checkout develop
    bash scripts/start.sh
  ```

 
### Deploying Oppia-ml on VM instance

1. Clone this repo in oppia-ml folder of your VM instance:
  ```
    git clone https://github.com/oppia/oppia-ml.git
  ```

2. Install Supervisor on VM instance. Generally it can be installed by running simple pip command. You need superuser privileges to install it on VM. if this command does not work then follow instructions on official installation [page](http://supervisord.org/installing.html). 
  ```
    pip install supervisor
  ```

3. Navigate to oppia-ml folder in terminal and run following commands:
  ```
    bash scripts/deploy.sh
  ```

4. Add shared secret key in VM and in Oppia for secure communication.
  - Shared key on VM is added using GCE metadata. Select the VM instance in the GCP panel, then add two key - value pairs in metadata, one for `shared_secret_key` and other is `vm_id`. VM will automatically get the ID and secret from metadata.
  - Shared key on Oppia can be added by going to `/admin` page of your Oppia host. On this page go to the 'Config' tab where there will be one section for VMID and shared secret keys in which one can add as many `vm_id` and `shared_secret_key` as needed.

## Support
If you have any feature requests or bug reports, please log them on our [issue tracker](https://github.com/oppia/oppia-ml/issues/new?title=Describe%20your%20feature%20request%20or%20bug%20report%20succinctly&body=If%20you%27d%20like%20to%20propose%20a%20feature,%20describe%20what%20you%27d%20like%20to%20see.%0A%0AIf%20you%27re%20reporting%20a%20bug,%20please%20be%20sure%20to%20include%20the%20expected%20behaviour,%20the%20observed%20behaviour,%20and%20steps%20to%20reproduce%20the%20problem.%20Console%20copy-pastes%20and%20any%20background%20on%20the%20environment%20would%20also%20be%20helpful.%0A%0AThanks!).
 
Please report security issues directly to admin@oppia.org.
 
## Licence
The Oppia-ml code is released under the [Apache v2 license](https://github.com/oppia/oppia-ml/blob/master/LICENSE).
",,2021-05-22T18:11:54Z,5,3,9,"('prasanna08', 22), ('seanlip', 2), ('akumar1503', 1), ('anmolshkl', 1), ('rohitkatlaa', 1)","[4, 'Quality Education']"
betagouv/api.gouv.fr,Liste les API disponibles au sein de l'administration française,"# api.gouv.fr

[![Build, lint & tests](https://github.com/betagouv/api.gouv.fr/actions/workflows/pre-merge.yml/badge.svg)](https://github.com/betagouv/api.gouv.fr/actions/workflows/pre-merge.yml)
[![Accessibilité](https://github.com/betagouv/api.gouv.fr/actions/workflows/check-accessibility.yml/badge.svg)](https://github.com/betagouv/api.gouv.fr/actions/workflows/check-accessibility.yml)
[![Liens morts](https://github.com/betagouv/api.gouv.fr/actions/workflows/check-broken-links.yml/badge.svg)](https://github.com/betagouv/api.gouv.fr/actions/workflows/check-broken-links.yml)

**api.gouv.fr** catalogue les API produites par les administrations centrales, les
collectivités territoriales, les établissements publics… Chaque API est
associée à une courte description fonctionnelle, une documentation technique,
les modalités d'accès, d'éventuelles ressources supplémentaires et surtout des
liens vers les services qui l'utilisent.

## Derniers déploiements

[![Deploy - Staging](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-staging.yml/badge.svg)](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-staging.yml)

[![Deploy - Production](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-production.yml/badge.svg)](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-production.yml)

## Public visé

api.gouv.fr s'adresse avant tout aux créateurs de services, les consommateurs
d'API. Pour cela, nous facilitons la découverte, la compréhension et l'accès
aux API et à leurs producteurs.

Les fournisseurs, de leur côté, ont avec ce catalogue un moyen simple de faire connaître leurs API.

## Fournisseur d'API ? Envie de référencer une nouvelle API ?

### 1- Contacter le service
Veuillez prendre attache avec l'équipe en complétant le formulaire suivant :
[👉 Ajoutez votre API](https://api.gouv.fr/nouvelle-api) !

### 2- Créer la page de son API ([exemple](https://api.gouv.fr/les-api/api-particulier))

Pour ajouter votre API ou commenter un fichier dans ce dépôt, vous devez au préalable avoir un [compte Github](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F&source=header-home).

#### Créer la page à partir des templates disponibles

1. Se rendre dans le dossier [_data/api](https://github.com/betagouv/api.gouv.fr/tree/master/_data/api).
2. Créer le fichier de votre API `api-nom-de-la-nouvelle-api.md`.
3. Utiliser les templates à disposition, commentés avec une explication pour chacun des champs. Les champs indispensables sont indiqués par l'icône épingle (📍) dans le commentaire.

- **Pour les API en open data**, appuyez-vous sur le template [`template.api-opendata.md.example`](https://github.com/betagouv/api.gouv.fr/tree/master/_data/api/template.api-opendata.md.example).

- **Pour les API avec accès restreint**, appuyez-vous sur le template [`template.api-fermees.md.example`](https://github.com/betagouv/api.gouv.fr/tree/master/_data/api/template.api-fermees.md.example).

Pour plus de facililité, copier/coller tout le contenu du template dans votre fichier. Dans le cas où vous n'avez pas toutes les informations permettant de compléter les champs, vous pouvez supprimer le champ ou le commenter en ajoutant un `#` en début de ligne.

#### Champ `producer`

Pour que votre fiche API fonctionne, il est impératif qu'elle soit rattachée au nom d'un fournisseur référencé dans le dossier `api_gouv/_data/producteurs`. [🔎 Ajouter un fournisseur de données](#3--créermodifier-sa-fiche-fournisseur-de-données)

#### Champ `doc_tech_link` ou comment référencer son swagger

Pour ajouter votre swagger dans [API.gouv](https://api.gouv.fr/documentation), il vous faut mettre le lien URL vers le fichier openAPI.

- Si vous avez une URL publique permettant d'accéder au fichier OpenAPI, c'est ce lien que vous devez indiquer.
- Si votre swagger n'est pas disponible par une URL publique, nous pouvons héberger votre swagger pour le rendre disponible. Veuillez [nous contacter](https://api.gouv.fr/parcours-client?source=contact).

Pour ajouter un lien vers la documentation technique, veuillez utiliser le champ `doc_tech_external:`.

#### `account_link:` et `datapass_link:`quelle différence ?

Votre API est en accès restreint ? Deux champs sont à disposition pour renvoyer les usagers vers la demande d'habilitation :  

- `account_link:` vous permet d'ajouter l'URL de votre page de connexion (si il s'agit d'une demande de création de compte) ou de votre procédure d'habilitation.
- `datapass_link:` permet d'ajouter le lien vers le formulaire d'habilitation DataPass, produit opéré par la DINUM et permettant l'instruction de demandes d'accès à des données.


#### Entonoir d'éligibilité avec `access_page:`

Si votre API est uniquement accessible à un type de public, le champ `access_page` vous permet de créer un composant entonnoir pour vérifier si l'usager est éligible. Vous pouvez voir un exemple de ce parcours [ici](https://api.gouv.fr/les-api/api-statut-demandeur-emploi/demande-acces). Ce parcours est accessible après avoir cliqué sur le bouton ""Faire une demande d'habilitation"" sur la page de l'API.

##### Forme standard du champ :

```
access_page:
  - who: # Chaque ""who"" crée un bouton de premier niveau. Limitez-en le nombre pour que l'usager s'y retrouve.
      - Un particulier ou une entreprise # Label du bouton
    is_eligible: -1 # -1 signifie que ce public n'est pas elligible, la mention ""Désolé, vous n’êtes pas éligible 🚫"" sera affichée quand l'usager clique sur le bouton.
    description: |
      Seules les administrations sont habilitées à utiliser l'API XX.

      Rechercher une autre API
    # Cette description vient compléter la mention indiquée par le champ is_eligible.
  - who:
      - Une collectivité ou une administration
    is_eligible: 1 # 1 signifie que ce public est éligible, la mention ""Vous êtes éligible 👌"" sera affichée quand l'usager clique sur le bouton.
    description: |
      Conformément aux dispositions XXXX, seul le public XXX est habilité à pouvoir utiliser cette API.
      Pour obtenir un agrément, vous devrez **justifier de XXXX**, et vous engager à XXXX.

      Vous aurez besoin des informations suivantes pour compléter votre demande d'habilitation : 
      - Info 1
      - Info 2
      - Document 1

      Remplir une demande
  - who:
      - Un éditeur de logiciel
    is_eligible: -1
    description: |
      Si vous êtes **éditeur de logiciels, c'est à votre collectivité ou administration de faire sa demande d'habilitation.**

      Rechercher une autre API
```

##### Options du champ `description: |` :

Vous pouvez ajouter tout le contenu markdown que vous souhaitez dans le champ `description: |`. Biensûr, rester le plus concis possible est préférable pour que l'usager se repère. Voici quelques options que vous pouvez utiliser facilement : 

**Option 1 :**
Ajouter un bouton pour proposer de chercher une nouvelle API, en écrivant : `Rechercher une autre API`

**Option 2 :**
Spécialement pour les API utilisant Datapass comme formulaire d'habilitation, vous pouvez utiliser le composant [``](https://github.com/betagouv/api.gouv.fr/tree/master/components/richReactMarkdown/index.tsx) pour ajouter un paragraphe décrivant la liste des documents et informations qui seront demandés.
Il vous suffit de l'ajouter sur une ligne seule à l'intérieur du champ `description: |`, un exemple est visible dans [la forme standard du composant ci-dessus](#forme-standard-du-champ).


    Que va ajouter ce composant ?
    Ajouter ce composant, revient à ajouter le code suivant : 
      ```
      
            Pour remplir votre demande, vous aurez besoin : 
          
          
            de votre numéro SIRET
            du cadre juridique
            {service_description}
            des coordonnées de l'équipe
            
              des coordonnées de votre délégué à la protection des données et
              responsable de traitement
              {is_editeur &&  de l’entité pour laquelle vous opérez}
            
          
      ```


**Autres options :** Vous voulez aller plus loin ? Créer plus de niveaux d'information ? 
Les [API Particulier](https://api.gouv.fr/les-api/api-particulier) et [API Entreprise](https://api.gouv.fr/les-api/api-entreprise) sont un bon exemple dont vous pouvez vous inspirer.

Leur composant entonnoir, également configuré à la base dans le fichier principal, appelle d'autres composants disponibles dans le dossier [`api_gouv/components/questionTree`](https://github.com/betagouv/api.gouv.fr/tree/master/components/questionTree).


### 3- Créer/modifier sa fiche fournisseur de données

Si vous êtes un nouveau fournisseur de données, vous avez besoin de référencer votre organisation dans API.gouv :

- Créer la fiche fournisseur `fournisseur.md`, en l'ajoutant dans le dossier [`api_gouv/_data/producteurs`](https://github.com/betagouv/api.gouv.fr/tree/master/_data/producteurs)
- Dans ce fichier, copier/coller le template `template.fournisseur.md.example` ou ajouter au minimum : 
```
---
name: Nom complet du fournisseur
acronym: Nom court qui sera affiché en principal
type: Association | Entreprise privée 
logo: fichier.png
---
```
- Ajouter le logo au format .png dans le dossier `api_gouv/public/images/api-logo`. Nommer le logo sous le même nom qu'utilisé dans le fichier `fournisseur.md` au niveau du champ `logo:`.


## Comment ça marche ?

### Prérequis

[Node.js](https://nodejs.org/en/) >= 16

### Serveur de développement

tl;dr: `./bin/install.sh`

Cette application utilise [Next.js](https://github.com/zeit/next.js).

1. Installer les dépendances

```bash
npm i
```

2. Variables d’environnement

Afin de configurer le projet correctement, il est conseillé de créer un fichier `.env` avec les variables d’environnement nécessaires à l’application.

`.env` permet de persister les variables d’environnement de développement dans un fichier plutôt que de les définir dans le shell, mais les deux fonctionnent. Cela fonctionne avec [dotenv](https://github.com/motdotla/dotenv) et [next-runtime-dotenv](https://github.com/tusbar/next-runtime-dotenv).

Copier le fichier de configuration

```bash
cp .env.sample .env
```

3. Lancer le serveur de développement

```bash
./bin/local_dev.sh
```

Puis visitez http://localhost:3000/

Par défaut, il écoutera sur le port `3000`, pour changer, utiliser `npm run dev -p 4242`.

### Build de production

Cette application utilise [Next.js](https://github.com/zeit/next.js).

1. Installer les dépendances

```bash
npm i
```

2. Générer les bundles de production

```bash
npm run build
```

3. Lancer le serveur de production

```bash
PORT=3000 npm run start
```



### Tests

1. Linter

```bash
npm run lint
```

2. Tests unitaires

```bash
npm run test
```

3. Autres tests

```bash
// a11y
npm run check-accessibility

//404
npm run check-broken-links

// no link to datapass staging
npm run check-no-datapass-staging
```

### Miniatures

Avant chaque commit est lancé un script qui redimmensionne et compresse les images des pages de guides :

```bash
// a11y
npm run create-thumbnail
```

### Preview apps

Chaque pull request est déployé dans des [review app](https://devcenter.heroku.com/articles/github-integration-review-apps) sur [Heroku](https://dashboard.heroku.com/)

### Deploiement

Le déploiement se fait par [Github action](https://github.com/betagouv/api.gouv.fr/actions)

A chaque ""merge"" sur master :

- Laissez le déploiement se faire automatiquement sur [staging](https://staging.api.gouv.fr) via l'action [deploy-staging](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-staging.yml)
- Vérifiez vos changements sur [staging](https://staging.api.gouv.fr)
- Lancez manuellement le déploiement sur [production](https://api.gouv.fr) : sur [deploy-production](https://github.com/betagouv/api.gouv.fr/actions/workflows/deploy-production.yml) et cliquez sur ""Run workflow"" -> ""Run workflow""

NB: Si plusieurs déploiements sont déclenchés en même temps, seul le premier va jusqu'au bout. Les autres sont automatiquement interrompus.

","'api', 'government', 'platform'",2024-05-02T14:52:24Z,30,317,20,"('XavierJp', 950), ('etienne0101', 462), ('ThibautGery', 410), ('DorineLam', 239), ('Flightan', 162), ('rdubigny', 157), ('tvilmus', 103), ('jdesboeufs', 95), ('SchweisguthN', 60), ('skelz0r', 57), ('MattiSG', 56), ('patama', 52), ('Morendil', 46), ('EmploiStoreDeveloppeur', 41), ('Isalafont', 41), ('Samuelfaure', 27), ('mazalaigue', 27), ('arslanegharout', 25), ('VGrss', 21), ('teleboas', 21), ('dependabotbot', 18), ('Nico-Cisirh', 18), ('laem', 18), ('GilMPEIO', 17), ('Miryad3108', 17), ('Bernardstanislas', 17), ('fionnh', 16), ('MaelREBOUX', 15), ('AntoineAugusti', 14), ('l-vincent-l', 14)","[16, 'Peace, Justice and Strong Institutions']"
bayesimpact/bob-emploi,An application that provides personalized career and job search advice to jobseekers.,"_Si vous ne parlez pas anglais, contactez-nous directement à [contribuer@bob-emploi.fr](mailto:contribuer@bob-emploi.fr). Le code source et sa documentation sont en anglais afin de favoriser les contributions internationales._

# Bob Emploi
Bob Emploi is an application that uses algorithms to provide personalized advice to jobseekers.

For more information, check out the website at http://www.bob-emploi.fr.

Bob Emploi is developed and maintained by [Bayes Impact](http://www.bayesimpact.org), a non-profit organization whose purpose is to use technology to address social issues. The project is currently in active development and started in Spring 2016 (see [HISTORY.md](HISTORY.md) for the full project history).

Please subscribe to our [mailing list](https://groups.google.com/forum/#!forum/bob-emploi) in order to receive updates regarding the open sourcing of this project.

Bob Emploi is a React Single Page Application with a RESTful JSON API written in Python. The application is backed by a MongoDB which serves data that we pre-computed using the Python data science stack.

This repository is an automated export of our internal repository that we use for development.

## Licensing

We believe algorithms that aim to serve the public interest should be audit-able and transparent. In accordance with our mission of building a ""citizen-led public service"", most of the application will be open-source under a GPL v3 license.

## Documentation

### Repository Layout

This repository contains several components where each is located within a single folder on the root level. Currently existing folders are:

* `frontend`: Bob Emploi web application.
* `data_analysis`: Data analysis part of Bob Emploi.

Each folder contains a README file with further details regarding this application.

### Contributions

We welcome content and feature suggestions, which help us provide better and more granular advice to Bob users. These can be as simple as a cool job search tip you'd like to share with Bob users, or more sophisticated (for example if you have an idea for how to better suggest training programs based on a user's situation). You can send these to [contribuer@bob-emploi.fr](mailto:contribuer@bob-emploi.fr) either in French or in English.

If you want to contribute to Bob Emploi with a bug fix or feature suggestion, please follow the Github contribution workflow described in [here](https://guides.github.com/activities/contributing-to-open-source/#contributing). At the current stage however, the main intention of the open source repository is to be transparent about the inner workings of Bob. In the future we hope  for, and will actively seek out for, contributions from the community. When we are ready for that, we will add detailed contribution instructions and suggestions of what to work on to this repository.

## Installation / Local Development

All components / applications are packaged in [Docker Containers](https://www.docker.com/), which makes it extremely portable and reduces the setup of your development environment to a few simple commands.

1. Install Docker: use [these detailed instructions](https://www.docker.com/products/overview#/install_the_platform) for installation
2. Run application-specific docker command. For example:
  * To _run_ and _build_ the frontend locally: `docker-compose up -d frontend-dev` (Will make the application available at `http://localhost:3000`)
  * To run the _tests_: `docker-compose run --no-deps frontend-dev npm test`
  * ...

You can find the full list of commands in the README of each folder.

### Short Links

We use a Bayes internal URL shortener that allows us to create links like http://go/bob-design-doc to conveniently link resources from within the source code or documentation. These links are only intended for internal usage and would not work on your computer. We are currently in a process to vet which documents can be made public and will in the future find a solution to replace the _go links_ in the code by publicly available URLs.

## Open-Sourcing Status

### Core Application Code

The core application source code is published in the `frontend` folder of this repository. It is still under very active development and even major features and interfaces are likely to change.

### Components
We have currently opened two of the components we built as part of Bob Emploi:
- **[JobSuggest](https://github.com/bayesimpact/french-job-suggest)**, an autocomplete component for suggesting and selecting French job titles. It is currently based on the [ROME job taxonomy](http://www.pole-emploi.org/informations/open-data-pole-emploi-@/25799/view-category-25799.html?), which it enriches by a) properly gendering job titles and b) weighting job titles by frequency.
- **[PyEmploiStore](https://github.com/bayesimpact/python-emploi-store)**, a Python wrapper around Emploi Store Dev APIs, the platform managed by Pôle Emploi to share their public data.

### Data

We have opened some of the data analysis notebooks (folder `data_analysis/notebooks`) pertaining to the data sources used by the application. These are incomplete, but provide a first insight on the data our recommendations are based on. As these are critical to being able to assess the validity of Bob Emploi's recommendation, we will first focus our open-sourcing efforts on opening more notebooks.

In particular, some of our datasets contain private data, or belong to an external data partner, and as such notebooks analyzing them will be opened over time as we are able to individually vet them for any potential leakage of sensitive information. The [data README](data_analysis/data/README.md) contains a list of the main data sources used in the application along with a short description.
",,2023-05-23T03:51:00Z,6,139,21,"('pcorpet', 28), ('zozoens31', 5), ('dedan', 4), ('pyduan', 3), ('florianjourda', 2), ('rap2hpoutre', 1)","[8, 'Decent Work and Economic Growth']"
codeforboston/safe-water,We are using data science to analyze water quality issues.,"# Safe Water Project 🚰

The Safe Water Project is a team of volunteers at [Code for Boston](https://www.codeforboston.org/) who are using data modeling, data visualization and machine learning to predict, visualize, and share data about the presence of hazardous drinking and surface water contaminants in the United States. 

#### We are interested in collaborating with health and environmental organizations.

Representatives for nonprofit organizations and government agencies interested in our work should contact Andrew at andrew@codeforboston.org. Please feel free to also join our Slack channel at Code for Boston and meet the team.

## Our Work

### Institutional Partners

The Safe Water Project works with Boston-based community partners. These organizations have needs related to data infrastructure and data science, and we provide our expertise in assisting them.   

**Currently we work with the [Charles River Watershed Association](https://www.crwa.org/).** We help them deploying their predictive model, visualizing and communicating their data, as well as better understanding their constituents. You can learn more about this project [in the related folder](https://github.com/codeforboston/safe-water/tree/master/projects/crwa).

Most volunteers work in [python](http://python.org) with [pandas](https://pandas.pydata.org/). Some volunteers also use [R](https://www.r-project.org/), or PHP.

## Project Materials and Communications

- [Slack channel](https://cfb-public.slack.com): #water
  - [Join Code for Boston's Slack Workplace](https://communityinviter.com/apps/cfb-public/code-for-boston)
- [Google Drive - Notes and Research](https://drive.google.com/drive/folders/1FbQE9_NP664lkz4d-Xu4omijLl-HNklz)
- [Wiki - how to contribute etc.](https://github.com/codeforboston/safe-water/wiki)

## Join Us

If you are interested in volunteering for the Safe Drinking Water Project:

- __Join the Slack.__ The #water channel has various pinned items that will be of use for getting started.

- __Read the docs.__ The ``docs/`` folder contains more information about the project, plus steps on how to set up the project so you can start contributing. [Click here](docs/) for the docs.

- __Meet us on Tuesday nights.__ We would love to meet you! We meet alongside other Code for Boston teams every Tuesday in Kendall Square, Cambridge, MA. [Sign up to attend Code for Boston events here](https://www.meetup.com/Code-for-Boston/).
",,2022-12-08T07:45:34Z,26,40,11,"('de-la-viz', 53), ('anhase', 48), ('dwreeves', 26), ('fpaupier', 20), ('jmclellan', 17), ('LewisStaples', 12), ('wesen', 10), ('bhushan-choudhari', 9), ('EwanStephens', 6), ('jlangevin', 6), ('nicholasjin', 6), ('dependabotbot', 6), ('jsecol', 4), ('gitsper', 4), ('harrislapiroff', 4), ('aaevan', 3), ('srihari2761', 2), ('AshirBorah', 2), ('BabarBaig', 1), ('dawngraham', 1), ('dsurrao', 1), ('SJClarissa', 1), ('VictorClarke', 1), ('hiclef', 1), ('kkliu5', 1), ('laruec', 1)","[6, 'Clean Water and Sanitation']"
ourresearch/oadoi,The backend code that powers Unpaywall. support@unpaywall.org,,,2024-04-29T14:42:44Z,10,275,20,"('hpiwowar', 2357), ('richard-orr', 2069), ('jasonpriem', 771), ('nolanm1122', 368), ('caseydm', 262), ('h1-the-swan', 9), ('zuphilip', 2), ('a3nm', 1), ('nemobis', 1), ('leouieda', 1)","[4, 'Quality Education']"
osm-search/Nominatim,Open Source search based on OpenStreetMap data,"[![Build Status](https://github.com/osm-search/Nominatim/workflows/CI%20Tests/badge.svg)](https://github.com/osm-search/Nominatim/actions?query=workflow%3A%22CI+Tests%22)
[![codecov](https://codecov.io/gh/osm-search/Nominatim/branch/master/graph/badge.svg?token=8P1LXrhCMy)](https://codecov.io/gh/osm-search/Nominatim)

Nominatim
=========

Nominatim (from the Latin, 'by name') is a tool to search OpenStreetMap data
by name and address (geocoding) and to generate synthetic addresses of
OSM points (reverse geocoding). An instance with up-to-date data can be found
at https://nominatim.openstreetmap.org. Nominatim is also used as one of the
sources for the Search box on the OpenStreetMap home page.

Documentation
=============

The documentation of the latest development version is in the
`docs/` subdirectory. A HTML version can be found at
https://nominatim.org/release-docs/develop/ .

Installation
============

The latest stable release can be downloaded from https://nominatim.org.
There you can also find [installation instructions for the release](https://nominatim.org/release-docs/latest/admin/Installation), as well as an extensive [Troubleshooting/FAQ section](https://nominatim.org/release-docs/latest/admin/Faq/).

[Detailed installation instructions for current master](https://nominatim.org/release-docs/develop/admin/Installation)
can be found at nominatim.org as well.

A quick summary of the necessary steps:

1. Compile Nominatim:

        mkdir build
        cd build
        cmake ..
        make
        sudo make install

2. Create a project directory, get OSM data and import:

        mkdir nominatim-project
        cd nominatim-project
        nominatim import --osm-file 

3. Point your webserver to the nominatim-project/website directory.


License
=======

The source code is available under a GPLv2 license.


Contributing
============

Contributions, bugreport and pull requests are welcome.
For details see [contribution guide](CONTRIBUTING.md).


Questions and help
==================

For questions, community help and discussions you can use the
[Github discussions forum](https://github.com/osm-search/Nominatim/discussions)
or join the
[geocoding mailing list](https://lists.openstreetmap.org/listinfo/geocoding).
","'geocoding', 'openstreetmap', 'osm'",2024-05-02T11:46:43Z,30,2890,95,"('lonvia', 3660), ('mtmail', 349), ('twain47', 236), ('AntoJvlt', 38), ('tareqpi', 36), ('gemo1011', 34), ('robbe-haesendonck', 32), ('darkshredder', 28), ('markigail', 26), ('ThomasBarris', 22), ('woodpeck', 17), ('krahulreddy', 11), ('IrlJidel', 9), ('eyusupov', 8), ('Simon-Will', 7), ('miku0', 7), ('t-tomek', 6), ('biswajit-k', 6), ('roques', 6), ('nslxndr', 5), ('lujoh', 5), ('melvyn-sopacua', 4), ('Strvm', 4), ('otbutz', 4), ('RhinoDevel', 3), ('alfmarcua', 3), ('DaxServer', 3), ('matkoniecz', 3), ('netsgnut', 3), ('pawel-wroniszewski', 2)","[11, 'Sustainable Cities and Communities']"
tidepool-org/viz,Tidepool data visualization for diabetes device data.,"[![Build Status](https://img.shields.io/travis/com/tidepool-org/viz.svg)](https://travis-ci.com/tidepool-org/viz)

# @tidepool/viz

Tidepool data visualization for diabetes device data.

This README is focused on just the nuts & bolts of getting the code in this repository ready to develop locally in [blip](https://github.com/tidepool-org/blip 'GitHub: blip') or with [React Storybook](https://getstorybook.io/ 'React Storybook'). For more detailed information about the code in this repository, please see the [developer guide](./docs/StartHere.md).

#### Table of contents

- [Getting started](#getting-started)
- [Development](#development)
    - [Running locally with blip](#running-locally-with-blip)
    - [Running locally in React Storybook](#running-locally-in-react-storybook)
    - [Running the tests](#running-the-tests)
    - [Running the linter](#running-the-linter)
- [Production](#production)
    - [Publishing examples](#publishing-examples-to-github-pages-with-react-storybook)
    - [Publishing to npm](#building-and-publishing-to-npm)

* * * * *

## Getting started

After cloning this repository to your local machine, first make sure that you have at least node `6.x` and npm `4.x` installed. If you have a different major version of node installed, consider using [nvm](https://github.com/creationix/nvm 'GitHub: Node Version Manager') to manage and switch between multiple node (& npm) installations. If you have npm `3.x` installed (as it is by default with node `6.x`), then you can update to the latest npm `4.x` with `npm install -g npm@4`.

It's not an absolute requirement, but it is preferable to have [Yarn](https://yarnpkg.com 'Yarn') installed, as it provides dependency management features above and beyond what npm provides. Just follow [Yarn's installation instructions](https://yarnpkg.com/en/docs/install 'Yarn installation instructions') (hint: for Mac users with Homebrew installed, it's just `brew install yarn`).

Once your environment is setup with node `6.x` and npm `3.x` install the dependencies with Yarn:

```bash
$ yarn install
```

Or with npm if you're choosing not to use Yarn:

```bash
$ npm install
```

## Development

### Running locally with blip

To work on code in this repository within [blip](https://github.com/tidepool-org/blip 'Tidepool on GitHub: blip'), first do the following from your local blip repository (assuming blip/ and viz/ are sister directories):

```bash
$ npm link ../viz/
```

In this repository, start the build in watch mode:

```bash
$ npm start
```

Finally, back in your local blip repository, follow [the instructions for starting blip locally](http://developer.tidepool.io/blip/#running-locally 'Blip README: running locally').

### Running locally in React Storybook

If you're working at the component or view level outside of blip, you can work on component and view rendering code with [React Storybook](https://github.com/kadirahq/react-storybook 'GitHub: react-storybook').

If you're working on the diabetes data model rendering components, run:

```bash
$ npm run typestories
```

If you're working on any other components or views, run:

```bash
$ npm run stories
```

For more about the use of React Storybook in this repo, see [use of React Storybook](http://developer.tidepool.io/viz/Storybook.html '@tidepool/viz docs: React Storybook').

### Running the tests

To run the unit tests in [PhantomJS](http://phantomjs.org/ 'PhantomJS') (as they run on [Travis CI](https://travis-ci.com/ 'Travis CI')):

```bash
$ npm test
```

To have the tests run continuously with source and test code changes rebundled as you work:

```bash
$ npm run test-watch
```

To run the unit tests in your local Chrome browser (recommended for Tidepool developers before merging or publishing a release):

```bash
$ npm run browser-tests
```

### Running the linter

To run the code linter from the command line:

```bash
$ npm run lint
```

Generally speaking, Tidepool developers configure linting to run continuously in their text editor of choice, and we recommend this approach for development. You can easily find instructions online for running ESLint continuously in all of the popular text editors—SublimeText, Atom, Visual Studio Code, etc.

## Production

### Publishing examples to GitHub Pages with React Storybook

See [the publishing section](docs/misc/Docs.md#publishing) of the docs on docs.

### Building and publishing to `npm`

When a new feature(s) is/are complete (i.e., branch is synchronized with master, reviewed with a sign-off from another developer), it's time to publish the package to npm! Since this is one of our most recently created repositories, any member of the ""developers"" team in the `@tidepool` npm organization will be able to publish the package using his or her npm login. Steps to publishing are as follows:

1. create a tag on the approved pull request using the `mversion` tool with the `-m` option to auto-commit the version bump and tag (e.g., `$ mversion patch -m` for a patch version bump)
1. push the new commit and tag to the GitHub remote with `$ git push origin ` and `$ git push origin --tags`
1. check that the tag build has passed on [TravisCI](https://travis-ci.com/tidepool-org/viz)
1. `$ npm whoami` to check if you are logged in as yourself; if you are, skip to 7.
1. if you are logged in as `tidepool-robot`, log out with `$ npm logout`
1. then log in as yourself with `$ npm login`
1. publish the new version with `$ npm publish`; before the *actual* publish happens, the `yarn` install, linter, tests, and packaging webpack build will run since we have set those up through the `prepare` and `prepublishOnly` npm hooks in the package.json
1. merge the approved pull request to master
1. remember to bump the version appropriately in the package.json for the app (e.g., blip) requiring `@tidepool/viz` as a dependency!
",,2024-04-16T13:14:10Z,13,13,26,"('clintonium-119', 1689), ('jebeck', 587), ('krystophv', 308), ('jh-bate', 262), ('dependabotbot', 47), ('gniezen', 17), ('courtenayhuffman', 10), ('coyotte508', 8), ('derrickburns', 7), ('darinkrauss', 2), ('BCabon', 1), ('pazaan', 1), ('franck-fourel', 1)","[3, 'Good Health and Well-Being']"
openeemeter/eemeter,An open source python package for implementing and developing standard methods for calculating normalized metered energy consumption and avoided energy use.,"EEmeter: tools for calculating metered energy savings
=====================================================

.. image:: https://travis-ci.org/openeemeter/eemeter.svg?branch=master
  :target: https://travis-ci.org/openeemeter/eemeter
  :alt: Build Status

.. image:: https://img.shields.io/github/license/openeemeter/eemeter.svg
  :target: https://github.com/openeemeter/eemeter
  :alt: License

.. image:: https://readthedocs.org/projects/eemeter/badge/?version=master
  :target: https://eemeter.readthedocs.io/?badge=master
  :alt: Documentation Status

.. image:: https://img.shields.io/pypi/v/eemeter.svg
  :target: https://pypi.python.org/pypi/eemeter
  :alt: PyPI Version

.. image:: https://codecov.io/gh/openeemeter/eemeter/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/openeemeter/eemeter
  :alt: Code Coverage Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
  :target: https://github.com/ambv/black
  :alt: Code Style

---------------

**EEmeter** — an open source toolkit for implementing and developing standard
methods for calculating normalized metered energy consumption (NMEC) and
avoided energy use.

Background - why use the EEMeter library
----------------------------------------

At time of writing (Sept 2018), the OpenEEmeter, as implemented in the eemeter
package and sibling `eeweather package `_, contains the
most complete open source implementation of the
`CalTRACK Methods `_, which
specify a family of ways to calculate and aggregate estimates avoided energy
use at a single meter particularly suitable for use in pay-for-performance
(P4P) programs.

The eemeter package contains a toolkit written in the python langage which may
help in implementing a CalTRACK compliant analysis.

It contains a modular set of of functions, parameters, and classes which can be
configured to run the CalTRACK methods and close variants.

.. note::

    Please keep in mind that use of the OpenEEmeter is neither necessary nor
    sufficient for compliance with the CalTRACK method specification. For example,
    while the CalTRACK methods set specific hard limits for the purpose of
    standardization and consistency, the EEmeter library can be configured to edit
    or entirely ignore those limits. This is becuase the emeter package is used not
    only for compliance with, but also for *development of* the CalTRACK methods.

    Please also keep in mind that the EEmeter assumes that certain data cleaning
    tasks specified in the CalTRACK methods have occurred prior to usage with the
    eemeter. The package proactively exposes warnings to point out issues of this
    nature where possible.

Installation
------------

EEmeter is a python package and can be installed with pip.

::

    $ pip install eemeter

Features
--------

- Reference implementation of standard methods

  - CalTRACK Daily Method
  - CalTRACK Monthly Billing Method
  - CalTRACK Hourly Method

- Flexible sources of temperature data. See `EEweather `_.
- Candidate model selection
- Data sufficiency checking
- Model serialization
- First-class warnings reporting
- Pandas dataframe support
- Visualization tools

Roadmap for 2020 development
----------------------------

The OpenEEmeter project growth goals for the year fall into two categories:

1. Community goals - we want help our community thrive and continue to grow.
2. Technical goals - we want to keep building the library in new ways that make it
   as easy as possible to use.

Community goals
~~~~~~~~~~~~~~~

1. Develop project documentation and tutorials

A number of users have expressed how hard it is to get started when tutorials are
out of date. We will dedicate time and energy this year to help create high quality
tutorials that build upon the API documentation and existing tutorials.

2. Make it easier to contribute

As our user base grows, the need and desire for users to contribute back to the library
also grows, and we want to make this as seamless as possible. This means writing and
maintaining contribution guides, and creating checklists to guide users through the
process.


Technical goals
~~~~~~~~~~~~~~~

1. Implement new CalTRACK recommendations

The CalTRACK process continues to improve the underlying methods used in the
OpenEEmeter. Our primary technical goal is to keep up with these changes and continue
to be a resource for testing and experimentation during the CalTRACK methods setting
process.

2. Hourly model visualizations

The hourly methods implemented in the OpenEEMeter library are not yet packaged with
high quality visualizations like the daily and billing methods are. As we build and
package new visualizations with the library, more users will be able to understand,
deploy, and contribute to the hourly methods.

3. Weather normal and unusual scenarios

The EEweather package, which supports the OpenEEmeter, comes packaged with publicly
available weather normal scenarios, but one feature that could help make that easier
would be to package methods for creating custom weather year scenarios.

4. Greater weather coverage

The weather station coverage in the EEweather package includes full coverage of US and
Australia, but with some technical work, it could be expanded to include greater, or
even worldwide coverage.

License
-------

This project is licensed under [Apache 2.0](LICENSE).

Other resources
---------------

- `CONTRIBUTING `_: how to contribute to the project.
- `MAINTAINERS `_: an ordered list of project maintainers.
- `CHARTER `_: open source project charter.
- `CODE_OF_CONDUCT `_: Code of conduct for contributors.
","'building-energy', 'efficiency', 'energy', 'energy-data', 'energy-efficiency'",2024-04-23T14:46:55Z,30,208,23,"('philngo', 1579), ('jason-recurve', 122), ('tplagge', 119), ('ssuffian', 85), ('pyup-bot', 62), ('hshaban', 49), ('travis-recurve', 44), ('joydeep-recurve', 34), ('jfenna', 25), ('potash', 18), ('peterbolson', 12), ('mariano-recurve', 9), ('dyeager-recurve', 8), ('philngo-recurve', 7), ('jwickers', 5), ('natphi', 5), ('jglasskatz', 5), ('ericdill', 4), ('toshi09', 3), ('arpankotecha', 3), ('jpvelez', 2), ('tsennott', 2), ('mdrpheus', 2), ('lisbyers-recurve', 1), ('takkaria', 1), ('cathydeng', 1), ('kfogel', 1), ('rybalko', 1), ('marcrecurve', 1), ('marcpare', 1)","[7, 'Affordable and Clean Energy']"
JustFixNYC/tenants,"JustFix.nyc is a tool to document, organize, and take action in getting repairs made on your apartment.","This is the codebase for the JustFix.nyc platform, including the Tenant Webapp, Advocate Dashboard, and the back-end API for managing data. For more information about our organization, please visit [www.justfix.nyc](https://www.justfix.nyc).

## Getting Started

This guide assumes that you are using a [UNIX](http://i.imgur.com/uE6fkx7.gif) system (most likely macOS), but everything is available on Windows if you follow the appropriate guides thru the links below.

There are two ways to get started. Both require a bit of common setup.

### Common setup

1. Get a copy of the code!

  ```
  git clone https://github.com/JustFixNYC/tenants.git
  ```

2. You should have a `development.js` copy from me. (If not, email me at [dan@justfix.nyc](mailto:dan@justfix.nyc)). Place that file in `config/env`.

Now you have the option to set up and run everything locally, or use Docker.

### Option 1: Run everything locally

#### Build tools and languages

0. Open terminal. Some of these steps may require `sudo` in order to install.

1. Install [Node.js](https://nodejs.org/en/) (v4.4.x is ideal) and make sure everything works:

  ```
  node -v
  ```

3. Update [npm](https://www.npmjs.com/) and make sure everything works:

  ```
  npm install -g npm@latest
  npm -v
  ```

4. Download and install [MongoDB](https://www.mongodb.com/download-center). Make sure that it runs on the default port (27017)

5. Use npm to install [bower](http://bower.io/) and [grunt](http://gruntjs.com/). You'll probably need sudo for this (using `-g` affects your machine globally).

  ```
  sudo npm install -g bower grunt-cli
  ```   

6. Set up [git](https://help.github.com/articles/set-up-git/) if you don't already have it.

7. Set up Ruby and Sass to compile CSS. See [here](https://github.com/gruntjs/grunt-contrib-sass#sass-task) for instructions.

#### Download and install libraries

Use npm to install the needed back-end and buildtool libraries. It should trigger a `bower install` for front-end dependencies automatically. Make sure you're in the root directory for the project - i.e. the same level as the `package.json` file.

  ```
  npm install
  ```  

#### Start everything up

1. To start, you'll need an active mongodb instance running. I like to do this in a folder within `app` - e.g. `app/mongodb` but it can be run globally or anywhere else.

  ```
  mongod --dbpath . &
  ```
Open a new terminal window and make sure it's running:
  ```
  ps aux | grep mongo
  ```
You should see the process running, as well as the `grep mongo` command process you just ran. Ex:

  ```
  dan@Dans-MacBook tenants (master) $ ps aux | grep mongo
  dan               4072   0.3  0.0  2583596   6272   ??  S    16Feb17  48:14.37 mongod --dbpath .
  dan              24473   0.0  0.0  2444056    796 s001  S+    4:26PM   0:00.01 grep mongo
  ```

2. After that, it should be quite simple to start the app (again, make sure this is from the root directory):
  ```
  grunt
  ```

#### Set environment variables

Set the `MONGODB_TEST_URI` environment variable to point to a test database
that will be wiped every time the test suite is run, e.g.:

```
export MONGODB_TEST_URI=mongodb://localhost:27017/unittests
```

### Option 2: Use Docker

1. Download [Docker Community Edition](https://www.docker.com/community-edition).

2. Run `bash docker-update.sh`.

   Note that in the future, you'll want to run this whenever you update the repository or switch branches, too, to make sure you have all the latest dependencies.

3. Run `docker-compose up`.

### Visit your local server

Now that you've got everything up and running, go to `http://localhost:3000` to see your development version! Grunt will watch for any changes you make to the code and automatically restart the server for live development.

### Tips on using Docker

If you decided to go the Docker route but aren't very familiar with Docker, here are some tips.

#### Running command-line tools

If you ever want to run an individual command-line tool on the project, such as a specific grunt task or linter, you can dive into your Docker's main `web` container by running:

```
docker-compose run web bash
```

You will be in the container's `/tenants` directory, which maps to the root of your repository on your local machine.

Alternatively, you can run individual commands just by running `docker-compose run web `. Some people do this so often that they create a shell alias called `dcr` that's short for `docker-compose run`.

#### Uninstalling or starting from scratch

If you ever get your Docker setup into a weird state where nothing works, or if you're done with the project and want to free all resources used by Docker, run `docker-compose down -v`.

You'll then need to re-run `bash docker-update.sh` set everything up again.

### Running the test suite

To run the test suite, run:

```
npm test
```

### More Questions

Check out [MEAN.JS](http://meanjs.org/docs/0.3.x/) - will have more tutorials on the architecture setup and things for troubleshooting.


## License

JustFix.nyc uses the GNU General Public License v3.0 Open-Source License. See `LICENSE.md` file for the full text.
","'civic-tech', 'civictech', 'mean', 'mean-stack', 'meanstack', 'non-profit', 'nonprofit', 'nyc'",2020-07-17T09:30:23Z,3,15,7,"('romeboards', 540), ('Meegan', 220), ('toolness', 17)","[10, 'Reduced Inequalities']"
camicroscope/caMicroscope,Digital pathology image viewer with support for human/machine generated annotations and markups.,"
  


caMicroscope is a web-based biomedical image and data viewer, with a strong emphasis on cancer pathology WSI (Whole Slide Imaging).
This guide has sections for different kinds of use of the platform. The [User Guide](#user-guide) covers the basics on how to use caMicroscope viewer. nanoBorb covers nanoBorb, the version of caMicroscope designed as a standalone application for individual users without a server. [Hosted Setup](#hosted-setup) covers how to set up caMicroscope for multiple users on a server. [Developer Guide](#developer-guide) covers the broad strokes on how to add new functionality to caMicroscope.

![View Slides](docs/img/View.gif)
![Measure Features](docs/img/Measure.gif)
![Annotate Areas of Interest](docs/img/Draw.gif)
![Alternate Annotation Method](docs/img/Paint.gif)
![Automatic Object Detection](docs/img/Segment.gif)
![Test Classification Models](docs/img/Predict.gif)

# User Guide

## Selecting an Image
Depending on what is providing the image metadata, a different login process may be necessary. For public instances, no log in is necessary, and you can proceed to view slides. Use of other tools, such as annotations may or may not require login in this case.
For slim instances, login should be done through a redirect directly. For pathDB instances, login should be done on the login link on the main page.
At this point, select a collection, if applicable, and proceed to open or ""view"" the image of your choice.

## Viewing an Image
Once an image is open, you can pan around the image by either clicking and dragging (when no conflicting tool, such as the pen, is open), or by moving the red bounding box in the viewport in the bottom right.
Zooming can be accomplished through the scroll wheel, pinch events on a touch screen, by using the zoom slider or its associated buttons, or by clicking on the zoom number and inputting a different number.

## Using Tools
The toolbar is in the top-left of the main content window. Use the toolbar buttons to manipulate the slide. To close any toolbar button, click the same button again or a new button.

| Tool  | Name        | Function  |
| ----- |-------------| -----|
| ![](https://fonts.gstatic.com/s/i/materialicons/apps/v4/24px.svg)      | Annotations | Opens the Annotation panel, where you can select which annotation set to view, name that annotation set, add optional notes about the annotation set, save the annotation set, and reset the panel to its original state. |
| ![](https://fonts.gstatic.com/s/i/materialicons/view_list/v4/24px.svg)      | Layer Manager      | Opens the Layers Manager panel, where you can select which layers to view. |
| ![](https://fonts.gstatic.com/s/i/materialicons/home/v4/24px.svg)      | Home      | Return to the data table so that you can open another slide.|
| ![](https://fonts.gstatic.com/s/i/materialicons/create/v4/24px.svg)      | Draw      |  Draw thin lines, thick lines, or polygons on the image. Annotations can also be computer aided using the Smart-pen tool. Draw them, stretch them, remove them. To maintain the integrity of measurements, avoid drawing shapes that overlap or intersect one another. |
| ![](https://fonts.gstatic.com/s/i/materialicons/colorize/v4/24px.svg)      | Preset Labels      |  Use a preset annotation type immediately to quickly annotate a slide consistently. |
| ![](https://fonts.gstatic.com/s/i/materialicons/search/v4/24px.svg)       | Magnifier      |The Magnifier works like a magnifying glass and allows you to see the slide at normal magnification (1.0), low magnification (0.5), or high magnification (2.0). Click a magnification level and place the bounding box on the area of the slide you want to magnify. |
| ![](https://fonts.gstatic.com/s/i/materialicons/space_bar/v4/24px.svg)      | Measurement      | Drag this tool on the slide to learn the measurement in micrometers. |
| ![](https://fonts.gstatic.com/s/i/materialicons/share/v4/24px.svg)      | Share View      |Opens a window with a URL to the current presentation state of the slide including the magnification level, layers that are currently open, and your position on the image.|
| ![](https://fonts.gstatic.com/s/i/materialicons/view_carousel/v4/24px.svg)      | Side by Side Viewer     |Shows the Layer Manager panel, the left and right layers, and inset window. For the right and left layer, select which layer you want to view. |
| ![](https://fonts.gstatic.com/s/i/materialicons/satellite/v4/24px.svg)      | Heatmap     | For a slide with heatmap data, opens the choices of heatmaps available, as well as ways of displaying the heatmaps. The gradient shows all of the values on the selected spectrum for the field you selected. Contains a heatmap edit pen function.|
| ![](https://fonts.gstatic.com/s/i/materialicons/label/v4/24px.svg)      | Labeling      |Use this tool to draw a circle or rectangle around a tumor region, measure an area on the slide, download labels, and submit a bug report. The Labeling tool has its own toolbar with tools in the following order from left to right: return to the previous slide, place a square on the slide, place a circle on the slide, measure an area, download labels, and submit a bug report. Click the left arrow at the far right of the toolbar to hide it, then click the right arrow to show it. |
| ![](https://fonts.gstatic.com/s/i/materialicons/timeline/v6/24px.svg)      | Segment      | This tool allows you to display, count, and export nuclear segmentations on the image. Clicking this tool opens the following custom toolbar. |
| ![](https://fonts.gstatic.com/s/i/materialicons/aspect_ratio/v4/24px.svg)      | Model      | Show results from a pre-trained tensorflow compatible model on a ROI of the slide. |
| ![](https://fonts.gstatic.com/s/i/materialicons/get_app/v4/24px.svg)      | Download Slide      | Download the slide image to your system |
| ![](https://fonts.gstatic.com/s/i/materialicons/playlist_add_check/v8/24px.svg)      | Mark Reviewed      | Use to signify the completion of review of a slide. |
| ![](https://fonts.gstatic.com/s/i/materialicons/bug_report/v4/24px.svg)      | Bug Report      | Report a bug or give feedback. |
| ![](https://fonts.gstatic.com/s/i/materialicons/camera_enhance/v4/24px.svg)      | Slide Capture      | Click to take a screenshot of the slide and annotations on it. |
| ![](https://fonts.gstatic.com/s/i/materialicons/help/v4/24px.svg)      | Tutorial      | Click to view a guided tour of the viewer tools. |


## Toolbar Shortcuts

| Tool         | Shortcut  |
|------------- |-----------|
| Annotation   |  Ctrl + a |
| Magnifier    |  Ctrl + m |
| Measurement    |  Ctrl + r |
| Side-by-Side |  Ctrl + s |
| Close all tools |  ESC   |

# Hosted Setup
The full distribution repository for hosted caMicroscope is [here](https://github.com/camicroscope/Distro/).
run with `docker-compose -f caMicroscope.yml up`

this will build all services and run in the foreground.
Use `docker-compose -f caMicroscope.yml build` to rebuild the services.

Once everything is up, go to \:4010/ to see the landing page.

# Other Resources
- **Slack:** 
- **Discussion mailing list:** 
- **Sample Tensorflow Models:** 

# Developer Guide
We are collecting feedback to write this section in more detail. Please add your suggestions [here](https://github.com/camicroscope/caMicroscope/issues/267).

caMicroscope is open source software. Any involvement and contribution with the caMicroscope project is greatly appreciated. Feel free to get directly involved in any of the repositories in the caMicroscope organization. New developers may find the notes in [CONTRIBUTING](https://github.com/camicroscope/caMicroscope/blob/master/CONTRIBUTING.md) helpful to start contributing to caMicroscope.

It is highly recommended to make any changes off of the develop branch of a repository, and, when ready, create a PR to the develop branch of the source repository. Before sending the PR, make sure that there are no linting errors by running ```npm install``` and then ```npm run lint```  to see the errors and ```npm run lint-fix``` to automatically fix the errors in the repository folder.

Source code organization ie the file structure of caMicroscope can be found in [file structure](https://github.com/camicroscope/caMicroscope/blob/master/docs/file_structure.md)

## Fast Local Changes
When using the hosted setup, you can have the distribution host the changes from your local. Follow these steps :
- Clone this repository, the [Caracal repository](https://github.com/camicroscope/Caracal/) and [the distribution](https://github.com/camicroscope/Distro/) in the same parent directory
- Set the build to build your local changes instead of the hosted git versions by editing the ca-back container section of your develop.yml. Replace the build context section with the path to your caracal checkout (""../Caracal""), and add `- ../caMicroscope:/src/camicroscope` to the volumes.
- Remove this line from 'Dockerfile' in Caracal repository :
```
RUN git clone https://github.com/${fork:-camicroscope}/camicroscope.git --branch=${viewer:-master}
```
- In Distro repository, enter the following commands :
```
docker-compose -f develop.yml build
docker-compose -f develop.yml up
```
","'camicroscope', 'digital-pathology', 'slide-images', 'whole-slide-imaging'",2024-05-02T16:57:20Z,30,224,26,"('birm', 1299), ('tdiprima', 343), ('nanli-emory', 244), ('bridge2014', 173), ('lastlegion', 103), ('Lincolnnus', 93), ('akhil-rana', 92), ('ajasniew', 91), ('akazer2', 80), ('jbalsamo', 71), ('cjchirag7', 29), ('leoarc', 22), ('r7rohan', 20), ('JasoxLee', 18), ('mgautam98', 17), ('Hemansh31', 17), ('Insiyaa', 15), ('dependabotbot', 13), ('CGDogan', 12), ('vunh', 12), ('gbengaoluwadahunsi', 11), ('sohamsshah', 10), ('amritansh22', 10), ('Vedant1202', 10), ('AbdulRashidReshamwala', 9), ('vikasgola', 8), ('pranavbudhwant', 7), ('Akhilesh-max', 7), ('Lochipi', 6), ('AkhileshAdithya', 6)","[3, 'Good Health and Well-Being']"
OpenRefine/OpenRefine,"OpenRefine is a free, open source power tool for working with messy data and improving it","# OpenRefine

[![DOI](https://zenodo.org/badge/6220644.svg)](https://zenodo.org/badge/latestdoi/6220644)
[![Join the chat at https://gitter.im/OpenRefine/OpenRefine](https://badges.gitter.im/OpenRefine/OpenRefine.svg)](https://gitter.im/OpenRefine/OpenRefine) 
[![Snapshot release](https://github.com/OpenRefine/OpenRefine/actions/workflows/snapshot_release.yml/badge.svg)](https://github.com/OpenRefine/OpenRefine/actions/workflows/snapshot_release.yml) [![Coverage Status](https://coveralls.io/repos/github/OpenRefine/OpenRefine/badge.svg?branch=master)](https://coveralls.io/github/OpenRefine/OpenRefine?branch=master) [![Translation progress](https://hosted.weblate.org/widgets/openrefine/-/svg-badge.svg)](https://hosted.weblate.org/engage/openrefine/?utm_source=widget)

OpenRefine is a Java-based power tool that allows you to load data, understand it,
clean it up, reconcile it, and augment it with data coming from
the web. All from a web browser and the comfort and privacy of your own computer.

Official website: **https://openrefine.org**

Community forum: **https://forum.openrefine.org**

[](https://openrefine.org)

## Download

* [OpenRefine Releases](https://github.com/OpenRefine/OpenRefine/releases)

## Snapshot releases

You can download snapshots of the development version of OpenRefine.
To do so, you need to be logged in to GitHub. Then, click on the first item with a green tick / check mark on [this page](https://github.com/OpenRefine/OpenRefine/actions/workflows/snapshot_release.yml) and scroll down to the Artifacts section to find the version that matches your operating system.

## Run from source

If you have cloned this repository to your computer, you can run OpenRefine with:

* `./refine` on Mac OS and Linux
* `refine.bat` on Windows

This requires [JDK 11](https://adoptium.net/) or newer, [Apache Maven](https://maven.apache.org/) and [NPM](https://www.npmjs.com/) 16 or newer.

## Documentation

* [User Manual](https://openrefine.org/docs)
* [FAQ](https://github.com/OpenRefine/OpenRefine/wiki/FAQ)

## Contributing to the project

* [Developers Guide & Architecture](https://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Developers)
* [Contributing Guide](https://github.com/OpenRefine/OpenRefine/blob/master/CONTRIBUTING.md)
* [Project Governance](https://github.com/OpenRefine/OpenRefine/blob/master/GOVERNANCE.md)

## Contact us

* [Community forum](https://forum.openrefine.org)
* [Twitter](https://www.twitter.com/openrefine)
* [Gitter](https://gitter.im/OpenRefine/OpenRefine)
* [Matrix (bridged from Gitter)](https://matrix.to/#/#OpenRefine_OpenRefine:gitter.im)

## Licensing and legal issues

OpenRefine is open source software and is licensed under the BSD license located in the [LICENSE.txt](LICENSE.txt). See the folder `licenses` for information on open source libraries that OpenRefine depends on.

## Credits

This software was created by Metaweb Technologies, Inc. and originally written and conceived by [David Huynh](https://github.com/dfhuynh). Metaweb Technologies, Inc. was acquired by Google, Inc. in July 2010 and the product was renamed Google Refine. In October 2012, it was renamed OpenRefine as it transitioned to a community-driven project.

Since 2020, OpenRefine is fiscally sponsored by [Code for Science and Society](https://www.codeforsociety.org/) (CS&S).

See [CONTRIBUTING.md](./CONTRIBUTING.md) for instructions on how to contribute yourself.
","'data-analysis', 'data-science', 'data-wrangling', 'datacleaning', 'datacleansing', 'datajournalism', 'datamining', 'java', 'journalism', 'opendata', 'reconciliation', 'wikidata'",2024-05-03T03:51:04Z,30,10499,474,"('wetneb', 1698), ('tfmorris', 949), ('dfhuynh', 838), ('dependabotbot', 834), ('weblate', 550), ('stefanom', 396), ('jackyq2015', 260), ('thadguidry', 227), ('elebitzero', 203), ('ostephens', 170), ('isaomatsunami', 143), ('dependabot-previewbot', 115), ('iainsproat', 76), ('Abbe98', 69), ('SantosSi', 68), ('allanaaa', 55), ('kushthedude', 55), ('trnstlntk', 45), ('darecoder', 44), ('magdmartin', 42), ('antoine2711', 40), ('afkbrb', 36), ('joanneong', 32), ('Blakko', 32), ('fgiroud', 32), ('comradekingu', 30), ('SannitaSSJ', 26), ('elroykanye', 26), ('msaby', 20), ('lisa761', 19)","[17, 'Partnerships for the Goals']"
enketo/enketo-express,The full-fledged Enketo web application for the ODK ecosystem,"# Enketo Express

### ⚠️ Enketo Express code and issues have moved to [the Enketo monorepo](https://github.com/enketo/enketo) ⚠️

---

_The [Enketo Smart Paper](https://enketo.org) web application._ It can be used directly by form servers or used as inspiration for building applications that wrap [Enketo Core](https://github.com/enketo/enketo-core). See [this diagram](https://enketo.org/develop/) for a summary of how the different Enketo components are related.

**To get started visit our [technical documentation](https://enketo.github.io/enketo-express).**

### Project status

Enketo was initiated in 2009 by Martijn van de Rijdt as a web-based alternative or complement to [ODK Collect](https://docs.getodk.org/collect-intro/). It has become a core component of the ODK ecosystem and been adopted by several organizations beyond that ecosystem.

As of 2022, Enketo is maintained by the [ODK team](https://getodk.org/about/team.html) (primarily [Trevor Schmidt](https://github.com/eyelidlessness/)). Martijn continues to provide advice and continuity. The ODK project sets priorities in collaboration with its [Technical Advisory Board](https://getodk.org/about/ecosystem.html).

Our current primary goals are:

-   Increasing alignment with ODK Collect, particularly in service of submission edits.
-   Improving error messages so that users can get out of bad states.
-   Improving long-term maintainability by modernizing code bases, removing code duplication, and simplifying state mutation.

Feature requests and project discussion are welcome on the [ODK forum](https://forum.getodk.org/).

### Translation

The user interface was translated by: Oleg Zhyliak (Ukrainian), Karol Kozyra (Swedish), Badisches Rotes Kreuz (German), Serkan Tümbaş (Turkish), Hélène Martin (French), Gurjot Sidhu(Hindi, Panjabi), ""Abcmen"" (Turkish), Otto Saldadze, Makhare Atchaidze, David Sichinava, Elene Ergeshidze (Georgian), Nancy Shapsough (Arabic), Noel O'Boyle (French), Miguel Moreno (Spanish), Tortue Torche (French), Bekim Kajtazi (Albanian), Marc Kreidler (German), Darío Hereñú (Spanish), Viktor S. (Russian), Alexander Torrado Leon (Spanish), Peter Smith (Portugese, Spanish), Przemysław Gumułka (Polish), Niklas Ljungkvist, Sid Patel (Swedish), Katri Jalava (Finnish), Francesc Garre (Spanish), Sounay Phothisane (Lao), Linxin Guo (Chinese), Emmanuel Jean, Renaud Gaudin (French), Trần Quý Phi (Vietnamese), Reza Doosti, Hossein Azad, Davood Mottalee (Persian), Tomas Skripcak (Slovak, Czech, German), Daniela Baldova (Czech), Robert Michael Lundin (Norwegian), Margaret Ndisha, Charles Mutisya (Swahili), Panzero Mauro (Italian), Gabriel Kreindler (Romanian), Jason Reeder, Omar Nazar, Sara Sameer, David Gessel (Arabic), Tino Kreutzer (German), Wasilis Mandratzis-Walz (German, Greek), Luis Molina (Spanish), Martijn van de Rijdt (Dutch).

_Send a message if you'd like to contribute! We use an easy web interface provided by [Transifex](https://www.transifex.com/projects/p/enketo-express/)._

### Releases

1. Create release PR
1. Check [Dependabot](https://github.com/enketo/enketo-express/security/dependabot) for alerts
1. Run `npm update`
    - Check if `node-forge` has been updated and if so, verify encrypted submissions end-to-end
1. Run `npm audit`
    - Run `npm audit fix --production` to apply most important fixes
1. Update version in `package.json`
    - Bump to major version if consumers have to make changes.
1. Run `npm i`
1. Run `npm test`
1. Run `npm run build-docs`
1. Update `CHANGELOG.md`
1. Merge PR with all changes
1. Create GitHub release
1. Tag and publish the release
    - GitHub Action will publish it to npm

### Funding

The development of this application is now led by [ODK](https://getodk.org) and funded by customers of the ODK Cloud hosted service.

Past funders include [KoBo Toolbox (Harvard Humanitarian Initiative)](http://www.kobotoolbox.org), [iMMAP](http://immap.org), [OpenClinica](https://openclinica.com), [London School of Hygiene and Tropical Medicine](https://opendatakit.lshtm.ac.uk/), [DIAL Open Source Center](https://www.osc.dial.community/) and [Enketo LLC](https://www.linkedin.com/company/enketo-llc). Also see [Enketo Core sponsors](https://github.com/enketo/enketo-core#sponsors).

### License

See [the license document](https://github.com/enketo/enketo-express/blob/master/LICENSE) for this application's license.

Note that some of the libraries used in this app have a different license.

Note the 'Powered by Enketo' footer requirement as explained in [enketo-core](https://github.com/enketo/enketo-core#license). This requirement is applicable to all Enketo apps, including this one, unless an exemption was granted.

The Enketo logo and Icons are trademarked by [Enketo LLC](https://www.linkedin.com/company/enketo-llc) and should only be used for the 'Powered by Enketo' requirement mentioned above (if applicable). To prevent infringement simply replace the logo images in [/public/images](https://github.com/enketo/enketo-express/blob/master/public/images) with your own or contact [Enketo LLC](mailto:info@enketo.org) to discuss the use inside your app.

### Change log

See [change log](https://github.com/enketo/enketo-express/blob/master/CHANGELOG.md)
","'enketo', 'form', 'odk', 'survey', 'webform', 'xform'",2023-11-29T05:18:42Z,27,115,17,"('MartijnR', 1461), ('eyelidlessness', 133), ('lognaturel', 49), ('yanokwa', 47), ('jnm', 23), ('noliveleger', 10), ('alxndrsn', 4), ('magicznyleszek', 4), ('bufke', 4), ('ivermac', 3), ('dependabotbot', 3), ('punkch', 3), ('vituslehner', 1), ('pld', 1), ('ktmatzdorf', 1), ('theywa', 1), ('ukanga', 1), ('tinok', 1), ('roman-holovin', 1), ('rhunwicks', 1), ('joshuaberetta', 1), ('jdugh', 1), ('xanf', 1), ('FrankApiyo', 1), ('edrex', 1), ('kant', 1), ('duvld', 1)","[16, 'Peace, Justice and Strong Institutions']"
rubyforgood/human-essentials,"Human Essentials is an inventory management system for diaper, incontinence, and period-supply banks. It supports them in distributing to partners, tracking inventory, and reporting stats and analytics.","# Human Essentials












 
 
















## Mission 💖

Human Essentials is an inventory management system built to address the needs of [Diaper Banks](https://nationaldiaperbanknetwork.org/diaper-need/) as directly and explicitly as possible and adapted to meet the needs of other Essentials Banks. Essentials Banks maintain inventory, receive donations and other human essentials supplies (e.g. diapers, period supplies), and issue distributions to community partner organizations. Like any non-profit, they also need to perform reports on this data and have day-to-day operational information they need. This application aims to serve those needs and facilitate the general operations of the Diaper Banks (e.g., using barcode readers, scale weighing, inventory audits).

## Impact 🌟

Human Essentials has over 200 registered banks across the United States at **no cost** to them. It is currently helping over **3 million** children receive diapers and over **400k** period supply recipients receive period supplies. Our team is in partnership with the [National Diaper Bank Network (NDBN)](https://nationaldiaperbanknetwork.org/) and can be found in their annual conference that brings numerous of non-profit organizations that distribute essential products to people.

We are proud of our achievements up to date but there is much more to do! This is where you come in...

## Ruby for Good

Human Essentials is one of many projects initiated and run by Ruby for Good. You can find out more about Ruby for Good at https://rubyforgood.org

## Digital Public Good 🎉

The [Digital Public Goods Alliance](https://digitalpublicgoods.net/registry/) recognizes Human Essentials as a digital public good (DPG). This project supports the following Sustainable Development Goals:
* [SDG 1](https://sdgs.un.org/goals/goal1) - End poverty in all its forms everywhere
* [SDG 3](https://sdgs.un.org/goals/goal3) - Ensure healthy lives and promote well-being for all at all ages
* [SDG 10](https://sdgs.un.org/goals/goal10) - Reduce inequality within and among countries

Use as an Organization or Contribute as an Individual/Team to this Project:
- [NGO Adoption Info](ngo.md) - information about how to use this DPG
- [Skills Based Volunteering Info](sbv.md) - information about how to volunteer

# Welcome Contributors! 👋

Thanks for checking us out! If you're new here, here are some things you should know:
- Issues tagged ""Help Wanted"" are self-contained and great for new contributors
- Pull Requests are reviewed within a week or so
- Ensure your build passes (`rubocop -a` is often necessary) and addresses the issue requirements
- This project relies entirely on volunteers, so please be patient with communication

### Join us on slack 💬
You can sign up [here](https://join.slack.com/t/rubyforgood/shared_invite/zt-21pyz2ab8-H6JgQfGGI0Ab6MfNOZRIQA) and find us in #human-essentials. Many helpful members are available to answer your questions. Just ask, and someone will be there to help you!

##  Getting Started 🛠️

1. Install Ruby
   - Install the version specified in [`.ruby-version`](.ruby-version).
   - Visit the [Install Ruby on Rails](https://gorails.com/setup/osx/12-monterey) guide by GoRails for Ubuntu, Windows, and macOSX setup. ⚠️ Follow only the Installing Ruby step, as our project setup differs ⚠️ It is highly recommended you use a ruby version manager such as [rbenv](https://github.com/rbenv/rbenv), [asdf](https://asdf-vm.com/), or [rvm](https://rvm.io/).
   - Verify that your Ruby installation works by running `ruby -v`.
2. Install Postgres
   - Follow one of these guides: [MacOSX](https://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-ruby-on-rails-application-on-macos), [Ubuntu](https://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-ruby-on-rails-application-on-ubuntu-18-04).
     - Do you develop on Windows? We'd love to hear (and for you to submit a PR explaining) how you do it. 🙏🏻
   - Create a `database.yml` file on `config/` directory with your database configurations. You can also copy the existing files called [`database.yml.example`](config/database.yml.example) and [`.env.example`](.env.example) and change the credentials.
3. Run `bin/setup`
4. Run `bin/start` and visit http://localhost:3000/ to see the human essentials page.
5. Login as a sample user with these default credentials (which also work for [staging](https://staging.humanessentials.app/)):


   Super Users 🦸🏽‍♀️ 

  ```
    username: superadmin@example.com
    password: password!
  ```



   Bank Users 🏦 

  ```
    Organization Admin
       Email: org_admin1@example.com
    Password: password!

    User
    Email: user_1@example.com
    Password: password!
  ```



   Partner Users 👥 

  ```
    Verified Partner
    Email: verified@example.com
    Password: password!

    Invited Partner
    Email: invited@pawneehomeless.com
    Password: password!

    Unverified Partner
    Email: unverified@pawneepregnancy.com
    Password: password!

    Recertification Required Partner
    Email: recertification_required@example.com
    Password: password!
  ```


## Troubleshooting 👷🏼‍♀️

Please let us know by opening up an issue! We have many new contributors come through and it is likely what you experienced will happen to them as well.

- *""My RBENV installation didn't work!""* - The rbenv repository provides a [rbenv-doctor script](https://github.com/rbenv/rbenv-installer#rbenv-doctor) to verify the installation and check if a ruby version is installed

## Contributing Guidelines 🤝

Please feel free to contribute! Priority will be given to pull requests that address outstanding issues and have appropriate test coverage. Focus on issues tagged with the next milestone for higher priority.

To contribute:
* Identify an unassigned issue
* Assign the issue to yourself to avoid duplicated efforts (or request assignment by adding a comment)
* Fork the repo if you're not a contributor yet
* Create a new branch for the issue using the format `XXX-brief-description-of-feature`, where `XXX` is the issue number
* If you create a new model run `bundle exec annotate` from the root of the app
* Create tests to validate that your work fixes the Issue (if you need help with this, please reach out!)
* Commit locally using descriptive messages that indicate the affected parts of the app
* Ensure all tests pass successfully; if any fail, fix the issues causing the failures
* Make a final commit if tests needed fixing
* Push up the branch
* Create a pull request and indicate the addressed issue in the title

### Squashing Commits

Consider the balance of ""polluting the git log with commit messages"" vs. ""providing useful detail about the history of changes in the git log"". If you have several smaller commits that serve a one purpose, you are encouraged to squash them into a single commit. There's no hard and fast rule here about this (for now), just use your best judgement. Please don't squash other people's commits. Everyone who contributes here deserves credit for their work! :)

### Pull Request Merging

At this point, someone will work with you on doing a code review. If the automated tests gives :+1: to the PR merging, we can then do any additional (staging) testing as needed. Finally if all looks good the core team will merge your code in; if your feature branch was in this main repository, the branch will be deleted after the PR is merged. Deploys are currently done about once a week!

### In-flight Pull Requests

Sometimes we want to get a PR up there and going so that other people can review it or provide feedback, but maybe it's incomplete. This is OK, but if you do it, please tag your PR with `in-progress` label so that we know not to review / merge it.

### Becoming a Repo Contributor

Users that are frequent contributors and are involved in discussion (join the slack channel! :)) may be given direct Contributor access to the Repo so they can submit Pull Requests directly instead of Forking first.

### Stay Scoped

Try to keep your PRs limited to one particular issue, and don't make changes that are out of scope for that issue. If you notice something that needs attention but is out of scope, please [create a new issue](https://github.com/rubyforgood/human-essentials/issues/new).
## Debugging
If starting server directly, via `rail s` or `rail console`, or built-in debugger in RubyMine, then you can use `binding.pry` to debug. Drop the pry where you want the execution to pause.

If starting via Procfile with `bin/start`, then drop a ``binding.remote_pry`` into the line where you want execution to pause at. Then run ``pry-remote`` in the terminal to connect to it.
https://github.com/Mon-Ouie/pry-remote


## Testing 🧪
### Writing Tests/Specs
- Run all the tests with `bundle exec rspec`
- Run a single test with `bundle exec rspec {path_to_test_name}_spec.rb`

Make sure all tests run clean & green before submitting a Pull Request. If you are inexperienced in writing tests or get stuck on one, please reach out for help :). You probably don't need to write new tests when simple re-stylings are done (ie. the page may look slightly different but the Test suite is unaffected by those changes).

*Tip: If you need to skip a failing test, place `pending(""Reason you are skipping the test"")` into the `it` block rather than skipping with `xit`. This will allow rspec to deliver the error message without causing the test suite to fail.*

```ruby
  it ""works!"" do
    pending(""Need to implement this"")
    expect(my_code).to be_valid
  end
```

### Writing Browser/System/Feature Specs

If you need to see a browser/system spec run in the browser, you can use the following env variable:

```
NOT_HEADLESS=true bundle exec rspec
```

We've added [magic_test](https://github.com/bullet-train-co/magic_test) which makes creating browser specs much easier. It allows you to record actions on the browser running the specs and easily paste them into the spec. You can do this by adding `magic_test` within your system spec:
```rb
 it ""does some browser stuff"" do
   magic_test
 end
```
and run the spec using this command: `MAGIC_TEST=1 NOT_HEADLESS=true bundle exec rspec `

**See videos of it in action [here](https://twitter.com/andrewculver/status/1366062684802846721)**

# Deployment Process 🚀
The human-essentials & partner application should ideally be deployed on a weekly or bi-weekly schedule depending on the merged updates in the main branch. This is the process we take to deploy updates from our main branch to our servers.

### Requirements
- SSH access to our servers (usually granted to core maintainers)
- Login credentials to our [Mailchimp](https://mailchimp.com/) account


### Tag & Release
1. Push a tag with the appropriate semantic versioning. Refer to the [releases](https://github.com/rubyforgood/human-essentials/releases) for the correct versioning. For example, if the last release was `2.1.0` and you're making a hotfix, use `2.1.1`

    ```sh
    git tag x.y.z
    git push --tags
    ```
2. Publish a release associated to that tag pushed up in the previous step [here](https://github.com/rubyforgood/human-essentials/releases/new). Include details about the release's updates (we use this to notify our stakeholders on updates via email).

### Running delayed jobs

Run delayed jobs locally with the `rake jobs:work` command. This is necessary to view any emails in your browser. Alternatively, you can run a specific delayed job by opening a Rails console and doing something like:

```ruby
Delayed::Job.last.invoke_job
```

You can replace the `last` query with any other query (e.g. `Delayed::Job.find(123)`).

### Additional Notes

- Only commit the schema.rb only if you have committed anything that would change the DB schema (i.e. a migration).

# Acknowledgements
Thanks to Rachel (from PDX Diaperbank) for all of her insight, support, and assistance with this application, and Sarah ( http://www.sarahkasiske.com/ ) for her wonderful design and CSS work at Ruby For Good '17!

# License
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper?ref=badge_large)

# ✨ Contributors ✨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):





  
    
      Edwin Mak💻 📆 🚇 🤔 💬 🛡️
      Sean Marcia💻 💼 💵 🔍 📋
      Aaron H📆 💻
      Dmitry💻
      Adam Bachman💻
      JC Avena💻
      mdworken💻 📆
    
    
      Marius Pop💻
      Elayne💻
      Amy Detwiler💻
      Gia💻
      Amina Adewusi💻
      albertchae💻 🤔
      Keith Walters💻
    
    
      Chase Southard💻
      Brock Wilcox💻
      danquill💻
      Keith Bennett💻
      Benjamin Reynolds💻
      jtu0💻
      Juarez Lustosa💻
    
    
      Julien A.💻
      Emerson Manabu Araki💻
      Ben Klang💻
      karolina💻
      Josh Cano💻
      Julian Macmang💻
      Philip DeFraties💻
    
    
      GabrielRMuller💻
      Lucas Hiago💻
      Lanya Butler💻
      Eduardo Moreira💻
      Alicia Barrett💻
      Bob Forcha💻
      William Murphy💻
    
    
      Kate Donaldson💻
      Matthew Russell Dodds💻
      Allison McMillan💻
      Ashley Jean💻
      Eduardo Alencar💻
      Thomas Hart💻
      Bart Agapinan💻
    
    
      Monique💻
      Valerie Woolard💻
      zak-kay💻
      Jason LaHatte💻
      Dave Tapley💻
      Meghan💻
      Andy Thackray💻
    
    
      Felipe Lovato Flores💻
      Marcel Kooi💻
      Lee Sharma💻
      Scott Steele💻 🤔
      Sam Weerasinghe💻
      Gerald Abrencillo💻
      Rodolfo Santos💻
    
    
      Gabriel Baldão💻
      Melanie White💻
      Melissa Miller💻
      Marc Heiligers💻
      Joe Hunt💻
      Meg Gutshall💻
      Andrew H Schwartz💻
    
    
      Joseph Glass💻
      Reese Williams💻
      Santiago Perez💻
      Sirius Dely💻
      Heather Herrington💻
      Moacir Guedes💻
      CraigJZ💻
    
    
      Semih Arslanoğlu💻
      Mauricio de Lima💻
      David Curtis💻
      Natalia Galán💻
      Anderson Fernandes💻
      Gabriel Belgamo💻
      Jorge David C.T Junior💻
    
    
      Seth Lieberman💻
      Jorge Oliveira Santos💻
      Drinks💻
      Bruno Castro💻
      Carlos Palhares💻
      Clifton McIntosh💻
      Daniel Beigelman💻
    
    
      Meg Viar💻
      Svetlana Vileshina💻
      Ben Reed💻
      Emily Giurleo💻
      Alem Getu💻
      Dejan Bjeloglav💻
      Cassiano Blonski Sampaio💻
    
    
      Greg💻
      finn💻
      Jayson Mandani💻
      Stanley Liu💻
      Curtis Bartell💻
      Libby Rodriguez💻
      joshuacgraves💬 📆
    
    
      Himanshu💻
      Mohamed Hegab💻
      Alejandro AR💻
      hatsu💻
      Matt Glover💻
      js-sapphire💻
      lasitha💻
    
    
      Patrick McClernan💻
      Marc Bellingrath💻
      Daniel Orner💻
      Bob Mazanec💻
      Mark Yen💻
      Rachael Wright-Munn💻
      Ítalo Matos💻
    
    
      Alvaro Sanchez Diaz💻
      mbrundige💻
      Robert Greene💻
    
  







This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

","'hacktoberfest', 'help-wanted', 'non-profit', 'open-source', 'rails', 'rubyforgood', 'sdg-1', 'sdg-10', 'sdg-3'",2024-05-03T04:21:33Z,30,426,54,"('seanmarcia', 720), ('armahillo', 533), ('dependabotbot', 525), ('dependabot-previewbot', 271), ('edwinthinks', 253), ('dorner', 224), ('awwaiid', 208), ('cielf', 167), ('mdworken', 117), ('abachman', 85), ('jadekstewart3', 81), ('IlinDmitry', 79), ('cancelei', 77), ('jcavena', 71), ('elasticspoon', 70), ('lokisk1155', 57), ('mlpinit', 51), ('Aliciawyse', 40), ('kaylahrose', 36), ('giacoelho', 34), ('ejuten', 34), ('cattywampus', 33), ('kev-kev', 31), ('Nirvikalpa108', 31), ('albertchae', 29), ('danquill', 28), ('chaserx', 27), ('cheddar-godaddy', 25), ('dependabot-support', 25), ('keithrbennett', 22)","[10, 'Reduced Inequalities']"
ShelterTechSF/askdarcel-api,"Backend of the ""Ask Darcel"" app.","# askdarcel-api [![Build Status](https://travis-ci.org/ShelterTechSF/askdarcel-api.svg?branch=master)](https://travis-ci.org/ShelterTechSF/askdarcel-api)

This project exposes the API endpoints for supporting the askdarcel-web project, which is built using a Ruby on Rails API Server


## Onboarding information

[Dev Role Description](https://www.notion.so/sheltertech/Developer-Engineer-Role-Description-ShelterTech-AskDarcel-SFServiceGuide-Tech-Team-7fd992a20f864698a43e3882a66338bb)

[Technical Onboarding & Team Guidelines](https://www.notion.so/sheltertech/Technical-Onboarding-and-Team-Guidelines-a06d5543495248bfb6f17e233330249e)


## Docker-based Development Environment (Recommended)

### Requirements

Docker Community Edition (CE) >= 17.06

Docker Compose >= 1.18

Download and install the version of [Docker for your OS](https://www.docker.com/community-edition#/download).


#### Creating the `.env` file

The `.env` file is a de facto file format that allows you to specify environment
variables that can be read by an application. It makes it easier to pass
environment variables to an application without manually having to set them in
the environment. It is supported by:
- [Docker](https://code.visualstudio.com/docs/python/environments) (built in)
- [NodeJS](https://www.npmjs.com/package/dotenv) (as a library)
- [Ruby](https://github.com/bkeepers/dotenv) (as a library)

In the root of the repo cloned to your local machine, create a file named `.env` with the credentials listed in [this
document](https://www.notion.so/sheltertech/API-Keys-Env-variables-3913e9074b61403c860d1a4649060e4f).


### Set up the project

This is not a full guide to Docker and Docker Compose, so please consult other
guides to learn more about those tools.

```sh
# Build (or rebuild) Docker images
$ docker-compose build

# Start the database container (in the background with -d)
$ docker-compose up -d db

# Generate random database fixtures
$ docker-compose run --rm api rake db:setup db:populate

# Start the Rails development server in the api container (in the foreground)
$ docker-compose up api

# Stop all containers, including background ones
$ docker-compose stop
```


### Running Postman tests from the command line

```sh
# Reset DB with initial database fixtures
$ docker-compose run --rm api rake db:setup db:populate

# Run Docker container that executes Postman CLI tool named newman
$ docker-compose run --rm postman
```


### Alternative database setup

```sh
# Populate the database with a direct copy of the live staging database.
# - Ask technical team for the staging database password.
$ docker-compose run -e STAGING_DB_PASSWORD= --rm api rake db:setup db:import_staging
```



",,2024-05-02T07:09:32Z,30,8,7,"('lgarofalo', 142), ('schroerbrian', 82), ('richardxia', 65), ('jjfreund', 63), ('jfhamlin', 33), ('katerina-kossler', 28), ('lexholden', 26), ('twolfe2', 25), ('drcaramelsyrup', 17), ('dependabotbot', 15), ('cliffcrosland', 13), ('quanhuynh', 11), ('james7770', 10), ('shannonrdunn', 10), ('wi11', 8), ('Maxastuart', 7), ('bryanh210', 6), ('quanwin', 5), ('GeorgeCloud', 5), ('alexanderturinske', 4), ('iopkelvin', 3), ('meltingmelon', 3), ('tsyaeger', 3), ('kumquatexpress', 2), ('Raawr', 2), ('zjipsen', 2), ('fayceltouili', 1), ('Akhtam', 1), ('rtindru', 1), ('trucnguyen', 1)","[1, 'No Poverty']"
openml/OpenML,Open Machine Learning,"[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

OpenML: Open Machine Learning
=============================
Welcome to the OpenML GitHub page! :tada:

Contents:

* [Who are we?](#who-are-we)
* [What is OpenML?](#what-is-openml)
     * [Benefits for Science](#benefits-for-science)
     * [Benefits for Scientists](#benefits-for-scientists)
     * [Benefits for Society](#benefits-for-society)
* [Get involved](#get-involved)


## Who are we?
We are a group of people who are excited about open science, open data and machine learning. 
We want to make machine learning and data analysis **simple**, **accessible**, **collaborative** and **open** with an optimal **division of labour** between computers and humans. 

## What is OpenML?
Want to learn about OpenML or get involved? Please do and [get in touch](openmlHQ@googlegroups.com) in case of questions or comments! :incoming_envelope:

- Getting started:
    - Check out the [OpenML Website](https://www.openml.org) to get a first impression of what OpenML is 
    - The OpenML [Documentation page](https://openml.github.io/OpenML/) gives an introduction in details and features, as well as
    - OpenML's different [APIs](https://openml.github.io/OpenML/APIs) and [integrations](https://openml.github.io/OpenML/sklearn/) so that everyone can work with their favorite tool. 
- How to contribute: https://github.com/openml/OpenML/blob/master/CONTRIBUTING.md 
- Citation and Honor Code: https://www.openml.org/terms
- Communication / Contact: https://github.com/openml/OpenML/wiki/Communication-Channels

OpenML is an online machine learning platform for sharing and organizing data, machine learning algorithms and experiments. It is designed to create a frictionless, networked ecosystem, that you can readily integrate into your existing processes/code/environments, allowing people all over the world to collaborate and build directly on each other’s latest ideas, data and results, irrespective of the tools and infrastructure they happen to use. 

As an open science platform, OpenML provides important benefits for the science community and beyond.

### Benefits for Science
Many sciences have made significant breakthroughs by adopting online tools that help organizing, structuring and analyzing scientific data
online. Indeed, any shared idea, question, observation or tool may be noticed by someone who has just the right expertise to spark new ideas, answer open questions, reinterpret observations or reuse data and tools in unexpected new ways. Therefore, sharing research results and collaborating online as a (possibly cross-disciplinary) team enables scientists to quickly build on and extend the results of others,
fostering new discoveries.

Moreover, ever larger studies become feasible as a lot of data are already available. Questions such as “Which hyperparameter is important to tune?”, “Which is the best known workflow for analyzing this data set?” or “Which data sets are similar in structure to my own?” can be answered in minutes by reusing prior experiments, instead of spending days setting up and running new experiments.

### Benefits for Scientists
Scientists can also benefit personally from using OpenML. For example, they can save time, because OpenML assists in many routine and tedious duties: finding data sets, tasks, flows and prior results, setting up experiments and organizing all experiments for further analysis. Moreover, new experiments are immediately compared to the state of the art without always having to rerun other people’s
experiments. 

Another benefit is that linking one’s results to those of others has a large potential for new discoveries (see, for instance, Feurer et al. 2015; Post et al. 2016; Probst et al. 2017), leading to more publications and more collaboration with other scientists all over the world.

Finally, OpenML can help scientists to reinforce their reputation by making their work (published or not) visible to a wide group of people and by showing how often one’s data, code and experiments are downloaded or reused in the experiments of others.

### Benefits for Society
OpenML also provides a useful learning and working environment for students, citizen scientists and practitioners. Students and citizen scientist can easily explore the state of the art and work together with top minds by contributing their own algorithms and experiments. Teachers can challenge their students by letting them compete on OpenML tasks or by reusing OpenML data in assignments. Finally, machine learning practitioners can explore and reuse the best solutions for specific analysis problems, interact with the scientific community or efficiently try out many possible approaches.

-------------------
## Get involved

OpenML has grown into quite a big project. We could use many more hands to help us out :wrench:. 

- **You want to contribute?**: Awesome! Check out our wiki page on [how to contribute](https://github.com/openml/OpenML/wiki/How-to-contribute) or [get in touch](https://github.com/openml/OpenML/wiki/Communication-Channels). There may be unexpected ways for how you could help. We are open for any ideas. 
- **You want to support us financially?**: YES! Getting funding through conventional channels is very competitive, and we are happy about every small contribution. Please send an email to openmlHQ@googlegroups.com!


-------------------
## GitHub organization structure

OpenML's code distrubuted over different repositories to simplify development. Please see their individual readme's and issue trackers of you like to contribute. These are the most important ones:

- **[openml/OpenML](https://github.com/openml/openml)**: The OpenML web application, including the REST API.
- **[openml/openml-python](https://github.com/openml/openml-python)**: The Python API, to talk to OpenML from Python scripts (including scikit-learn).
- **[openml/openml-r](https://github.com/openml/openml-r)**: The R API, to talk to OpenML from R scripts (inclusing mlr).
- **[openml/java](https://github.com/openml/java)**: The Java API, to talk to OpenML from Java scripts.
- **[openml/openml-weka](https://github.com/openml/openml-weka)**: The WEKA plugin, to talk to OpenML from the WEKA toolbox.





","'citizen-scientists', 'collaboration', 'datasets', 'hacktoberfest', 'machine-learning', 'open-science', 'opendata', 'science'",2024-04-05T08:51:24Z,25,638,48,"('janvanrijn', 1278), ('joaquinvanschoren', 545), ('berndbischl', 110), ('vumaasha', 50), ('sahithyaravi', 49), ('ltorgo', 35), ('dominikkirchhoff', 34), ('WilliamRaynaut', 34), ('mwever', 16), ('beaugogh', 10), ('prabhant', 8), ('mfeurer', 5), ('PGijsbers', 5), ('giuseppec', 3), ('josvandervelde', 3), ('HeidiSeibold', 3), ('ArlindKadra', 3), ('hildeweerts', 2), ('mehdijamali', 2), ('hofnerb', 1), ('meissnereric', 1), ('GitteVW', 1), ('LennartPurucker', 1), ('waffle-iron', 1), ('ledell', 1)","[4, 'Quality Education']"
OperationCode/front-end,Operation Code's website,"![](https://i.imgur.com/mW5LUGV.jpg)



[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Twitter Follow](https://img.shields.io/twitter/follow/operation_code.svg?style=social&label=Follow&style=social)](https://twitter.com/operation_code)

[![CircleCI](https://circleci.com/gh/OperationCode/front-end/tree/main.svg?style=svg)](https://circleci.com/gh/OperationCode/front-end/tree/main)
[![Maintainability](https://api.codeclimate.com/v1/badges/5010b82ce5d8e319a597/maintainability)](https://codeclimate.com/github/OperationCode/front-end/maintainability)
[![Cypress.io tests](https://img.shields.io/badge/cypress.io-tests-green.svg?style=flat-square)](https://cypress.io)

[See unblocked, unassigned issues](https://github.com/OperationCode/front-end/issues?q=is%3Aopen+is%3Aissue+-label%3A%22Status%3A+Blocked%22+no%3Aassignee). We love our labels - feel free to filter issues to find what you want to work on.

# Welcome!

This is the front-end application for [OperationCode](https://operationcode.org). We highly recommend [joining our organization](https://operationcode.org/join) to receive an invite to our Slack team. After registering, you'll receive a Slack invite via email and will want to join the `#oc-projects` channel. You can get help from multiple professional developers, including people who have worked on the application since day 1!

Before contributing, please review our [Contributing Guide](CONTRIBUTING.md).

Designers, please review our [Contributing Guide for Designers](CONTRIBUTING_TO_DESIGN.md)

## Quick Start

If you're unsure of how to start this app or code for it, don't worry! You're our target audience!
Please read our [Contributing Guide](CONTRIBUTING.md) to learn everything you need to be able to ask the right questions on our Slack team.

**In continuing with the quick start instructions, it is assumed that you are no stranger to React applications, the JavaScript ecosystem, and standard GitHub workflows such as forking, cloning, and branching.**

Our entire UI library is documented via [![Storybook](https://github.com/storybookjs/brand/blob/8d28584c89959d7075c237e9345955c895048977/badge/badge-storybook.svg)](http://storybook.operationcode.org)

Required versions of tools used within the repo:

- Node: See [.nvmrc](https://github.com/OperationCode/front-end/blob/main/.nvmrc)
- `yarn@1`
- `git@2.17.1` or greater

```sh
# Install dependencies
yarn

# Run local development
yarn dev

# Use Storybook as a workbench when developing new components
yarn storybook

# Run all unit tests
yarn test

# Run all Cypress tests (make sure your dev server is running)
yarn test:e2e

# Create all the necessary files/folders for a new, reusable component
yarn create-component $ComponentName

# Create the necessary file with a small boilerplate for a new page
yarn create-page $PageName
```

## Open Source Gratitude

We appreciate the following tools/companies that are providing us a service or platform for free or a heavily discounted rate.

Sorted alphabetically:

---

### Chromatic



Thanks to [Chromatic](https://www.chromatic.com/) for providing the visual testing platform that helps us review UI changes and catch visual regressions.

---

### LogRocket



Thanks to [LogRocket](https://logrocket.com/) for providing time-saving context on every error and insight into our user's behavior,

---

### Sentry



Thanks to [Sentry](https://getsentry.io) for a wonderful experience with cataloguing and managing errors.

---

### Vercel



Thanks to [Vercel](https://vercel.com) for hosting and continuous deployment of all our web applications.
","'beginner-friendly', 'css', 'cypress', 'hacktoberfest', 'hacktoberfest2017', 'hacktoberfest2018', 'hacktoberfest2019', 'hacktoberfest2020', 'jest', 'next', 'postcss', 'react', 'storybook'",2024-05-02T13:27:13Z,30,368,27,"('kylemh', 2774), ('dependabot-previewbot', 171), ('AllenAnthes', 103), ('greenkeeperbot', 96), ('markchernov', 85), ('calincionca35', 80), ('manthonyg', 77), ('juliantrueflynn', 73), ('chrisgalvan', 58), ('chynh', 46), ('angtlin', 33), ('subhajit20', 31), ('chrismgonzalez', 31), ('sumitparakh', 29), ('clsoar', 28), ('dependabotbot', 28), ('danielasannino', 28), ('JohnGoure', 26), ('dirtyredz', 24), ('cbituin', 24), ('PeterEckIII', 23), ('jmayergit', 22), ('lambk', 22), ('onprem', 20), ('mergifybot', 20), ('greenkeeperio-bot', 17), ('LeviButcher', 17), ('recondesigns', 16), ('jfie5', 15), ('sethbergman', 12)","[10, 'Reduced Inequalities']"
etalab/croquemort,A micro service to check dead links efficiently and asynchronously. In use at https://www.data.gouv.fr/,"# Croquemort

## Vision

The aim of this project is to provide a way to check HTTP resources: hunting 404s, updating redirections and so on.

For instance, given a website that stores a list of external resources (html, images or documents), this product allows the owner to send its URLs in bulk and retrieve information for each URL fetched in background (status code and useful headers for now). This way he can be informed of dead links or outdated resources and acts accordingly.

The name comes from the [French term](https://fr.wikipedia.org/wiki/Croque-mort) for [Funeral director](https://en.wikipedia.org/wiki/Funeral_director).


## Language

The development language is English. All comments and documentation should be written in English, so that we don't end up with “franglais” methods, and so we can share our learnings with developers around the world.


## History

We started this project on May, 2015 for [data.gouv.fr](http://data.gouv.fr/).

We open-sourced it since the beginning because we want to design things in the open and involve citizens and hackers in our developments.


## Installation

We’re using these technologies: RabbitMQ and Redis. You have to install and launch these dependencies prior to install and run the Python packages.

Once installed, run these commands to setup the project:

```shell
$ python3 -m venv ~/.virtualenvs/croquemort
$ source ~/.virtualenvs/croquemort/bin/activate
$ pip3 install -r requirements/develop.pip
```

You're good to go!


## Usage

First you have to run the `http` service in order to receive incoming HTTP calls. You can run it with this command:

```shell
$ nameko run croquemort.http
starting services: http_server
Connected to amqp://guest:**@127.0.0.1:5672//
```

Then launch the `crawler` in a new shell that will fetch the submitted URL in the background.

```shell
$ nameko run croquemort.crawler
starting services: url_crawler
Connected to amqp://guest:**@127.0.0.1:5672//
```

You can optionnaly use the proposed configuration (and tweak it) to get some logs (`INFO` level by default):

```shell
$ nameko run --config config.yaml croquemort.crawler
```

You can enable in the config file more workers for the crawler (from 10 (default) to 50):
```yaml
max_workers: 50
```



### Browsing your data

At any time, you can open `http://localhost:8000/` and check the availability of your URLs collections within a nice dashboard that allows you to filter by statuses, content types, URL schemes, last updates and/or domains. There is even a CSV export of the data you are currently viewing if you want to script something.


### Fetching one URL

Now you can use your favorite HTTP client (mine is [httpie](https://github.com/jakubroztocil/httpie)) to issue a POST request againt `localhost:8000/check/one` with the URL as a parameter:

```shell
$ http :8000/check/one url=""https://www.data.gouv.fr/fr/""
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 28
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:21:50 GMT

{
  ""url-hash"": ""u:fc6040c5""
}
```

The service returns a URL hash that will be used to retrieve informations related to that URL:

```shell
$ http :8000/url/u:fc6040c5
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 335
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:22:57 GMT

{
  ""etag"": """",
  ""checked-url"": ""https://www.data.gouv.fr/fr/"",
  ""final-url"": ""https://www.data.gouv.fr/fr/"",
  ""content-length"": """",
  ""content-disposition"": """",
  ""content-md5"": """",
  ""content-location"": """",
  ""expires"": """",
  ""final-status-code"": ""200"",
  ""updated"": ""2015-06-03T16:21:52.569974"",
  ""last-modified"": """",
  ""content-encoding"": ""gzip"",
  ""content-type"": ""text/html"",
  ""charset"": ""utf-8""
}
```

Or you can use the URL passed as a GET parameter (less error prone):

```shell
$ http GET :8000/url url=https://www.data.gouv.fr/fr/
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 335
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:23:35 GMT

{
  ""etag"": """",
  ""checked-url"": ""https://www.data.gouv.fr/fr/"",
  ""final-url"": ""https://www.data.gouv.fr/fr/"",
  ""content-length"": """",
  ""content-disposition"": """",
  ""content-md5"": """",
  ""content-location"": """",
  ""expires"": """",
  ""final-status-code"": ""200"",
  ""updated"": ""2015-06-03T16:21:52.569974"",
  ""last-modified"": """",
  ""content-encoding"": ""gzip"",
  ""content-type"": ""text/html"",
  ""charset"": ""utf-8""
}
```

Both return the same amount of information.


### Fetching many URLs

You can also use your  HTTP client to issue a POST request againt `localhost:8000/check/many` with the URLs and the name of the group as parameters:

```shell
$ http :8000/check/many urls:='[""https://www.data.gouv.fr/fr/"",""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png""]' group=""datagouvfr""
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 30
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:24:00 GMT

{
  ""group-hash"": ""g:efcf3897""
}
```

This time, the service returns a group hash that will be used to retrieve informations related to that group:

```shell
$ http :8000/group/g:efcf3897
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 941
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:26:04 GMT

{
  ""u:179d104f"": {
    ""content-encoding"": """",
    ""content-disposition"": """",
    ""group"": ""g:efcf3897"",
    ""last-modified"": ""Tue, 31 Mar 2015 14:38:37 GMT"",
    ""content-md5"": """",
    ""checked-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-status-code"": ""200"",
    ""expires"": """",
    ""content-type"": ""image/png"",
    ""content-length"": ""280919"",
    ""updated"": ""2015-06-03T16:24:00.405636"",
    ""etag"": ""\""551ab16d-44957\"""",
    ""content-location"": """"
  },
  ""name"": ""datagouvfr"",
  ""u:fc6040c5"": {
    ""content-disposition"": """",
    ""content-encoding"": ""gzip"",
    ""group"": ""g:efcf3897"",
    ""last-modified"": """",
    ""content-md5"": """",
    ""content-location"": """",
    ""content-length"": """",
    ""expires"": """",
    ""content-type"": ""text/html"",
    ""charset"": ""utf-8"",
    ""final-status-code"": ""200"",
    ""updated"": ""2015-06-03T16:24:02.398105"",
    ""etag"": """",
    ""checked-url"": ""https://www.data.gouv.fr/fr/""
    ""final-url"": ""https://www.data.gouv.fr/fr/""
  }
}
```

Or you can use the group name passed as a GET parameter (less error prone):

```shell
$ http GET :8000/group/ group=datagouvfr
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 335
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:23:35 GMT

{
  ""etag"": """",
  ""checked-url"": ""https://www.data.gouv.fr/fr/"",
  ""final-url"": ""https://www.data.gouv.fr/fr/"",
  ""content-length"": """",
  ""content-disposition"": """",
  ""content-md5"": """",
  ""content-location"": """",
  ""expires"": """",
  ""final-status-code"": ""200"",
  ""updated"": ""2015-06-03T16:21:52.569974"",
  ""last-modified"": """",
  ""content-encoding"": ""gzip"",
  ""content-type"": ""text/html"",
  ""charset"": ""utf-8""
}
```

Both return the same amount of information.


### Redirect handling

Both when fetching one and many urls, croquemort has basic support of HTTP redirections. First, croquemort follows eventual redirections to the final destination (`allow_redirects` option of the `requests` library). Further more, croquemort stores some information about the redirection: the first redirect code and the final url. When encountering a redirection, the JSON response looks like this (note `redirect-url` and `redirect-status-code`):

```json
{
  ""checked-url"": ""https://goo.gl/ovZB"",
  ""final-url"": ""http://news.ycombinator.com"",
  ""final-status-code"": ""200"",
  ""redirect-url"": ""https://goo.gl/ovZB"",
  ""redirect-status-code"": ""301"",
  ""etag"": """",
  ""content-length"": """",
  ""content-disposition"": """",
  ""content-md5"": """",
  ""content-location"": """",
  ""expires"": """",
  ""updated"": ""2015-06-03T16:21:52.569974"",
  ""last-modified"": """",
  ""content-encoding"": ""gzip"",
  ""content-type"": ""text/html"",
  ""charset"": ""utf-8""
}
```


### Filtering results

You can filter results returned for a given group by header (or status) with the `filter_` prefix:

```shell
$ http GET :8000/group/g:efcf3897 filter_content-type=""image/png""
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 539
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:27:07 GMT

{
  ""u:179d104f"": {
    ""content-encoding"": """",
    ""content-disposition"": """",
    ""group"": ""g:efcf3897"",
    ""last-modified"": ""Tue, 31 Mar 2015 14:38:37 GMT"",
    ""content-md5"": """",
    ""checked-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-status-code"": ""200"",
    ""expires"": """",
    ""content-type"": ""image/png"",
    ""content-length"": ""280919"",
    ""updated"": ""2015-06-03T16:24:00.405636"",
    ""etag"": ""\""551ab16d-44957\"""",
    ""content-location"": """"
  },
  ""name"": ""datagouvfr""
}
```

You can exclude results returned for a given group by header (or status) with the `exclude_` prefix:

```shell
$ http GET :8000/group/g:efcf3897 exclude_content-length=""""
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 539
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:27:58 GMT

{
  ""u:179d104f"": {
    ""content-encoding"": """",
    ""content-disposition"": """",
    ""group"": ""g:efcf3897"",
    ""last-modified"": ""Tue, 31 Mar 2015 14:38:37 GMT"",
    ""content-md5"": """",
    ""checked-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-url"": ""https://www.data.gouv.fr/s/images/2015-03-31/d2eb53b14c5f4e6690e150ea7be40a88/cover-datafrance-retina.png"",
    ""final-status-code"": ""200"",
    ""expires"": """",
    ""content-type"": ""image/png"",
    ""content-length"": ""280919"",
    ""updated"": ""2015-06-03T16:24:00.405636"",
    ""etag"": ""\""551ab16d-44957\"""",
    ""content-location"": """"
  },
  ""name"": ""datagouvfr""
}
```

Note that in both cases, the `http` and the `crawler` services return interesting logging information for debugging (if you pass the `--config config.yaml` option to the `run` command).


### Computing many URLs

You can programmatically register new URLs and groups using the RPC proxy. There is an example within the `example_csv.py` file which computes URLs from a CSV file (one URL per line).

```shell
$ PYTHONPATH=. python tests/example_csv.py --csvfile path/to/your/file.csv --group groupname
Group hash: g:2752262332
```

The script returns a group hash that you can use through the HTTP interface as documented above.


### Frequencies

You may want to periodically check existing groups of URLs in the background. In that case launch the `timer` service:

```shell
$ nameko run croquemort.timer
starting services: timer
Connected to amqp://guest:**@127.0.0.1:5672//
```

You can now specify a `frequency` parameter when you `POST` against `/check/many` or when you launch the command via the shell:

```shell
$ PYTHONPATH=. python example_csv.py --csvfile path/to/your/file.csv --group groupname --frequency hourly
Group hash: g:2752262332
```

There are three possibilities: ""hourly"", ""daily"" and ""monthly"". If you don't specify any you'll have to refresh URL checks manually. The `timer` service will check groups with associated frequencies and refresh associated URLs accordingly.


### Webhook

Instead of polling the results endpoints to get the results of one or many URLs checks, you can ask Croquemort to call a webhook when a check is completed.

```shell
$ nameko run croquemort.webhook
starting services: webhook_dispatcher
Connected to amqp://guest:**@127.0.0.1:5672//
```

You can now specify a `callback_url` parameter when you `POST` against `/check/one` or `/check/many`.

```shell
$ http :8000/check/one url=""https://www.data.gouv.fr/fr/"" callback_url=""http://example.org/cb""
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 28
Content-Type: text/plain; charset=utf-8
Date: Wed, 03 Jun 2015 14:21:50 GMT

{
  ""url-hash"": ""u:fc6040c5""
}
```

When the check is completed, a `POST` request should be emitted to `http://example.org/cb` with the metadata of the check. The webhook service expects a successfull (e.g. 200) HTTP status code. If not, it will retry (by default) 5 times, waiting at first 10 seconds before retrying then increasing the delay by a factor of 2 at each try. You can customize those values by setting the variables `WEBHOOK_NB_RETRY`, `WEBHOOK_DELAY_INTERVAL` and `WEBHOOK_BACKOFF_FACTOR`.

```json
{
  ""data"": {
    ""checked-url"": ""http://yahoo.fr"",
    ""final-url"": ""http://yahoo.fr"",
    ""group"": ""g:a80c20d4"",
    ""frequency"": ""hourly"",
    ""final-status-code"": ""200"",
    ""updated"": ""2017-07-10T12:50:20.219819"",
    ""etag"": """",
    ""expires"": ""-1"",
    ""last-modified"": """",
    ""charset"": ""utf-8"",
    ""content-type"": ""text/html"",
    ""content-length"": """",
    ""content-disposition"": """",
    ""content-md5"": """",
    ""content-encoding"": ""gzip"",
    ""content-location"": """"
  }
}
```


### Migrations

You may want to migrate some data over time with the `migrations` service:

```shell
$ nameko run croquemort.migrations
starting services: migrations
Connected to amqp://guest:**@127.0.0.1:5672//
```

You can now run a nameko shell:

```shell
$ nameko shell
>>> n.rpc.migrations.split_content_types()
>>> n.rpc.migrations.delete_urls_for('www.data.gouv.fr')
>>> n.rpc.migrations.delete_urls_for('static.data.gouv.fr')
```

The `split_content_types` migration is useful if you use Croquemort prior to the integration of the report: we use to store the whole string without splitting on the `charset` leading to fragmentation of the Content-types report graph.

The `delete_urls_for` is useful if you want to delete all URLs related to a given `domain` you must pass as a paramater: we accidently checked URLs that are under our control so we decided to clean up in order to reduce the size of the Redis database and increase the relevance of reports.

The `migrate_from_1_to_2` (meta migration for `migrate_urls_redirect` and `add_hash_prefixes`) is used to migrate your database from croquemort `v1` to `v2`. In `v2` there are breaking changes from `v1` on the API JSON schema for a check result:
- `url` becomes `checked-url`
- `status` becomes `final-status-code`

You are encouraged to add your own generic migrations to the service and share those with the community via pull-requests (see below).


## Contributing

We’re really happy to accept contributions from the community, that’s the main reason why we open-sourced it! There are many ways to contribute, even if you’re not a technical person.

We’re using the infamous [simplified Github workflow](http://scottchacon.com/2011/08/31/github-flow.html) to accept modifications (even internally), basically you’ll have to:

* create an issue related to the problem you want to fix (good for traceability and cross-reference)
* fork the repository
* create a branch (optionally with the reference to the issue in the name)
* hack hack hack
* commit incrementally with readable and detailed commit messages
* submit a pull-request against the master branch of this repository

We’ll take care of tagging your issue with the appropriated labels and answer within a week (hopefully less!) to the problem you encounter.

If you’re not familiar with open-source workflows or our set of technologies, do not hesitate to ask for help! We can mentor you or propose good first bugs (as labeled in our issues). Also welcome to add your name to Credits section of this document.


### Submitting bugs

You can report issues directly on Github, that would be a really useful contribution given that we lack some user testing on the project. Please document as much as possible the steps to reproduce your problem (even better with screenshots).


### Adding documentation

We’re doing our best to document each usage of the project but you can improve this file and add you own sections.


### Hacking backend

Hello fellow hacker, it’s good to have you on board! We plan to implement these features in a reasonable future, feel free to pick the one you want to contribute too and declare an issue for it:

* verifying mimetypes, extensions, sizes, caches, etc
* periodical fetching
* reporting for a group of URLs


### Testing

Before submitting any pull-request, you must ensure tests are passing.
You should add tests for any new feature and/or bugfix.
You can run tests with the following command:
```shell
$ python -m pytest tests/
```

You must have rabbitmq and redis running to pass the test.

A ``docker-compose.yml`` file is provided to be quickly ready:
```shell
$ docker-compose up -d
Creating croquemort_redis_1...
Creating croquemort_rabbitmq_1...
$ python -m pytest tests/
```

In the case you use your own middleware with different configuration,
you can pass this configuration as py.test command line arguments:
```shell
python -m pytest tests/ --redis-uri=redis://myredis:6379/0 --amqp-uri=amqp://john:doe@myrabbit
```

Read the py.test help to see all available options:
```shell
python -m pytest tests/ --help
```


## Versioning

Version numbering follows the [Semantic versioning](http://semver.org/) approach.


## License

We’re using the [MIT license](https://tldrlegal.com/license/mit-license).


## Credits

* [David Larlet](https://larlet.fr/david/)
* [Alexandre Bulté](http://alexandre.bulte.net/)
",,2020-06-12T09:24:58Z,6,34,6,"('abulte', 85), ('pyup-bot', 61), ('davidbgk', 40), ('noirbizarre', 38), ('AntoineAugusti', 1), ('geoffreyaldebert', 1)","[17, 'Partnerships for the Goals']"
JustFixNYC/who-owns-what,Who owns what in nyc?,"[![CircleCI](https://circleci.com/gh/JustFixNYC/who-owns-what.svg?style=svg)](https://circleci.com/gh/JustFixNYC/who-owns-what)

# Who owns what in nyc?

The Who owns What project is a new resource for community organizers and tenant leaders to demystify property ownership and shell company networks across New York City.

With this website, you can find crucial information about who is responsible for your building. The site utilizes a database of 160k other properties to connect the dots and discover other properties that your landlord might own or be associated with. Use this tool to discover what buildings in your neighborhood to organize in, what communities your landlord might be targeting, and if your building might be financially overleveraged.

![Imgur](http://i.imgur.com/cYw4gyU.jpg)

**This project is currently in active development!**

## Architecture

This site is built on top of the critical work done by [@aepyornis](https://github.com/aepyornis) on the [nycdb](https://github.com/nycdb/nycdb) project, which is used to cleanly extract, sanitize, and load [HPD Registration data](http://www1.nyc.gov/site/hpd/about/open-data.page) into a PostgreSQL instance.

Backend logic and data manipulation is largely handled by making calls to PostgreSQL functions and prebuilding results into tables whenever possible to avoid complex queries made per-request. for the SQL code that provides this functionality, see:

- the [hpd-registration](https://github.com/nycdb/nycdb/tree/master/src/nycdb/sql/hpd_registrations) scripts of `nycdb`, and
- the [sql](./sql) directory of this repository.

#### Backend

The backend of the app is a simple Django app that connects to Postgres.

#### Frontend

The frontend of the app (`/client`) is built on top of [create-react-app](https://github.com/facebookincubator/create-react-app). See [`/client/README.md`](client/README.md) for all the info you might need.

## Setup

In order to set things up, you'll need to copy `.env.sample` to `.env` and
edit it as needed:

```
cp .env.sample .env
```

In particular, make sure you configure the `DATABASE_URL` environment variable.

Then you'll want to set up and enter a Python 3 virtual environment:

```
python3 -m venv venv
source venv/bin/activate  # Or 'venv\Scripts\activate' on Windows
pip install -r requirements-dev.txt
```

Then you'll need to load data into the database. If you want to use
real data, which takes a long time to load, you can do so with:

```
python dbtool.py builddb
```

Alternatively, you can load a small test dataset with:

```
python dbtool.py loadtestdata
```

After that, make sure you have Node 12 or higher installed as well as [yarn](https://yarnpkg.com/en/), and then run:

```
cd client
yarn
```

This will grab dependencies for the client.

## Running in development

You will need to run two separate terminals; one for the back-end and another for the front-end.

To run the back-end API:

```
python manage.py runserver
```

The server will listen at http://localhost:8000 by default, though you probably
won't need to visit it unless you're manually testing out the API.

To run the front-end:

```
cd client
yarn start
```

You can visit your local dev instance at http://localhost:3000.

## Alternative: Docker-based development

As an alternative to the aforementioned setup, you can use
[Docker](https://www.docker.com/get-started).

First create an `.env` file and edit it as needed:

```
cp .env.sample .env
```

Note that you don't need to change `DATABASE_URL` if you
just want to use the test database.

Now run:

```
docker-compose run app python dbtool.py loadtestdata
```

This will build a nycdb with test data, which is must faster
than downloading the whole nycdb. You can, however, opt to
download the whole thing by running
`docker-compose run app python dbtool.py builddb`, but be
prepared, as it will take a while!

Once you've done that, run:

```
bash docker-update.sh
```

(You will want to re-run that whenever you update your git repository, too.)

Then start up the server:

```
docker-compose up
```

Eventually, you should see a message that says ""You can now view client in the browser.""

Visit http://localhost:3000 and you should be good to go! If
you installed test data, you can see useful results by
clicking on the ""All Year Management"" portfolio on the
home page.

Note: If you would like to connect your Docker instance to an external postgres database, you
can update the `DATABASE_URL` [server-side env variable](https://github.com/JustFixNYC/who-owns-what/blob/master/.env.sample) with your remote db's connection URI.

## Tests

Back-end tests can be run via the Python virtualenv:

```
pytest
```

If you're using Docker, this can be done via `docker-compose run app pytest`.

See [`/client/README.md`](client/README.md) for more details on front-end
tests.

## Black

[Black][] is a formatting tool similar to Prettier, but for Python code.

Before committing or pushing to GitHub, you may want to run the following
to ensure that any files you've changed are properly formatted:

```
black .
```

Note that if you don't either use this or some kind of editor plug-in
before pushing to GitHub, continuous integration will fail.

[Black]: https://black.readthedocs.io/

## Deploying

Package client-side assets through:

```
cd client && yarn build
```

You will need to deploy `client/build` to a static file server.

## Cross-browser testing

We use BrowserStack Live to make sure that our sites work across browsers, operating systems, and devices.

![BrowserStack](https://www.browserstack.com/images/layout/browserstack-logo-600x315.png)

## Updating data

Updating WoW's data is straighforward, unless a new dataset is needed or the schema 
of an existing dataset changes. Previously this was necessary every year with new 
versions of the PLUTO dataset (now there is a version on Open Data with automatic 
updates and a stable schema), but can also happen unpredicitably when an agency 
decides to change the schema of an existing dataset.

To use new data, you'll need to update a few things:

1. Update the [NYCDB][] revision WoW and its test suite use
   at [`requirements-dev.txt`][].
2. Update the list of NYCDB datasets WoW depends on at
   [`who-owns-what.yml`][].
3. Update any SQL to refer to the new dataset's tables.
4. Any new or updated datasets may need new scaffolding
   for WoW's test suite to continue functioning. This
   means you may need to run the
   [`tests/generate_factory_from_csv.py`][] tool to
   create new factories in the `tests/factories`
   folder. You may also need to add new test data to
   the `tests/data` directory in order for tests to
   continue working.

An example of all this in practice can be seen in [#209][],
which upgrades WoW from PLUTO 18v2 to 19v2.

Note also that the
[justfixnyc/nycdb-k8s-loader](https://github.com/justfixnyc/nycdb-k8s-loader)
project may be useful for keeping the WoW database up-to-date on a day-to-day
basis.

[nycdb]: https://github.com/nycdb/nycdb
[`requirements-dev.txt`]: requirements-dev.txt
[`who-owns-what.yml`]: who-owns-what.yml
[`tests/generate_factory_from_csv.py`]: tests/generate_factory_from_csv.py
[#209]: https://github.com/JustFixNYC/who-owns-what/pull/209

## License

JustFix uses the GNU General Public License v3.0 Open-Source License. See `LICENSE.md` file for the full text.

## Code of Conduct

Read about JustFix's code of conduct as an organization on our [Mission page](https://www.justfix.org/our-mission/).
","'civic-tech', 'civictech', 'mapbox-gl', 'mapbox-gl-js', 'open-data', 'opendata', 'postgresql', 'reactjs'",2024-04-30T18:57:30Z,17,176,8,"('sraby', 281), ('romeboards', 209), ('toolness', 143), ('austensen', 72), ('dependabotbot', 47), ('JustFix-org', 16), ('shakao', 10), ('sylvansson', 5), ('kiwansim', 4), ('Sjones21', 3), ('samaratrilling', 2), ('amagnasco', 1), ('jessehall3', 1), ('kfinn', 1), ('LC15', 1), ('steve52', 1), ('abromanos', 1)","[10, 'Reduced Inequalities']"
enketo/enketo-core,The engine that powers Enketo Tools - Use it to develop your own enketo-powered app.,"[![npm version](https://badge.fury.io/js/enketo-core.svg)](http://badge.fury.io/js/enketo-core) ![Build Status](https://github.com/enketo/enketo-core/actions/workflows/npmjs.yml/badge.svg)

# Enketo Core

### ⚠️ Enketo Core code and issues have moved to [the Enketo monorepo](https://github.com/enketo/enketo) ⚠️

***

The engine that powers [Enketo Express](https://github.com/enketo/enketo-express) and various third party tools including [this selection](https://enketo.org/about/adoption/).

Enketo's form engine is compatible with tools in the ODK ecosystem and complies with its [XForms specification](https://getodk.github.io/xforms-spec/) though not all features in that specification have been implemented yet.

This repo is meant to be used as a building block for any Enketo-powered application. See [this page](https://enketo.org/develop/#libraries) for a schematic overview of a real-life full-fledged data collection application and how Enketo Core fits into this.

## Project status

As of 2022, Enketo is maintained by the [ODK team](https://getodk.org/about/team.html) (primarily [Trevor Schmidt](https://github.com/eyelidlessness/)). Martijn, its original author, continues to provide advice and continuity. The ODK project sets priorities in collaboration with its [Technical Advisory Board](https://getodk.org/about/ecosystem.html).

Broader context is available in [the Enketo Express repository](https://github.com/enketo/enketo-express#project-status).

## Browser support

The following browsers are officially supported:

-   latest Android webview on latest Android OS
-   latest WKWebView on latest iOS
-   latest version of Chrome/Chromium on Mac OS, Linux, Windows, Android and iOS
-   latest version of Firefox on Mac OS, Windows, Linux, Android and iOS
-   latest version of Safari on Mac OS, Windows, and on the latest version of iOS
-   latest version of Microsoft Edge

We have to admit we do not test on all of these, but are committed to fixing browser-specific bugs that are reported for these browsers. Naturally, older browsers versions will often work as well - they are just not officially supported.

### Performance (live)

See [graphs](https://github.com/enketo/enketo-core-performance-monitor#live-results)

## Usage as a library

1. Install with `npm install enketo-core --save` or include as a git submodule.
2. Develop a way to perform an [XSL Transformation](https://enketo.org/develop/#transformation) on OpenRosa-flavoured XForms inside your app. The transformation will output an XML instance and a HTML form. See [enketo-transformer](https://github.com/enketo/enketo-transformer) for an available library/app to use or develop your own.
3. Add [themes](./src/sass) to your stylesheet build system (2 stylesheets per theme, 1 is for `media=""print""`).
4. Override [the files under ""browser""](./package.json), e.g. using [aliasify](https://www.npmjs.com/package/aliasify) with your app-specific versions.
5. Main methods illustrated in code below:

```javascript
// assumes the enketo-core package is mapped from the node_modules folder
import { Form } from 'enketo-core';

// The XSL transformation result contains a HTML Form and XML instance.
// These can be obtained dynamically on the client, or at the server/
// In this example we assume the HTML was injected at the server and modelStr
// was injected as a global variable inside a  tag.

// required HTML Form DOM element
const formEl = document.querySelector('form.or');

// required object containing data for the form
const data = {
    // required string of the default instance defined in the XForm
    modelStr: globalXMLInstance,
    // optional string of an existing instance to be edited
    instanceStr: null,
    // optional boolean whether this instance has ever been submitted before
    submitted: false,
    // optional array of external data objects containing:
    // {id: 'someInstanceId', xml: XMLDocument}
    external: [],
    // optional object of session properties
    // 'deviceid', 'username', 'email', 'phonenumber', 'simserial', 'subscriberid'
    session: {},
};

// Form-specific configuration
const options = {};

// Instantiate a form, with 2 parameters
const form = new Form(formEl, data, options);

// Initialize the form and capture any load errors
let loadErrors = form.init();

// If desired, scroll to a specific question with any XPath location expression,
// and aggregate any loadErrors.
loadErrors = loadErrors.concat(form.goTo('//repeat[3]/node'));

// submit button handler for validate button
$('#submit').on('click', function () {
    // clear non-relevant questions and validate
    form.validate().then(function (valid) {
        if (!valid) {
            alert('Form contains errors. Please see fields marked in red.');
        } else {
            // Record is valid!
            const record = form.getDataStr();

            // reset the form view
            form.resetView();

            // reinstantiate a new form with the default model and no options
            form = new Form(formSelector, { modelStr: modelStr }, {});

            // do what you want with the record
        }
    });
});
```

## Global Configuration

Global configuration (per app) is done in [config.json](./config.json) which is meant to be overridden by a config file in your own application (e.g. by using rollup).

### maps

The `maps` configuration can include an array of Mapbox TileJSON objects (or a subset of these with at least a `name`, `tiles` (array) and an `attribution` property, and optionally `maxzoom` and `minzoom`). You can also mix and match Google Maps layers. Below is an example of a mix of two map layers provided by OSM (in TileJSON format) and Google maps.

```
[
  {
    ""name"": ""street"",
    ""tiles"": [ ""http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png"" ],
    ""attribution"": ""Map data © OpenStreetMap contributors""
  },
  {
    ""name"": ""satellite"",
    ""tiles"": ""GOOGLE_SATELLITE""
  }
]
```

For GMaps layers you have the four options as tiles values: `""GOOGLE_SATELLITE""`, `""GOOGLE_ROADMAP""`, `""GOOGLE_HYBRID""`, `""GOOGLE_TERRAIN""`. You can also add other TileJSON properties, such as minZoom, maxZoom, id to all layers.

### googleApiKey

The Google API key that is used for geocoding (in the geo widgets' search box). Can be obtained [here](https://console.developers.google.com/project). Make sure to enable the _GeoCoding API_ service. If you are using Google Maps layers, the same API key is used. Make sure to enable the _Google Maps JavaScript API v3_ service as well in that case (see next item).

### validateContinuously

This setting with the default `false` value determines whether Enketo should validate questions immediately if a related value changes. E.g. if question A has a constraint that depends on question B, this mode would re-validate question A if the value for question B changes. **This mode will slow down form traversal.** When set to `false` that type of validation is only done at the end when the Submit button is clicked or in Pages mode when the user clicks Next.

### validatePage

This setting with default `true` value determines whether the Next button should trigger validation of the current page and block the user from moving to the next page if validation fails.

### swipePage

This setting with default `true` value determines whether to enable support for _swiping_ to the next and previous page for forms that are divided into pages.

## Form Configuration

Per-form configuration is done by adding an (optional) options object as 3rd parameter when instantiating a form.

### Print only the ""relevant"" parts of the form

If `printRelevantOnly` is set to `true` or not set at all, printing the form only includes what is visible, ie. all the groups and questions that do not have a `relevant` expression or for which the expression evaluates to `true`.

```
new Form(formselector, data, {
  printRelevantOnly: false
});
```

### Explicitly set the default form language

The `language` option overrides the default languages rules of the XForm itself. Pass any valid and present-in-the-form IANA subtag string, e.g. `ar`.

## How to develop Enketo Core

1. install prerequisites:

-   Volta (optional, but recommended)
-   Node.js 16 and npm 6 (Node.js 14 is also supported)
-   [grunt-cli](https://gruntjs.com/getting-started)

2. install dependencies with `npm install`
3. build with `grunt` (`npx grunt`)
4. start built-in auto-reloading development server with `npm start`
5. browse to [http://localhost:8005](http://localhost:8005/) and load an XForm url with the `xform` queryparameter or load a local from the /tests/forms folder in this repo
6. run tests with `npm test` (headless chrome) and `npm run test-browsers` (browsers); **note:** running tests updates the coverage badge in README.md, but these changes should not be committed except when preparing a release
7. adding the querystring `touch=true` and reducing the window size allows you to simulate mobile touchscreens

### Notes for JavaScript Developers

-   When creating new functions/Classes, make sure to describe them with JSDoc comments.
-   JavaScript style see [ESLint](./eslintrc.json) config files. The check is added to the grunt `test` task. You can also manually run `grunt eslint:fix` to fix style issues.
-   Testing is done with Mocha and Karma (all: `grunt karma`, headless: `grunt karma:headless`, browsers: `grunt karma:browsers`)
-   Tests can be run in watch mode for [TDD](https://en.wikipedia.org/wiki/Test-driven_development) workflows with `npm run test-watch`, and support for debugging in [VSCode](https://code.visualstudio.com/) is provided. For instructions see [Debugging test watch mode in VSCode](./#debugging-test-watch-mode-in-vscode) below
-   When making a pull request, please add tests where relevant

#### Debugging test watch mode in VSCode

Basic usage:

1. Go to VSCode's ""Run and Debug"" panel
2. Select ""Test (watch + debug)""
3. Click the play button

Optionally, you can add a keyboard shortcut to select launch tasks:

1. Open the keyboard shortcuts settings (cmd+k cmd+s on Mac, ctrl+k ctrl+s on other OSes)
2. Search for `workbench.action.debug.selectandstart`
3. Click the + button to add your preferred keybinding keybinding

### Notes for CSS Developers

The core can be fairly easily extended with alternative themes.
See the _plain_, the _grid_, and the _formhub_ themes already included in [/src/sass](./src/sass).
We would be happy to discuss whether your contribution should be a part of the core, the default theme or be turned into a new theme.

For custom themes that go beyond just changing colors and fonts, keep in mind all the different contexts for a theme:

1. non-touchscreen vs touchscreen (add ?touch=true during development)
2. default one-page-mode and multiple-pages-mode
3. right-to-left form language vs left-to-right form language (!) - also the UI-language may have a different directionality
4. screen view vs. print view
5. questions inside a (nested) repeat group have a different background
6. large screen size --> smaller screen size ---> smallest screen size
7. question in valid vs. invalid state

### Widgets in Enketo Core

Widgets extend the [Widget class](https://github.com/enketo/enketo-core/blob/master/src/js/widget.js). This is an example:

(see full functioning example at [/src/widget/example/my-widget.js](https://github.com/enketo/enketo-core/blob/master/src/widget/example/my-widget.js)

```js
import Widget from '../../js/widget';

/*
 * Make sure to give the widget a unique widget class name and extend Widget.
 */
class MyWidget extends Widget {
    /*
     * The selector that determines on which form control the widget is instantiated.
     * Make sure that any other widgets that target the same from control are not interfering with this widget by disabling
     * the other widget or making them complementary.
     * This function is always required.
     */
    static get selector() {
        return '.or-appearance-my-widget input[type=""number""]';
    }

    /*
     * Initialize the widget that has been instantiated using the Widget (super) constructor.
     * The _init function is called by that super constructor unless that constructor is overridden.
     * This function is always required.
     */
    _init() {
        // Hide the original input
        this.element.classList.add('hide');

        // Create the widget's DOM fragment.
        const fragment = document.createRange().createContextualFragment(
            `
                
            `
        );
        fragment.querySelector('.widget').appendChild(this.resetButtonHtml);

        // Only when the new DOM has been fully created as a HTML fragment, we append it.
        this.element.after(fragment);

        const widget = this.element.parentElement.querySelector('.widget');
        this.range = widget.querySelector('input');

        // Set the current loaded value into the widget
        this.value = this.originalInputValue;

        // Set event handlers for the widget
        this.range.addEventListener('change', this._change.bind(this));
        widget
            .querySelector('.btn-reset')
            .addEventListener('click', this._reset.bind(this));

        // This widget initializes synchronously so we don't return anything.
        // If the widget initializes asynchronously return a promise that resolves to `this`.
    }

    _reset() {
        this.value = '';
        this.originalInputValue = '';
        this.element.classList.add('empty');
    }

    _change(ev) {
        // propagate value changes to original input and make sure a change event is fired
        this.originalInputValue = ev.target.value;
        this.element.classList.remove('empty');
    }

    /*
     * Disallow user input into widget by making it readonly.
     */
    disable() {
        this.range.disabled = true;
    }

    /*
     * Performs opposite action of disable() function.
     */
    enable() {
        this.range.disabled = false;
    }

    /*
     * Update the language, list of options and value of the widget.
     */
    update() {
        this.value = this.originalInputValue;
    }

    /*
     * Obtain the current value from the widget. Usually required.
     */
    get value() {
        return this.element.classList.contains('empty') ? '' : this.range.value;
    }

    /*
     * Set a value in the widget. Usually required.
     */
    set value(value) {
        this.range.value = value;
    }
}

export default MyWidget;
```

Some of the tests are common to all widgets, and can be run with a few lines:

(see full functioning example at [/test/spec/widget.example.spec.js](https://github.com/enketo/enketo-core/blob/master/test/spec/widget.example.spec.js))

```js
import ExampleWidget from '../../src/widget/example/my-widget';
import { runAllCommonWidgetTests } from '../helpers/testWidget';

const FORM = `
        
    `;
const VALUE = '2';

runAllCommonWidgetTests(ExampleWidget, FORM, VALUE);
```

### DO

-   use the rank widget as a more complex example that uses the best practices (some other widgets use an older style)
-   add an `_init` function to your widget that either returns nothing or a Promise (if it initializes asynchronously)
-   include a widget.my-widget.spec.js file in the /test folder
-   run at least the standardized common widget tests by doing: TBD
-   make the widget responsive up to a minimum window width of 320px
-   ensure the widget's scss and js file is/are loaded in widgets.js and \_widgets.scss respectively
-   if hiding the original input element, it needs to load the default value `this.originalInputValue` into the widget
-   if hiding the original input element, keep its value syncronized using `this.originalInputValue = ...`
-   if hiding the original input element, it needs to listen for the `applyfocus` event on the original input and focus the widget
-   if hiding the original input element, the widget value needs to update when the original input updates due to a calculation or becoming non-relevant (update)
-   apply the `widget` css class to the top level elements it adds to the DOM (but not to their children)
-   new input/select/textarea elements inside widgets should have the `ignore` class to isolate them from the Enketo form engine
-   include `enable()`, `disable()` and `update()` method overrides. See the Widget class.
-   if the widget needs tweaks or needs to be disabled for mobile use, use support.js to detect this and override the static `condition()` function in Widget.js.
-   allow clearing of the original input (i.e. setting value to '')
-   if the widget does not get automatic (built-in HTML) focus, trigger a `fakefocus` event to the original input when the widget gets focus (rarely required, but see rank widget)

#### DON'T

-   do not include jQuery, React, Vue or any other general purpose libraries or frameworks

### Events in Enketo Core

##### inputupdate

Fired on a form control when it is programmatically updated and when this results in a change in value

#### xforms-value-changed

Fired on a form control when it is updated directly by the user and when this results in a change in value

##### invalidated

Fired on a form control when it has failed constraint, datatype, or required validation.

##### dataupdate

Fired on model.$events, when a single model value has changed its value, a repeat is added, or a node is removed. It passes an ""update object"". This event is propagated for external use by firing it on the form.or element as well.

##### odk-instance-first-load

Fired on model.events when a new record (instance) is loaded for the first time. It's described here: [odk-instance-first-load](https://getodk.github.io/xforms-spec/#event:odk-instance-first-load).

##### odk-new-repeat

Fired on a newly added repeat. It's described here: [odk-instance-first-load](https://getodk.github.io/xforms-spec/#event:odk-new-repeat).

##### removerepeat

Fired on the repeat or repeat element immediately following a removed repeat.

##### removed

Fired on model.events, when a node is removed. It passes an ""update object"". This event is propagated for external use by firing it on the form.or element as well.

##### goto-irrelevant

Fired on form control when an attempt is made to 'go to' this field but it is hidden from view because it is non-relevant.

##### goto-invisible

Fired on form control when an attempt is made to 'go to' this field but it is hidden from view because it is has no form control.

##### pageflip

Fired when user flips to a new page, on the page element itself.

##### edited

Fired on form.or element when user makes first edit in form. Fires only once.

##### validation-complete

Fired on form.or element when validation completes.

##### progress-update

Fired when the user moves to a different question in the form.

## Release

1. Create release PR
1. Update `CHANGELOG.md`
1. Update version in `package.json`
    - Bump to major version if consumers have to make changes.
1. Check [Dependabot](https://github.com/enketo/enketo-core/security/dependabot) for alerts
1. Run `npm update`
    - Check if `node-forge` has been updated and if so, verify encrypted submissions end-to-end
    - If `enketo-transformer` has been updated, change `Form.requiredTransformerVersion`
1. Run `npm audit`
    - Run `npm audit fix --production` to apply most important fixes
1. Run `npm i`
1. Run `npm test`
1. Merge PR with all changes
1. Create GitHub release
1. Tag and publish the release
    - GitHub Action will publish it to npm

## Sponsors

The development of this library is now led by [ODK](https://getodk.org) and funded by customers of the ODK Cloud hosted service.

Past sponsors include:

-   [OpenClinica](https://www.openclinica.com/)
-   [Sustainable Engineering Lab at Columbia University](http://modi.mech.columbia.edu/)
-   [WHO - HRP project](http://www.who.int/reproductivehealth/topics/mhealth/en/index.html)
-   [Santa Fe Insitute & Slum/Shack Dwellers International](http://www.santafe.edu/)
-   [Enketo LLC](http://www.linkedin.com/company/enketo-llc)
-   [iMMAP](http://immap.org)
-   [KoBo Toolbox (Harvard Humanitarian Initiative)](https://kobotoolbox.org)
-   [Ona](https://ona.io)
-   [Medic](https://medic.org/)
-   [Esri](https://esri.com)
-   [DIAL Open Source Center](https://www.osc.dial.community/)

## License

See [license](https://github.com/enketo/enketo-core/blob/master/LICENSE) document and additional clause below:

Any product that uses enketo-core is required to have a ""Powered by Enketo"" footer, according to the specifications below, on all screens in which enketo-core or parts thereof, are used, unless explicity exempted from this requirement by Enketo LLC in writing. Partners and sponsors of the Enketo Project, listed on [https://enketo.org/about/sponsors/](https://enketo.org/about/sponsors/) and on [this page](#sponsors) are exempted from this requirements and so are contributors listed in [package.json](https://github.com/enketo/enketo-core/blob/master/package.json).

The aim of this requirement is to force adopters to give something back to the Enketo project, by at least spreading the word and thereby encouraging further adoption.

Specifications:

1. The word ""Enketo"" is displayed using Enketo's logo.
2. The minimum font-size of ""Powered by"" is 12 points.
3. The minimum height of the Enketo logo matches the font-size used.
4. The Enketo logo is hyperlinked to https://enketo.org

Example:

Powered by 
","'enketo', 'form', 'odk', 'survey', 'webform', 'xform'",2023-11-29T00:47:40Z,28,79,19,"('MartijnR', 1806), ('eyelidlessness', 153), ('alxndrsn', 49), ('lognaturel', 45), ('yanokwa', 19), ('theywa', 15), ('nanzhang01', 9), ('gushil', 8), ('jkuester', 8), ('dependabotbot', 6), ('magicznyleszek', 6), ('FrankApiyo', 4), ('ukanga', 2), ('vimemo', 2), ('delcroip', 2), ('joshuaberetta', 2), ('jdugh', 2), ('mrjones-plip', 1), ('cloudchen', 1), ('JGreenlee', 1), ('latin-panda', 1), ('abbyad', 1), ('peterp', 1), ('sgigh', 1), ('yogeshsr', 1), ('garethbowen', 1), ('ivermac', 1), ('tnagorra', 1)","[16, 'Peace, Justice and Strong Institutions']"
instedd/pollit,"Pollit helps you use the ease of text messaging to conduct surveys reaching your audience at their convenience. It will guide each participant step by step through your survey, collecting the results in real time. Using text messaging allows you to scale the number of people you reach and lets them answer anytime, anywhere.","Pollit
======

[![Build Status](https://travis-ci.org/instedd/pollit.svg)](https://travis-ci.org/instedd/pollit)

[Pollit](pollit.instedd.org) helps you use the ease of text messaging to conduct surveys reaching your audience at their convenience. It will guide each participant step by step through your survey, collecting the results in real time. Using text messaging allows you to scale the number of people you reach and lets them answer anytime, anywhere.

Installation
============

Simply clone the repository and fill the following settings file before starting the server with rails server command:

    config/settings.yml
    config/nuntium.yml
    config/database.yml
    config/guisso.yml
    config/hub.yml

Configuring Nuntium
-------------------

Nuntium settings are located in `config/nuntium.yml`.

The Nuntium Application associated to a Pollit instance must be configured in this way:
  * Application name: the value of `application` in `config/nuntium.yml`
  * Interface configuration:
    * HTTP Post callback: `/nuntium/receive_at`
    * User: the value of `at_post_user` in `config/nuntium.yml`
    * Password: the value of `at_post_password` in `config/nuntium.yml`
  * Delivery acknowledgement:
    * HTTP Post: `/nuntium/delivery_callback`
    * User: the value of `at_post_user` in `config/nuntium.yml`
    * Password: the value of `at_post_password` in `config/nuntium.yml`

Development
===========

Docker development
------------------

`docker-compose.yml` file build a development environment mounting the current folder and running rails in development environment.

Run the following commands to have a stable development environment.

```
$ docker-compose run --rm --no-deps web bundle install
$ docker-compose up -d db
$ docker-compose run --rm web rake db:setup
$ docker-compose up
```

To setup and run test, once the web container is running:

```
$ docker-compose exec web bash
root@web_1 $ rake
```

API
===

Pollit provides a simple RESTful read-only API for querying Answers, Polls, Questions and Respondents.

Authentication
--------------

Authentication is handled via [GUISSO](https://github.com/instedd/guisso), allowing access via [both OAuth and basic auth](https://github.com/instedd/alto_guisso_rails#allow-oauth-and-basic-authentication-with-guisso-credentials). If the user is currently logged in to the application, all requests to the API from the browser will also work, facilitating the exploration of the API.

Formats
-------

All endpoints support both JSON and XML format. The extension used in the URL will determine which format is returned by the API.

Endpoints
---------

* List all user Polls
```
http://pollit.instedd.org/api/polls.json
```

* Get a poll given its numeric ID
```
http://pollit.instedd.org/api/polls/ID.json
```

* List all respondents from a Poll
```
http://pollit.instedd.org/api/polls/POLL_ID/respondents.json
```

* Get a respondent given its numeric ID
```
http://pollit.instedd.org/api/polls/POLL_ID/respondents/ID.json
```

* List all questions from a Poll
```
http://pollit.instedd.org/api/polls/POLL_ID/questions.json
```

* Get a question given its numeric ID
```
http://pollit.instedd.org/api/polls/POLL_ID/questions/ID.json
```

* List all answers from a Poll
```
http://pollit.instedd.org/api/polls/POLL_ID/answers.json
```

* Get an answer given its numeric ID
```
http://pollit.instedd.org/api/polls/POLL_ID/answers/ID.json
```

Entities
--------

**Poll**

```json
{
  ""confirmation_word"": ""Yes"",
  ""created_at"": ""2011-11-04T19:11:53Z"",
  ""current_occurrence"": null,
  ""description"": ""A test poll"",
  ""form_url"": ""URL_TO_FORM"",
  ""goodbye_message"": ""Thank you for your answers!"",
  ""id"": 1,
  ""owner_id"": 1,
  ""post_url"": ""URL_FOR_ANSWERS"",
  ""recurrence"": {
    ""start_time"": ""2015-01-01T12:00:00+00:00"",
    ""rrules"": [],
    ""rtimes"": [],
    ""extimes"": []
  },
  ""status"": ""started"",
  ""title"": ""Test poll"",
  ""updated_at"": ""2015-01-01T12:00:00+00:00"",
  ""welcome_message"": ""Answer 'yes' if you want to participate in this poll.""
}
```

**Respondent**

```json
{
  ""confirmed"": true,
  ""created_at"": ""2015-01-01T20:00:00Z"",
  ""current_question_id"": null,
  ""current_question_sent"": true,
  ""id"": 1,
  ""phone"": ""PHONE_NUMBER"",
  ""poll_id"": 1,
  ""pushed_at"": ""2015-01-01T20:00:00Z"",
  ""pushed_status"": ""succeeded"",
  ""updated_at"": ""2015-01-01T20:00:00Z""
}
```

**Question**

```json
{
  ""collects_respondent"": false,
  ""created_at"": ""2015-01-01T20:00:00Z"",
  ""description"": ""Enter your name"",
  ""field_name"": ""FIELD_NAME"",
  ""id"": 1,
  ""kind"": ""text"",
  ""numeric_max"": null,
  ""numeric_min"": null,
  ""options"": [],
  ""poll_id"": 1,
  ""position"": 1,
  ""title"": ""What is your name?"",
  ""updated_at"": ""2015-01-01T20:00:00Z""
}
```

**Answer**

```json
{
  ""id"": 1,
  ""question_id"": 1,
  ""question_title"": ""What is your name?"",
  ""respondent_phone"": ""PHONE_NUMBER"",
  ""occurrence"": null,
  ""timestamp"": ""2015-01-01T20:00:00Z"",
  ""response"": ""John Doe""
}
```

Intercom
--------

Pollit supports Intercom as its CRM platform. To load the Intercom chat widget, simply start Pollit with the env variable `INTERCOM_APP_ID` set to your Intercom app id (https://www.intercom.com/help/faqs-and-troubleshooting/getting-set-up/where-can-i-find-my-workspace-id-app-id).

Pollit will forward any conversation with a logged user identifying them through their email address. Anonymous, unlogged users will also be able to communicate.

If you don't want to use Intercom, you can simply omit `INTERCOM_APP_ID` or set it to ''.

To test the feature in development, add the `INTERCOM_APP_ID` variable and its value to the `environment` object inside the `web` service in `docker-compose.yml`.",,2023-05-24T19:36:41Z,10,1,14,"('spalladino', 360), ('bcardiff', 21), ('waj', 19), ('asterite', 17), ('pablobrusco', 6), ('ggiraldez', 4), ('pmallol', 2), ('macoca', 1), ('matiasgarciaisaia', 1), ('mmuller', 1)","[17, 'Partnerships for the Goals']"
dimagi/commcare-cloud,Tools for standing up and managing a CommCare HQ server environment,"# CommCare-Cloud

[![commcare-cloud tests](https://github.com/dimagi/commcare-cloud/actions/workflows/tests.yml/badge.svg)](https://github.com/dimagi/commcare-cloud/actions/workflows/tests.yml)

CommCare-Cloud is a suite of tools for standing up and managing a production
[CommCare HQ](https://github.com/dimagi/commcare-hq/) server environment.

For documentation and installation instructions, visit
[https://commcare-cloud.readthedocs.io/](https://commcare-cloud.readthedocs.io/)
or the `docs/source` directory of this repository.


# More Information

* See something you'd like to improve?  We welcome [contributions](CONTRIBUTING.md).
* To set up a local development environment for CommCare HQ, view the
  [instructions](https://github.com/dimagi/commcare-hq/blob/master/DEV_SETUP.md)
  in that repository.
",'ansible',2024-05-03T12:32:12Z,30,29,34,"('dannyroberts', 4593), ('snopoke', 3823), ('emord', 1431), ('shyamkumarlchauhan', 938), ('millerdev', 874), ('benrudolph', 805), ('sanjay2916', 746), ('javierwilson', 655), ('sravfeyn', 649), ('pvisweswar', 587), ('calellowitz', 525), ('NitigyaS', 467), ('gherceg', 440), ('pr33thi', 416), ('skodde', 361), ('czue', 311), ('esoergel', 278), ('AmitPhulera', 250), ('TylerSheffels', 239), ('biyeun', 197), ('kaapstorm', 195), ('mkangia', 185), ('proteusvacuum', 177), ('rohit-dimagi', 172), ('mohantybibek', 127), ('orangejenny', 110), ('cewing', 81), ('Charl1996', 76), ('nickpell', 75), ('rameshganne', 71)","[3, 'Good Health and Well-Being']"
energy-data/energydata.info,energydata.info - open data and analytics for a sustainable energy future,"# Contributing

## Requirements

- [VirtualBox 5](http://virtualbox.org)
- [Vagrant](https://www.vagrantup.com/)
- This repo

## Development

### Initial provisioning
1. Clone this repo
2. `cd energydata.info`
2. `git submodule update --init --recursive` 
2. Copy `development.ini.sample` to `development.ini`
3. Change variables in development.ini:
   - beaker.session.secret
   - app.instance.uuid
   - ckanext.s3.*
   - Configure smtp for email
4. Start the VM with `vagrant up`
5. `vagrant ssh` to ssh into the VM.
5. Fix the permissions: `sudo usermod -aG docker $(whoami) && newgrp docker`
   (this should be done by build_box.sh, but currently fails)
6. `source venv/bin/activate` which will allow you to use the datacats command
7. `cd /vagrant`
8. `datacats init` to initialize the environment and choose an admin password

### Booting
1. In the repo directory `vagrant up`. 
2. Point your web browser to 192.168.101.99 (the address of the VM)
3. Modifications to the CSS & templates will be reloaded automatically using vagrant sync. Adding templates or changing configuration files will not cause a server restart, check the next section to restart the server

### Restarting the server

If you're adding new templates or new functionality to CKAN, you might need to restart the CKAN server.

1. `vagrant ssh` to ssh into the VM
2. `source venv/bin/activate` which will allow you to use the datacats command
3. `cd /vagrant` (this is the synced folder with the git repo)
4. `./reload-server.sh` to reload the server

### Styles (Less)

Less is compiled in the local machine, not vagrant.
The resulting styles should be committed to the repo.
 
#### Requirements

- Node (v4.2.x) & Npm ([nvm](https://github.com/creationix/nvm) usage is advised)

> The versions mentioned are the ones used during development. It could work with newer ones.

After these basic requirements are met, run the following commands in the website's folder:
```
npm install
```

#### Watch for changes

```
npm run less-watch
```
Starts the watcher and recompiles when files change.


## Deploy instructions
This assumes a base OS of Ubuntu 16.04

1. Clone this repo on the target machine
2. Follow the instructions in `build-box/build_box.sh`, changing `vagrant` to this repository's source directory
3. Copy `development.ini.sample` to `development.ini`
4. Change variables in development.ini:
   - beaker.session.secret
   - app.instance.uuid
   - ckanext.s3.*
   - Configure smtp for email
5. `cd wbg-energydata`
6. `datacats init` and choose an admin password
7. `datacats reload`
8. `datacats paster -d celeryd`
9. For HTTPS, install Let's Encrypt and uncomment the HTTPS section of the nginx configuration

### New iteration
To deploy a new iteration of the data platform to the production environment, follow these steps:

1. ssh into the machine
2. `cd wbg-energydata`
3. `git checkout master`
4. `git pull origin master`
5. [only if you enable a new plugin] `datacats install`
6. `./reload-server.sh` to restart the server

### Setting up the harvester
1. `cd wbg-energydata`
2. `datacats tweak --add-redis`
3. `datacats reload`
4. `cd ckanext-harvest`
5. `datacats paster harvester initdb`
6. `datacats paster -d harvester gather_consumer`
7. `datacats paster -d harvester fetch_consumer`

## Adding a new DCAT-JSON ArcGIS harvest source
1. `cd wbg-energydata/ckanext-harvest`
2. `datacats paster harvester source energy-gis-arcgis http://data.energy-gis.opendata.arcgis.com/data.json dcat-json ""Energy Sector GIS Working Group"" true energy-sector-gis-working-group`
3. `datacats paster harvester job-all` to create and run a harvest job

",,2017-12-07T19:51:39Z,8,24,7,"('olafveerman', 126), ('kamicut', 112), ('ricardoduplos', 55), ('Naigege', 19), ('danielfdsilva', 4), ('anandthakker', 2), ('carderne', 2), ('drewbo', 1)","[13, 'Climate Action']"
betagouv/leximpact-client,leximpact-client a été migré vers une autre organisation,"# Introduction

## [FR] Introduction

Ceci est le code source de l'interface utilisateur de LexImpact.

LexImpact permet aux administrations, aux parlementaires et à la société civile de simuler l'impact _ex ante_ des réformes au système socio-fiscal.
* [Appels à candidatures](https://entrepreneur-interet-general.etalab.gouv.fr/defis/2019/leximpact.html)
* [Fiche produit](https://beta.gouv.fr/startups/leximpact.html)
* [LexImpact Beta](https://leximpact.beta.gouv.fr)

LexImpact est constitué de deux parties :

- [leximpact-server](https://github.com/betagouv/leximpact-server/) : application en python utilisant [OpenFisca-France](https://github.com/openfisca/openfisca-france) permettant de mettre en place une API Web répondant à des questions sur l'impact de modifications de la loi fiscale,
- Ici même, [leximpact-client](https://github.com/betagouv/leximpact-client/) : interface web communiquant avec l'API qui met à disposition des usagers un site web permettant de définir des modifications de la loi et d'en visulaliser les résultats calculés par l'API.


## [EN] Introduction

This is the source code of LexImpact user interface.

LexImpact allows civil servants, policy makers and citizens to simulate the _ex ante_ impact of a reform to a country's tax-benefit system.
* [Call for candidates (FR)](https://entrepreneur-interet-general.etalab.gouv.fr/defis/2019/leximpact.html)
* [Elevator pitch (FR)](https://beta.gouv.fr/startups/leximpact.html)
* [LexImpact Beta](https://leximpact.beta.gouv.fr)

LexImpact has two components:

- [leximpact-server](https://github.com/betagouv/leximpact-server/): a Python application using [OpenFisca-France](https://github.com/openfisca/openfisca-france) and providing a Web API responding to requests on the impact of a change of the tax law,
- Here, [leximpact-client](https://github.com/betagouv/leximpact-client/): a web interface interacting with leximpact-server API and providing to the users a web site to set law tax changes and see the results calculated by the API.

# How to use

```shell
npm install
npm run dev
```

## Configuration file `.env`

A file name `.env` is necessary for the client to work properly. The file `.env.example` can be copied into it.

Here ate the environment variables that you have to set:
- `API_URL`: leximpact-client is just a web interface that does not do computations by itself, but needs to be provided a working [leximpact-server](https://github.com/betagouv/leximpact-server/) Web API to fetch results. As of v`1.0.0`, a working API example can be found on: https://api.leximpact.beta.gouv.fr
- `PORT`: describes the port that the client will be setup to (e.g. the website will be accessible from http://127.0.0.1: if the client is run locally). If ommited, defaults to `9001`


# Snapshot testing with Jest

Snapshot tests are a very useful tool whenever you want to make sure your UI does not change unexpectedly.

> A typical snapshot test case for a mobile app renders a UI component, takes a snapshot, then compares it to a reference snapshot file stored alongside the test. The test will fail if the two snapshots do not match: either the change is unexpected, or the reference snapshot needs to be updated to the new version of the UI component.


For more information, please see the Jest [official documentation](https://jestjs.io/docs/en/snapshot-testing).

Here is the command to run these tests:

```shell
npm run test
```

To update the reference snapshots, run:

```shell
npm run test --updateSnapshots
```

This is similar to: `./node_modules/.bin/jest --updateSnapshots`

# Screenshot



# Icons & Emoji sources

- [Twitter Emoji](https://iconify.design/icon-sets/twemoji/)
- [MaterialUI Icons](https://material.io/resources/icons)

# Documentation

- [MaterialUI v3.9.9 Documentation](https://v3.material-ui.com/getting-started/installation/)
",,2022-12-09T20:48:48Z,5,3,5,"('DorineLam', 187), ('magemax', 147), ('sandcha', 81), ('sixertoy', 59), ('LoicPoullain', 4)","[10, 'Reduced Inequalities']"
opendatacube/datacube-core,Open Data Cube analyses continental scale Earth Observation data through time,"Open Data Cube Core
===================

.. image:: https://github.com/opendatacube/datacube-core/workflows/build/badge.svg
    :alt: Build Status
    :target: https://github.com/opendatacube/datacube-core/actions

.. image:: https://codecov.io/gh/opendatacube/datacube-core/branch/develop/graph/badge.svg
    :alt: Coverage Status
    :target: https://codecov.io/gh/opendatacube/datacube-core

.. image:: https://readthedocs.org/projects/datacube-core/badge/?version=latest
    :alt: Documentation Status
    :target: http://datacube-core.readthedocs.org/en/latest/

Overview
========

The Open Data Cube Core provides an integrated gridded data
analysis environment for decades of analysis ready earth observation
satellite and related data from multiple satellite and other acquisition
systems.

Documentation
=============

See the `user guide `__ for
installation and usage of the datacube, and for documentation of the API.

`Join our Slack `__ if you need help
setting up or using the Open Data Cube.

Please help us to keep the Open Data Cube community open and inclusive by
reading and following our `Code of Conduct `__.

Requirements
============

System
~~~~~~

-  PostgreSQL 10+
-  Python 3.9+

Developer setup
===============

1. Clone:

   -  ``git clone https://github.com/opendatacube/datacube-core.git``

2. Create a Python environment for using the ODC.  We recommend `Mambaforge `__ as the
   easiest way to handle Python dependencies.

::

   mamba env create -f conda-environment.yml
   conda activate cubeenv

3. Install a develop version of datacube-core.

::

   cd datacube-core
   pip install --upgrade -e .

4. Install the `pre-commit `__ hooks to help follow ODC coding
   conventions when committing with git.

::

   pre-commit install

5. Run unit tests + PyLint

Install test dependencies using:

   ``pip install --upgrade -e '.[test]'``

If install for these fails, please lodge them as issues.

Run unit tests with:

   ``./check-code.sh``

   (this script approximates what is run by GitHub Actions. You can
   alternatively run ``pytest`` yourself).

6. **(or)** Run all tests, including integration tests.

   ``./check-code.sh integration_tests``

   -  Assumes a password-less Postgres database running on localhost called

   ``pgintegration``

   -  Otherwise copy ``integration_tests/integration.conf`` to
      ``~/.datacube_integration.conf`` and edit to customise.

   - For instructions on setting up a password-less Postgres database, see
      the `developer setup instructions `__.


Alternatively one can use the ``opendatacube/datacube-tests`` docker image to run
tests. This docker includes database server pre-configured for running
integration tests. Add ``--with-docker`` command line option as a first argument
to ``./check-code.sh`` script.

::

   ./check-code.sh --with-docker integration_tests


To run individual tests in a docker container

::

    docker build --tag=opendatacube/datacube-tests-local --no-cache --progress plain -f docker/Dockerfile .

    docker run -ti -v $(pwd):/code opendatacube/datacube-tests-local:latest pytest integration_tests/test_filename.py::test_function_name


Developer setup on Ubuntu
~~~~~~~~~~~~~~~~~~~~~~~~~

Building a Python virtual environment on Ubuntu suitable for development work.

Install dependencies:

::

    sudo apt-get update
    sudo apt-get install -y \
        autoconf automake build-essential make cmake \
        graphviz \
        python3-venv \
        python3-dev \
        libpq-dev \
        libyaml-dev \
        libnetcdf-dev \
        libudunits2-dev


Build the python virtual environment:

::

    pyenv=""${HOME}/.envs/odc""  # Change to suit your needs
    mkdir -p ""${pyenv}""
    python3 -m venv ""${pyenv}""
    source ""${pyenv}/bin/activate""
    pip install -U pip wheel cython numpy
    pip install -e '.[dev]'
    pip install flake8 mypy pylint autoflake black
","'gdal', 'gis', 'hacktoberfest', 'netcdf', 'numpy', 'python', 'raster', 'remote-sensing', 'scientific-computing'",2024-05-01T02:06:01Z,30,488,54,"('v0lat1le', 1142), ('omad', 1125), ('Kirill888', 1090), ('jeremyh', 657), ('SpacemanPaul', 440), ('andrewdhicks', 395), ('alex-ip', 222), ('uchchwhash', 154), ('pindge', 151), ('petewa', 137), ('alexgleith', 73), ('woodcockr', 68), ('simonaoliver', 56), ('rtaib', 53), ('harshurampur', 42), ('Ariana-B', 35), ('RichardScottOZ', 34), ('rowanwins', 27), ('robbibt', 27), ('awalshie', 25), ('whatnick', 23), ('Spaxe', 18), ('benjimin', 18), ('pre-commit-cibot', 17), ('zhang01GA', 15), ('GypsyBojangles', 12), ('mpaget', 11), ('snowman2', 10), ('bellemae', 8), ('dependabotbot', 8)","[17, 'Partnerships for the Goals']"
Bahmni/bahmni-core,Core OpenMRS modules for Bahmni (including ERP & ELIS Atom Feed Clients),"# OpenMRS module bahmnicore

This module provides necessary services for running Bahmni

## Build

[![BahmniCore-master Actions Status](https://github.com/Bahmni/bahmni-core/workflows/Java%20CI%20with%20Maven/badge.svg)](https://github.com/Bahmni/bahmni-core/actions)

### Prerequisite
    JDK 1.8
    ruby 2.2+
    RubyGems
    Compass 1.0.3 (gem install compass)
    
### Clone the repository and build the omod
   
    git clone https://github.com/bahmni/bahmni-core
    cd bahmni-core
    ./mvnw clean install
    
## Deploy

Copy ```bahmni-core/bahmnicore-omod/target/bahmnicore-omod-VERSION-SNAPSHOT.omod``` into OpenMRS modules directory and restart OpenMRS
","'emr', 'hacktoberfest', 'hospital-management-system', 'java', 'social-impact', 'tech4good'",2024-05-02T13:14:38Z,30,115,56,"('mihirk', 166), ('shruthidipali', 103), ('endeepak', 99), ('preethi29', 96), ('HemanthEverest', 80), ('johnstoecker', 76), ('limitless-horizon', 70), ('rohanpoddar', 68), ('sidtharthanan', 62), ('indraneelr', 60), ('sushmitharaos', 57), ('vinkesh', 56), ('abishek91', 53), ('vikashgupta2000', 51), ('angshu', 46), ('sumanmaity112', 45), ('aj-jaswanth', 44), ('SwathiVarkala', 44), ('padmavati', 43), ('pushpadumpala', 42), ('binduak', 40), ('santhubairamcs', 35), ('Gaurav-Deshkar', 34), ('petmongrels', 30), ('mujir-tw', 29), ('mujir', 29), ('hanisha93', 28), ('pankajladhar', 27), ('arathyjan', 26), ('preethisadagopan', 23)","[3, 'Good Health and Well-Being']"
wevote/WeVoteServer,"We Vote's API application server written in Django/Python. Election data pulled from many sources, used by https://github.com/wevote/WebApp and https://github.com/wevote/WeVoteCordova and https://github.com/wevote/Campaigns.","[![Build Status](https://travis-ci.org/wevote/WeVoteServer.svg?branch=master)](https://travis-ci.org/wevote/WeVoteServer) [![Coverage Status](https://coveralls.io/repos/wevote/WeVoteServer/badge.svg?branch=master&service=github)](https://coveralls.io/github/wevote/WeVoteServer?branch=master) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

# README for WeVoteServer

This WeVoteServer repository contains a Python/Django-powered API endpoints server. We take in ballot data from 
Google Civic API, Ballotpedia, Vote Smart, MapLight, TheUnitedStates.io and the Voting Information Project. We then serve
it up to voters, and let voters Support/Oppose and Like ballot items. We are also building tools to capture
and share voter guide data.

You can see our current alpha version for recent national and some reigonal elections here:  https://WeVote.US/

To get started as a We Vote developer, sign the Contributor License Agreement.

## Installing Python/Django API Server

To install and develop the WeVote API server, follow the instructions below based on your preferred environment. 

1. Use these [Simplified Instructions for Mac](docs/README_MAC_SIMPLIFIED_INSTALL.md) leveraging the free (and powerful) PyCharm IDE and debugger (**recommended!**) 

2. [Install directly](docs/README_API_INSTALL.md) on your Linux machine (or use WSL on Windows)

3. Use [Docker](docs/README_API_INSTALL_DOCKER.md)

## Installing We Vote Mobile Web Application (React/Flux)

The website front end application is powered by the [We Vote WebApp](https://github.com/wevote/WebApp)

We distribute the We Vote WebApp in an Apache Cordova wrapper, with some native features, to provide [iOS and Android](https://github.com/wevote/WeVoteCordova) apps.

See the iOS [We Vote 2018 Ballot, @WeVote](https://itunes.apple.com/us/app/we-vote-2018-ballot-wevote/id1347335726?mt=8) in the iTunes app store for iPhones and iPads.

See the Android [We Vote 2018 Ballot, @WeVote](https://play.google.com/store/apps/details?id=org.wevote.cordova&hl=en_US) in the Google Play store for most Android phone and tablet devices.

We also have a [ReactNative for iOS and Android](https://github.com/wevote/WeVoteReactNative) that is currently on hold.


## After Installation: Working with WeVoteServer Day-to-Day

[Read about working with WeVoteServer on a daily basis](docs/README_WORKING_WITH_WE_VOTE_SERVER.md)

Need to upgrade Python?

3a. [Install Python/Django on Mac](docs/README_API_INSTALL_PYTHON_MAC.md)

3b. [Install Python/Django on Linux](docs/README_API_INSTALL_PYTHON_LINUX.md)


If you need to test donations and have not updated your openssl and pyopenssl during install and setup, you will need
[to update your local](docs/README_DONATION_SETUP.md).

See [Instructions for Scheduled Tasks](docs/README_SCHEDULED_TASKS.md)

[How to run the WeVoteServer tests](docs/README_DJANGO_TESTS.md)

[How to run the WeVoteServer Locust Load Tests](loadtest/README.md)

## Join Us

We meet weekly on Google hangouts, and on a team Slack. Please contact Dale.McGrew@WeVote.US for more information.


",,2024-05-03T13:38:05Z,30,50,21,"('DaleMcGrew', 4097), ('SailingSteve', 583), ('pertrai1', 156), ('edward-ly', 103), ('jainanisha90', 91), ('josephevans', 62), ('nirvanabear', 35), ('brandta-1', 32), ('bharathdn', 29), ('fi0rini', 28), ('eayoungs', 23), ('snyk-bot', 21), ('Eduardo445', 20), ('gknorman', 20), ('udii', 17), ('smcvey7', 16), ('nyette', 14), ('nicogreenarry', 14), ('upluke', 11), ('hesamshayegan', 9), ('avanijadeja', 8), ('dependabotbot', 8), ('pbnjcub', 8), ('Farid-Amirli', 7), ('irascible', 7), ('abdulrahimq', 6), ('rileychou', 5), ('jangsoo', 5), ('stilwellmg', 5), ('kotaaaa', 4)","[16, 'Peace, Justice and Strong Institutions']"
JustFixNYC/eviction-free-nyc,Eviction Free NYC! This website helps tenants in NYC navigate the eviction process.,"# Eviction Free NYC!

This site was built in conjunction with the [Right to Counsel Coalition](https://www.righttocounselnyc.org/) as a new resource for NYC tenants to navigate the new RTC law, learn how to respond to an eviction notice, and access legal aid services.

This site is built on top of [GatsbyJS](https://www.gatsbyjs.org/). 

![product shots](https://i.imgur.com/TVZV2Qe.jpg)


### Getting the site up-and-running

Before installing anything, you will want to check out the
repository and create an `.env.development` file from the
template:

```
cp .env.development.sample .env.development
```

Now edit `.env.development` as needed.

You can now develop using either the local installation of
node on your computer, or via Docker.

#### Option 1: Developing with your local installation of node

Make sure that you have `node >= v10` and `yarn >= 1.17` running. In a terminal window, type `node --version` and hit ENTER, then `yarn --version` and hit ENTER to get this info.

Once your environment is setup, you'll need to download libraries (make sure you're in the root directory):

```
yarn install
```

Then run the following to start your dev environment!

```
yarn netlify:develop
```

You should be able to access your development server at http://localhost:8001/.

#### Option 2: Developing with Docker

Get [Docker][] and run:

```
docker-compose run app yarn
docker-compose up
```

Then visit http://localhost:8001/ in your browser and you
should be good to go.

[Docker]: https://docker.com/

### Deploying

See the [deploy instructions](https://www.gatsbyjs.org/tutorial/part-one/#deploying-gatsbyjs-websites) on the GatsbyJS site to decide what's best for you!

### Serverless functionality

The files in `src/serverless-functions` are accessible as
[Netlify Functions][], both during development and in production.
For instance, this means that a file at `src/serverless-functions/foo.ts` can be accessed at `/.netlify/functions/foo`.

[Netlify Functions]: https://docs.netlify.com/functions/overview/

### Textbot

A EFNYC textbot based off [textit-webhook-fun][] is in the `src/textbot` directory. It can be run in the terminal with:

```
node run-textbot.js
```

For production, it's exposed via a serverless function at `/.netlify/functions/textbot`.  Because we currently use RapidPro/TextIt as our primary SMS gateway, the RapidPro flow in `src/textbot/sample-rapidpro-textbot-flow.json` needs to be imported in order to interface with it.

[textit-webhook-fun]: https://github.com/JustFixNYC/textit-webhook-fun

### Attribution

The starter for this site was [gatsby-contentful-i18n](https://github.com/mccrodp/gatsby-contentful-i18n), built by [mccrodp](https://github.com/mccrodp)! Many thanks!

Also a huge thanks to the team at [Contentful](https://www.contentful.com/) for their generous support.

![thanks contentful!](https://www.contentful.com/assets/images/badges/dark.png)

We use BrowserStack Live to make sure that our sites work across browsers, operating systems, and devices.

![BrowserStack](https://www.browserstack.com/images/layout/browserstack-logo-600x315.png)

## License

JustFix.nyc uses the GNU General Public License v3.0 Open-Source License. See `LICENSE.md` file for the full text.

## Code of Conduct

Read about JustFix's code of conduct as an organization on our [Mission page](https://www.justfix.nyc/our-mission/), as well as on the Right to Counsel's [About page](https://www.righttocounselnyc.org/about)
",,2023-01-11T18:38:16Z,5,48,5,"('romeboards', 130), ('sraby', 32), ('abromanos', 31), ('toolness', 24), ('dependabotbot', 9)","[10, 'Reduced Inequalities']"
SEED-platform/seed,Standard Energy Efficiency Data (SEED) Platform™ is a web-based application that helps organizations easily manage data on the energy performance of large groups of buildings.,"## Standard Energy Efficiency Data (SEED) Platform™

[![Build Status][build-img]][build-url] [![Coverage Status][coveralls-img]][coveralls-url]

The SEED Platform is a web-based application that helps organizations easily
manage data on the energy performance of large groups of buildings. Users can
combine data from multiple sources, clean and validate it, and share the
information with others. The software application provides an easy, flexible,
and cost-effective method to improve the quality and availability of data to
help demonstrate the economic and environmental benefits of energy efficiency,
to implement programs, and to target investment activity.

The SEED application is written in Python/Django, with AngularJS, Bootstrap,
and other javascript libraries used for the front-end. The back-end database
is required to be PostgreSQL.

The SEED web application provides both a browser-based interface for users to
upload and manage their building data, as well as a full set of APIs that app
developers can use to access these same data management functions. From a
running server, the Swagger API documentation can be found at `/api/swagger`
or from the front end by clicking the API documentation link in the sidebar.

### Installation

- Production on Amazon Web Service: See [Installation Notes][production-aws-url]
- Development on Mac OSX: [Installation Notes][development-mac-osx]
- Development using Docker: [Installation Notes][development-docker]

### Starting SEED Platform

In production the following two commands will run the web server (uWSGI) and
the background task manager (Celery) with:

```
bin/start_uwsgi.sh
bin/start_celery.sh
```

In development mode, you can start the web server (uWSGI) and the background
task manager (Celery) with:

```
./manage.py runserver
celery -A seed worker -l INFO -c 4 --max-tasks-per-child 1000 -EBS django_celery_beat.schedulers:DatabaseScheduler
```

### Developer Resources

- Source code documentation is on the [SEED website][code-documentation] and there are links to [older versions][code-documentations-links] as needed.
- Several notes regarding Django and AngularJS integration: See [Developer Resources][developer-resources]

#### Testing

- Running tests: See [Testing Notes][developer-testing-notes]

### Copyright

See the information in the [LICENSE.md](LICENSE.md) file.

[code-documentation]: https://seed-platform.org/code_documentation/latest/
[code-documentation-links]: https://seed-platform.org/developer_resources/
[development-docker]: https://github.com/SEED-platform/seed/blob/develop/docs/source/setup_docker.rst
[development-mac-osx]: https://github.com/SEED-platform/seed/blob/develop/docs/source/setup_osx.rst
[production-aws-url]: http://www.github.com/seed-platform/seed/wiki/Installation
[developer-resources]: https://github.com/SEED-platform/seed/blob/develop/docs/source/developer_resources.rst
[developer-testing-notes]: https://github.com/SEED-platform/seed/blob/develop/docs/source/developer_resources.rst#testing
[build-img]: https://github.com/SEED-platform/seed/workflows/CI/badge.svg?branch=develop
[build-url]: https://github.com/SEED-platform/seed/actions?query=branch%3Adevelop
[coveralls-img]: https://coveralls.io/repos/github/SEED-platform/seed/badge.svg?branch=HEAD
[coveralls-url]: https://coveralls.io/github/SEED-platform/seed?branch=HEAD
","'buildings', 'commercial', 'energy'",2024-05-03T15:30:20Z,30,105,27,"('nllong', 2716), ('axelstudios', 1094), ('macintoshpie', 1067), ('aviveiros11', 561), ('perryr16', 541), ('nicholasserra', 445), ('mmclark', 372), ('lainsworth8801', 343), ('haneslinger', 264), ('Myoldmopar', 247), ('adrian-lara', 231), ('maronnax', 216), ('danielbressan', 208), ('rgm', 201), ('danielmcquillen', 162), ('kflemin', 160), ('anchapin', 92), ('fablet', 72), ('StephenCzarnecki', 53), ('dependabotbot', 34), ('dangunter', 27), ('ebeers-png', 26), ('laurenliz22', 14), ('rHorsey', 13), ('meghasandesh', 13), ('nmerket', 12), ('juliecov', 10), ('rebrownlbnl', 6), ('philngo', 6), ('anyaelena', 6)","[11, 'Sustainable Cities and Communities']"
radiantearth/stac-spec,SpatioTemporal Asset Catalog specification - making geospatial assets openly searchable and crawlable,"


[![CircleCI](https://circleci.com/gh/radiantearth/stac-spec.svg?style=svg)](https://circleci.com/gh/radiantearth/stac-spec)

## About

The SpatioTemporal Asset Catalog (STAC) family of specifications aim to 
standardize the way geospatial asset metadata is structured and queried. 
A ""spatiotemporal asset"" is any file that represents information about 
the Earth at a certain place and time. The original focus was on scenes 
of satellite imagery, but the specifications now cover a broad variety of uses, 
including sources such as aircraft and drone and data such as hyperspectral optical, 
synthetic aperture radar (SAR), video, point clouds, lidar, digital elevation 
models (DEM), vector, machine learning labels, and composites like NDVI and 
mosaics. STAC is intentionally designed with a minimal core and flexible 
extension mechanism to support a broad set of use cases. This specification 
has matured over the past several years, and is used in [numerous production 
deployments](https://stacindex.org/catalogs). 

This is advantageous to providers of geospatial data, as they can simply use a
well-designed, standard format and API without needing to design their own proprietary one.
This is advantageous to consumers  of geospatial data, as they can use existing libraries 
and tools to access metadata, instead of needing to write new code to interact 
with each data provider's proprietary formats and APIs. 

The STAC specifications define related JSON object types connected by link 
relations to support a [HATEOAS](https://en.wikipedia.org/wiki/HATEOAS)-style traversable
interface and a [RESTful](https://en.wikipedia.org/wiki/Representational_state_transfer) API
providing additional browse and search interfaces.
Typically, several STAC specifications are composed together to create an implementation. 
The **Item**, **Catalog**, and **Collection** specifications define a minimal core 
of the most frequently used JSON object types. Because of the hierarchical structure 
between these objects, a STAC catalog can be implemented in a completely 'static' 
manner as a group of hyperlinked Catalog, Collection, and Item URLs, enabling data 
publishers to expose their data as a browsable set of files. If more complex query 
abilities are desired, such as spatial or temporal predicates, the 
**STAC API** [specification](https://github.com/radiantearth/stac-api-spec/) can be 
implemented as a web service interface to query over a group of STAC objects, usually 
held in a database.

To the greatest extent possible, STAC uses and extends existing specifications. 
The most important object in STAC is an **Item**, which is simply a [GeoJSON](http://geojson.org) **Feature** 
with a well-defined set of additional attributes (""foreign members""). The **STAC API** 
extends the **[OGC API - Features - Part 1: Core](http://docs.opengeospatial.org/is/17-069r3/17-069r3.html)** 
with additional web service endpoints and object attributes.

## Current version and branches

The [master branch](https://github.com/radiantearth/stac-spec/tree/master) is the 'stable' 
version of the spec. It is currently version **1.0.0** of the specification. The STAC specification 
follows [Semantic Versioning](https://semver.org/), so any breaking change will require the spec to 
go to 2.0.0. 

The [dev](https://github.com/radiantearth/stac-spec/tree/dev) branch is where active development 
takes place, and may have inconsistent examples. Whenever dev stabilizes, a release is cut and we 
merge `dev` in to `master`. So `master` should be stable at any given time. 
More information on how the STAC development process works can be found in 
[process.md](process.md).

## Communication

Our [gitter channel](https://gitter.im/SpatioTemporal-Asset-Catalog/Lobby) is 
the best place to ask questions or provide feedback. The majority of communication about the evolution of 
the specification takes place in the [issue tracker](https://github.com/radiantearth/stac-spec/issues) and in 
[pull requests](https://github.com/radiantearth/stac-spec/pulls).

## In this Repository

This repository contains the core object type specifications, [examples](examples/), 
validation schemas, and documentation about the context and plans for the evolution of the 
specification. Each folder contains a README explaining the layout of the folder, 
the main specification document, examples, and validating schemas. 

Additionally, the [STAC API specification](https://github.com/radiantearth/stac-api-spec/) 
provides API endpoints, based on the [OGC API - Features](http://docs.opengeospatial.org/is/17-069r3/17-069r3.html) standard,
that enable clients to search for Item objects that match their filtering criteria. 

The **Item**, **Catalog**, **Collection**, and **STAC API** specifications are intended to be 
used together, but are designed so each piece is small, self-contained, and reusable in other contexts.

- **[Overview](overview.md)** describes the three core object type specifications and how they relate to one another.
- **[Item Specification](item-spec/)** defines a STAC **Item**, which is a [GeoJSON](http://geojson.org) **Feature**
  with additional fields (""foreign members"") for attributes like time and links to related entities and assets 
  (including thumbnails). This is the core entity that describes the data to be discovered.
- **[Catalog Specification](catalog-spec/)** specifies a structure to link various STAC Items together to be crawled or browsed. It is a
  simple, flexible JSON file of links to Items, Catalogs or Collections that can be used in a variety of ways.
- **[Collection Specification](collection-spec/)** provides additional information about a spatio-temporal collection of data.
  In the context of STAC it is most likely a related group of STAC Items that is made available by a data provider.
  It includes things like the spatial and temporal extent of the data, the license, keywords, etc.
  It enables discovery at a higher level than individual Item objects, providing a simple way to describe sets of data.
- **[Examples](examples/):** The *[examples/](examples/)* folder contains examples for all three specifications, linked together to form two 
  complete examples. Each spec and extension links in to highlight particular files that demonstrate key concepts.
- **[Extensions](extensions/README.md)** describe how STAC can use extensions that extend the functionality of the core spec or 
  add fields for specific domains. Extensions can be published anywhere,
  although the preferred location for public extensions is in the [GitHub `stac-extensions` organization](https://github.com/stac-extensions).
- **Additional documents:** The supporting documents include a complementary [best practices](best-practices.md) 
  document, and information on contributing (links in the next section). We also maintain a [changelog](CHANGELOG.md) of
  what was modified in each version. 

## Contributing

Anyone building software that catalogs imagery or other geospatial assets is welcome to collaborate.
Beforehand, please review our [guidelines for contributions](CONTRIBUTING.md) and [code of conduct](CODE_OF_CONDUCT.md). 
You may also be interested in our overall [process](process.md), and the [principles](principles.md) that guide our 
collaboration
","'earth-observation', 'earth-observation-catalogue', 'satellite-imagery', 'satellites'",2024-02-14T19:04:02Z,30,730,72,"('cholmes', 926), ('m-mohr', 822), ('matthewhanson', 481), ('hgs-msmith', 47), ('jbants', 45), ('joshfix', 42), ('jisantuc', 38), ('mojodna', 37), ('emmanuelmathot', 37), ('constantinius', 36), ('fredliporace', 32), ('lossyrob', 32), ('scisco', 26), ('anayeaye', 25), ('philvarner', 24), ('jeffnaus', 18), ('davidraleigh', 17), ('alexgleith', 15), ('nrweir', 14), ('aaronxsu', 11), ('vmx', 7), ('hobu', 6), ('kbgg', 5), ('daveluo', 5), ('notthatbreezy', 5), ('rouault', 4), ('francbartoli', 4), ('CloudNiner', 3), ('danlopez00', 3), ('schwehr', 3)","[17, 'Partnerships for the Goals']"
LoopKit/LoopKit,A set of iOS tools for building closed-loop insulin delivery apps,"# LoopKit

[![Build Status](https://travis-ci.org/LoopKit/LoopKit.svg?branch=master)](https://travis-ci.org/LoopKit/LoopKit)
[![Carthage compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage)
[![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://loop.zulipchat.com)

LoopKit is a set of tools to speed up development of your own closed-loop insulin delivery app. It is agnostic to treatment decisions and input sources.

## Loop

[Loop](https://github.com/LoopKit/Loop) is a full-featured app built on top of LoopKit.

## LoopKit provides

* Data storage and retrieval, using HealthKit and Core Data as appropriate
* Protocol types for representing data models in a flexible way
* Common calculation algorithms like Insulin On Board
* Boilerplate user interfaces like editing basal rate schedules and carb entry

## LoopKit does not provide

* Treatment decisions: Your Diabetes May Vary.
* Device communications: Device-specific libraries are maintained separately.

# License

LoopKit is available under the MIT license. See the LICENSE file for more info.

# Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](https://github.com/LoopKit/LoopKit/blob/master/CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.
",,2024-04-22T13:57:40Z,30,357,90,"('ps2', 809), ('loudnate', 256), ('nhamming', 147), ('Camji55', 69), ('mpangburn', 59), ('novalegra', 37), ('rickpasetto', 21), ('SwiftlyNoah', 18), ('darinkrauss', 11), ('gestrich', 11), ('marionbarker', 8), ('Kdisimone', 8), ('ArwainK', 7), ('elnjensen', 5), ('damonbayer', 5), ('bill-foreflight', 4), ('cfaagaard', 3), ('dm61', 3), ('sharpfive', 2), ('Joerg-Schoemer', 2), ('mylma', 2), ('ryanarana-dexcom', 2), ('diggabyte', 2), ('dnzxy', 1), ('jh-bate', 1), ('itsmojo', 1), ('bjorkert', 1), ('whiten', 1), ('panctronic', 1), ('vpdn', 1)","[3, 'Good Health and Well-Being']"
NREL/EnergyPlus,"EnergyPlus™ is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption and water use in buildings.","EnergyPlus [![](https://img.shields.io/github/release/NREL/energyplus.svg)](https://github.com/NREL/EnergyPlus/releases/latest)
==========

[![](https://img.shields.io/github/downloads/NREL/EnergyPlus/latest/total?color=5AC451)](https://github.com/NREL/EnergyPlus/releases/latest)
[![](https://img.shields.io/github/downloads/nrel/energyplus/total.svg?color=5AC451&label=downloads_since_v8.1)](https://github.com/NREL/EnergyPlus/releases)

This is the EnergyPlus Development Repository.  EnergyPlus™ is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption and water use in buildings.

## Contact/Support

 - The Department of Energy maintains a [public website for EnergyPlus](https://energyplus.net) where you can find much more information about the program.
 - For detailed developer information, consult the [wiki](https://github.com/nrel/EnergyPlusTeam/wiki).
 - Many users (and developers) of EnergyPlus are active on [Unmet Hours](https://unmethours.com/), so that's a great place to start if you have a question about EnergyPlus or building simulation.
 - For more in-depth, developer-driven support, please utilize the [EnergyPlus Helpdesk](https://energyplushelp.freshdesk.com/).

## Testing

[![](https://github.com/NREL/EnergyPlus/workflows/Custom%20Check/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/custom_check.yml) 
[![](https://github.com/NREL/EnergyPlus/workflows/Documentation/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/documentation.yml) 
[![](https://github.com/NREL/EnergyPlus/workflows/CppCheck/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/cppcheck.yml)

Every commit and every release of EnergyPlus undergoes rigorous testing.
The testing consists of building EnergyPlus, of course, then there are unit tests, integration tests, API tests, and regression tests.
Since 2014, most of the testing has been performed by our bots ([Tik-Tok](https://github.com/nrel-bot), [Gort](https://github.com/nrel-bot-2), and [Marvin](https://github.com/nrel-bot-3)), using a fork of the [Decent CI](https://github.com/lefticus/decent_ci) continuous integration system.
We are now adapting our efforts to use the Github Actions system to handle more of our testing processes.
In the meantime, while Decent CI is still handling the regression and bulkier testing, results from Decent CI are still available on the testing [dashboard](https://myoldmopar.github.io/EnergyPlusBuildResults/).

## Releases

[![](https://github.com/NREL/EnergyPlus/workflows/Windows%20Releases/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/windows_release.yml) 
[![](https://github.com/NREL/EnergyPlus/workflows/Mac%20Releases/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/mac_release.yml) 
[![](https://github.com/NREL/EnergyPlus/workflows/Linux%20Releases/badge.svg)](https://github.com/NREL/EnergyPlus/actions/workflows/linux_release.yml)

EnergyPlus is released twice annually, usually in March and September.
It is recommended all use of EnergyPlus in production workflows use these formal, public releases.
Iteration **(pre-)releases** may be created during a development cycle, however users should generally avoid these, as input syntax may change which won't be supported by the major release version transition tools, and could require manual intervention to remedy.
If an interim release is intended for active use by users, such as a bug-fix-only or performance-only re-release, it will be clearly specified on the release notes and a public announcement will accompany this type of release.
Our releases are now built by Github Actions.

## Core Documentation

Program documentation is installed alongside the program, with the pdfs also available [online](https://energyplus.net/documentation).
Big Ladder also produces html based documentation [online](http://bigladdersoftware.com/epx/docs/).

## API Documentation

[![Read the Docs](https://img.shields.io/readthedocs/energyplus?label=docs%20%28latest%29&color=5AC451)](https://energyplus.readthedocs.io/en/latest/)
[![Read the Docs](https://img.shields.io/readthedocs/energyplus?label=docs%20%28stable%29&color=5AC451)](https://energyplus.readthedocs.io/en/stable/)

An API has been developed to allow access to internal EnergyPlus functionality and open up the possibility for new workflow opportunities around EnergyPlus.
A C API is developed to expose the C++ functions, then Python bindings are built on top of that to maximize the accessibility.
Documentation is being built and posted on ReadTheDocs and that documentation will continue to be expanded over time as the API grows.
The badges above here show the status, and link out to, the `latest` documentation (most recent commit to the `develop` branch) as well as the `stable` documentation (most recent release tag).

## License & Contributing Development

[![](https://img.shields.io/badge/license-BSD--3--like-5AC451.svg)](https://github.com/NREL/EnergyPlus/blob/develop/LICENSE.txt)

EnergyPlus is available under a BSD-3-like license.
For more information, check out the [license file](https://github.com/NREL/EnergyPlus/blob/develop/LICENSE.txt).
The EnergyPlus team accepts contributions to EnergyPlus source, utilities, test files, documentation, and other materials distributed with the program.
The current EnergyPlus contribution policy is now available on the EnergyPlus [contribution policy page](https://www.energyplus.net/contributing).
If you are interested in contributing, please start there, but feel free to reach out to the team.

## Building EnergyPlus

A detailed description of compiling EnergyPlus on multiple platforms is available on the [wiki](https://github.com/NREL/EnergyPlus/wiki/Building-EnergyPlus).
Also, as we are adapting to using Github Actions, the recipes for building EnergyPlus can be found in our [workflow files](https://github.com/NREL/EnergyPlus/tree/develop/.github/workflows).
",,2024-05-03T14:46:06Z,30,1018,99,"('Myoldmopar', 6575), ('mjwitte', 3967), ('rraustad', 2758), ('mitchute', 2142), ('jcyuan2020', 1638), ('jmarrec', 1531), ('dareumnam', 1438), ('jmythms', 1386), ('JasonGlazer', 1342), ('Nigusse', 1259), ('xuanluo113', 1019), ('mbadams5', 828), ('yujiex', 823), ('lgu1234', 785), ('RKStrand', 755), ('nealkruis', 661), ('jasondegraw', 632), ('nmerket', 570), ('matthew-larson', 550), ('lefticus', 538), ('EnergyArchmage', 497), ('brianlball', 356), ('rongpengzhang', 322), ('yzhou601', 284), ('michael-okeefe', 280), ('kbenne', 262), ('amirroth', 242), ('vidanovic', 224), ('lymereJ', 216), ('DeadParrot', 208)","[7, 'Affordable and Clean Energy']"
nvaccess/nvda,"NVDA, the free and open source Screen Reader for Microsoft Windows","# NVDA

NVDA (NonVisual Desktop Access) is a free, open source screen reader for Microsoft Windows.
It is developed by NV Access in collaboration with a global community of contributors.
To learn more about NVDA or download a copy, visit the main [NV Access](http://www.nvaccess.org/) website.

Please note: the NVDA project has a [Citizen and Contributor Code of Conduct](CODE_OF_CONDUCT.md). NV Access expects that all contributors and other community members will read and abide by the rules set out in this document while participating or contributing to this project.

The NVDA project is guided by a [product vision statement and set of principles](./projectDocs/product_vision.md).
The vision and principles should be always considered when planning features and prioritizing work.

NVDA is available under a modified GNU General Public License version 2.
Please refer to [our license](./copying.txt) for more information.
 
## NVDA Community
* [Support and information for NVDA users](https://www.nvaccess.org/get-help/)
* [Report an issue or feature request](./projectDocs/issues/readme.md)
* [Getting add-ons](https://www.nvaccess.org/files/nvda/documentation/userGuide.html#AddonsManager)
* [More important links and community information](./projectDocs/community/readme.md)

## Contributing to NVDA
If you would like to contribute to NVDA, you can read more information in our [contributing guide](./.github/CONTRIBUTING.md).
This includes information on reporting issues, triaging issues, testing, translating, contributing code/documentation and creating add-ons.
","'accessibility', 'blind', 'screen-reader'",2024-05-03T14:55:37Z,30,1935,143,"('jcsteh', 4178), ('michaelDCurran', 3937), ('seanbudd', 642), ('feerrenrut', 622), ('LeonarddeR', 364), ('mhameed', 300), ('josephsl', 277), ('CyrilleB79', 180), ('lukaszgo1', 172), ('JulienCochuyt', 112), ('ragb', 97), ('codeofdusk', 89), ('Surveyor123', 38), ('dinakar-td', 35), ('dkager', 31), ('nishimotz', 29), ('XLTechie', 29), ('mltony', 26), ('aaclause', 24), ('gerald-hartig', 23), ('bramd', 21), ('dhankuta', 19), ('ondrosik', 17), ('nvdaes', 16), ('Adriani90', 16), ('hwf1324', 15), ('burmancomp', 15), ('gregjozk', 14), ('zahyur', 14), ('Nardol', 14)","[3, 'Good Health and Well-Being']"
utopic-studio/hanselygretelVR,VR game of the story of Hansel & Gretel,,,2020-01-07T22:28:05Z,4,0,3,"('gabocorrea', 107), ('Pandalex1', 60), ('inostropo', 20), ('Loptharr', 9)","[4, 'Quality Education']"
rubyforgood/playtime,"Supporting children and families experiencing homelessness in Washington, DC.  Live app - https://wishlist.playtimeproject.org Organization Website:","

# Homeless Children's Playtime Project
[![Build Status](https://api.travis-ci.org/rubyforgood/playtime.svg?branch=master)](https://travis-ci.org/rubyforgood/playtime) [![View performance data on Skylight](https://badges.skylight.io/status/vXHx56uMnWP7.svg)](https://oss.skylight.io/app/applications/vXHx56uMnWP7)

*The mission of the [Homeless Children's Playtime Project][HCPP] is to nurture
healthy child development and reduce the effects of trauma among children
living in temporary housing programs in Washington, DC.*

*Playtime seeks to create a city that provides every opportunity for children
in families experiencing homelessness to succeed by ensuring consistent
opportunities to play and learn, offering support services for families, and
advocating for affordable housing and safe shelter.*

[HCPP]: http://www.playtimeproject.org/

### About This App

The goal of this application is to allow supporters to donate toys and other
items that help advance the work of Playtime Project's work in family shelters
throughout DC. This application will allow donors to view the organization's
Amazon wish lists, add items, track contributions, and aid staff in following
up with donors.



### About Ruby for Good

This project was born at Ruby for Good 2017.

Ruby for Good is an annual event based out of the DC-metro area where Ruby
programmers from all over the globe get together for a long weekend to build
projects that help our communities. For more information about Ruby for Good,
[visit the website] or [check out the other projects].

[visit the website]: http://rubyforgood.org/
[check out the other projects]: http://rubyforgood.org/yearbook.html


## Join the Team

This application is a work in progress, and we encourage you to jump in!

To get started, follow our [Getting Started](#getting-started) guide to set up
your computer, then check out [our GitHub Issues][issues] to see what issues are
currently in the pipeline. You can find our contribution guidelines in
[CONTRIBUTING]. Get in touch with [@micahbales] for more details.

To join the conversation, join the [#playtime][slack-channel] channel
on Slack ([get an invite][slack-invite]).

* [Getting Started](#getting-started)
    * [Quick Start](#quick-start)
    * [Getting Amazon OAuth Working Locally](#getting-amazon-oauth-working-locally)
    * [Getting Amazon Product Advertising API Working Locally](#getting-amazon-product-advertising-api-working-locally)
* [Testing](#testing)
* [Environment](#environment)

[issues]: https://github.com/rubyforgood/playtime/issues
[@micahbales]: https://github.com/micahbales
[slack-channel]: https://rubyforgood.slack.com/messages/playtime
[slack-invite]: https://rubyforgood.herokuapp.com/
[CONTRIBUTING]: .github/CONTRIBUTING.md

### Getting Started

#### Quick Start

By the end of this section, you should have the project and dependencies
installed on your local system. For information on how to contribute, see
[Contributing][CONTRIBUTING].

You need:

  - Ruby 2.4.1
  - Rails 5.1
  - Postgres >=9.1
  - A JavaScript runtime, we recommend Node.js

You will need Node.js (which comes bundled with NPM) and it is easiest to
install on Mac, Windows, or Linux [with your favorite package manager][node-pkg]
or by downloading directly from [nodejs.org][node-direct].

[node-pkg]:    https://nodejs.org/en/download/package-manager/
[node-direct]: https://nodejs.org/en/download/

You will also need Git, Ruby, Rails, and PostgreSQL. If you have Git and
Postgres in some version or another you're probably set. But:

  - If you're working on **a fresh Windows machine**, you're best off installing
    Ruby and Rails with the [Rails Installer]. For PostgreSQL, you'll want to
    install [EnterpriseDB].

  - If you have **a fresh Mac OS X machine**, just follow these
    [directions][Go Rails] (if you're running a different OS version, switch to
    the correct version). These instructions include PostgreSQL.

  - If you are using **Ubuntu Linux**, use this [Rails Apps Guide] to get set
    up with Ruby, Rails, and Node.js and this guide to
    [install PostgreSQL][Postgres Ubuntu].

  - Otherwise, Google for instructions for your OS of choice. Let us know if
    you get stuck!

[Rails Installer]: http://railsinstaller.org
[EnterpriseDB]: https://www.enterprisedb.com/downloads/postgres-postgresql-downloads#windows

[Go Rails]: https://gorails.com/setup/osx

[Rails Apps Guide]: http://railsapps.github.io/installrubyonrails-ubuntu.html
[Postgres Ubuntu]: https://www.postgresql.org/download/linux/ubuntu/


Navigate to your desired working directory. Then from a command prompt:

  ```bash
  $ git clone http://github.com/rubyforgood/playtime
  $ cd playtime
  $ bin/setup
  $ rails server
  ```

Then navigate to `http://localhost:3000` in your browser to view the app.

#### Getting Amazon OAuth Working Locally (Optional)

By the end of this section, you should be able to create an account/login to
the app on your local machine.

This application uses Amazon OAuth for authentication. In order to create an
account or login locally:

1. Follow [these instructions][Amazon OAuth Instructions] to create an Amazon
   app. This is required for logging in/creating

2. If you don't have a `.env` file, copy the sample .env configuration:

    ```bash
    # from the root project directory
    $ cp .env.sample .env
    ```

3. In `.env`, fill in the id and secret keys for your Amazon app (Client ID and
   Client Secret on your Amazon App Console):

    ```bash
    # .env
    AMAZON_CLIENT_ID=""your client id""
    AMAZON_CLIENT_SECRET=""your client secret""
    # ...
    ```

4. *(Optional: Setting up your admin account)* If you want your development
   account to be an admin, you can set that up by setting the admin environment
   variables:

   ```bash
   # .env
   ADMIN_NAME=""your name""
   ADMIN_AMAZON_EMAIL=""the email you use for your amazon.com account""
   ```

   Next, run `rake users:initialize_admin`. If you've already logged in, your
   account will be promoted to an admin. If you haven't, your new admin account
   will be created.

   You can also change your user role by using `rails console`.

5. Start your Rails app with `rails server`. You're ready to OAuth!

[Amazon OAuth Instructions]: https://github.com/wingrunr21/omniauth-amazon#prereqs


#### Getting Amazon Product Advertising API Working Locally

By the end of this section, you should be able to search Amazon products and
add items to wishlists on your local machine (when your logged-in user is an
admin or site manager).

**This step is only required for site managers and admins searching/adding
Amazon products.** If your issue doesn't involve working with the Amazon
Product API, you'll be fine without this step.

Admins and site managers can add new items to their wishlists by searching
the Amazon Product API. To get this search working locally, you'll need to
create a few Amazon accounts and add some more `.env` configuration:

  1. First, [register for an Amazon Associate account][Amazon Associate]. Order
     matters here; be sure to do this before step #2!

  2. Next, [become a Product Advertising API Developer][Product Advertising API].

  3. Last, using the AWS credentials you downloaded (or viewed) in step #2,
     update your `.env` configuration:

       ```bash
       # .env
       # ...
       AWS_ACCESS_KEY=""your access key id goes here""
       AWS_SECRET_KEY=""your secret access key goes here""
       AWS_ASSOCIATES_TAG=""playtim009-20"" # <- same for everyone
       ```
     You'll only be able to download your secret key once, so make sure you put
     it in a safe place! Otherwise, you'll have to log in and regenerate your
     credentials.

Restart the server and you're done–you can search to your heart's content!

[Amazon Associate]: http://docs.aws.amazon.com/AWSECommerceService/latest/DG/becomingAssociate.html
[Product Advertising API]: http://docs.aws.amazon.com/AWSECommerceService/latest/DG/becomingDev.html


### Testing

All specs must pass in order for a PR to be accepted. Specs and associated
checks can be run with

  ```bash
  $ bin/rake
  ```

Specs can be found in the `spec` folder, and they follow the typical `rspec`
directory structure. Some notes:

  - We're using FactoryBot to generate objects, and those files can be found
    in `spec/factories/.rb`.

  - Support files, including helper methods, gem initialization/configuration,
    etc. can be found in `spec/support`.

  - While we use factories instead of fixtures to generate objects, static
    test files (ex. images or API responses) can be found in `fixtures`.

  - External HTTP requests can be mocked with WebMock. To mark a spec as
    containing an external request, use the `:external` tag.


### Environment

We manage environment variables using `.env`. This file should be created when
your run `bin/setup`, but you can always copy `.env.sample` if it gets deleted.
For changes to `.env` to take effect, you'll need to restart your server.

**Remember: `.env` should never be checked into source control!**

  ```bash
  # Annotated .env example

  # The client id and secret are used for OAuth (creating accounts/logging in).
  # Setup instructions can be found above in ""Getting Started with OAuth""
  AMAZON_CLIENT_ID=""your amazon client id""
  AMAZON_CLIENT_SECRET=""your amazon client secret key""

  # The access key and secret are used for interacting with the Amazon Product
  # API. Setup instructions can be found above in ""Getting Amazon Product
  # Advertisement API Working Locally""
  AWS_ACCESS_KEY=""your aws access key""
  AWS_SECRET_KEY=""your aws secret key""

  # If you want to force Amazon login instead of developer in development environment
  # set it to true.
  FORCE_AMAZON_LOGIN=false

  # Code for generating affiliate links from search (same for everyone)
  AWS_ASSOCIATES_TAG=""playtim009-20""

  # Default rack env and port (same for everyone)
  RACK_ENV=""development""
  PORT=3000

  # Admin account details for `rake users:initialize_admin`
  ADMIN_NAME=""your name""
  ADMIN_EMAIL=""the email you use for your amazon.com account""
  ```
","'nonprofit', 'rails', 'ruby-for-good'",2019-03-03T02:10:32Z,30,87,11,"('leesharma', 100), ('micahbales', 96), ('flemdizzle', 67), ('seanmarcia', 21), ('Brennaleker', 19), ('invacuo', 9), ('filipewl', 7), ('mxygem', 7), ('stacimcwilliams', 6), ('jwieringa', 6), ('gabteles', 6), ('UJKhokhar', 5), ('josephbhunt', 4), ('K4sku', 3), ('rowrowrowrow', 3), ('zr2d2', 2), ('teneresa', 2), ('ramyaa', 2), ('Maxscores', 2), ('D3vil0p3r', 1), ('anacarolinacastro', 1), ('ChanChar', 1), ('monoliths', 1), ('JacksonIsaac', 1), ('jaquesdias', 1), ('khwilo', 1), ('viniciusilveira', 1), ('yock', 1), ('zahra-io', 1), ('memcmahon', 1)","[10, 'Reduced Inequalities']"
dimagi/commcare-android,"Offline First Android software client for CommCare, the world's largest platform for designing, managing, and deploying robust mobile applications to frontline workers worldwide","# commcare-android

CommCare is an easily customizable, open source mobile platform that supports frontline workers in low-resource settings. By replacing paper-based forms, frontline workers can use CommCare to track and support their clients with registration forms, checklists, SMS reminders, and multimedia.

This repository represents the Android version of CommCare. It depends on the [CommCare Core](https://github.com/dimagi/commcare-core) repository, which contains the XForm engine and case/lookup table implementations.

## Setup

To set up an Android dev environment for commcare-android, do the following:

- Install [Android Studio](https://developer.android.com/sdk/index.html).
- Install Java 17 if you don't have it yet. For ease of test suite setup ([see below](#tests)) OpenJDK is preferred over Oracle's version of Java. 

Go ahead and open Android Studio if this is your first time using it;
it may take you through some sort of setup wizard, and it's nice to get that out of the way. If it's not the first time, ensure that there are no references to [removed Java options](https://docs.oracle.com/en/java/javase/17/docs/specs/man/java.html#removed-java-options) in your environment, most commonly found are *MaxPermSize* and *PermSize*

Android Studio's default project space is `~/AndroidStudioProjects` so I'm going to use that in the example.
CommCare Android depends on CommCare Core, and CommCare Android expects the core directory to live side by side
in your directory structure. You can acheive this with the following commands (in bash):

```bash
cd ~/AndroidStudioProjects
mkdir CommCare
cd CommCare
git clone https://github.com/dimagi/commcare-android.git
git clone https://github.com/dimagi/commcare-core.git
```

- Open Android Studio
- If this is your first time using Android Studio, click ""Config"" and setup the Android SDK.
- Download the Android 12 (API 31) SDK Platform and the Google APIs for 31.
- Now go back to the Android Studio Welcome dashboard and click ""Import project (Eclipse ADT, Gradle, etc.)""
- Select AndroidStudioProjects > CommCare > commcare-android and hit OK
- Click ""OK"" to use the Gradle wrapper
- Wait while Android Studio spins its wheels
- Download any build dependencies that the SDK Manager tells you you need.
- Disable _Instant Run_ found in Settings > Build, Execution, Deployment > Instant Run. (It does not play well with multidexing, which we have enabled, or with some of the processes we have set up for Google Services)

## Building

Now you're basically ready to go. To build CommCare Android and get it running on your phone,
plug in an android phone that

- is [in developer mode has USB debugging enabled](https://developer.android.com/tools/device.html#setting-up)
- doesn't have CommCare Android installed on it

In Android Studio, hit the build button (a green ""play"" symbol in the toolbar).
The first build will take a minute.
Then it'll ask you what device to run it on

- Make sure your screen is unlocked (or else you'll see [something like this](https://gist.github.com/dannyroberts/6d8d57ff4d5f9a1b70a5))
- select your device

Enjoy!

### Building from the command-line

CommCare has several different build variants. The normal build variant is `commcare` and can built built from the command-line with the following command:

```bash
cd commcare-android
./gradlew assembleCommcareDebug
# the apk can now be found in the build/outputs/apk/ directory
```

## Unit Tests

The commcare-android repository uses [Robolectric](http://robolectric.org/), which provides mocks, allowing you to run Android specific code on your local machine.

### Run unit-tests from the command-line

```bash
cd commcare-android
./gradlew testCommcareDebug
```

and view the results from the output file generated.

### Run unit-tests from Android Studio

Create a new Android Studio JUnit Build configuration using the following steps.

- Click _Run -> Edit Configruations_ and create a new JUnit configuration by pressing the green plus button.
- Set _Name_ to ""commcare android test suite""
- Set _Test kind_ to ""All in directory""
- set _Directory_ to `/absolute/path/to/commcare-android/app/unit-tests/src/`
- Right click on this directory and click the ""Create 'All Tests'"" option that should be listed more than half-way down the list.
- Set _VM options_ to `-ea -noverify`
- Set _Working directory_ to `/absolute/path/to/commcare-android/app/`
- Set _Use classpath of module_ to *app*
- Click `OK` to finish creating the configuration.
- Select the ""commcare android test suite"" under the configuration drop down to the left of the green play button.
- Press the green play button to run the tests.

## Instrumentation Tests

The commcare-android repository uses [Espresso](https://developer.android.com/training/testing/espresso/) to write UI tests.
You need to have two keys in your `gradle.properties` before being able to run any instrumentation tests. **But make sure you never commit these keys to github.**
```
HQ_API_USERNAME=
HQ_API_PASSWORD=
```

### Run instrumentation-tests from the command-line

```bash
cd commcare-android
./gradlew connectedCommcareDebugAndroidTest
```

It's also a common requirement to run a particular test, such as when you’re fixing a bug or developing a new test. You can achieve the same in command-line using: 

```bash
./gradlew connectedCommcareDebugAndroidTest -Pandroid.testInstrumentationRunnerArguments.class=
```

You can view the results from the output file generated.

### Run instrumentation-tests from Android Studio

Before running tests from Android-Studio make sure you've disabled animations in your device. Note, this is only required when you're running tests from Android Studio 
```
Go to Setting -> Developer Options, and under the Drawing section, switch all of the following options:

Window animation scale -> off
Transition animation scale -> off
Animator duration scale -> off
```

Create a new Android Studio _Android Instrumented Test_ Build configuration using the following steps.

- Click _Run -> Edit Configruations_ and create a new _Android Instrumented Test_ configuration by pressing the green plus button.
- Set _Name_ to ""commcare android instrumentation tests""
- Set _Test kind_ to ""All in Package""
- set _Package_ to `org.commcare.androidTests`
- Click `OK` to finish creating the configuration.
- Select the ""commcare android instrumentation tests"" under the configuration drop down to the left of the green play button.
- Press the green play button to run the tests.

### Run instrumentation-tests skipped on browserstack

```bash
cd commcare-android
./gradlew connectedCommcareDebugAndroidTest -Pandroid.testInstrumentationRunnerArguments.notAnnotation=org.commcare.annotations.BrowserstackTests
```

### Code Style Settings

In order to comply with code style guidelines we follow, please use [Commcare Coding Style file](https://github.com/dimagi/commcare-android/blob/master/.android_studio_settings/codestyles/CommCare%20Coding%20Style.xml) and [Commcare Inspection Profile](https://github.com/dimagi/commcare-android/blob/master/.android_studio_settings/inspection/CommCare%20Inpsection%20Profile.xml) as your respective code style and inpection profile in Android Studio. To do so follow these instructions 

1. Copy the config files to your Android Studio installation as follows (Replace AndroidStudio3.0 with the respective directory for the AS version you are using) - 

```
cp .android_studio_settings/inspection/CommCare\ Inpsection\ Profile.xml ~/Library/Preferences/AndroidStudio3.0/inspection/.

cp .android_studio_settings/codestyles/CommCare\ Coding\ Style.xml ~/Library/Preferences/AndroidStudio3.0/codestyles/.  

```

2.  Restart Android Studio

3.  Go to AS preferences -> Editor -> Code Style  and select Scheme as 'Commcare Coding Style' and to AS preferences -> Editor -> Inspections and select Profile as 'Commcare Inspection Profile'

### Common Errors

#### If you experience the following exception when running individual tests from Android Studio Editor on Mac

```
No such manifest file: build/intermediates/bundles/debug/AndroidManifest.xml
```
If you are on a Mac, you will probably need to configure the default JUnit test runner configuration in order to work around a bug where IntelliJ / Android Studio does not set the working directory to the module being tested. This can be accomplished by editing the run configurations, Defaults -> JUnit and changing the working directory value to $MODULE_DIR$

#### Error on attempt to install CommCare app on phone: _Unknown failure during app install_

Android Monitor in Android Studio shows the following exceptions:
```
java.lang.RuntimeException: CommCare ran into an issue deserializing data while inflating type
    ...
Caused by: org.javarosa.core.util.externalizable.DeserializationException:
No datatype registered to serialization code [4b a9 e5 89]
```

Resolution:

- Disable _Instant Run_ found in Settings > Build, Execution, Deployment > Instant Run.
- Maybe also edit ~/.gradle/gradle.properties (may not exist) and add a line like `org.gradle.jvmargs=-Xmx1536M` if the build fails due to OOM or you see a message like the following during the build:

      To run dex in process, the Gradle daemon needs a larger heap.
      It currently has 1024 MB.
      For faster builds, increase the maximum heap size for the Gradle daemon to at least 1536 MB.
      To do this set org.gradle.jvmargs=-Xmx1536M in the project gradle.properties.

- Click _Run 'app'_ to rebuid and run on phone.
","'android', 'global-development', 'global-health', 'java', 'mhealth', 'mobile-data-collection', 'social-impact', 'xforms'",2024-05-03T13:33:27Z,26,35,29,"('philomates', 3884), ('amstone326', 2469), ('shubham1g5', 1713), ('ctsims', 1480), ('ShivamPokhriyal', 730), ('orangejenny', 634), ('avazirna', 325), ('sjain28', 159), ('rristovic', 52), ('dcluna', 40), ('dependabotbot', 26), ('kbo001', 24), ('lu16j', 20), ('biyeun', 20), ('dannyroberts', 18), ('Charl1996', 12), ('OrangeAndGreen', 4), ('millerdev', 3), ('benrudolph', 2), ('snopoke', 2), ('stephherbers', 2), ('amsagoff', 1), ('czue', 1), ('proteusvacuum', 1), ('emord', 1), ('NoahCarnahan', 1)","[3, 'Good Health and Well-Being']"
avniproject/avni-server,Backend APIs for Avni,"# avni-server


### Developer machine setup
See instructions available at https://avni.readme.io/docs/developer-environment-setup-ubuntu#server-side-components


## Build Status

[![CircleCI](https://dl.circleci.com/status-badge/img/gh/avniproject/avni-server/tree/master.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/gh/avniproject/avni-server/tree/master)
","'java', 'postgresql', 'spring-boot'",2024-05-03T11:26:30Z,28,5,14,"('petmongrels', 687), ('vindeolal', 596), ('vinayvenu', 388), ('himeshr', 369), ('mihirk', 368), ('hithacker', 292), ('1t5j0y', 201), ('sidtharthanan', 178), ('mahalakshme', 151), ('charl3sj', 107), ('sidsamanvay', 87), ('arjunk', 55), ('amolnirmalpsl', 40), ('amarkamthe', 31), ('swapnil106111', 26), ('anandkothariPSL', 25), ('sachsk', 23), ('vedfordev', 11), ('yenugukeerthana', 8), ('himeshr-egov', 7), ('rsatishm', 7), ('thundersparkf', 3), ('mohan-13', 2), ('nupoorkhandelwal', 2), ('Balamuruganjeevi', 1), ('drsureshreddyg', 1), ('riyaTw', 1), ('ak2502', 1)","[16, 'Peace, Justice and Strong Institutions']"
codeforamerica/brigade,The Code for America Brigade Website,"# The Code for America Brigade Website

Code for America Brigades are local volunteer groups that bring together community members to help make government work better. Brigades use technology to build new tools to help with local civic issues. Code for America supports Brigade chapters with resources, tools, and access to the wider civic technology movement.

This repo is for the Brigade website [https://brigade.codeforamerica.org/].

## Installation
The Code for America Brigade site is built on [Flask](http://flask.pocoo.org/) and Python with a little bit of Javascript. The `brigade/views.py` file describes the routes. The `brigade/templates` files have the HTML templates.

Set up a [Python virtual environment](https://github.com/codeforamerica/howto/blob/master/Python-Virtualenv.md).

```
python3 -m venv env
source env/bin/activate
```

Install the [required libraries](https://github.com/codeforamerica/howto/blob/master/Python-Virtualenv.md#install-packages).

```
pip install -r requirements.txt
```

### Install Node and frontend dependencies with npm

    brew install node
    npm install

During development, run webpack.

    ./node_modules/.bin/webpack --watch

Then run the server in debug mode:

    FLASK_ENV=development FLASK_APP=brigade.wsgi flask run

The server will be available at `http://localhost:5000/`.

or run it using [Honcho and the `Procfile`](https://github.com/codeforamerica/howto/blob/master/Procfile.md):

    honcho start

You can also run unit tests like this:

    python manage.py runtests

## Goals
This website is meant to:
* Explain what the Brigade program is
* Help people find their local Brigade
* Show off the fine works of the Brigades
* Provide tools that help Brigade work
* Make it easy to start a new Brigade

## Project Search
The [Project Search](http://www.codeforamerica.org/brigade/projects) page is a new service we built to search across thousands of civic technology projects. Go try it out, we think its pretty useful.

Read more at [README-Project-Search.md](README-Project-Search.md)

## History
The Brigade program started in 2012 as an experiment, largely copying the success of [Chi Hack Night](https://chihacknight.org/) (known at the time as [Open Gov Hack Night](https://web.archive.org/web/20150504114341/http://www.opengovhacknight.org/)).

This website is on its third version. V1 Was a Rails site with many contributors. It served the Brigade well as it was growing. As Code for America became better at supporting the volunteer groups, we needed something different.

The [CfAPI](http://github.com/codeforamerica/cfapi) was built as reaction to how Brigades were operating themselves. We now meet them where they are, instead of trying to get them to log into our site.

V2 was powered by the CfAPI and worked great, yet was built quickly with PHP and Javascript. It was kind of a cobweb of dependent parts.

V3, the current site, is meant to simplify the code and make it easier for Brigade members to get involved in building the Brigade site.


Contacts
--------
* Tom Dooner ([@tdooner](https://github.com/tdooner))

Copyright
---------
Copyright (c) 2015–2020 Code for America.
",,2023-03-01T17:38:59Z,30,100,108,"('ondrae', 384), ('tdooner', 372), ('fritzjooste', 262), ('hannahyoung', 141), ('rmcastil', 121), ('brielleplump', 117), ('tmaybe', 78), ('mick', 66), ('zsiec', 55), ('joelmahoney', 46), ('thedelchop', 23), ('moniquebt', 23), ('ryanatwork', 22), ('abhinemani', 20), ('milafrerichs', 17), ('hotgazpacho', 13), ('eunicekokor', 11), ('ycombinator', 9), ('migurski', 9), ('awesomizer', 8), ('sferik', 8), ('mapmeld', 8), ('noneck', 8), ('davecap', 6), ('brennandunn', 5), ('verythorough', 3), ('arielcharney', 3), ('ahelkit', 3), ('prestonrhea', 3), ('glassresistor', 3)","[16, 'Peace, Justice and Strong Institutions']"
get-alex/SublimeLinter-contrib-alex,SublimeLinter plugin for Alex,"# SublimeLinter-contrib-alex

![](screenshot.png)

This linter plugin for [SublimeLinter](https://github.com/SublimeLinter/SublimeLinter) provides an interface to [Alex](https://alexjs.com). It will be used with files that have the ""Plain text"" & ""Markdown"" syntax.


## Installation

SublimeLinter must be installed in order to use this plugin.

Please use [Package Control](https://packagecontrol.io) to install the linter plugin.

Before installing this plugin, you must ensure that `alex` is installed on your system:

```
$ npm install --global alex
```

In order for `alex` to be executed by SublimeLinter, you must ensure that its path is available to SublimeLinter. The docs cover [troubleshooting PATH configuration](https://sublimelinter.readthedocs.io/en/latest/troubleshooting.html#finding-a-linter-executable).


## Settings

- [SublimeLinter settings](https://sublimelinter.readthedocs.org/en/latest/settings.html)
- [Linter settings](https://sublimelinter.readthedocs.org/en/latest/linter_settings.html)


## License

MIT © [Sindre Sorhus](https://sindresorhus.com)
",,2018-10-05T16:20:21Z,1,58,10,"('sindresorhus', 5)","[11, 'Sustainable Cities and Communities']"
mysociety/alaveteli,Provide a Freedom of Information request system for your jurisdiction,"# Welcome to Alaveteli!

[![CI](https://img.shields.io/github/actions/workflow/status/mysociety/alaveteli/ci.yml?label=CI)](http://github.com/mysociety/alaveteli/actions?query=workflow%3ACI)
[![RuboCop](https://img.shields.io/github/actions/workflow/status/mysociety/alaveteli/rubocop.yml?label=RuboCop)](https://github.com/mysociety/alaveteli/actions?query=workflow%3ARuboCop)
[![Coverage Status](https://img.shields.io/coveralls/github/mysociety/alaveteli/develop)](https://coveralls.io/r/mysociety/alaveteli)
[![Code Climate](https://img.shields.io/codeclimate/maintainability-percentage/mysociety/alaveteli)](https://codeclimate.com/github/mysociety/alaveteli)
[![Installability: Gold](http://img.shields.io/badge/installability-gold-ffd700.svg ""Installability: Gold"")](http://mysociety.github.io/installation-standards.html)

This is an open source project to create a standard, internationalised
platform for making Freedom of Information (FOI) requests in different
countries around the world. The software started off life as
[WhatDoTheyKnow](https://www.whatdotheyknow.com), a website produced by
[mySociety](http://mysociety.org) for making FOI requests in the UK.

We hope that by joining forces between teams across the world, we can
all work together on producing the best possible software, and help
move towards a world where governments approach transparency as the
norm, rather than the exception.

Please join our [developers mailing list](https://groups.google.com/group/alaveteli-dev)
and introduce yourself, or drop a line to hello@alaveteli.org to let us know
that you're using Alaveteli.

There's lots of useful information and documentation (including a blog)
on [the project website](http://alaveteli.org). There's background
information and notes on [our
wiki](https://github.com/mysociety/alaveteli/wiki/Home/), and upgrade
notes in the [`doc/`
folder](https://github.com/mysociety/alaveteli/tree/master/doc/CHANGES.md)

## Installing

We've been working hard to make Alaveteli easy to install and re-use anywhere. Please
see [the project website](http://alaveteli.org) for instructions on installing Alaveteli.

## Compatibility

Every Alaveteli commit is tested by GitHub Actions on the [following Ruby platforms](https://github.com/mysociety/alaveteli/blob/develop/.github/workflows/ci.yml#L27-L29)

* ruby-3.0
* ruby-3.1
* ruby-3.2

If you use a ruby version management tool (such as RVM or .rbenv) and want to use the default development version used by the Alaveteli team (currently 3.0.4), you can create a `.ruby-version` symlink with a target of `.ruby-version.example` to switch to that automatically in the project directory.

## How to contribute

If you find what looks like a bug:

* Check the [GitHub issue tracker](http://github.com/mysociety/alaveteli/issues/)
  to see if anyone else has reported issue.
* If you don't see anything, create an issue with information on how to reproduce it.

If you want to contribute an enhancement or a fix:

* Fork the project on GitHub.
* Make a topic branch from the develop branch.
* Make your changes with tests.
* Commit the changes without making changes to any files that aren't related to your enhancement or fix.
* Send a pull request against the develop branch.

Looking for the latest stable release? It's on the
[master branch](https://github.com/mysociety/alaveteli/tree/master).

We have some more notes for developers [on the project site](http://alaveteli.org/docs/developers/).

## Examples

* [WhatDoTheyKnow](https://www.whatdotheyknow.com)
* [KiMitTud](http://kimittud.atlatszo.hu)
* [Informace Pro Všechny](http://www.infoprovsechny.cz)
* [fyi.org.nz](https://fyi.org.nz)

See more at [alaveteli.org](http://alaveteli.org/deployments/).

## Acknowledgements

Thanks to [Browserstack](https://www.browserstack.com/) who let us use their
web-based cross-browser testing tools for this project.

This product includes GeoLite data created by MaxMind, available from
http://www.maxmind.com.
","'access-to-information', 'alaveteli', 'freedom-of-information', 'right-to-know'",2024-05-01T11:33:46Z,30,383,35,"('garethrees', 4130), ('crowbot', 3408), ('gbp', 2960), ('lizconlan', 1345), ('sebbacon', 906), ('mlandauer', 530), ('dependabotbot', 471), ('henare', 465), ('robinhouston', 435), ('wrightmartin', 307), ('mhl', 295), ('alexander-griffen', 138), ('dependabot-previewbot', 48), ('zarino', 38), ('selishta', 34), ('dependabot-support', 30), ('dracos', 19), ('equivalentideas', 18), ('lucascumsille', 13), ('laurentS', 12), ('scjody', 9), ('wombleton', 9), ('sagepe', 8), ('hazelesque', 7), ('schlos', 7), ('chrismytton', 6), ('pcc', 6), ('andrewblack', 4), ('HelenWDTK', 4), ('petterreinholdtsen', 4)","[16, 'Peace, Justice and Strong Institutions']"
SEL-Columbia/formhub,Mobile Data Collection made easy.,"Formhub
=======
.. image:: https://api.travis-ci.org/SEL-Columbia/formhub.png?branch=master
  :target: https://travis-ci.org/SEL-Columbia/formhub

Getting Started
---------------

* To build and install from source, follow the `Installation Guide `_

* Alternatively, you can just use these pre-built server images instead:

  * The public Formhub Amazon Machine Image (AMI) to `Run Your Own Formhub Instances on Amazon Web Services (AWS) `_

  * The public Formhub Virtual Disk Image (VDI) to `Run Your Own Formhub Instances on VirtualBox `_

Contributing
------------

If you would like to contribute code please read
`Contributing Code to Formhub `_.

Code Structure
--------------

Formhub is written in `Python `_, using the `Django Web Framework `_. 

In Django terms, an ""app"" is a bundle of Django code, including models and views, that lives together in a single Python package and represents a full Django application.

Formhub consists of three Django apps:

* odk_logger - This app serves XForms to ODK Collect and receives
  submissions from ODK Collect. This is a stand alone application.

* odk_viewer - This app provides a
  csv and xls export of the data stored in odk_logger. This app uses a
  data dictionary as produced by pyxform. It also provides a map and
  single survey view.

* main - This app is the glue that brings odk_logger and odk_viewer
  together.

Internationalization and Localization
-------------------------------------

Formhub can be presented in specific languages and formats, customized for specific audiences.

These examples were derived from `Django's Internationalization and Localization Documentation `_ and there is also a good explanation in `The Django Book's Chapter on Internationalization `_.

To generate a locale from scratch, e.g. Spanish:

.. code-block:: sh

    $ django-admin.py makemessages -l es -e py,html,email,txt ;
    $ for app in {main,odk_viewer} ; do cd ${app} && django-admin.py makemessages -d djangojs -l es && cd - ; done

To update PO files

.. code-block:: sh

    $ django-admin.py makemessages -a ;
    $ for app in {main,odk_viewer} ; do cd ${app} && django-admin.py makemessages -d djangojs -a && cd - ; done

To compile MO files and update live translations

.. code-block:: sh

    $ django-admin.py compilemessages ;
    $ for app in {main,odk_viewer} ; do cd ${app} && django-admin.py compilemessages && cd - ; done

",,2015-07-30T13:40:39Z,25,259,49,"('ukanga', 1096), ('larryweya', 1013), ('dorey', 1011), ('amarder', 815), ('mejymejy', 281), ('prabhasp', 157), ('rgaudin', 150), ('modilabs-bumblebee', 108), ('prajjwol', 101), ('TomCoder', 84), ('vr2262', 78), ('mberg', 75), ('katembu', 46), ('modilabs-starscream', 44), ('Topol', 21), ('codcrazi', 18), ('myf', 18), ('dpapathanasiou', 11), ('jmwohl', 4), ('iwillig', 2), ('jeluxama', 2), ('kiorky', 2), ('Snkz', 1), ('rubenarslan', 1), ('simod', 1)","[4, 'Quality Education']"
chaynHQ/soulmedicine,Soul Medicine a multilingual digital service designed to deliver critical safety information and supportive messaging in bite-sized pieces.,"# Soul Medicine

[![CI](https://github.com/chaynHQ/soulmedicine/actions/workflows/ci.yml/badge.svg)](https://github.com/chaynHQ/soulmedicine/actions/workflows/ci.yml)

Through notes of love and information, Soul Medicine offers bite-sized support to survivors, and an easy system to find meaningful support. This service has resources on opening a bank account, managing money, regaining confidence after trauma, coping with stress, and practicing assertive communication. It also contains the written [Bloom](https://bloom.chayn.co/) courses. Soul Medicine gives users the option to follow their own pathway and receive disguised safeguarded emails at a specific time.

**Currently in active development**

## Development and Testing

- See our [documentation](docs/README.md) if you're interested in developing or testing this application.
- To make a contribution, please read our Contributing Guidelines in [CONTRIBUTING.md](CONTRIBUTING.md).

# Get Involved

If you would like to help Chayn and receive special access to our organization and volunteer opportunities, please [visit our Getting Involved guide](https://www.chayn.co/get-involved). We'll get back to you to schedule an onboarding call. Other ways to get involved and support us are [donating](https://www.paypal.me/chaynhq), starring this repo and making an open-source contribution here on GitHub, and supporting us on social media! 

Stay up to date with Chayn:

- Website - [Chayn](https://www.chayn.co/)
- Twitter - [@ChaynHQ](https://twitter.com/ChaynHQ)
- Instagram - [@chaynhq](https://www.instagram.com/chaynhq/)
- Youtube - [Chayn Team](https://www.youtube.com/channel/UC5_1Ci2SWVjmbeH8_USm-Bg)
- LinkedIn - [@chayn](https://www.linkedin.com/company/chayn)

**Thank you for your interest in SoulMedicine! 🙏**
","'dpg', 'opensourceforgood', 'public-goods', 'ruby', 'sdg'",2024-05-02T21:04:50Z,14,30,7,"('sophieheb', 483), ('jits', 436), ('tarebyte', 271), ('dependabotbot', 46), ('kyleecodes', 18), ('issyl0', 9), ('fahadiqbal1', 6), ('eleanorreem', 3), ('annarhughes', 2), ('swetha-charles', 2), ('ATREAY', 1), ('seanmarcia', 1), ('snyk-bot', 1), ('aybeedee', 1)","[5, 'Gender Equality']"
dimagi/commcare-hq,"CommCareHQ is the server backend for CommCare, the world's largest platform for designing, managing, and deploying robust, offline-first, mobile applications to frontline workers worldwide","CommCare HQ
===========

CommCare HQ is a web application for building complex, customizable, frontline worker solutions.

It includes multi-tenant application building, user management, data collection, and reporting.

CommCare HQ apps work with [CommCare mobile](https://github.com/dimagi/commcare-android) and its
bundled [web application platform](https://github.com/dimagi/formplayer).

### Key Components

+ CommCare application builder
+ OpenRosa compliant XForms designer
+ SMS integration
+ Domain/user/mobile worker management
+ XForms data collection
+ Case management
+ Over-the-air (ota) restore of user and cases
+ Integrated web and email reporting

### More Information

+ To try CommCare you can use [this production instance of hosted CommCare](https://www.commcarehq.org/).
+ For setting up a local CommCare HQ developer environment, see [Setting up CommCare HQ for Developers](https://github.com/dimagi/commcare-hq/blob/master/DEV_SETUP.md).
+ For setting up a production CommCare HQ environment, check out [CommCare Cloud](https://commcare-cloud.readthedocs.io/), our toolkit for deploying and maintaining CommCare servers.
+ Additional documentation is available on [ReadTheDocs](https://commcare-hq.readthedocs.io/).
+ We welcome contributions, see [Contributing](CONTRIBUTING.rst) for more.
+ Questions?  Contact the CommCare community at our [forum](https://forum.dimagi.com/).

*CommCare is built by [Dimagi](https://dimagi.com/) and our open-source contributors.*
",'commcare-hq',2024-05-03T15:32:12Z,144,481,47,"('czue', 17291), ('orangejenny', 16346), ('snopoke', 14987), ('dannyroberts', 11929), ('biyeun', 11323), ('esoergel', 7721), ('emord', 6967), ('millerdev', 5787), ('mkangia', 4998), ('kaapstorm', 4817), ('sravfeyn', 4455), ('proteusvacuum', 4007), ('calellowitz', 3651), ('NoahCarnahan', 3576), ('gherceg', 2562), ('AmitPhulera', 2361), ('dmyung', 1970), ('yedi', 1839), ('benrudolph', 1797), ('Rohit25negi', 1651), ('twymer', 1449), ('nickpell', 1371), ('stephherbers', 1163), ('mwhite', 1086), ('zandre-eng', 1022), ('Charl1996', 979), ('Jtang-1', 866), ('mjriley', 854), ('gbova', 773), ('pr33thi', 727), ('niryariv', 692), ('joxl', 658), ('challabeehyv', 645), ('minhaminha', 600), ('AddisonDunn', 561), ('jmtroth0', 556), ('MartinRiese', 537), ('nospame', 514), ('ShubhamBansal1997', 508), ('jingcheng16', 494), ('kennknowles', 455), ('saiprakash-v', 410), ('TylerSheffels', 402), ('rowenaluk', 400), ('Robert-Costello', 397), ('akashkj', 286), ('dependabotbot', 275), ('SmittieC', 267), ('ctsims', 254), ('agrzywinski', 251), ('RynhardPietersen', 237), ('lwyszomi', 230), ('pxwxnvermx', 225), ('msienkiewicz', 198), ('rickypchen', 182), ('marissahrrsn', 146), ('ternus', 136), ('adil-uddin', 131), ('Sktank', 109), ('ajeety4', 102), ('dominicochoa-dimagi', 101), ('katembu', 96), ('solleks', 82), ('cws1121', 81), ('shubham1g5', 68), ('amstone326', 59), ('kmtracey', 55), ('michelleliang1', 55), ('dhiraj-beehyv', 54), ('skodde', 53), ('mrgriscom', 53), ('kmbrgandhi', 52), ('charlesfleche', 51), ('AliRizvi1', 35), ('mchampanis', 33), ('xbryanc', 33), ('dmydlo', 32), ('pawelreise', 30), ('dchukhin', 27), ('dborowiecki', 25), ('andyasne', 24), ('ShivamPokhriyal', 24), ('bagnolil', 23), ('sudh-kvantum', 22), ('knguyen142', 19), ('amsagoff', 18), ('javierwilson', 18), ('awalkowiak', 16), ('LambdaLearner', 16), ('shyamkumarlchauhan', 15), ('MaciejChoromanski', 15), ('mkaczmarczyksoldevelo', 15), ('satyaakam', 15), ('SunainaWalia', 14), ('dslowikowski', 13), ('philomates', 12), ('jnoraky', 12), ('tsinkala', 11), ('craigappl', 10), ('njuki', 9), ('jc00ke', 8), ('karmuno', 5), ('dszafranek', 5), ('theism', 5), ('NitigyaS', 5), ('knittingarch', 5), ('pyup-bot', 5), ('jjackson', 4), ('fyniac', 3), ('fazledyn-or', 3), ('christineawhite', 3), ('robin824', 2), ('prem-fissionhq', 2), ('modonnell729', 2), ('kcowger', 2), ('idiene', 2), ('dimagilg', 2), ('akshita-sh', 2), ('vladimiroff', 2), ('krsdimagi', 2), ('dankohn', 2), ('balajirags', 1), ('bderenzi', 1), ('dependabot-previewbot', 1), ('dlanedimagi', 1), ('omari-funzone', 1), ('rigambhir', 1), ('roboton', 1), ('sophiamcg', 1), ('stellastjarna', 1), ('sujayravikumar', 1), ('scottwedge', 1), ('zbidi', 1), ('akyogi', 1), ('vishal-beehyv', 1), ('ukanga', 1), ('gitter-badger', 1), ('Sim4n6', 1), ('piyushmadan', 1), ('markstory', 1), ('yuriiz', 1), ('greggles', 1), ('braxton-p', 1), ('benjaoming', 1)","[3, 'Good Health and Well-Being']"
ESIPFed/sweet,Official repository for Semantic Web for Earth and Environmental Terminology (SWEET) Ontologies,"# SWEET Ontologies

[![Chat on Slack](https://img.shields.io/badge/chat-on%20slack-ff69b4.svg)](https://esip-all.slack.com/) on channel #sweetontology 



# Introduction
Official repository for Semantic Web for Earth and Environmental Terminology (SWEET) Ontologies.

## Accessing SWEET (base ontology)
This table provides links to access the base SWEET ontology, which contains import directives to bring in all the other ontologies. 
If you want to browse the entire content SWEET, see the next section below.

  
    
      View in pyLODE
      View in WebVowl
      View in ESIP COR
    
    
      View in GitHub
      View raw RDF
      Download RDF
    
  


## Accessing SWEET (all content)

The base URL for SWEET is [http://sweetontology.net/sweetAll](http://sweetontology.net/sweetAll). You can use this URL in any
OWL-compliant tool, including applications such as Protégé or TopBraid as well as ontology tools such as ROBOT.



  
    
      SWEET in BioPortal
      View all classes
      Download RDF
    
  


# What is SWEET?
SWEET is a highly modular ontology suite with ~6000 concepts in ~200 separate ontologies covering Earth system science. SWEET is a mid-level ontology and consists of nine top-level concepts that can be used as a foundation for domain-specific ontologies that extend these top-level SWEET components. SWEET’s own domain-specific ontologies, which extend the upper level ontologies, can provide users interested in further developing a particular domain with a solid set of concepts to get started. SWEET ontologies are written in [W3C Turtle](https://www.w3.org/TR/turtle/); the Terse RDF Triple Language.

# SWEET as Linked Data
Details of how HTTP requests against http://sweetontology.net are processed can be found [here](https://github.com/ESIPFed/sweet/wiki/sweetontology.net).

# SWEET IRI Patterns for Ontologies and their Terms
See the relevant [wiki documentation](https://github.com/ESIPFed/sweet/wiki/SWEET-IRI-Patterns-for-Ontologies-and-their-Terms).

# Community
SWEET is governed by the [ESIP Semantic Technologies Committee](http://wiki.esipfed.org/index.php/Semantic_Technologies) (STC) meaning that all proposed changes are evaluated by a number of subject matter experts. If you would like to learn more about SWEET, or are interested in joining the community, please join our [community mailing list](https://lists.esipfed.org/mailman/listinfo/esip-semantictech).

# Recognition of SWEET Contributors
[Ensure you are recognized for your contributions](https://github.com/ESIPFed/sweet/wiki/Recognition-of-SWEET-Contributors).

# Development
Please see the [CONTRIBUTING documentation](https://github.com/ESIPFed/sweet/blob/master/CONTRIBUTING.md).

Additionally, if you wish to discuss SWEET issues with the STC, please contact us via the [WG email list](http://lists.esipfed.org/mailman/listinfo/esip-semantictech). 

# Using local copies of ontology
The sweetall.ttl ontology imports all the other sweet components via URL. If you are offline, or working on updates that require using the local copies of the ontology files, copy the catalog-v001.xml file from the root directory of the repository into the src directory before opening sweetAll.ttl in Protege. 

# License/Copyright
SWEET is available under the [CC0 1.0 Universal Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).

  <a rel=""license""
     href=""http://creativecommons.org/publicdomain/zero/1.0/"">
    
  
  
  To the extent possible under law,
  <a rel=""dcterms:publisher""
     href=""https://github.com/ESIPFed/sweet"">
    SWEET Ontology Developers
  have waived all copyright and related or neighboring rights to the
  Semantic Web for Earth and Environmental Terminology (SWEET) Ontology Suite.

A copy of the CC0-1.0 ships with this repository.

Prior to SWEET 3.5.0, SWEET was licensed under the Apache License v2. For more information on the change, see [here](https://github.com/ESIPFed/sweet/issues/173).
","'earth', 'earth-science', 'environment', 'environment-variables', 'ontologies', 'owl', 'owl-ontology', 'sweet-ontologies'",2024-01-14T23:17:00Z,16,108,33,"('brandonnodnarb', 134), ('lewismc', 121), ('smrgeoinfo', 11), ('carueda', 6), ('graybeal', 6), ('pbuttigieg', 4), ('rduerr', 4), ('charlesvardeman', 3), ('jmkeil', 3), ('ashepherd', 2), ('esip', 2), ('ignazio1977', 2), ('cmungall', 2), ('esip-lab', 1), ('bradh', 1), ('r0sek', 1)","[13, 'Climate Action']"
opencrvs/opencrvs-core,A global solution to civil registration," 

OpenCRVS
 A digital public good for civil registration

Report an issue  ·  Join our community  ·  Read our documentation  ·  www.opencrvs.org

[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0)





- [OpenCRVS](#opencrvs)
- [Important! Please read](#important-please-read)
- [Become part of the OpenCRVS Community](#become-part-of-the-opencrvs-community)
- [License](#license)



## OpenCRVS

[OpenCRVS](https://www.opencrvs.org/) (Civil registration & Vital Statistics) is a digital public good to help achieve universal civil registration and evidence-based decision making in all country contexts.

> **We are on a mission to ensure that every individual on the planet is recognised, protected and provided for from birth.**



## Important! Please read

To proceed, refer to our [documentation](http://documentation.opencrvs.org). It contains all the information you need.

## Become part of the OpenCRVS Community

We want to see OpenCRVS implemented across the world. We can’t do this alone. Through the OpenCRVS Community, we are uniting experts in civil registration and other interested parties.

[Visit our website](https://www.opencrvs.org)

[Join our community](https://community.opencrvs.org)

We greatly appreciate any contributions to opencrvs-core

Raise an issue or join our community to ask for a new feature

1. Raise an issue
2. Involve yourself in design discussions
3. Co-create development tasks with our team
4. Fork the repo
5. Create a branch with your changes
6. Submit a PR
7. Address any comments the core contributors may have
8. The core contributors will merge the code once it is ready, thanks for you contribution to ensure every individual on the planet is recognised, protected and provided for from birth!

By contributing to the OpenCRVS code, you are conforming to the terms of the [license](https://www.opencrvs.org/license).



## License

This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at https://mozilla.org/MPL/2.0/.

OpenCRVS is also distributed under the terms of the Civil Registration & Healthcare Disclaimer located at http://opencrvs.org/license.

Copyright (C) The OpenCRVS Authors located at https://github.com/opencrvs/opencrvs-core/blob/master/AUTHORS.
",,2024-05-03T15:35:43Z,47,77,18,"('euanmillar', 2834), ('mushrafulhoque-dsi', 1766), ('rikukissa', 1398), ('tahmidrahman-dsi', 1193), ('Zangetsu101', 982), ('rcrichton', 968), ('yeasinhossain-dsi', 889), ('Sadman-Ilham', 736), ('sahriartoufiq', 697), ('sadmananik', 508), ('atiqzaman-dsi', 471), ('tofaelahmed-dsi', 264), ('mgorabbani', 254), ('rabiulislamanik', 209), ('emon-swe-sust', 202), ('Nil20', 181), ('naftis', 171), ('renovatebot', 142), ('armanbhuiyan-dsi', 138), ('jpye-finch', 115), ('dependabotbot', 79), ('soumita-dsi', 71), ('maacpiash', 70), ('nsamadavid', 69), ('nafistiham', 64), ('wilrona', 61), ('kayumuzzaman', 59), ('Eezi', 56), ('modupeadeonojobi', 52), ('sifulovi', 47), ('afrida67', 34), ('bausmeier', 30), ('jamil314', 29), ('youshamahmood96', 27), ('CornelMaze', 27), ('unixdev', 22), ('favour234', 12), ('septlowe', 8), ('KaliGreen', 6), ('anamulhaquemollah', 4), ('n1koo', 4), ('dependabot-previewbot', 4), ('adrienramiliharivelo', 3), ('eduffus', 2), ('tertek', 2), ('madewulf', 1), ('davidNsama', 1)","[17, 'Partnerships for the Goals']"
team-asr/od-Data,"Repository for Open Drone Data, including aerial imagery and analytics","# od-Data
Repository for Open Drone Data, including aerial imagery and analytics

![alt tag](https://user-images.githubusercontent.com/1425839/33735588-c4386eca-dba0-11e7-8c2f-b987f33ba90a.png)

od-Data repo hosts aerial imagery and associated artifacts including:
a. raw aerial images
b. Processed images.
c. Analytics of the images e.g. vegetation indecies, terrain maps, etc
d. Flight plans and flight maps

## Usage
To use the data herein, you need photogrametry software, such as the OpenDrone Map application suite, : https://github.com/team-asr/WebODM. Working knowledge of photogrametry software is required,
and requires at the least basic skills in GIS.
We've included a toolkit to extract image metadata, in the folder ImageMetadata (https://github.com/team-asr/od-Data/tree/master/ImageMetadata). Visit the folder to find
invocation instructions. The toolkit is developed using the Java programming language and a working knowledge of the language isn't required.

We've been testing out algorithms that can be invoked independently with python installed, on the folder algorithms (https://github.com/team-asr/od-Data/tree/master/algorithms). Invocation instructions
can be found therein. A working knowledge of python is assumed.

## Note
Some of the data will become available as they are uploaded by the team.

A link is being created to facilitate updates from the main server directly available to the ASR website, as these are done immediately as missions occur. 

## Support/ Engagement

We'd love to hear from you. You can post a comment or request for additional information using team-asr(at)as-research.org or visit www.as-research.org and use the contacts page.

## Licensing Terms:
Work and material herein is released under a GNU General Public License, and a Creative Commons Attribution 4.0 (CC-BY-4.0) License. Copies of license terms can be found here: https://www.gnu.org/licenses/gpl.html and https://creativecommons.org/licenses/by/4.0/

## Data Standards
Aerial Images are stored following JPEG standard where the data is in its raw form. TIFF Standard is used in some processed data

## Data Privacy and Security
During aerial missions we discuss the areas under investigation with local authorities for notification of the general public that areas identified for aerial survery will be overflown. This is then broadcasted in the local area radio station and license sought through the relevant Civil Aviation Authority for permission. All areas under investigation are solely properties of farmers who have been requested permission for overflight of their farm, and consent forms duly signed and recorded prior overflight.
For more information on our security and data privacy policy, please logon to https://as-research.org/data-and-privacy/

",,2022-12-08T01:28:30Z,1,1,2,"('gichangA', 25)","[2, 'Zero Hunger']"
JustFixNYC/tenants2,The JustFix tenant platform!,"[![CircleCI](https://circleci.com/gh/JustFixNYC/tenants2.svg?style=svg)](https://circleci.com/gh/JustFixNYC/tenants2)
[![Maintainability](https://api.codeclimate.com/v1/badges/de475123649c132f858b/maintainability)](https://codeclimate.com/github/JustFixNYC/tenants2/maintainability)
[![Test Coverage](https://api.codeclimate.com/v1/badges/de475123649c132f858b/test_coverage)](https://codeclimate.com/github/JustFixNYC/tenants2/test_coverage)

This is the JustFix Tenant Platform.

In addition to this README, please feel free to consult the
[project wiki][], which contains details on the project's
principles and architecture, development tips, and more.

[project wiki]: https://github.com/JustFixNYC/tenants2/wiki

## Getting started

**Note**: It's highly recommended you follow the
[Developing with Docker](#developing-with-docker) instructions, as it
makes development much easier. But if you'd really rather set
everything up without Docker, read on!

You'll need Python 3.8.2 and [pipenv][], as well as Node 12, yarn, and
[Git Large File Storage (LFS)][git-lfs]. You will also need to
set up Postgres version 10 or later, and it will need the PostGIS
extension installed.

If you didn't have Git LFS installed before cloning the repository,
you can obtain the repository's large files by running `git lfs pull`.

First create an environment file and optionally edit it as you
see fit:

```
cp .justfix-env.sample .justfix-env
```

Since you're not using Docker, you will want to set `DATABASE_URL`
to your Postgres instance, using the [Database URL schema][].

Then set up the front-end and configure it to
continuously re-build itself as you change the source code:

```
yarn
yarn start
```

Then, in a separate terminal, you'll want to instantiate
your Python virtual environment and enter it:

```
pipenv install --dev --python 3.8
pipenv shell
```

(Note that if you're on Windows and have `bash`, you
might want to run `pipenv run bash` instead of
`pipenv shell`, to avoid a bug whereby command-line
history doesn't work with `cmd.exe`.)

Then start the app:

```
python manage.py migrate
python manage.py runserver
```

Then visit http://localhost:8000/ in your browser.

[Database URL schema]: https://github.com/kennethreitz/dj-database-url#url-schema

### Creating an Admin User

You'll want to create an admin user account to access the App's ""Admin Site."" Django has this functionality preset, so just navigate to the root directory and use the Django command for creating a super user:

```
python manage.py createsuperuser
```

The following prompts on your terminal window will set up the account for you. Once created, visit http://localhost:8000/admin and log in with your new credentials to access the Admin Site.


### Production dependencies

Some of this project's dependencies are cumbersome
to install on some platforms, so they're not installed
by default.

However, they are present in the Docker development
environment (described below), and they are
required to develop some functionality, as well as
for production deployment. They can be installed via:

```
pipenv run pip install -r requirements.production.txt
```

These dependencies are described below.

#### WeasyPrint

[WeasyPrint][] is used for PDF generation. If it's
not installed during development, then any PDF-related
functionality will fail.

Instructions for installing it can be found on the
[WeasyPrint installation docs][].

[WeasyPrint]: http://weasyprint.org/
[WeasyPrint installation docs]: https://weasyprint.readthedocs.io/en/latest/install.html

## Running tests

To run the back-end Python/Django tests, use:

```
pytest
```

To run the front-end Node/TypeScript tests, use:

```
yarn test
```

You can also use `yarn test:watch` to have Jest
continuously watch the front-end tests for changes and
re-run them as needed.

## Prettier

We use [Prettier][] to automatically format some of our non-Python code.
Before committing or pushing to GitHub, you may want to run the following
to ensure that any files you've changed are properly formatted:

```
yarn prettier:fix
```

Note that if you don't either use this or some kind of editor plug-in
before pushing to GitHub, continuous integration will fail.

[Prettier]: https://prettier.io

## Black

[Black][] is a formatting tool similar to Prettier, but for Python code.

Before committing or pushing to GitHub, you may want to run the following
to ensure that any files you've changed are properly formatted:

```
black .
```

Note that if you don't either use this or some kind of editor plug-in
before pushing to GitHub, continuous integration will fail.

[Black]: https://black.readthedocs.io/

## Environment variables

For help on environment variables related to the
Django app, run:

```
python manage.py envhelp
```

Alternatively, you can examine
[project/justfix_environment.py](project/justfix_environment.py).

For the Node front-end:

* `NODE_ENV`, can be set to `production` for production or any
  other value for development.
* See [frontend/webpack/webpack-defined-globals.d.ts](frontend/webpack/webpack-defined-globals.d.ts) for more values.

## Common data

Some data that is shared between the front-end and back-end is
in the [`common-data/`](common-data/) directory.  The
back-end generally reads this data in JSON format, while the
front-end reads a TypeScript file that is generated from
the JSON.

A utility called `commondatabuilder` is used to generate the
TypeScript file. To execute it, run:

```
node commondatabuilder.js
```

You will need to run this whenever you make any changes to
the underlying JSON files.

If you need to add a new common data file, see
[`common-data/config.ts`](common-data/config.ts), which
defines how the conversion from JSON to TypeScript occurs.

## GraphQL

The communication between server and client occurs via [GraphQL][]
and has been structured for type safety. This means that we'll
get notified if there's ever a mismatch between the server's
schema and the queries the client is generating.

[GraphQL]: https://graphql.org/

### Interactive environment (GraphiQL)

To manually experiment with GraphQL queries, use the interactive in-browser
environment called **GraphiQL**, which is built-in to the development
server.  It can be accessed via the ""Developer"" menu at the top-right of
almost any page on the site, or directly at `http://localhost:8000/graphiql`.

### Server-side GraphQL schema

The server uses [Graphene-Django][] for its GraphQL needs. It also
uses a custom ""schema registry"" to make it easier to define new
fields and mutations on the schema; see
[`project/schema_registry.py`](project/schema_registry.py) for
documentation on how to use it.

The JSON representation of the schema is in `schema.json` and
is automatically regenerated by the development server,
though developers can manually regenerate it via
`python manage.py graphql_schema` if needed.

[Graphene-Django]: http://docs.graphene-python.org/projects/django/en/latest/

### Client-side GraphQL queries

Client-side GraphQL code is generated as follows:

1. Raw queries are in `frontend/lib/queries/` and given a `.graphql`
   extension.  Currently, they must consist of **one** query,
   mutation, or fragment that has the same name as the base name of the file.
   For instance, if the file is called `SimpleQuery.graphql`,
   then the contained query should be called `SimpleQuery`, e.g.:

    ```graphql
    query SimpleQuery($thing: String) {
        hello(thing: $thing)
    }
    ```

2. Some GraphQL queries are automatically generated based on
   the configuration in
   [`frontend/lib/queries/autogen-config.toml`](frontend/lib/queries/autogen-config.toml).

3. The querybuilder, which runs as part of `yarn start`, will notice
   changes to any of these raw queries *or* `autogen-config.toml`
   *or* the server's `schema.json`, and do the following:

    1. It automatically generates any GraphQL queries that need
       generating.

    2. It runs [Apollo Code Generation][] to validate the raw queries
       against the server's GraphQL schema and create TypeScript
       interfaces for them.

    3. For queries and mutations, it adds a function to the TypeScript
       interfaces that is responsible for performing the query in a
       type-safe way.
       
    4. The resultant TypeScript interfaces and/or function is written
       to a file that is created next to the original `.graphql` file
       (e.g., `SimpleQuery.ts`).

    If the developer prefers not to rely on `yarn start`
    to automatically rebuild queries for them, they can also manually
    run `node querybuilder.js`.

At this point the developer can import the final TS file and use the query.

[Apollo Code Generation]: https://github.com/apollographql/apollo-cli#code-generation

## Developing with Docker

You can alternatively develop the app via [Docker][], which
means you don't have to install any dependencies. However,
Docker takes a bit of time to learn how to use.

After you install Docker on your machine, open up `Settings(gear icon) > Resources > Advanced`
and make sure you give it at least 8 GB of memory to play with.
If you don't, you might get an out-of-memory error when attempting
to build and/or run the Docker image.

You'll also need [Git Large File Storage (LFS)][git-lfs].
On a Mac with Homebrew, that's
```
brew install git-lfs
git lfs install
```
If you didn't have Git LFS installed before cloning the repository,
you can obtain the repository's large files by running `git lfs pull`.

As with the non-Docker setup, you'll first want to create an environment
file and optionally edit it as you see fit:

```
cp .justfix-env.sample .justfix-env
```

Then, to set everything up, run:

```
bash docker-update.sh
```

Then run:

```
docker-compose up
```

This will start up all services and you can visit
http://localhost:8000/ to visit the app.

[Docker]: https://docs.docker.com/install/

### Updating the containers

Whenever you update your repository via e.g. `git pull` or
`git checkout`, you should update your containers by running:

```
bash docker-update.sh
```

### Starting over

If your Docker setup appears to be in an irredeemable state
and `bash docker-update.sh` doesn't fix it--or
if you just want to free up extra disk space used up by
the app--you can destroy everything by running:

```
docker-compose down -v
```

Note that this may delete all the data in your
instance's database.

At this point you can re-run `bash docker-update.sh` to set
everything up again.

### Accessing the app container

To access the app container, run:

```sh
docker-compose run app bash
```

This will run an interactive bash session inside the main app container.
In this container, the `/tenants2` directory is mapped to the root of
the repository on your host; you can run any command, like `python manage.py`
or `pytest`, from there. Specifically, within this bash session is where you can [create an Admin User](#creating-an-admin-user) to access the App's Admin Site. 


## Changing the Dockerfile

Development, production, and our continuous integration
pipeline ([CircleCI][]) use a built image from the
`Dockerfile` on Docker Hub as their base to ensure
[dev/prod parity][].

Changes to `Dockerfile` should be pretty infrequent, as
they define the lowest level of our application's software
stack, such as its Linux distribution. However, changes
do occasionally need to be made.

Whenever you change the `Dockerfile`, you will need to
push the new version to Docker Hub and change the
tag in a few files to correspond to the new version you've pushed.

To push your new version, you will need to:

1. Come up with a unique tag name; preferably one that isn't
   [already taken][].  (While you can use an existing one, it's
   recommended that you create a new one so that other pull
   requests using the existing one don't break.)

   For the rest of these instructions we'll assume your new
   tag is called `0.1`.

2. Run `docker build -t justfixnyc/tenants2_base:0.1 .`
   to build the new image.

3. Run `docker push justfixnyc/tenants2_base:0.1` to
   push the new image to Docker Hub.

4. In `Dockerfile.web`, `docker-services.yml`, `.circleci/config.yml`,
   and `.devcontainer/Dockerfile`, edit the references to
   `justfixnyc/tenants2_base` to point to the new tag.

[CircleCI]: https://circleci.com/
[already taken]: https://hub.docker.com/r/justfixnyc/tenants2_base/tags/

## Deployment

### **To deploy on Justfix architecture**
See the wiki section on [Deployment](https://github.com/JustFixNYC/tenants2/wiki/Deployment).

### **To deploy outside of Justfix's deployment architecture**
The app uses the [twelve-factor methodology][], so
deploying it should be relatively straightforward.

At the time of this writing, however, the app's
runtime environment does need *both* Python and Node
to execute properly, which could complicate matters.

A Python 3 script, `deploy.py`, is located in the
repository's root directory and can assist with
deployment. It has no dependencies other than
Python 3.

#### **Deploying to Heroku via Docker**

It's possible to deploy to Heroku using their
[Container Registry and Runtime][].  To build
and push the container to their registry, run:

```
python3 deploy.py heroku
```

You'll likely want to use [Heroku Postgres][] as your
ndatabase backend.

## Internationalization

This project uses the [PO file format][] to store most of its
localization data in the [`locales`](locales/) directory.

[PO file format]: https://www.gnu.org/software/gettext/manual/html_node/PO-Files.html

### Back-end internationalization

The back-end uses the [Django translation framework][] for internationalization.
To extract messages for localization, run:

```
yarn django:makemessages
```

One `.po` files have been updated, the catalogs can be compiled with:

```
yarn django:compilemessages
```

[Django translation framework]: https://docs.djangoproject.com/en/3.0/topics/i18n/translation/

### Front-end internationalization

The front-end uses [Lingui][] for internationalization. To extract
messages for localization, run:

```
yarn lingui:extract
```

Once `.po` files have been updated, the catalogs can be compiled to JS
with:

```
yarn lingui:compile
```

[Lingui]: https://lingui.js.org/

### Garbling message catalogs

When internationalizing a piece of code, it can be difficult to
tell if one missed any strings because of how our internationalization
frameworks fall back to English when our strings haven't been translated
yet.

To compensate for this, we provide a way to ""garble"" message catalogs with
nonsense localizations, which makes it easier to tell whether all our strings
have been properly internationalized.  When enabled, it looks like this:

> 

To activate the garbling, run:

```
yarn l10n:garble
```

**Be careful about making commits while the message catalogs are garbled!**
Because garbling changes the actual `.po` files, and because those files
are version-controlled, any commits you make while garbling is active may
accidentally commit garbled message catalogs.

When you're done using the garbled codebase, you can un-garble the message
catalogs by running:

```
yarn l10n:ungarble
```

## Optional integrations

The codebase has a number of optional integations with third-party services
and data sources. Run `python manage.py envhelp` for a listing of all
environment variables related to them.

### NYCHA offices

You can load all the NYCHA offices into the database via:

```
python manage.py loadnycha nycha/data/Block-and-Lot-Guide-08272018.csv
```

Once imported, any users from NYCHA who file a letter of complaint will
automatically have their landlord address populated.

Note that the CSV loaded by this command was originally generated by
the [JustFixNYC/nycha-scraper](https://github.com/JustFixNYC/nycha-scraper)
tool. It can be re-used to create new CSV files that may be more up-to-date
than the one in this repository.

### NYC geographic regions

The tenant assistance directory, known within the project as `findhelp`, needs
shapefiles of New York City geographic regions to allow staff to define
the catchment areas of tenant resources. These shapefiles can be loaded via
the following command:

```
python manage.py loadfindhelpdata
```

The shapefile data is stored within the repository using Git LFS
and has the following provenance:

* `findhelp/data/ZIP_CODE_040114` - https://data.cityofnewyork.us/Business/Zip-Code-Boundaries/i8iw-xf4u
* `findhelp/data/Borough-Boundaries.geojson` - https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm
* `findhelp/data/Community-Districts.geojson` - https://data.cityofnewyork.us/City-Government/Community-Districts/yfnk-k7r4
* `findhelp/data/ZillowNeighborhoods-NY` - https://www.zillow.com/howto/api/neighborhood-boundaries.htm
* `findhelp/data/nys_counties.geojson` - http://gis.ny.gov/gisdata/inventories/details.cfm?DSID=927 (reprojected into the WGS 84 CRS and converted to GeoJson via QGIS)

[pipenv]: https://docs.pipenv.org/
[git-lfs]: https://git-lfs.github.com/
[twelve-factor methodology]: https://12factor.net/
[multiple buildpacks]: https://devcenter.heroku.com/articles/using-multiple-buildpacks-for-an-app
[Heroku Postgres]: https://www.heroku.com/postgres
[Container Registry and Runtime]: https://devcenter.heroku.com/articles/container-registry-and-runtime
[dev/prod parity]: https://12factor.net/dev-prod-parity

### Celery

You can optionally integrate the app with Celery to ensure that some long-running
tasks will not cause web requests to time out.

If you're using Docker, Celery isn't enabled by default. To enable it, you need
to extend the default Docker Compose configuration with `docker-compose.celery.yml`.
For details on this, see Docker's documentation on [Multiple Compose files][].

For example, to start up all services with Celery integration enabled, you can run:

```
docker-compose -f docker-compose.yml -f docker-compose.celery.yml up
```

[Multiple Compose files]: https://docs.docker.com/compose/extends/

### NoRent.org website

The codebase can also serve an entirely different website, NoRent.org.

To view this alternate website, you'll need to either add a new
[Django Site model][] or modify the built-in default one to have
a name that includes the text ""NoRent"" somewhere in it (the match is
case-insensitive, so it can be ""norent"" or ""NORENT"", etc).

To do this:
1. [Edit your `/etc/hosts` file](https://www.howtogeek.com/howto/27350/beginner-geek-how-to-edit-your-hosts-file/) to map localhost.norent to 127.0.0.1.
Your file should have the following line: 
```
127.0.0.1       localhost.norent
```
2. Add an additional Site model (in addition to the default one).
You can do this by going to http://localhost:8000/admin/sites/site/ and clicking ""add site"". Set `domain` to localhost.norent:8000 and set `name` to norent.
It should look like this:
```
DOMAIN NAME                    DISPLAY NAME
localhost.norent:8000          NoRent
localhost.laletterbuilder:8000 LaLetterBuilder
```

This will allow you to access NoRent at `http://localhost.norent:8000/` and
LA Letter Builder at `http://localhost.laletterbuilder:8000/`.

In general, if you add a new Django Site model, you'll need to make sure it
has a domain that matches whatever domain you're visiting the
site at, or else the code won't be able to map your request to
the new Site you added. The display name matters too - it will be matched by
a regular expression in `site_util.py`, so make sure that regex will match your
display name. Best practice is not to include spaces.

[Django Site model]: https://docs.djangoproject.com/en/3.0/ref/contrib/sites/#django.contrib.sites.models.Site
",,2024-03-25T21:32:11Z,14,23,6,"('toolness', 2474), ('sraby', 221), ('shakao', 89), ('dependabotbot', 53), ('samaratrilling', 51), ('romeboards', 38), ('JustFix-org', 13), ('kiwansim', 11), ('jokudasai', 8), ('austensen', 3), ('atreni', 2), ('LC15', 2), ('Sjones21', 2), ('abromanos', 2)","[10, 'Reduced Inequalities']"
openfisca/openfisca-france,French tax and benefit system for OpenFisca,"# OpenFisca-France

[![Newsletter](https://img.shields.io/badge/newsletter-subscribe!-informational.svg?style=flat)](mailto:contact%40openfisca.org?subject=Subscribe%20to%20your%20newsletter%20%7C%20S'inscrire%20%C3%A0%20votre%20newsletter&body=%5BEnglish%20version%20below%5D%0A%0ABonjour%2C%0A%0AVotre%C2%A0pr%C3%A9sence%C2%A0ici%C2%A0nous%C2%A0ravit%C2%A0!%20%F0%9F%98%83%0A%0AEnvoyez-nous%20cet%20email%20pour%20que%20l'on%20puisse%20vous%20inscrire%20%C3%A0%20la%20newsletter.%20%0A%0AAh%C2%A0!%20Et%20si%20vous%20pouviez%20remplir%20ce%20petit%20questionnaire%2C%20%C3%A7a%20serait%20encore%20mieux%C2%A0!%0Ahttps%3A%2F%2Fgoo.gl%2Fforms%2F45M0VR1TYKD1RGzX2%0A%0AAmiti%C3%A9%2C%0AL%E2%80%99%C3%A9quipe%20OpenFisca%0A%0A%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%20ENGLISH%20VERSION%20%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%0A%0AHi%2C%20%0A%0AWe're%20glad%20to%20see%20you%20here!%20%F0%9F%98%83%0A%0APlease%20send%20us%20this%20email%2C%20so%20we%20can%20subscribe%20you%20to%20the%20newsletter.%0A%0AAlso%2C%20if%20you%20can%20fill%20out%20this%20short%20survey%2C%20even%20better!%0Ahttps%3A%2F%2Fgoo.gl%2Fforms%2FsOg8K1abhhm441LG2%0A%0ACheers%2C%0AThe%20OpenFisca%20Team)
[![Twitter](https://img.shields.io/badge/twitter-follow%20us!-9cf.svg?style=flat)](https://twitter.com/intent/follow?screen_name=openfisca)
[![Slack](https://img.shields.io/badge/slack-join%20us!-blueviolet.svg?style=flat)](mailto:contact%40openfisca.org?subject=Join%20you%20on%20Slack%20%7C%20Nous%20rejoindre%20sur%20Slack&body=%5BEnglish%20version%20below%5D%0A%0ABonjour%2C%0A%0AVotre%C2%A0pr%C3%A9sence%C2%A0ici%C2%A0nous%C2%A0ravit%C2%A0!%20%F0%9F%98%83%0A%0ARacontez-nous%20un%20peu%20de%20vous%2C%20et%20du%20pourquoi%20de%20votre%20int%C3%A9r%C3%AAt%20de%20rejoindre%20la%20communaut%C3%A9%20OpenFisca%20sur%20Slack.%0A%0AAh%C2%A0!%20Et%20si%20vous%20pouviez%20remplir%20ce%20petit%20questionnaire%2C%20%C3%A7a%20serait%20encore%20mieux%C2%A0!%0Ahttps%3A%2F%2Fgoo.gl%2Fforms%2F45M0VR1TYKD1RGzX2%0A%0AN%E2%80%99oubliez%20pas%20de%20nous%20envoyer%20cet%20email%C2%A0!%20Sinon%2C%20on%20ne%20pourra%20pas%20vous%20contacter%20ni%20vous%20inviter%20sur%20Slack.%0A%0AAmiti%C3%A9%2C%0AL%E2%80%99%C3%A9quipe%20OpenFisca%0A%0A%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%20ENGLISH%20VERSION%20%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%0A%0AHi%2C%20%0A%0AWe're%20glad%20to%20see%20you%20here!%20%F0%9F%98%83%0A%0APlease%20tell%20us%20a%20bit%20about%20you%20and%20why%20you%20want%20to%20join%20the%20OpenFisca%20community%20on%20Slack.%0A%0AAlso%2C%20if%20you%20can%20fill%20out%20this%20short%20survey%2C%20even%20better!%0Ahttps%3A%2F%2Fgoo.gl%2Fforms%2FsOg8K1abhhm441LG2.%0A%0ADon't%20forget%20to%20send%20us%20this%20email!%20Otherwise%20we%20won't%20be%20able%20to%20contact%20you%20back%2C%20nor%20invite%20you%20on%20Slack.%0A%0ACheers%2C%0AThe%20OpenFisca%20Team)
[![Python](https://img.shields.io/pypi/pyversions/openfisca-france.svg)](https://pypi.python.org/pypi/openfisca-france)
[![PyPi](https://img.shields.io/pypi/v/openfisca-france.svg?style=flat)](https://pypi.python.org/pypi/openfisca-france)
[![Gitpod](https://camo.githubusercontent.com/1eb1ddfea6092593649f0117f7262ffa8fbd3017/68747470733a2f2f676974706f642e696f2f627574746f6e2f6f70656e2d696e2d676974706f642e737667)](https://gitpod-referer.now.sh/api/gitpod-referer-redirect)

## [EN] Introduction

OpenFisca is a versatile microsimulation free software. This repository contains the OpenFisca model of the French tax and benefit system. Therefore, the working language here is French. You can however check the [general OpenFisca documentation](https://openfisca.org/doc/) in English!
> We host a public instance of of the [OpenFisca-France Web API](https://api.fr.openfisca.org/latest/). Learn more about its endpoint in the [Swagger documentation](https://legislation.fr.openfisca.org/swagger).
> If you need to run large amount of calculations, or add extensions, you should [host your own instance](#servez-openfisca-france-avec-lapi-web-openfisca).

## [FR] Introduction

[OpenFisca](https://www.openfisca.fr/) est un logiciel libre de micro-simulation. Ce dépôt contient la modélisation du système social et fiscal français. Pour plus d'information sur les fonctionnalités et la manière d'utiliser OpenFisca, vous pouvez consulter la [documentation générale](https://openfisca.org/doc/).
> Nous mettons à disposition une instance publique de [l'API Web OpenFisca-France](https://api.fr.openfisca.org/latest/). Découvrez ses capacité sur sa [documentation Swagger](https://legislation.fr.openfisca.org/swagger).
> Si vous avez besoin de réaliser un grand nombre de calculs ou d'ajouter des extensions, vous pouvez [servir votre propre instance](#servez-openfisca-france-avec-lapi-web-openfisca).

## API Web publique : interrogez OpenFisca-France sans installation

OpenFisca met à disposition une [API Web publique](https://openfisca.org/doc/openfisca-web-api/endpoints.html) qui ne demande aucune installation.
Utilisez l'API publique si vous souhaitez :
- accéder à un paramètre (Ex : [le montant du Smic horaire brut](https://api.fr.openfisca.org/latest/parameter/marche_travail.salaire_minimum.smic.smic_b_horaire)) ;
- consulter une formule de calcul (Ex : [le calcul de l'allocation de base des allocations familiales](https://api.fr.openfisca.org/latest/variable/af_base)) ;
- faire des calculs sur une situation (Ex : le calcul du coût du travail).

L'ensembles des endpoints sont décrits dans la [documentation Swagger](https://legislation.fr.openfisca.org/swagger).

[L'explorateur de législation](https://legislation.fr.openfisca.org/swagger) contient la liste des paramètres et variables disponibles.

## Installation

Ce paquet requiert [Python 3.9](https://www.python.org/downloads/release/python-390/) et [pip](https://pip.pypa.io/en/stable/installing/) ou [conda](https://www.anaconda.com/products/individual).

Plateformes supportées :
- distributions GNU/Linux (en particulier Debian and Ubuntu) ;
- Mac OS X ;
- Windows : Nous recommandons d'utiliser [conda](https://www.anaconda.com/products/individual), voir la procédure ci-dessous [Installez un environnement virtuel avec conda](./README.md#installez-un-environnement-virtuel-avec-conda)  ; OpenFisca fonctionne également dans le [sous-système Windows pour Linux (WSL)](https://docs.microsoft.com/fr-fr/windows/wsl/install). Dans ce dernier cas, il suffit de suivre la procédure pour Linux car vous êtes alors dans un environnement Linux.

Pour les autres OS : si vous pouvez exécuter Python et Numpy, l'installation d'OpenFisca devrait fonctionner.

### Installez un environnement virtuel avec Pew

Nous recommandons l'utilisation d'un [environnement virtuel](https://virtualenv.pypa.io/en/stable/) (_virtualenv_) avec un gestionnaire de _virtualenv_ tel que [Pew](https://github.com/berdario/pew). Vous pouvez aussi utiliser le gestionnaire d'environnemnt officiel de Python : [venv](https://docs.python.org/3/library/venv.html).

- Un _[virtualenv](https://virtualenv.pypa.io/en/stable/)_ crée un environnement pour les besoins spécifiques du projet sur lequel vous travaillez.
- Un gestionnaire de _virtualenv_, tel que [Pew](https://github.com/berdario/pew), vous permet de facilement créer, supprimer et naviguer entre différents projets.

Pour installer Pew, lancez une fenêtre de terminal et suivez ces instructions :

```sh
python --version # Python 3.9.0 ou plus récent devrait être installé sur votre ordinateur.
# Si non, téléchargez-le sur http://www.python.org et téléchargez pip.
```

```sh
pip install --upgrade pip
pip install pew
```
Créez un nouveau _virtualenv_ nommé **openfisca** et configurez-le avec python 3.9 :

```sh
pew new openfisca --python=python3.9
# Si demandé, répondez ""Y"" à la question sur la modification du fichier de configuration de votre shell
```
Le  _virtualenv_  **openfisca** sera alors activé, c'est-à-dire que les commandes suivantes s'exécuteront directement dans l'environnement virtuel. Vous verrez dans votre terminal :

```sh
Installing setuptools, pip, wheel...done.
Launching subshell in virtual environment. Type 'exit' or 'Ctrl+D' to return.
```

Informations complémentaires :
- sortez du _virtualenv_ en tapant `exit` (or Ctrl-D) ;
- re-rentrez en tapant `pew workon openfisca` dans votre terminal.

Bravo :tada: Vous êtes prêt·e à installer OpenFisca-France !

Nous proposons deux procédures d'installation. Choisissez l'installation A ou B ci-dessous en fonction de l'usage que vous souhaitez faire d'OpenFisca-France.

### A. Installation minimale (pip install)

Suivez cette installation si vous souhaitez :
- procéder à des calculs sur une large population ;
- créer des simulations fiscales ;
- écrire une extension au-dessus de la législation française (exemple : les extensions de [Paris](https://github.com/sgmap/openfisca-paris) et [Rennes](https://github.com/sgmap/openfisca-rennesmetropole) ;
- servir OpenFisca-France avec l'API Web OpenFisca.

Pour pouvoir modifier OpenFisca-France, consultez l'[Installation avancée](#b-installation-avancée-git-clone).

#### Installer OpenFisca-France avec pip install

Dans votre _virtualenv_, vérifiez les pré-requis :

```sh
python --version  # Devrait afficher ""Python 3.9.xx"".
#Si non, vérifiez que vous passez --python=python3.9 lors de la création de votre environnement virtuel.
```

```sh
pip --version  # Devrait afficher au moins 9.0.x
#Si non, exécutez ""pip install --upgrade pip"".
```
Installez OpenFisca-France :

```sh
pip install openfisca-france && pip install openfisca-core[web-api]
```
> _Note: La deuxième partie de la commande, à partir du `&&`, est optionnelle. Elle vous permet d'installer l'API Web d'OpenFisca._

Félicitations :tada: OpenFisca-France est prêt à être utilisé !

#### Prochaines étapes

- Apprenez à utiliser OpenFisca avec nos [tutoriels](https://openfisca.org/doc/) (en anglais).
- Hébergez et servez votre instance d'OpenFisca-France avec l'[API Web OpenFisca](#servez-openfisca-france-avec-lapi-web-openfisca).

En fonction de vos projets, vous pourriez bénéficier de l'installation des paquets suivants dans votre _virtualenv_ :
- pour installer une extension ou écrire une législation au-dessus d'OpenFisca-France, consultez la [documentation sur les extensions](https://openfisca.org/doc/contribute/extensions.html) (en anglais) ;
- pour représenter graphiquement vos résultats, essayez la bibliothèque [matplotlib](http://matplotlib.org/) ;
- pour gérer vos données, découvrez la bibliothèque [pandas](http://pandas.pydata.org/).

### B. Installation avancée (Git Clone)

Suivez cette installation si vous souhaitez :
- enrichir ou modifier la législation d'OpenFisca-France ;
- contribuer au code source d'OpenFisca-France.

#### Cloner OpenFisca-France avec Git

Premièrement, assurez-vous que [Git](https://www.git-scm.com/) est bien installé sur votre machine.

Dans votre _virtualenv_, assurez-vous que vous êtes dans le répertoire où vous souhaitez cloner OpenFisca-France.

Vérifiez les pré-requis :

```sh
python --version  # Devrait afficher ""Python 3.9.xx"".
#Si non, vérifiez que vous passez --python=python3.9 lors de la création de votre environnement virtuel.
```

```sh
pip --version  # Devrait afficher au moins 23.0.
#Si non, exécutez ""pip install --upgrade pip"".
```

Clonez OpenFisca-France sur votre machine :

```sh
git clone https://github.com/openfisca/openfisca-france.git
cd openfisca-france
pip install --editable .[dev] && pip install openfisca-core[web-api]
```

Vous pouvez vous assurer que votre installation s'est bien passée en exécutant :

```sh
pytest tests/test_basics.py # Ces test peuvent prendre jusqu'à 60 secondes.
```
:tada: OpenFisca-France est prêt à être utilisé !

### Installez un environnement virtuel avec conda

Nous conseillons cette procédure pour les personnes utilisant Windows et n'ayant pas d'environnement Python fonctionnel. Elle fonctionne également sous Linux et macOS.

Ceci vous permet d'obtenir en une seule installation :
- Python
- Le gestionnaire de paquets [Anaconda.org](https://docs.anaconda.com/anacondaorg/user-guide/)
- Le gestionnaire d'environnement Python virtuel : [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)

Voici les étapes à suivre :

- Installer la version communautaire/gratuite en suivant la procédure décrite sur [le site Anaconda](https://www.anaconda.com/products/individual). A noter que Anaconda occupe beaucoup d'espace disque (>2 Go), vous pouvez installer à la place [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) qui occupe beaucoup moins d'espace disque. Cependant vous n'aurez pas l'interface graphique [Anaconda Navigator](https://docs.anaconda.com/anaconda/navigator/index.html) qui vous permet de gérer vos environnements. Si vous êtes habituée à gérer les choses en ligne de commande, préfèrez Miniconda.
- Depuis le menu démarrer, exécuter `Anaconda Powershell Prompt`. Ou utiliser votre shell préféré avec Miniconda, il vous faudra peut-être utiliser la commande `conda init`, mais conda vous le dira.
- Exécuter les commandes suivantes dans le shell:
  - Ajouter `conda-forge` comme channel par défaut : `conda config --add channels conda-forge && conda config --set channel_priority strict `
  - Créer un environnement virtuel dédié : `conda create --name openfisca python=3.9`
  - Activer l'environnement : `conda activate openfisca`
  - Installer OpenFisca : `conda install openfisca-france`

:tada: OpenFisca-France est prêt à être utilisé !

Ensuite, pour quitter l'environnement OpenFisca : `conda deactivate`

Pour y revenir : `conda activate openfisca`

A noter que OpenFisca-France est présent sur [conda-forge](https://anaconda.org/conda-forge/openfisca-france) et sur un _channel_ dédié [openfisca](https://anaconda.org/openfisca/openfisca-france). C'est conda-forge qui est mis en avant dans cette documentation, car accessible par défaut dans les installations Anaconda.

A noter que l'installation d'Openfisca-France peut lever une erreur sur certaines machines Windows à cause de la longueur des chemins de certains fichiers de paramètres et des restrictions de longueur de chemin sous Windows. Une option possible pour résoudre ce problème est de lever cette limite (voir cette documentation pour [Windows >=10](https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry#enable-long-paths-in-windows-10-version-1607-and-later))

#### Prochaines étapes

- Pour enrichir ou faire évoluer la législation d'OpenFisca-France, lisez _[Coding the Legislation](https://openfisca.org/doc/coding-the-legislation/index.html)_ (en anglais).
- Pour contribuer au code, lisez le _[Contribution Guidebook](https://openfisca.org/doc/contribute/index.html)_ (en anglais).

## Testing

Pour faire tourner les tests d'OpenFisca-France, exécutez la commande suivante :

```sh
make test
```

## Style

Ce dépôt adhère à un style de code précis, et on vous invite à le suivre pour que vos contributions soient intégrées au plus vite.

L'analyse de style est déjà exécutée avec `make test`. Pour le faire tourner de façon indépendante :

```sh
make check-style
```

Pour corriger les erreurs de style de façon automatique:

```sh
make format-style
```

Pour corriger les erreurs de style de façon automatique à chaque fois que vous faites un _commit_ :

```sh
touch .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit

tee -a .git/hooks/pre-commit << END
#!/bin/sh
#
# Automatically format your code before committing.
exec make format-style
END
```

## Servez OpenFisca-France avec l'API Web OpenFisca

Il est possible de servir l'API Web d'OpenFisca-France sur votre propre serveur :

```sh
openfisca serve
```

Pour en savoir plus sur la commande `openfisca serve` et ses options, consultez la [documentation de référence](https://openfisca.org/doc/openfisca-python-api/openfisca_serve.html).

Testez votre installation en requêtant la commande suivante :

```sh
curl ""http://localhost:5000/parameter/marche_travail.salaire_minimum.smic.smic_b_horaire""
```
Vous devriez avoir le resultat suivant :
```JSON
{
  ""description"": ""Smic horaire brut"",
  ""id"": ""marche_travail.salaire_minimum.smic.smic_b_horaire"",
  ""values"": {
    ""2001-08-01"": 6.67,
    ""2002-07-01"": 6.83,
    ""2003-07-01"": 7.19,
    ""2004-07-01"": 7.61,
    ""2005-07-01"": 8.03,
    ""2006-07-01"": 8.27,
    ""2007-07-01"": 8.44,
    ""2008-05-01"": 8.63,
    ""2008-07-01"": 8.71,
    ""2009-07-01"": 8.82,
    ""2010-01-01"": 8.86,
    ""2011-01-01"": 9.0,
    ""2011-12-01"": 9.19,
    ""2012-01-01"": 9.22,
    ""2012-07-01"": 9.4,
    ""2013-01-01"": 9.43,
    ""2014-01-01"": 9.53,
    ""2015-01-01"": 9.61,
    ""2016-01-01"": 9.67,
    ""2017-01-01"": 9.76
  }
}
```

:tada: Vous servez OpenFisca-France via l'API Web OpenFisca !

Pour en savoir plus, explorez [la documentation de l'API Web](https://legislation.fr.openfisca.org/swagger).

Vous pouvez activer le suivi des visites sur votre instance via Piwik avec _[le Tracker API OpenFisca](https://github.com/openfisca/tracker)_ (en anglais).

## Stratégie de versionnement

Le code d'OpenFisca-France est déployé de manière continue et automatique. Ainsi, à chaque fois que le code de la législation évolue sur la branche principale `master`, une nouvelle version est publiée.

De nouvelles versions sont donc publiées très régulièrement. Cependant, la différence entre deux versions consécutives étant réduite, les efforts d'adaptation pour passer de l'une à l'autre sont en général très limités.

Par ailleurs, OpenFisca-France respecte les règles du [versionnement sémantique](http://semver.org/). Tous les changements qui ne font pas l'objet d'une augmentation du numéro majeur de version sont donc garantis rétro-compatibles.

> Par exemple, si mon application utilise la version `13.1.1`, je sais qu'elle fonctionnera également avec la version `13.2.0`. En revanche, il est possible qu'une adaptation soit nécessaire sur mon client pour pouvoir utiliser la version `14.0.0`.

Enfin, les impacts et périmètres des évolutions sont tous documentés sur le [CHANGELOG](CHANGELOG.md) du package. Ce document permet aux contributeurs de suivre les évolutions et d'établir leur propre stratégie de mise à jour.

## Contributeurs

Voir la [liste des contributeurs](https://github.com/openfisca/openfisca-france/graphs/contributors).
","'better-rules', 'france', 'legislation-as-code', 'microsimulation', 'rules-as-code'",2024-05-03T14:21:42Z,88,245,35,"('Sasha-Laniece', 2295), ('benjello', 1822), ('eraviart', 988), ('fpagnoux', 884), ('sandcha', 497), ('Morendil', 422), ('AhmedZemzami', 411), ('DorineLam', 383), ('guillett', 362), ('clallemand', 234), ('laem', 210), ('benoit-cty', 188), ('HAEKADI', 184), ('jdesboeufs', 181), ('MattiSG', 171), ('Anna-Livia', 164), ('Allan-CodeWorks', 154), ('sylvainipp', 146), ('mtifarine', 137), ('frtomas', 136), ('TomSopra', 116), ('bouvard', 101), ('claireleroy', 93), ('LouisePD', 92), ('cbenz', 89), ('pierrehvz', 78), ('nanocom', 77), ('jsantoul', 74), ('GautierMerit', 69), ('bfabre01', 57), ('Kout95', 53), ('pzuldp', 50), ('fjacquetin', 47), ('adrienpacifico', 44), ('lukas-puschnig', 42), ('aguillouzouic', 33), ('LucasDetre', 29), ('edarin', 28), ('b-michaud', 27), ('bixiou', 27), ('SophieIPP', 23), ('bouxtehouve', 23), ('JoDuGa', 22), ('Cugniere', 21), ('AlexisEidelman', 19), ('robinguill', 19), ('kendrickherz', 18), ('malkaguillot', 16), ('bilalchoho', 14), ('dependabot-previewbot', 14), ('alexsegura', 13), ('LucileIPP', 12), ('magemax', 12), ('Lolajossipp', 11), ('dusylvain', 11), ('Shamzic', 11), ('marchand-laetitia-msa', 10), ('QuentinMadura', 10), ('PiGo86', 10), ('monbocal', 9), ('dependabot-support', 8), ('ManonLger', 7), ('desoindx', 7), ('Valandr', 6), ('baptou12', 6), ('clbe', 5), ('yasmine-glitch', 5), ('NolwennLoisel', 4), ('magopian', 4), ('Flightan', 4), ('Bardyl', 3), ('Nodraak', 2), ('denismerigoux', 2), ('GerEaton', 2), ('Pyke75', 2), ('Gentux', 2), ('tducret', 2), ('nico5655', 2), ('lauradelmas8', 1), ('elsaperdrix', 1), ('bchoho', 1), ('gitter-badger', 1), ('stanislasrybak', 1), ('RBLEC', 1), ('ronnix', 1), ('PhunkyBob', 1), ('pblayo', 1), ('Guinaume59', 1)","[16, 'Peace, Justice and Strong Institutions']"
opensrp/opensrp-client-core,Core OpenSRP Android client library,"[![Android CI with Gradle](https://github.com/opensrp/opensrp-client-core/actions/workflows/ci.yml/badge.svg)](https://github.com/opensrp/opensrp-client-core/actions/workflows/ci.yml)
[![Coverage Status](https://coveralls.io/repos/github/opensrp/opensrp-client-core/badge.svg?branch=master)](https://coveralls.io/github/opensrp/opensrp-client-core?branch=master)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/98bae20e1d9a4fcbb7da594a57705b9a)](https://www.codacy.com/gh/opensrp/opensrp-client-core/dashboard?utm_source=github.com&utm_medium=referral&utm_content=OpenSRP/opensrp-client-core&utm_campaign=Badge_Grade)

[![Dristhi](opensrp-core/res/drawable-mdpi/login_logo.png)](https://smartregister.atlassian.net/wiki/dashboard.action)

# Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Why OpenSRP?](#why-opensrp)
- [Website](#website)
- [Developer Documentation](#developer-documentation)
  - [Pre-requisites](#pre-requisites)
  - [Installation Devices](#installation-devices)
  - [How to install](#how-to-install)
  - [Developer Guides](#developer-guides)
  - [Wiki](#wiki)
  - [Uses](#uses)

# Introduction

OpenSRP Client Core App/Module basically provides basic functionality such as networking, security, database access, common widgets, utilities, domain objects, service layer, broadcast receivers and syncing.

# Features

It provides:

1. Domain objects for forms and database.
2. Mappers between the different domains
3. Basic and advanced networking capabilities to securely and efficiently connect to an OpenSRP backend
4. Sync abilities that handle and maintain data consistency between the client and backend
5. Data creation, edit, retrieval and deletion capabilities on the client device
6. Security services that maintain global data and application security
7. Utilities used for file storage, caching, image rendering, logging, session management and number conversions
8. Access to tailored android views/widgets for OpenSRP
9. Device-to-device sharing of application data - This includes events, clients and profile images

# Why OpenSRP?

1. It provides client access on Android phones which are easily available and acquirable
2. It can work with minimal or no internet connection
3. It provides enhanced security
4. It primarily integrates with OpenMRS
5. It is tailored to be used by health workers who regularly provide outreach services
6. It generates custom reports eg. HIA 2
7. It manages stock levels for stock provided to the health workers
8. It implements the WHO-recommended **z-score** for child growth monitoring
9. It provides device-to-device sharing of medical records in areas without an internet connection.

# Website

If you are looking for more information regarding OpenSRP as a platform checkout the [OpenSRP Site](http://smartregister.org/)

# Developer Documentation

This section will provide a brief description on how to build and install the application from the repository source code.

## Pre-requisites

1. Make sure you have Java 1.7 to 1.8 installed
2. Make sure you have Android Studio installed or [download it from here](https://developer.android.com/studio/index.html)

## Installation Devices

1. Use a physical Android device to run the app
2. Use the Android Emulator that comes with the Android Studio installation (Slow & not advisable)
3. Use Genymotion Android Emulator
   - Go [here](https://www.genymotion.com/) and register for genymotion account if none. Free accounts have limitations which are not counter-productive
   - Download your OS Version of VirtualBox at [here](https://www.virtualbox.org/wiki/Downloads)
   - Install VirtualBox
   - Download Genymotion & Install it
   - Sign in to the genymotion app
   - Create a new Genymotion Virtual Device
     - **Preferrable & Stable Choice** - API 22(Android 5.1.0), Screen size of around 800 X 1280, 1024 MB Memory --> eg. Google Nexus 7, Google Nexus 5

## How to install

1. Import the project into Android Studio by: **Import a gradle project** option
   _All the plugins required are explicitly stated, therefore it can work with any Android Studio version - Just enable it to download any packages not available offline_
1. Open Genymotion and Run the Virtual Device created previously.
1. Run the app on Android Studio and chose the Genymotion Emulator as the ` Deployment Target`

### Developer Guides

If you want to contribute please refer to these resources:

- [Getting started with OpenSRP](https://smartregister.atlassian.net/wiki/spaces/Documentation/pages/6619148/Getting+started+with+OpenSRP)
- [Setup Instructions](https://smartregister.atlassian.net/wiki/spaces/Documentation/pages/6619255/Setup+Instructions)
- [Complete OpenSRP Developer's Guide](https://smartregister.atlassian.net/wiki/spaces/Documentation/pages/6619193/OpenSRP+Developer%27s+Guide)
- [Peer-to-Peer Library Guide](https://smartregister.atlassian.net/wiki/spaces/Documentation/pages/1139212418/Android+Peer-to-peer+sync+library?atlOrigin=eyJpIjoiYWE5NmM1ZTk3MGQ2NGU4OWE0ZTdmM2U2YTFjODg2YTAiLCJwIjoiYyJ9)

### Wiki

If you are looking for detailed guides on how to install, configure, contribute and extend OpenSRP visit [OpenSRP Wiki](https://smartregister.atlassian.net/wiki)

# Uses

OpenSRP Client core has been used in several modules and applications:

- [OpenSRP Path application](https://github.com/OpenSRP/opensrp-client-path)
- [OpenSRP KIP application](https://github.com/OpenSRP/opensrp-client-kip)
- [OpenSRP Growth monitoring library](https://github.com/OpenSRP/opensrp-client-growth-monitoring)
- [OpenSRP Immunization library](https://github.com/OpenSRP/opensrp-client-immunization)
- [OpenSRP Native Form library](https://github.com/OpenSRP/opensrp-client-native-form)

# Main Functions

## 1. Security

Security is provided in the following:

- Network - It supports SSL certificates from **[Let's Encrypt](https://letsencrypt.org/)** CA
- Data access - Only registered providers are able to view and manipulate records
- Data encryption - The database on the android client is encrypted with 256-bit AES encryption using [SQLCipher](https://guardianproject.info/code/sqlcipher/).

The security classes can be found in `org.smartregister.ssl`

Under the cryptography package we have CryptographicHelper class whose instance exposes methods

_**byte[] encrypt(byte[] input, String keyAlias)**_ For encryption of a byte array input with key

_**byte[] decrypt(byte[] encrypted, String keyAlias)**_ For decryption of encrypted byte array with key

_**Key getKey(String keyAlias)**_ For retrieving a generated key stored in the Android keystore

_**void generateKey(String keyAlias)**_ For key generation using a Key Alias parameter for use by Android keystore

- NB: \* This class depends on `AndroidLegacyCryptography` class and the `AndroidMCryptography` class which both implement the above in different ways depending on the SDK version.
  `AndroidLegacyCryptography` has method implementation that are used when the SDK version is less than API level 23
  
The sample app has examples of how these methods have been implemented. The code for it can be found in
the [MainActivity](https://github.com/opensrp/opensrp-client-core/blob/master/sample/src/main/java/org/smartregister/sample/MainActivity.java) class.

## 2. Data management

This app provides data management.

It implements both plain and secure data storage. Classes implementing secure storage extend `SQLiteOpenHelper`, `Repository` or `BaseRepository`.

The rest use the SQLite helpers provided in the Android SDK.

For this reason, there are multiple implementations for storing the same model(s).

| Class                       | Represents                                 |
| --------------------------- | ------------------------------------------ |
| `EventClientRepository`     | Events                                     |
| `AlertRepository`           | Alerts                                     |
| `ChildRepository`           | Children                                   |
| `ClientRepository`          | Clients/Patients                           |
| `DetailsRepository`         | Details                                    |
| `EligibleCoupleRepository`  | Eligible couples                           |
| `EventRepository`           | Events                                     |
| `FormDataRepository`        | Form data                                  |
| `FormsVersionRepository`    | Form version                               |
| `ImageRepository`           | Image locations                            |
| `MotherRepository`          | Mothers                                    |
| `ServiceProvidedRepository` | Provided service to the patient            |
| `SettingsRepository`        | App settings eg. connection configurations |
| `TimelineEventRepository`   | Timeline events                            |

The data management classes can be found in `org.smartregister.repository`

## 3. Networking

This app provides the following networking capabilities:

| Class                        | Represents                                                                                                             |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `OpensrpSSLHelper`           | SSL Connection helper                                                                                                  |
| `OpenSRPImageLoader`         | Asynchronous image downloader                                                                                          |
| `HttpAgent`                  | Synchronous networking class with username\password ([Basic Auth](https://tools.ietf.org/html/rfc2617)) access support |
| `ConnectivityChangeReceiver` | Network status detection by a broadcast receiver                                                                       |
| `GZipEncodingHttpClient`     | GZip encoding and decoding capabilities                                                                                |
| `Session`                    | Session management                                                                                                     |
| `UserService`                | User authentication & client-server time synchronization                                                               |

The networking classes can be found in:

- `org.smartregister.service`
- `org.smartregister.util`
- `org.smartregister.client`
- `org.smartregister.ssl`
- `org.smartregister.view.receiver`

## 4. Domain Objects

This app provides the following domain objects:

| Class                   | Represents                                                                                                                                                                                                                           |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `Address`               | Location address containing map coordinates                                                                                                                                                                                          |
| `BaseDataObject`        | Data object with datestamps, void flag, related void details and server version                                                                                                                                                      |
| `BaseEntity`            | Extends `BaseDataObject` to include the `baseEntityId`, `identifiers`, `addresses` and `attributes` that are common in OpenSRP models                                                                                                |
| `Client`                | Represents a patient in OpenSRP eg. a child. It contains relevant patient details and extends `BaseEntity`                                                                                                                           |
| `ColumnAttribute`       | Represents a column using the type, is-primary-key and is-index properties. It is used in the `EventClientRepository` class to define and access columns in the appropriate table                                                    |
| `Event`                 | It represents an event in OpenSRP which are mainly [encounters](#https://github.com/OpenSRP/opensrp-client-native-form#encounter-types) eg. Birth Registration, Death. It extends the `BaseDataObject` and provides other properties |
| `Obs`                   | It represents an observation in an `Event` _above_                                                                                                                                                                                   |
| `Query`                 | It represents a data query and enables creation of queries using an OOP approach                                                                                                                                                     |
| `FormData`              | It represents form fields, their inputs and any sub-forms                                                                                                                                                                            |
| `FormField`             | It represents a single form question/field with a name, value and source                                                                                                                                                             |
| `FormInstance`          | It represents a `FormData` of a specific definition version                                                                                                                                                                          |
| `FormSubmission`        | It represents the status of a form before or after submission. It therefore contains other metadata such as client version and server verion.                                                                                        |
| `SubForm`               | It represents a form inside another form                                                                                                                                                                                             |
| `Alert`                 | It represents a notification about an encounter which is due or overdue the expected time                                                                                                                                            |
| `ANM`                   | It represents a health services provider                                                                                                                                                                                             |
| `Child`                 | It represents a child                                                                                                                                                                                                                |
| `EligibleCouple`        | It represents an eligible couple                                                                                                                                                                                                     |
| `FormDefinitionVersion` | It represents a form version                                                                                                                                                                                                         |
| `Mother`                | It represents a mother                                                                                                                                                                                                               |
| `Photo`                 | It represents a photo by storing the file path & resource id                                                                                                                                                                         |
| `ProfileImage`          | It represents the photo of an entity                                                                                                                                                                                                 |
| `Report`                | It represents a report                                                                                                                                                                                                               |
| `MontlyReport`          | It represents a monthly report                                                                                                                                                                                                       |
| `Response`              | It represents an HTTP response with status & payload                                                                                                                                                                                 |
| `ServiceProvided`       | It represents a service that was provided to a patient                                                                                                                                                                               |
| `TimelineEvent`         | It represents an event within a patient's life eg. birth                                                                                                                                                                             |

The domain object classes can be found in `org.smartregister.domain`. There are several domains namely: global domain, form and database domain.

## 5. Sync

This app provides the following sync capabilities:

- Periodic syncing based on network connection
- Updating views with updated information

The sync classes can be found in `org.smartregister.sync`

## 6. Utilities

This app provides the following utilities:

| Class                     | Provides                                                                                                                                                                                               |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `BitmapImageCache`        | Improved image caching                                                                                                                                                                                 |
| `Cache`                   | Data caching and modifications listener                                                                                                                                                                |
| `FileUtilities`           | File storage utility                                                                                                                                                                                   |
| `FloatUtil`               | Float conversion utility                                                                                                                                                                               |
| `FormSubmissionBuilder`   | Form submission builders                                                                                                                                                                               |
| `FormUtils`               | Form generation and manipulation utility                                                                                                                                                               |
| `IntegerUtil`             | Integer conversion utility                                                                                                                                                                             |
| `JsonFormUtils`           | JSON form data extractor and injector                                                                                                                                                                  |
| `OpenSRPImageLoader`      | Asynchronous image downloader with thread-safe image caching                                                                                                                                           |
| `Session`                 | Session manager                                                                                                                                                                                        |
| `StringUtil`              | String manipulation utility                                                                                                                                                                            |
| `TimelineEventComparator` | Timeline event comparator                                                                                                                                                                              |
| `Utils`                   | Date conversion, android preference manipulator, view generator, metrics humanizer among other basic utility functions.                                                                                |
| `AppExecutors`            | Provides implementation of the executor interface that allows grouping request. Grouping tasks like this avoids the effects of task starvation (e.g. disk reads don't wait behind webservice requests) |

The utility classes can be found under `org.smartregister.util`

## 7. Services

This app provides business logic for operations as follows:

| Class                       | Business logic related to |
| --------------------------- | ------------------------- |
| `ActionService`             | Actions                   |
| `AlertService`              | Alerts                    |
| `AllFormVersionSyncService` | Form versions             |
| `ANMService`                | Health service providers  |
| `BeneficiaryService`        | Beneficiaries             |
| `ChildService`              | Children                  |
| `Drishti`                   | Form submissions          |
| `MotherService`             | Mothers                   |
| `ServiceProvidedService`    | Services provided         |

The service classes can be found in `org.smartregister.service`

## 8. App Localization

This app provides capability to support multiple languages.

Check out the sample app to see how to implement language switching.

Ensure each class in your app extends (directly or indirectly) a class in client-core. If it doesn't then extend MultiLanguageActivity instead of AppCompatActivity.

## 9. Data Compression

The package `compression` contains an interface `ICompression` whose methods are Implemented using the GZIPCompression class (which uses a GZIP implementation)
Other compression Algorithms can be used by adding a new class implementing the interface methods.

Methods in the ICompression interface are

_**byte[] compress(String rawString)**_ Compress the given string input

_**String decompress(byte[] compressedBytes)**_ Decompress a byte array of compressed data

_**void compress(String inputFilePath, String compressedOutputFilepath)**_ Compress file in file path `inputFilePath` and output to location `compressedOutputFilepath`

_**void decompress(String compressedInputFilePath, String decompressedOutputFilePath)**_ Decompress file in file path `compressedInputFilePath` and output to location `decompressedOutputFilePath`

## 10. Bootstrap View Generation

You can quickly bootstrap view generation that take the format of a `Register` or `Profile` using the package `org.smartregister.view`. Views that render a Register can extend the `BaseRegisterFragment`
that implements basic register functionality such as searching, listing, sorting and counting number of records on the generic base view whilst reading a `cursor` object.

For views that display a generic List of items but require heavier customization, using the `org.smartregister.view.fragment.BaseListFragment` allows you to render any generic list while providing a context
aware background executor and error handling that only requires provisioning or `Callable` function to act as a data source.
Check the Sample app's `ReportFragment` that consumes a `Retrofit` response and renders a list of objects.

## Configurability

By placing a file named `app.properties` in your implementation assets folder (See sample app) , one can configure certain aspects of the app

### Configurable Settings

| Configuration                            | Type    | Default | Description                                                                                                                                         |
| ---------------------------------------- | ------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `system.toaster.centered`                | Boolean | false   | Position toaster(s) at the center of the view(s)                                                                                                    |
| `disable.location.picker.view`           | Boolean | false   | Disables LocationPicker View                                                                                                                        |
| `location.picker.tag.shown`              | Boolean | false   | Hides/Shows the location tag in the location picker tree view                                                                                       |
| `encrypt.shared.preferences`             | Boolean | false   | Enable/disables encrypting SharedPreferences                                                                                                        |
| `allow.offline.login.with.invalid.token` | Boolean | false   | Allow offline login when token is no longer valid after a successful login when online and user is forcefully logged out                            |
| `enable.search.button`                   | Boolean | false   | Enable/Disable search to be triggered only after clicking the search icon in `org.smartregister.view.fragment.BaseRegisterFragment` or its subclass |
| `feature.profile.images.disabled`            | Boolean | false    | Disable profile image capturing and rendering     |",,2023-10-05T06:52:16Z,36,16,53,"('githengi', 780), ('ekigamba', 552), ('Rkareko', 428), ('ndegwamartin', 385), ('vincent-karuri', 298), ('bennsimon', 176), ('dubdabasoduba', 143), ('Andati', 135), ('qiarie', 135), ('rkodev', 133), ('cozej4', 98), ('manutarus', 86), ('hilpitome', 59), ('hamza-vd', 52), ('pld', 50), ('mahmud6390', 47), ('paulinembabu', 45), ('ellykits', 44), ('raihan-mpower', 39), ('kaderchowdhury', 37), ('allan-on', 32), ('junaidwarsivd', 27), ('qaziabubakar-vd', 20), ('LZRS', 20), ('owais-vd', 18), ('transifex-integrationbot', 9), ('SebaMutuku', 7), ('rehammuzzamil', 7), ('faidvd', 5), ('kuthatikk', 3), ('aleesha711', 3), ('issyzac', 3), ('craigappl', 2), ('zzainulabidin', 2), ('bmalina', 1), ('keymane', 1)","[3, 'Good Health and Well-Being']"
instedd/resourcemap,,"# Resource Map

[Resource Map](http://instedd.org/technologies/resource-map/) helps people track
their work, resources and results geographically in a collaborative environment
accessible from anywhere.

Resource Map is a free, open-source tool that helps you make better decisions by
giving you better insight into the location and distribution of your resources.
With Resource Map, you and your team can collaboratively record, track, and
analyze resources at a glance using a live map. Resource Map works with any
computer or cell phone with text messaging capability, putting up-to-the-minute
alerts and powerful resource management always within reach, wherever you go.

Please refer to [the wiki](https://github.com/instedd/resourcemap/wiki) for more
information.


## Development Setup

### Dependencies

Resource Map is a standard Rails application, but also needs the following
services to run:

* [Elasticsearch](http://elastic.co/products/elasticsearch) 1.7
* [Redis](http://redis.io)
* [MySQL](http://www.mysql.com)

### Installation on Mac OS X

Use [Homebrew](http://brew.sh) to retrieve and install the required
dependencies.

Install the 1.7 series of elasticsearch (Homebrew defaults to the latest version
which may introduce compatibility problems):

    brew install homebrew/versions/elasticsearch17

Then install Redis with:

    brew install redis

Likewise, MySQL server installation is straightforward:

    brew install mysql

### Installation on Ubuntu Linux

Install Elasticsearch from the official download site using .deb packages by
following the instructions given at https://www.elastic.co/guide/en/elasticsearch/reference/1.7/setup-repositories.html

For Redis server, you can use the version provided in the distribution:

    sudo apt-get install redis-server

Likewise for MySQL:

    sudo apt-get install mysql-server


### Rails Setup

The current required Ruby version is 2.1.2. We recommend you use a Ruby version
manager to handle parallel installations of different Ruby versions.
[rbenv](https://github.com/rbenv/rbenv) and [RVM](http://rvm.io) are both
supported.

1. Install the bundle:

    ```
    bundle install
    ```

2. Create and setup de database

   ```
   bundle exec rake db:setup
   ```

## Running in development

Once the application has been setup, run the application in development mode:

    bundle exec rails server

To run the background jobs (through [resque](https://github.com/resque/resque))
execute:

    bundle exec rake resque:work

### Running the tests

Resource Map has unit tests, acceptance tests (using
[Capybara](https://github.com/jnicklas/capybara)) and Javascript tests.

Execute the unit tests through [Rspec](http://rspec.info):

    bundle exec rspec

To run the acceptance tests you need to have a recent version of Firefox, since
Capybara is configured to use the Selenium driver with Firefox.

    bundle exec rspec -t js spec/integration

Keep in mind that the acceptance tests are kind of out-of-date. Many of them
will pass, but lots of them are marked as `pending` - the coverage isn't that
good to rely on them.

Finally, Javascript tests are run through [Jasmine](http://jasmine.github.io/).
Start the Jasmine server with:

    bundle exec rake jasmine

And open a browser tab in [http://localhost:8888](http://localhost:8888)


### Docker development

`docker-compose.yml` file build a development environment mounting the current folder and running rails in development environment.

Run the following commands to have a stable development environment.

```
$ docker compose run --rm --no-deps web bundle install
$ docker compose up -d db
$ docker compose run --rm web rake db:setup
$ docker compose up
```

To setup and run test, once the web container is running:

```
$ docker compose exec web bash
root@web_1 $ rake
```

## Intercom

Resourcemap supports Intercom as its CRM platform. To load the Intercom chat widget, simply start Resourcemap with the env variable `INTERCOM_APP_ID` set to your Intercom app id (https://www.intercom.com/help/faqs-and-troubleshooting/getting-set-up/where-can-i-find-my-workspace-id-app-id).

Resourcemap will forward any conversation with a logged user identifying them through their email address. Anonymous, unlogged users will also be able to communicate.

If you don't want to use Intercom, you can simply omit `INTERCOM_APP_ID` or set it to `''`.

To test the feature in development, add the `INTERCOM_APP_ID` variable and its value to the `environment` object inside the `web` service in `docker-compose.yml`.

# Upload files using Google Sheets Links

## Overview

Sometimes users won't upload files in the usual way (using the file explorer to select a CSV file), but by providing a Google Spread sheet link.
At server side, link is validated and its content fetched using `Google::Apis::SheetsV4::SheetsService`.
Finally, content of the sheet is written into a CSV file, which is stored same way as the other files (same directory and naming convention).
Therefore, uploading a file through a Google Sheet link yields the same result as downloading the contents of the sheet as CSV and uploading that file in the usual way.

## Setup

Users can only uploads links that belongs to public Google Sheets. Though [Google Sheets API v4](https://developers.google.com/sheets/api/guides/authorizing) doesn't require an `OAuth 2.0 token` to authorize the requests, it does demands an `API_KEY` as a means of authentication. Therefore, in the next subsection we'll review how to create a `GOOGLE_SHEET_API_KEY` in a Project.

### Obtaining a Google Sheet API KEY

1. Create a Google Project or get into an existing one
2. Navigate to `Credentials`
3. Create a new `API_KEY` or select an existing one

At this point we still have to enable our `API_KEY` obtained in step (3) to use `Google Sheets API v4`. Otherwise, if you attempt a request using the `API_KEY` to authenticate yourself (e.g try to read the content of a public spreadsheet), you'll obtain the following error:

```
{
  ""error"": {
    ""code"": 403,
    ""message"": ""Google Sheets API has not been used in project {project-id} before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/sheets.googleapis.com/overview?project=project-id then retry""
    ""status"": ""PERMISSION_DENIED"",
    ""details"": [
      ...
    ]
  }
}
```

4. Navigate to https://console.developers.google.com/apis/api/sheets.googleapis.com/overview?project=#{project-id}, as pointed out by the error message. Don't forget to replace _project-id_ with the actual _id_ of the project.
5. Enable `Google Sheets API v4` in your project
6. Wait a few minutes until changes take effect

At this point your `API_KEY` will be ready to authenticate `Google Sheets API v4` requests.

### Setting `GOOGLE_SHEET_API_KEY`

For `DEVELOPMENT`, add `GOOGLE_SHEET_API_KEY` in `settings.local.yml`.
For `PRODUCTION`, add `GOOGLE_SHEET_API_KEY` along with the other variables set in settings.yml
`GOOGLE_SHEET_API_KEY` is used by `SpreadsheetService` class to authenticate `Google Sheets API v4` requests.

",,2024-02-29T05:49:28Z,22,14,12,"('carohadad', 886), ('asterite', 705), ('kensreng', 279), ('lmatayoshi', 151), ('fciccioli', 126), ('mmuller', 123), ('juanboca', 116), ('waj', 104), ('gbatiston', 79), ('matiasgarciaisaia', 66), ('ggiraldez', 53), ('ysbaddaden', 16), ('bcardiff', 16), ('kakada', 13), ('juanedi', 7), ('pmallol', 6), ('pablobrusco', 5), ('flbulgarelli', 4), ('macoca', 3), ('devduarte', 1), ('spalladino', 1), ('manumoreira', 1)","[17, 'Partnerships for the Goals']"
getodk/collect,ODK Collect is an Android app for filling out forms. It's been used to collect billions of data points in challenging environments around the world. Contribute and make the world a better place! ✨📋✨,"# ODK Collect
![Platform](https://img.shields.io/badge/platform-Android-blue.svg)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build status](https://circleci.com/gh/getodk/collect.svg?style=shield&circle-token=:circle-token)](https://circleci.com/gh/getodk/collect)
[![Slack](https://img.shields.io/badge/chat-on%20slack-brightgreen)](https://slack.getodk.org)

ODK Collect is an Android app for filling out forms. It is designed to be used in resource-constrained environments with challenges such as unreliable connectivity or power infrastructure. ODK Collect is part the ODK project, a free and open-source set of tools which help organizations author, field, and manage mobile data collection solutions. Learn more about ODK and its history [here](https://getodk.org/) and read about example ODK deployments [here](https://forum.getodk.org/c/showcase).

ODK Collect renders forms that are compliant with the [ODK XForms standard](https://getodk.github.io/xforms-spec/), a subset of the [XForms 1.1 standard](https://www.w3.org/TR/xforms/) with some extensions. The form parsing is done by the [JavaRosa library](https://github.com/getodk/javarosa) which Collect includes as a dependency.

Please note that the `master` branch reflects ongoing development and is not production-ready.

## Table of Contents
* [Learn more about ODK Collect](#learn-more-about-odk-collect)
* [Release cycle](#release-cycle)
* [Downloading builds](#downloading-builds)
* [Suggesting new features](#suggesting-new-features)
* Contributing
  * [Contributing code](#contributing-code)
  * [Contributing translations](#contributing-translations)
  * [Contributing testing](#contributing-testing)
* Developing
  * [Setting up your development environment](#setting-up-your-development-environment)
  * [Testing a form without a server](#testing-a-form-without-a-server)
  * [Using APIs for local development](#using-apis-for-local-development)
  * [Debugging JavaRosa](#debugging-javarosa)
  * [Troubleshooting](#troubleshooting)
* [Creating signed releases for Google Play Store](#creating-signed-releases-for-google-play-store)

## Learn more about ODK Collect
* ODK website: [https://getodk.org](https://getodk.org)
* ODK Collect usage documentation: [https://docs.getodk.org/collect-intro/](https://docs.getodk.org/collect-intro/)
* ODK forum: [https://forum.getodk.org](https://forum.getodk.org)
* ODK developer Slack chat: [https://slack.getodk.org](https://slack.getodk.org)

## Release cycle

Releases are planned to happen every 2-3 months (resulting in ~4 releases a year). Soon before (or just after) the end of one release cycle, the core team will plan a new set of work for the next release based on the [ODK Roadmap](https://getodk.org/roadmap), bugs and crashes identified in previous releases and other required or preemptive maintenance. This work will be broken down into Github Issues (for things that aren't already) by [@seadowg](https://github.com/seadowg) and is then added into Collect's prioritised [backlog](https://github.com/orgs/getodk/projects/9/views/8) for the core team (and any external contributors) to work on day to day. Sometimes issues will be assigned to core team members before they are actually started (moved to ""in progress"") to make it clear who's going to be working on what.

Once the majority of high risk or visible work is done for a release, a new beta will then be released to the Play Store by [@lognaturel](https://github.com/lognaturel) and that will be used for regression testing by [@getodk/testers](https://github.com/orgs/getodk/teams/testers). If any problems are found, the release is blocked until we can merge fixes. Regression testing should continue on the original beta build (rather than a new one with fixes) unless problems block the rest of testing. Once the process is complete, [@lognaturel](https://github.com/lognaturel) pushes the releases to the Play Store following [these instructions](#creating-signed-releases-for-google-play-store).

Fixes to a previous release should be merged to a ""release"" branch (`v2023.2.x` for example) so as to leave `master` available for the current release's work. If hotfix changes are needed in the current release as well then these can be merged in as a PR after hotfix releases (generally easiest as a single PR for the whole hotfix release). This approach can also be used if work for the next release starts before the current one is out - the next release continues on `master` while the release is on a release branch.

At the beginning of each release cycle, [@grzesiek2010](https://github.com/grzesiek2010) updates all dependencies that have compatible upgrades available and ensures that the build targets the latest SDK.

## Downloading builds
Per-commit debug builds can be found on [CircleCI](https://circleci.com/gh/getodk/collect). Login with your GitHub account, click the build you'd like, then find the APK in the Artifacts tab.

If you are looking to use ODK Collect, we strongly recommend using the [Play Store build](https://play.google.com/store/apps/details?id=org.odk.collect.android). Current and previous production builds can be found in [Releases](https://github.com/getodk/collect/releases).

## Suggesting new features
We try to make sure that all issues in the issue tracker are as close to fully specified as possible so that they can be closed by a pull request. Feature suggestions should be described [in the forum Features category](https://forum.getodk.org/c/features) and discussed by the broader user community. Once there is a clear way forward, issues should be filed on the relevant repositories. More controversial features will be discussed as part of the Technical Steering Committee's [roadmapping process](https://github.com/getodk/governance/blob/master/TSC-1/STANDARD-OPERATING-PROCEDURES.md#roadmap).

## Contributing code
Any and all contributions to the project are welcome. ODK Collect is used across the world primarily by organizations with a social purpose so you can have real impact!

Issues tagged as [good first issue](https://github.com/getodk/collect/labels/good%20first%20issue) should be a good place to start. There are also currently many issues tagged as [needs reproduction](https://github.com/getodk/collect/labels/needs%20reproduction) which need someone to try to reproduce them with the current version of ODK Collect and comment on the issue with their findings.

If you're ready to contribute code, see [the contribution guide](docs/CONTRIBUTING.md).

## Contributing translations
If you know a language other than English, consider contributing translations through [Transifex](https://www.transifex.com/getodk/collect/).

Translations are updated right before the first beta for a release and before the release itself. To update translations, download the zip from https://www.transifex.com/getodk/collect/strings/. The contents of each folder then need to be moved to the Android project folders. A quick script like [the one in this gist](https://gist.github.com/lognaturel/9974fab4e7579fac034511cd4944176b) can help. We currently copy everything from Transifex to minimize manual intervention. Sometimes translation files will only get comment changes. When new languages are updated in Transifex, they need to be added to the script above. Additionally, `ApplicationConstants.TRANSLATIONS_AVAILABLE` needs to be updated. This array provides the choices for the language preference in settings. Ideally the list could be dynamically generated.

## Contributing testing
All pull requests are verified on the following devices (ordered by Android version):
* [Huawei Y560-L01](http://www.gsmarena.com/huawei_y560-7829.php) - Android 5.1.1
* [Sony Xperia Z3 D6603](http://www.gsmarena.com/sony_xperia_z3-6539.php) - Android 6.0.1 (used irregularly)
* [Samsung Galaxy S7 SM-G930F](https://www.gsmarena.com/samsung_galaxy_s7-7821.php) - Android 7.0.0 (used irregularly)
* [Motorola Moto G4 Play](https://www.gsmarena.com/motorola_moto_g4_play-8104.php) - Android 7.1.1 (used irregularly)
* [LG Nexus 5X](https://www.gsmarena.com/lg_nexus_5x-7556.php) - Android 8.1
* [Xiaomi Redmi 7](https://www.gsmarena.com/xiaomi_redmi_7-9498.php) - Android 9.0 (used irregularly)
* [Samsung Galaxy M11 SM-M115F/DSN](https://www.gsmarena.com/samsung_galaxy_m11-10124.php) - Android 10.0
* [Google Pixel 3a](https://www.gsmarena.com/google_pixel_3a-9408.php) - Android 11.0

Our regular code contributors use these devices (ordered by Android version):
* [Samsung Galaxy Tab SM-T285](http://www.gsmarena.com/samsung_galaxy_tab_a_7_0_(2016)-7880.php) - Android 5.1.1 [@lognaturel](https://github.com/lognaturel)
* [Motorola G 5th Gen XT1671](https://www.gsmarena.com/motorola_moto_g5-8454.php) - Android 7.0 [@lognaturel](https://github.com/lognaturel)


The best way to help us test is to build from source! If you aren't a developer and want to help us test release candidates, join the [beta program](https://play.google.com/apps/testing/org.odk.collect.android)!

Testing checklists can be found on the [Collect testing plan](https://docs.google.com/spreadsheets/d/1ITmOW2MFs_8-VM6MTwganTRWDjpctz9CI8QKojXrnjE/edit?usp=sharing).

If you have finished testing a pull request, please use a template from [Testing result templates](.github/TESTING_RESULT_TEMPLATES.md) to report your insights.

## Setting up your development environment

1. Download and install [Git](https://git-scm.com/downloads) and add it to your PATH

1. Download and install [Android Studio](https://developer.android.com/studio/index.html) 

1. Fork the collect project ([why and how to fork](https://help.github.com/articles/fork-a-repo/))

1. Clone your fork of the project locally. At the command line:

        git clone https://github.com/YOUR-GITHUB-USERNAME/collect

    If you prefer not to use the command line, you can use Android Studio to create a new project from version control using `https://github.com/YOUR-GITHUB-USERNAME/collect`.

1. Use Android Studio to import the project from its Gradle settings. To run the project, click on the green arrow at the top of the screen.

1. Windows developers: continue configuring Android Studio with the steps in this document: [Developing ODK Collect on Windows](docs/WINDOWS-DEV-SETUP.md).

1. Make sure you can run unit tests by running everything under `collect_app/src/test/java` in Android Studio or on the command line:

    ```
    ./gradlew testDebug
    ```

1. Make sure you can run instrumented tests by running everything under `collect_app/src/androidTest/java` in Android Studio or on the command line:

    ```
    ./gradlew connectedAndroidTest
    ```
    **Note:** You can see the emulator setup used on CI in  `.circleci/config.yml`.

## Customizing the development environment

### Changing JVM heap size

You can customize the heap size that is used for compiling and running tests. Increasing these will most likely speed up compilation and tests on your local machine. The default values are specified in the project's `gradle.properties` and this can be overriden by your user level `gradle.properties` (found in your `GRADLE_USER_HOME` directory). An example `gradle.properties` that would give you a heap size of 4GB (rather than the default 1GB) would look like:

```
org.gradle.jvmargs=-Xmx4096m
```

## Testing a form without a server
When you first run Collect, it is set to download forms from [https://demo.getodk.org/](https://demo.getodk.org/), the demo server. You can sometimes verify your changes with those forms but it can also be helpful to put a specific test form on your device. Here are some options for that:

1. The `All question types` form from the default server is [here](https://docs.google.com/spreadsheets/d/1af_Sl8A_L8_EULbhRLHVl8OclCfco09Hq2tqb9CslwQ/edit#gid=0). You can also try [example forms](https://github.com/XLSForm/example-forms) and [test forms](https://github.com/XLSForm/test-forms) or [make your own](https://xlsform.org).

1. Convert the XLSForm (xlsx) to XForm (xml). Use the [ODK website](http://getodk.org/xlsform/) or [XLSForm Offline](https://gumroad.com/l/xlsform-offline) or [pyxform](https://github.com/XLSForm/pyxform).

1. Once you have the XForm, use [adb](https://developer.android.com/studio/command-line/adb.html) to push the form to your device (after [enabling USB debugging](https://www.kingoapp.com/root-tutorials/how-to-enable-usb-debugging-mode-on-android.htm)) or emulator.
	```
	adb push my_form.xml /sdcard/Android/data/org.odk.collect.android/files/forms
	```

1. Launch ODK Collect and tap `Fill Blank Form`. The new form will be there.

## Using APIs for local development

Certain functions in ODK Collect depend on cloud services that require API keys or authorization steps to work.  Here are the steps you need to take in order to use these functions in your development builds.

**Google Maps API**: When the ""Google Maps SDK"" option is selected in the ""User interface"" settings, ODK Collect uses the Google Maps API for displaying maps in the geospatial question types (GeoPoint, GeoTrace, and GeoShape).  To enable this API:
  1. [Get a Google Maps API key](https://developers.google.com/maps/documentation/android-api/signup).  Note that this requires a credit card number, though the card will not be charged immediately; some free API usage is permitted.  You should carefully read the terms before providing a credit card number.
  1. Edit or create `secrets.properties` and set the `GOOGLE_MAPS_API_KEY` property to your API key.  You should end up with a line that looks like this:
    ```
    GOOGLE_MAPS_API_KEY=AIbzvW8e0ub...
    ```

**Mapbox Maps SDK for Android**: When the ""Mapbox SDK"" option is selected in the ""User interface"" settings, ODK Collect uses the Mapbox SDK for displaying maps in the geospatial question types (GeoPoint, GeoTrace, and GeoShape).  To enable this API:
  1. [Create a Mapbox account](https://www.mapbox.com/signup/).  Note that signing up with the ""Pay-As-You-Go"" plan does not require a credit card.  Mapbox provides free API usage up to the monthly thresholds documented at [https://www.mapbox.com/pricing](https://www.mapbox.com/pricing).  If your usage exceeds these thresholds, you will receive e-mail with instructions on how to add a credit card for payment; services will remain live until the end of the 30-day billing term, after which the account will be deactivated and will require a credit card to reactivate.
  2. Find your access token on your [account page](https://account.mapbox.com/) - it should be in ""Tokens"" as ""Default public token"".
  3. Edit or create `secrets.properties` and set the `MAPBOX_ACCESS_TOKEN` property to your access token.  You should end up with a line that looks like this:
    ```
    MAPBOX_ACCESS_TOKEN=pk.eyJk3bumVp4i...
    ```
  4. Create a new secret token with the ""DOWNLOADS:READ"" secret scope and then add it to `secrets.properties` as `MAPBOX_DOWNLOADS_TOKEN`.

*Note: Mapbox will not be available as an option in compiled versions of Collect unless you follow the steps above. Mapbox will also not be available on x86 devices as the native libraries are excluded to reduce the APK size. If you need to use an x86 device, you can force the build to include x86 libs by include the `x86Libs` Gradle parameter. For example, to build a debug APK with x86 libs: `./gradlew assembleDebug -Px86Libs`.*

## Debugging JavaRosa

JavaRosa is the form engine that powers Collect. If you want to debug or change that code while running Collect you can deploy it locally with Maven (you'll need `mvn` and `sed` installed):

1. Build and install your changes of JavaRosa (into your local Maven repo):

```bash
./gradlew installLocal
```

1. Change `const val javarosa = javarosa_online` in `Dependencies.kt` to `const val javarosa = javarosa_local`

## Troubleshooting

#### Error when running Robolectric tests from Android Studio on macOS: `build/intermediates/bundles/debug/AndroidManifest.xml (No such file or directory)`
> Configure the default JUnit test runner configuration in order to work around a bug where IntelliJ / Android Studio does not set the working directory to the module being tested. This can be accomplished by editing the run configurations, Defaults -> JUnit and changing the working directory value to $MODULE_DIR$.

> Source: [Robolectric Wiki](https://github.com/robolectric/robolectric/wiki/Running-tests-in-Android-Studio#notes-for-mac).

#### Android Studio Error: `SDK location not found. Define location with sdk.dir in the local.properties file or with an ANDROID_HOME environment variable.`
When cloning the project from Android Studio, click ""No"" when prompted to open the `build.gradle` file and then open project.

#### Execution failed for task ':collect_app:transformClassesWithInstantRunForDebug'.

We have seen this problem happen in both IntelliJ IDEA and Android Studio, and believe it to be due to a bug in the IDE, which we can't fix.  As a workaround, turning off [Instant Run](https://developer.android.com/studio/run/#set-up-ir) will usually avoid this problem. The problem is fixed in Android Studio 3.5 with the new [Apply Changes](https://medium.com/androiddevelopers/android-studio-project-marble-apply-changes-e3048662e8cd) feature.

#### Moving to the main view if user minimizes the app
If you build the app on your own using Android Studio `(Build -> Build APK)` and then install it (from an `.apk` file), you might notice this strange behaviour thoroughly described: [#1280](https://github.com/getodk/collect/issues/1280) and [#1142](https://github.com/getodk/collect/issues/1142).

This problem occurs building other apps as well.

#### gradlew Failure: `FAILURE: Build failed with an exception.`

If you encounter an error similar to this when running `gradlew`:

```
FAILURE: Build failed with an exception

What went wrong:
A problem occurred configuring project ':collect_app'.
> Failed to notify project evaluation listener.
   > Could not initialize class com.android.sdklib.repository.AndroidSdkHandler
```

You may have a mismatch between the embedded Android SDK Java and the JDK installed on your machine. You may wish to set your **JAVA_HOME** environment variable to that SDK. For example, on macOS:

`export JAVA_HOME=""/Applications/Android\ Studio.app/Contents/jre/Contents/Home/""
`

Note that this change might cause problems with other Java-based applications (e.g., if you uninstall Android Studio).

#### gradlew Failure: `java.lang.NullPointerException (no error message).`
If you encounter the `java.lang.NullPointerException (no error message).` when running `gradlew`, please make sure your Java version for this project is Java 17.

This can be configured under **File > Project Structure** in Android Studio, or by editing `$USER_HOME/.gradle/gradle.properties` to set `org.gradle.java.home=(path to JDK home)` for command-line use.

#### `Unable to resolve artifact: Missing` while running tests

This is encountered when Robolectric has problems downloading the jars it needs for different Android SDK levels. If you keep running into this you can download the JARs locally and point Robolectric to them by doing:

```
./download-robolectric-deps.sh
```

## Creating signed releases for Google Play Store
Maintainers keep a folder with a clean checkout of the code and use [jenv.be](https://www.jenv.be) in that folder to ensure compilation with Java 17.

### Release prerequisites:

- a`local.properties` file in the root folder with the following:
  ```
  sdk.dir=/path/to/android/sdk
  ```

- the keystore file and passwords

- a `secrets.properties` file in the root project folder folder with the following:
  ```
  // secrets.properties
  RELEASE_STORE_FILE=/path/to/collect.keystore
  RELEASE_STORE_PASSWORD=secure-store-password
  RELEASE_KEY_ALIAS=key-alias
  RELEASE_KEY_PASSWORD=secure-alias-password
  ```

- a `google-services.json` file in the `collect_app/src/odkCollectRelease` folder. The contents of the file are similar to the contents of `collect_app/src/google-services.json`.

### Release checklist:

- update translations
- make sure CI is green for the chosen commit
- run `./gradlew releaseCheck`. If successful, a signed release will be at `collect_app/build/outputs/apk` (with an old version name)
- verify a basic ""happy path"": scan a QR code to configure a new project, get a blank form, fill it, open the form map (confirms that the Google Maps key is correct), send form
- verify new APK can be installed as update to previous version and that above ""happy path"" works in that case also
- create and publish scheduled forum post with release description
- write Play Store release notes, include link to forum post
- create a release with the correct version by tagging the commit and running `./collect_app:assembleOdkCollectRelease`
  - Tags for full releases must have the format `vX.X.X`. Tags for beta releases must have the format `vX.X.X-beta.X`.
- add a release to Github [here](https://github.com/getodk/collect/releases), generate release notes and attach the APK
- upload APK to Play Store
- if there was an active beta before release (this can happen with point releases), publish a new beta release to replace the previous one which was disabled by the production release
- backup dependencies for the release by downloading the `vX.X.X.tar` artifact from the `create_dependency_backup` job on Circle CI (for the release commit) and then uploading it to the ""Collect Dependency Backups"" folder in GetODK's Google Drive

## Compiling a previous release using backed-up dependencies

1. Download the `.tar` for relevant release tag
2. Extract `.local-m2` into the project directory:
    ```bash
    tar -xf maven.tar -C 
    ```
   
The project will now be able to fetch dependencies that are no longer available (but were used to compile the release) from the `.local-m2` Maven repo.

","'android', 'data-collection', 'global-development', 'global-health', 'java', 'mhealth', 'mobile-data-collection', 'odk', 'social-impact', 'xforms'",2024-05-03T11:04:03Z,157,699,62,"('grzesiek2010', 5385), ('seadowg', 4553), ('lognaturel', 1124), ('yanokwa', 771), ('mitchellsundt', 465), ('SaumiaSinghal', 456), ('shobhitagarwal1612', 448), ('zestyping', 189), ('chartung', 114), ('dcbriccetti', 69), ('cooperka', 67), ('max2me', 45), ('dimwight', 40), ('jd-alexander', 40), ('hypercubestart', 39), ('meletis', 35), ('breakbusyloop', 26), ('mmarciniak90', 24), ('chidauri', 21), ('batkinson', 20), ('heyjamesknight', 19), ('mapkon', 18), ('huangyz0918', 18), ('lakshyagupta21', 18), ('jnordling', 17), ('alxndrsn', 16), ('bartoszfilipowicz', 13), ('akshay-ap', 12), ('steelej-arm', 12), ('jbeorse', 12), ('TomRCummings', 10), ('nribeka', 10), ('pratikmjoshi', 9), ('rajatthakur', 9), ('kkrawczyk123', 8), ('dr0pdb', 8), ('jwishnie', 8), ('lindsay-stevens', 7), ('nap2000', 6), ('ItloMd9', 6), ('xsteelej', 6), ('ekigamba', 5), ('abhishek-wiai', 5), ('YuraLaguta', 5), ('MukundAnanthu', 5), ('ArmenApresyan8', 5), ('icemc', 4), ('avin-sharma', 4), ('qlands', 4), ('shreyasnbhat', 4), ('ShenJack', 4), ('getsanjeev', 4), ('yadav-rahul', 4), ('DreamyPhobic', 4), ('ajwad-shaikh', 4), ('rajkumaar23', 3), ('ggalmazor', 3), ('pranavgupta1234', 3), ('Thodoris1999', 3), ('siavash1986', 3), ('championoverthinker', 3), ('sanovikov71', 3), ('sashank27', 3), ('peterh-uk', 3), ('nshestiuk', 3), ('lokeshkvn', 3), ('dsrees', 3), ('Archaejohn', 3), ('abdulwd', 3), ('anudeepti2004', 2), ('hossamnasser938', 2), ('linl33', 2), ('muarachmann', 2), ('nikhilbalyan', 2), ('pedrop30', 2), ('ravi-kishan', 2), ('royrivnam', 2), ('shouryaj', 2), ('vkdrn', 2), ('SIMRAN1', 2), ('Unreal-Dreams', 2), ('nmpatsal', 2), ('trebla64', 2), ('ctsims', 2), ('Ishmeetsingh97', 2), ('JainilGada', 2), ('jamesjhu', 2), ('marcos-lg', 2), ('raks097', 2), ('rijuldhir', 2), ('srsudar', 2), ('Vivever', 2), ('vrjgamer', 2), ('spwoodcock', 1), ('ugoamanoh', 1), ('codedsun', 1), ('tronku', 1), ('shubhamkumar1739', 1), ('Shrreya', 1), ('shouryalala', 1), ('ShivamArora', 1), ('sanjitschouhan', 1), ('supreme96', 1), ('saurabhjn76', 1), ('rk635636', 1), ('rishsethia', 1), ('Snailed', 1), ('codetriage-readme-bot', 1), ('shldhll', 1), ('rkmittal', 1), ('paraschadha2052', 1), ('marcparicio', 1), ('lumarega', 1), ('krishnarai30', 1), ('jybaro', 1), ('gmn0105', 1), ('cleoGson', 1), ('clarlars', 1), ('cdwuestner', 1), ('carstendev', 1), ('vladalexgit', 1), ('VisheshVadhera', 1), ('vikasdesale', 1), ('iammvaibhav', 1), ('pkarira', 1), ('faizmalkani', 1), ('Divya063', 1), ('didicodes', 1), ('thedavidmccann', 1), ('kaftand', 1), ('charlesfleche', 1), ('btrajkovski', 1), ('Bastiantheone', 1), ('Diefenbaker', 1), ('arundhatigupta', 1), ('anilshatharashi', 1), ('andrewsiew2', 1), ('akashdobaria', 1), ('adithya321', 1), ('thejainabhi', 1), ('AbanoubGhadban', 1), ('owenkim', 1), ('konishon', 1), ('hallahan', 1), ('nani92', 1), ('NamanArora', 1), ('Midhun07', 1), ('MartijnR', 1), ('marshallb93', 1), ('mkne', 1), ('KiaFarhang', 1), ('kunalsingh3110', 1), ('nexusbr', 1), ('jovaee', 1), ('joeflack4', 1), ('isabelcosta', 1), ('Genius1237', 1)","[17, 'Partnerships for the Goals']"
openfoodsource/OFS_trunk,The main development branch of Open Food Source software for local food networks,"OFS_trunk
=========

The main development branch of Open Food Source software for local food networks


#### Links to the GitHub Repository
* [Milestones](https://github.com/openfoodsource/OFS_trunk/milestones)
* [Issues](https://github.com/openfoodsource/OFS_trunk/issues)
",,2018-03-05T19:42:46Z,1,14,8,"('guising', 147)","[12, 'Responsible Consumption and Production']"
openstreetmap/openstreetmap-website,The Rails application that powers OpenStreetMap,"# openstreetmap-website

[![Lint](https://github.com/openstreetmap/openstreetmap-website/workflows/Lint/badge.svg?branch=master&event=push)](https://github.com/openstreetmap/openstreetmap-website/actions?query=workflow%3ALint%20branch%3Amaster%20event%3Apush)
[![Tests](https://github.com/openstreetmap/openstreetmap-website/workflows/Tests/badge.svg?branch=master&event=push)](https://github.com/openstreetmap/openstreetmap-website/actions?query=workflow%3ATests%20branch%3Amaster%20event%3Apush)
[![Coverage Status](https://coveralls.io/repos/openstreetmap/openstreetmap-website/badge.svg?branch=master)](https://coveralls.io/r/openstreetmap/openstreetmap-website?branch=master)

This is `openstreetmap-website`, the [Ruby on Rails](http://rubyonrails.org/)
application that powers the [OpenStreetMap](https://www.openstreetmap.org) website and API.

This repository consists of:

* The web site, including user accounts, diary entries, user-to-user messaging.
* The XML- and JSON-based editing [API](https://wiki.openstreetmap.org/wiki/API_v0.6).
* The integrated version of the [iD](https://wiki.openstreetmap.org/wiki/ID) editor.
* The Browse pages - a web front-end to the OpenStreetMap data.
* The GPX uploads, browsing and API.

A fully-functional `openstreetmap-website` installation depends on other services, including map tile
servers and geocoding services, that are provided by other software. The default installation
uses publicly-available services to help with development and testing.

# License

This software is licensed under the [GNU General Public License 2.0](https://www.gnu.org/licenses/old-licenses/gpl-2.0.txt),
a copy of which can be found in the [LICENSE](LICENSE) file.

# Installation

`openstreetmap-website` is a Ruby on Rails application that uses PostgreSQL as its database, and has a large
number of dependencies for installation. For full details please see [INSTALL.md](INSTALL.md).

# Development

We're always keen to have more developers! Pull requests are very welcome.

* Bugs are recorded in the [issue tracker](https://github.com/openstreetmap/openstreetmap-website/issues).
* Translation is managed by [Translatewiki](https://translatewiki.net/wiki/Translating:OpenStreetMap).
* Local Chapters shown on the Communities page, and their translations, come from [osm-community-index](https://github.com/osmlab/osm-community-index).
* There is a [rails-dev@openstreetmap.org](https://lists.openstreetmap.org/listinfo/rails-dev) mailing list for development discussion.
* IRC - there is the #osm-dev channel on irc.oftc.net.

More details on contributing to the code are in the [CONTRIBUTING.md](CONTRIBUTING.md) file.

# Maintainers

* Tom Hughes [@tomhughes](https://github.com/tomhughes/)
* Andy Allan [@gravitystorm](https://github.com/gravitystorm/)
","'openstreetmap', 'rails-application', 'ruby'",2024-05-02T16:57:04Z,210,2030,104,"('tomhughes', 6579), ('gravitystorm', 1481), ('translatewiki', 623), ('AntonKhorev', 559), ('jfirebaugh', 483), ('systemed', 363), ('avar', 342), ('dependabotbot', 280), ('smsm1', 271), ('zerebubuth', 245), ('siebrand', 214), ('SteveC', 148), ('Nikerabbit', 103), ('bhousel', 98), ('aaronlidman', 86), ('apmon', 84), ('IgnoredAmbience', 79), ('stefanb', 74), ('John07', 63), ('crschmidt', 63), ('saintamh', 62), ('tmcw', 61), ('Firefishy', 60), ('mmd-osm', 59), ('samanpwbb', 56), ('simonpoole', 56), ('woodpeck', 50), ('dankarran', 48), ('jguthrie100', 45), ('quincylvania', 35), ('tyrasd', 35), ('migurski', 32), ('sbagroy986', 30), ('danstowell', 27), ('harry-wood', 25), ('mikelmaron', 23), ('pnorman', 23), ('edenh', 21), ('simon04', 20), ('milan-cvetkovic', 19), ('HolgerJeromin', 18), ('tlevine', 18), ('ENT8R', 16), ('grekko', 14), ('jsoref', 14), ('jalessio', 14), ('emersion', 12), ('johnmckerrell', 12), ('lonvia', 12), ('cflipse', 11), ('dudarev', 10), ('althio', 9), ('floscher', 9), ('mortenbruhn', 9), ('boubascript', 8), ('hikemaniac', 8), ('tordans', 8), ('ppawel', 7), ('matkoniecz', 7), ('randomjunk', 7), ('Zverik', 6), ('nickw1', 6), ('mvexel', 5), ('Robbendebiene', 5), ('deejay1', 5), ('mxdanger', 5), ('danieldegroot2', 5), ('nebulon42', 5), ('iandees', 5), ('damned', 5), ('balrog-kun', 5), ('grischard', 4), ('biswesh456', 4), ('karussell', 4), ('tuxayo', 4), ('ssinger', 4), ('mapmeld', 4), ('51114u9', 4), ('KristjanESPERANTO', 4), ('bithive', 4), ('sonalkr132', 4), ('IvanSanchez', 4), ('bezdna', 4), ('erictheise', 4), ('chrstnbwnkl', 4), ('mbrzakovic', 3), ('lectrician1', 3), ('willemarcel', 3), ('Dimitar5555', 3), ('tuckerrc', 3), ('PedaB', 3), ('andrewharvey', 3), ('tankhiwale', 3), ('stalker314314', 3), ('jgpacker', 3), ('josh-works', 3), ('k-yle', 2), ('mstock', 2), ('marcows', 2), ('Nakaner', 2), ('krubokrubo', 2), ('sladen', 2), ('rjmunro', 2), ('ToeBee', 2), ('wburns84', 2), ('hlfan', 2), ('plarus', 2), ('polarbearing', 2), ('rubynho', 2), ('LaoshuBaby', 2), ('ingalls', 2), ('openbrian', 2), ('Abijeet', 2), ('amire80', 2), ('benreyn', 2), ('spilth', 2), ('twain47', 2), ('claudiush', 2), ('mackerski', 2), ('enockseth', 2), ('seav', 2), ('JesseWeinstein', 2), ('IshmeetSingh06', 2), ('imi0', 2), ('hiddewie', 2), ('hanchao', 2), ('dittaeva', 2), ('frodrigo', 2), ('fredrik-lindseth', 2), ('francois2metz', 2), ('fazlerabbi37', 2), ('seanhussey', 1), ('SK53', 1), ('drolbr', 1), ('rjw62', 1), ('goldfndr', 1), ('rajdeep26', 1), ('rahulvaish', 1), ('RM87', 1), ('booo', 1), ('mormegil-cz', 1), ('jongleur1983', 1), ('pdgtips', 1), ('MaZderMind', 1), ('zbycz', 1), ('TheMarex', 1), ('NumbuhFour', 1), ('nilsnolde', 1), ('nikolas', 1), ('1ec5', 1), ('ukasiu', 1), ('wtrbtl03', 1), ('tkas', 1), ('tilmanb', 1), ('superDoss', 1), ('rugk', 1), ('nlaratta', 1), ('mtmail', 1), ('faebebin', 1), ('emacsen', 1), ('daganzdaanda', 1), ('ardean80', 1), ('ZhengLin-Li', 1), ('shirayuki', 1), ('jiaxuyang', 1), ('vincentdephily', 1), ('CommanderRoot', 1), ('thomersch', 1), ('xnyhps', 1), ('cortesimone', 1), ('drewda', 1), ('dorotheakazazi', 1), ('domoritz', 1), ('davidgumberg', 1), ('danrademacher', 1), ('robotscissors', 1), ('genodeftest', 1), ('c960657', 1), ('gaddman', 1), ('bmarchant', 1), ('urbalazs', 1), ('AndrewHain', 1), ('saerdnaer', 1), ('nuntius35', 1), ('wilfoa', 1), ('altilunium', 1), ('Self-Perfection', 1), ('wildintellect', 1), ('atomoil', 1), ('acconrad', 1), ('mikini', 1), ('MattHeard', 1), ('mnalis', 1), ('marjanvandekauter-tomtom', 1), ('malteger', 1), ('IOOI-SqAR', 1), ('kshitizkhanal7', 1), ('Mortein', 1), ('vool', 1), ('filbertkm', 1), ('KasperFranz', 1), ('dalzinho', 1), ('Janjko', 1), ('JamesKingdom', 1), ('Hufkratzer', 1), ('Gnonthgol', 1), ('frafra', 1), ('freundchen', 1), ('Edward17', 1), ('EdwardBetts', 1)","[17, 'Partnerships for the Goals']"
Gapminder/dollar-street-pages,Dollar Street public pages,"# Test coverage
[![Build Status](https://travis-ci.org/Gapminder/dollar-street-pages.svg?branch=development)](https://travis-ci.org/Gapminder/dollar-street-pages)
[![codecov.io](https://codecov.io/github/Gapminder/dollar-street-pages/coverage.svg?branch=development)](https://codecov.io/github/Gapminder/dollar-street-pages?branch=development)
[![Dependency Status](https://david-dm.org/Gapminder/dollar-street-pages.svg)](https://david-dm.org/Gapminder/dollar-street-pages)
[![devDependency Status](https://david-dm.org/Gapminder/dollar-street-pages/dev-status.svg)](https://david-dm.org/Gapminder/dollar-street-pages#info=devDependencies)

# DS Consumer

[![Throughput Graph](https://graphs.waffle.io/Gapminder/dollar-street-pages/throughput.svg)](https://waffle.io/Gapminder/dollar-street-pages/metrics)

# How to install
- `git clone`
- `npm i`
- `npm start`
- *hint*: on first start if you see `can to GET /`, wait for build and refresh pages

## Credits
Crossbrowser testing sponsored by [Browser Stack](https://www.browserstack.com)
[](https://www.browserstack.com)

",,2022-06-24T09:14:14Z,13,14,13,"('Betrozov', 561), ('Ugmaxie', 247), ('NepipenkoIgor', 218), ('daniloff200', 149), ('Miffael', 58), ('ludmilanesvitiy', 38), ('valorkin', 25), ('sergeyKhristenko', 22), ('korel-san', 5), ('Legionivo', 4), ('AntonOlkhovskyi', 4), ('Yuriy777', 3), ('sergeimelnik1980', 3)","[17, 'Partnerships for the Goals']"
moodle/moodle,Moodle - the world's open source learning platform,"# Moodle


  


[Moodle][1] is the World's Open Source Learning Platform, widely used around the world by countless universities, schools, companies, and all manner of organisations and individuals.

Moodle is designed to allow educators, administrators and learners to create personalised learning environments with a single robust, secure and integrated system.

## Documentation

- Read our [User documentation][3]
- Discover our [developer documentation][5]
- Take a look at our [demo site][4]

## Community

[moodle.org][1] is the central hub for the Moodle Community, with spaces for educators, administrators and developers to meet and work together.

You may also be interested in:

- attending a [Moodle Moot][6]
- our regular series of [developer meetings][7]
- the [Moodle User Association][8]

## Installation and hosting

Moodle is Free, and Open Source software. You can easily [download Moodle][9] and run it on your own web server, however you may prefer to work with one of our experienced [Moodle Partners][10].

Moodle also offers hosting through both [MoodleCloud][11], and our [partner network][10].

## License

Moodle is provided freely as open source software, under version 3 of the GNU General Public License. For more information on our license see

[1]: https://moodle.org
[2]: https://moodle.com
[3]: https://docs.moodle.org/
[4]: https://sandbox.moodledemo.net/
[5]: https://moodledev.io
[6]: https://moodle.com/events/mootglobal/
[7]: https://moodledev.io/general/community/meetings
[8]: https://moodleassociation.org/
[9]: https://download.moodle.org
[10]: https://moodle.com/partners
[11]: https://moodle.com/cloud
[12]: https://moodledev.io/general/license
",,2024-05-03T01:13:43Z,206,5401,424,"('stronk7', 6795), ('andrewnicols', 5557), ('danpoltawski', 5326), ('skodak', 4551), ('junpataleta', 2870), ('timhunt', 2351), ('sarjona', 2170), ('marinaglancy', 2112), ('mudrd8mz', 2012), ('paulholden', 1443), ('snake', 1250), ('mdjnelson', 1093), ('abgreeve', 1051), ('jleyva', 1016), ('ilyatregubov', 1000), ('lameze', 923), ('rezaies', 909), ('moodler', 720), ('HuongNV13', 635), ('ankitagarwal', 634), ('ryanwyllie', 581), ('danmarsden', 534), ('mouneyrac', 519), ('sammarshallou', 515), ('jamiepratt', 443), ('vmdef', 384), ('Chocolate-lightning', 317), ('andyjdavis', 315), ('wildgirl', 305), ('ferranrecio', 302), ('lucaboesch', 295), ('mickhawkins', 292), ('dmonllao', 233), ('brendanheywood', 200), ('meirzamoodle', 167), ('cameorn1730', 160), ('jsnfwlr', 160), ('ericmerrill', 155), ('sbourget', 144), ('davosmith', 142), ('roland04', 126), ('scara', 126), ('barbararamiro', 124), ('ppichet', 123), ('nadavkav', 120), ('polothy', 106), ('laurentdavid', 104), ('kabalin', 102), ('cescobedo', 102), ('rwijaya', 101), ('marxjohnson', 98), ('NeillM', 96), ('nebgor', 95), ('davewoloszyn', 92), ('dpalou', 87), ('gbateson', 84), ('mackensen', 83), ('safatshahin', 82), ('zig-moodle', 81), ('scriby', 81), ('FMCorz', 80), ('aolley', 79), ('andelacruz', 77), ('taboubi', 73), ('mr-russ', 72), ('dravek', 70), ('golenkovm', 69), ('sharidas', 68), ('micaherne', 67), ('grabs', 65), ('tonyjbutler', 62), ('crazyserver', 58), ('mkassaei', 58), ('gauts', 54), ('gjb2048', 54), ('andrewhancox', 52), ('iarenaza', 51), ('marsh0lion', 48), ('pauln', 47), ('bostelm', 45), ('xow', 43), ('zbdd', 41), ('pcharsle', 40), ('stevandoMoodle', 37), ('danielneis', 37), ('dmitriim', 37), ('raortegar', 36), ('abias', 35), ('claudevervoort', 34), ('jstabinger', 33), ('usqfowlerj', 32), ('tlock', 32), ('gaudreaj', 29), ('leonstr', 29), ('samhemelryk', 29), ('Dagefoerde', 29), ('rlorenzo', 28), ('Peterburnett', 28), ('cameron1729', 28), ('dthies', 27), ('peterRd', 26), ('piersharding', 26), ('ssj365', 25), ('mchurchward', 24), ('jboulen', 24), ('ziegenberg', 24), ('tobiasreischmann', 23), ('TomoTsuyuki', 23), ('srdjan-catalyst', 22), ('dcai', 21), ('matthewhilton', 21), ('simoncoggins', 21), ('Syxton', 20), ('sk-unikent', 20), ('mayankgupta', 20), ('juancs', 20), ('keevan', 20), ('andreabix', 19), ('vadimonus', 19), ('JBThong', 19), ('jrchamp', 18), ('tmuras', 18), ('AnupamaSarjoshi', 18), ('NoelDeMartin', 18), ('samchaffee', 18), ('nobelium', 18), ('Kathrin84', 18), ('t-schroeder', 18), ('sh-csg', 18), ('PhMemmel', 17), ('kwiliarty', 17), ('doctorlard', 17), ('rmady', 17), ('leblangi', 17), ('nicolasconnault', 16), ('OdyX', 16), ('andrewmadden', 15), ('Beedell', 15), ('srobotta', 15), ('jinhofer', 15), ('greg-or', 15), ('sensei-hacker', 14), ('doraemonyoung', 14), ('spvickers', 14), ('rjnl', 14), ('kanikagoyal', 13), ('lostrogit', 13), ('devang-gaur', 12), ('nwp90', 11), ('PhilippImhof', 11), ('rushi963', 11), ('sunner', 11), ('farhan6318', 11), ('MartinGauk', 10), ('michael-milette', 10), ('peterbulmer', 10), ('woolardfa', 9), ('euven', 9), ('apsdehal', 8), ('shashitechno', 8), ('jfederico', 8), ('tomlanyon', 8), ('oasychev', 8), ('ndunand', 8), ('mhughes2k', 8), ('marcghaly', 8), ('kenneth-hendricks', 8), ('jfilip', 8), ('fabiomsouto', 8), ('colin-umn', 8), ('brki', 8), ('arborrow', 8), ('alexmorrisnz', 8), ('mwehr', 7), ('ak4t0sh', 7), ('willylee', 7), ('pedrojordao', 7), ('pavelsokolov', 7), ('matt-catalyst', 7), ('jason-platts', 7), ('izendegi', 7), ('guillogo', 7), ('cdsmith-umn', 7), ('barrysspace', 7), ('danghieu1407', 6), ('ramdesh', 6), ('nhoobin', 6), ('satrun77', 6), ('marcusboon', 6), ('jswebster', 6), ('jamie-catalyst', 6), ('jackson-catalyst', 6), ('lougroshek', 6), ('amandadoughty', 6), ('santoshndev', 5), ('rajeshtaneja', 5), ('LukeCarrier', 5), ('larsbonczek', 5), ('kastashov', 5), ('jayeshanandani', 5), ('fwsl', 5), ('davidknu', 5), ('danowar2k', 5), ('damyon', 5), ('tasosb', 5), ('Fragonite', 5)","[4, 'Quality Education']"
teliportme/remixvr,RemixVR is a tool for collaboratively building customisable VR experiences.,"# RemixVR

[![codecov](https://codecov.io/gh/teliportme/remixVR/branch/master/graph/badge.svg)](https://codecov.io/gh/teliportme/remixVR)

## RemixVR

RemixVR is a tool to collaboratively build editable VR experiences.

The editable VR experiences are called VR templates. Anyone can create a VR template. Each VR template will have configurable options that can be selected by the user. These options allow the user to create custom VR experiences from a single VR template.

![RemixVR](https://media.giphy.com/media/KZfKUhK06Gc8KL0O6Y/giphy.gif)

For example, let's look at the [ObjectsVR](https://github.com/teliportme/remixvr/tree/master/themes/packages/objectsvr) template. It has three options. You can change the object, the 360 background and the sound it creates when hovering on the object.

In the initial version, these options are updated by editing the code. However, as we continue to develop RemixVR, we'll be including a visual way to update the options.

### Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Documentation

You can read [RemixVR docs](https://docs.remixvr.org/) to view all the documentation about the project.

### Demos

* [Atomic Bohr Model](https://bohrmodel-remixvr.netlify.com/)
* [360 VR](https://360vr-remixvr.netlify.com/)
* [Particles VR](https://particlevr-remixvr.netlify.com/)
* [Objects VR](https://objectsvr-remixvr.netlify.com/)

### Installing RemixVR

To install and setup RemixVR, please see the [installation guide](docs/installation.md#steps-to-setup-remixvr-on-your-computer).

#### Running a VR template

Once you have installed all the dependencies, to start a VR template, go to the template folder and in the terminal run `yarn start`.

For example, to start the ObjectsVR template, go to the template folder.

```bash
cd packages/objectsvr
```

Once you're inside the template folder, then start the development server.

```bash
yarn start
```

You can view the template by going to `http://localhost:8080/` on your browser.

### Running the tests

To run the test, navigate to the root folder of remixVR. Then run:

```bash
yarn run test
```

The test files are present inside the template folder. The test files ends with _.test.js_ in their file name. You can look at [ObjectsVR test](https://github.com/teliportme/remixVR/tree/3bfcac83b55bf003900a1b90e61a49466b3a5bf4/templates/packages/objectsvr/objectsvr.test.js) file to see an example.

### Built With

* [a-frame](https://aframe.io/)
* [webpack](https://webpack.js.org/)
* [karma](https://karma-runner.github.io/)
* [mocha](https://mochajs.org/)
* [chai](http://www.chaijs.com/)
* [sinon](http://sinonjs.org/)

### Contributing

Please read [CODE OF CONDUCT](https://github.com/teliportme/remixVR/tree/3bfcac83b55bf003900a1b90e61a49466b3a5bf4/CODE_OF_CONDUCT.md) for details on our code of conduct. Please read [CONTRIBUTING GUIDE](https://github.com/teliportme/remixVR/tree/3bfcac83b55bf003900a1b90e61a49466b3a5bf4/CONTRIBUTING.md) before contributing.

### Versioning

We use [SemVer](http://semver.org/) for versioning the VR templates.

### Authors

* [**Rison Simon**](https://risonsimon.com)

See also the list of [contributors](https://github.com/teliportme/remixVR/contributors) who participated in this project.

### License

This project is licensed under the GPL v3 License - see the [LICENSE.md](https://github.com/teliportme/remixVR/tree/3bfcac83b55bf003900a1b90e61a49466b3a5bf4/LICENSE.md) file for details.

","'daydream', 'oculus', 'threejs', 'virtual-reality', 'virtual-reality-experiences', 'virtualreality', 'vive', 'vr', 'vr-apps', 'web-vr', 'webvr', 'webxr'",2019-11-19T11:26:50Z,3,140,12,"('risonsimon', 431), ('dependabotbot', 8), ('Utopiah', 1)","[4, 'Quality Education']"
OneBusAway/onebusaway-android,The official Android app for OneBusAway,"# OneBusAway for Android [![Android CI Build](https://github.com/OneBusAway/onebusaway-android/actions/workflows/android.yml/badge.svg)](https://github.com/OneBusAway/onebusaway-android/actions/workflows/android.yml)

This is the official Android / Fire Phone app for [OneBusAway](https://onebusaway.org/), a project of the non-profit [Open Transit Software Foundation](https://opentransitsoftwarefoundation.org/)!

[<img src=""https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png""
      alt=""Get it on Google Play""
      height=""80"">](https://play.google.com/store/apps/details?id=com.joulespersecond.seattlebusbot)



OneBusAway for Android provides:

1. Real-time arrival/departure information for public transit
2. A browse-able map of nearby stops
3. A list of favorite bus stops
4. Reminders to notify you when your bus is arriving or departing
5. The ability to search for nearby stops or routes
6. Real-time multimodal trip planning, using real-time transit and bike share information (requires a regional [OpenTripPlanner](http://www.opentripplanner.org/) server)
6. Bike share map layer, which includes real-time availability information for floating bikes and bike rack capacity (requires a regional [OpenTripPlanner](http://www.opentripplanner.org/) server)
7. Issue reporting to any Open311-compliant issue management system (see [this page](ISSUE_REPORTING.md) for details)

OneBusAway for Android automatically keeps track of your most used stops and routes, and allows you to put shortcuts on your phone's home screen for any stop or route you choose.

## Alpha and Beta Testing

Get early access to new OneBusAway Android versions, and help us squash bugs! See our [Testing Guide](https://github.com/OneBusAway/onebusaway-android/blob/main/BETA_TESTING.md) for details.

## Build Setup

Want to build the project yourself and test some changes?  See our [build documentation](BUILD.md).

## Contributing

We welcome contributions to the project! Please see our [Contributing Guide](https://github.com/OneBusAway/onebusaway-android/blob/main/.github/CONTRIBUTING.md) for details, including Code Style Guidelines and Template.

## System Architecture

Curious what servers power certain features in OneBusAway Android?  Check out the [System Architecture page](SYSTEM_ARCHITECTURE.md).

## Deploying OneBusAway Android in Your City

There are two ways to deploy OneBusAway Android in your city:

1. **Join the OneBusAway [multi-region project](https://github.com/OneBusAway/onebusaway/wiki/Multi-Region)** - The easiest way to get started - simply set up your own OneBusAway server with your own transit data, and get added to the OneBusAway apps!  See [this page](https://github.com/OneBusAway/onebusaway/wiki/Multi-Region) for details.
2. **Deploy a rebranded version of OneBusAway Android as your own app on Google Play** - Requires a bit more maintenance, but it allows you to set up your own app on Google Play based on the OneBusAway Android source code. See [rebranding page](https://github.com/OneBusAway/onebusaway-android/blob/main/REBRANDING.md) for details.

## Testing Your Own OneBusAway/OpenTripPlanner servers

Did you just set up your own [OneBusAway](https://github.com/OneBusAway/onebusaway-application-modules/wiki) and/or [OpenTripPlanner](http://www.opentripplanner.org/) server?  You can test both in this app without compiling any Android code.  Just download the app from [Google Play](https://play.google.com/store/apps/details?id=com.joulespersecond.seattlebusbot), and see our [Custom Server Setup Guide](CUSTOM_SERVERS.md) for details.

## Permissions

In order to support certain features in OneBusAway, we need to request various permissions to access information on your device.  See an explanation of why each permission is needed [here](PERMISSIONS.md).

## Troubleshooting

Things not going well building the project?  See our [Troubleshooting](TROUBLESHOOTING.md) section.  If you're a user of the app, check out our [FAQ](FAQ.md).

## OneBusAway Project

Want to learn more about the [OneBusAway project](https://onebusaway.org/), a project of the non-profit [Open Transit Software Foundation](https://opentransitsoftwarefoundation.org/)? [Read up on the entire Application Suite](https://github.com/OneBusAway/onebusaway-application-modules) and/or [learn more about the mobile apps](https://github.com/OneBusAway/onebusaway-application-modules/wiki/Mobile-App-Design-Considerations).
","'android', 'java', 'onebusaway', 'open-transit-software-foundation', 'public-transportation', 'transit'",2024-04-30T17:19:54Z,35,462,33,"('barbeau', 1609), ('paulcwatts', 562), ('cagryInside', 63), ('themonki', 38), ('aaronbrethorst', 27), ('acrown-msft', 23), ('amrhossamdev', 23), ('deweyx', 15), ('bbodenmiller', 11), ('sdjacobs', 6), ('bendu', 5), ('millanp', 4), ('sebesti0n', 3), ('carvalhorr', 3), ('sjgllghr', 2), ('kungharrison', 2), ('hayato1234', 2), ('gitter-badger', 2), ('gourabsingha1', 2), ('bridgette', 2), ('yonichanowitz', 1), ('tjasz', 1), ('paulnabanita1', 1), ('tmsyrt', 1), ('sausag3', 1), ('therajanmaurya', 1), ('Pokechu22', 1), ('mttmllns', 1), ('k7lim', 1), ('findjigar', 1), ('israteneda', 1), ('egconley', 1), ('Elyanne', 1), ('charles-b-stb', 1), ('Adithya-hv', 1)","[11, 'Sustainable Cities and Communities']"
get-alex/atom-linter-alex,"Linter plugin using alex to catch insensitive, inconsiderate writing","# atom-linter-alex

Deprecated.
[Atom is archived](https://github.blog/2022-06-08-sunsetting-atom/).

[Git is still intact](https://github.com/get-alex/atom-linter-alex/tree/b9f58d4).
",,2022-06-18T15:30:01Z,4,86,5,"('wooorm', 91), ('steelbrain', 2), ('blakeembrey', 1), ('greenkeeperio-bot', 1)","[11, 'Sustainable Cities and Communities']"
getodk/briefcase,ODK Briefcase is a Java application for fetching and pushing forms and their contents. It helps make billions of data points from ODK portable. Contribute and make the world a better place! ✨💼✨,"## ⚠️ ODK Briefcase is no longer being updated. If you're using Briefcase for CSV exports, data decryption, or automation, please use [ODK Central](https://github.com/getodk/central) instead. ⚠️

# ODK Briefcase
![Platform](https://img.shields.io/badge/platform-Java-blue.svg)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build status](https://circleci.com/gh/getodk/briefcase.svg?style=shield&circle-token=:circle-token)](https://circleci.com/gh/getodk/briefcase)
[![Slack](https://img.shields.io/badge/chat-on%20slack-brightgreen)](https://slack.getodk.org)

ODK Briefcase is a desktop application that can locally store survey results gathered with [ODK Collect](https://docs.getodk.org/collect-intro/). It can also be used to make local copies and CSV exports of data from [ODK Aggregate](https://docs.getodk.org/aggregate-intro/) (or compatible servers) and push data to those servers.

ODK Briefcase is part of ODK, a free and open-source set of tools which help organizations author, field, and manage mobile data collection solutions. Learn more about the ODK project and its history [here](https://getodk.org) and read about example ODK deployments [here](https://forum.getodk.org/c/showcase).

* ODK website: [https://getodk.org](https://getodk.org)
* ODK Briefcase usage instructions: [https://docs.getodk.org/briefcase-intro/](https://docs.getodk.org/briefcase-intro/)
* ODK forum: [https://forum.getodk.org](https://forum.getodk.org)
* ODK developer Slack chat: [https://slack.getodk.org](https://slack.getodk.org)
* ODK developer wiki: [https://github.com/getodk/getodk/wiki](https://github.com/getodk/getodk/wiki)

## Setting up your development environment

1. Fork the briefcase project ([why and how to fork](https://help.github.com/articles/fork-a-repo/))

1. Clone your fork of the project locally. At the command line:

        git clone https://github.com/YOUR-GITHUB-USERNAME/briefcase

We recommend using [IntelliJ IDEA](https://www.jetbrains.com/idea/) for development. On the welcome screen, click `Import Project`, navigate to your briefcase folder, and select the `build.gradle` file. Use the defaults through the wizard. Once the project is imported, IntelliJ may ask you to update your remote maven repositories. Follow the instructions to do so.

If you're using IntelliJ IDEA, we also recommend you [import the code style scheme](https://www.jetbrains.com/help/idea/copying-code-style-settings.html) for Briefcase at `config/codestyle/codestyle.xml`. Once you activate that scheme, use the automatic reformatting tool to produce code that will comply with the checkstyle rules of the project.

The main class is `org.opendatakit.briefcase.ui.MainBriefcaseWindow`. This repository also contains code for three smaller utilities with the following main classes:
- `org.opendatakit.briefcase.ui.CharsetConverterDialog` converts CSVs to UTF-8
- `org.opendatakit.briefcase.ui.MainClearBriefcasePreferencesWindow` clears Briefcase preferences
- `org.opendatakit.briefcase.ui.MainFormUploaderWindow` uploads blank forms to Aggregate instances

There might be some compile errors in the IDE about a missing class `BuildConfig`. That class is generated by gradle and the warnings can be ignored.

If you are working with [encrypted forms](https://docs.getodk.org/encrypted-forms/) you may get an `InvalidKeyException`. This is because you do not have an unlimited crypto policy enabled in Java. Do this:

* Java 8 Update 151 or later: Set `crypto.policy=unlimited` in `$JAVA_HOME/jre/lib/security/java.security`
* Java 8: Install [Unlimited Strength Policy Files 8](http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html) in `$JAVA_HOME/jre/lib/security`
* Java 7: Install [Unlimited Strength Policy Files 7](http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html) in `$JAVA_HOME/jre/lib/security`

## Running the project

To run the project, go to the `View` menu, then `Tool Windows > Gradle`. `run` will be in `odk-briefcase > Tasks > application > run`. Double-click `run` to run the application. This Gradle task will now be the default action in your `Run` menu.

You must use the Gradle task to run the application because there is a generated class (`BuildConfig`) that IntelliJ may not properly import and recognize.

To package a runnable jar, use the `jar` Gradle task.

To try the app, you can use the demo server. In the window that opens when running, choose Connect, then fill in the URL [https://opendatakit.appspot.com](https://opendatakit.appspot.com) leave username and password blank.

## Logging
Briefcase uses [SLF4J](https://www.slf4j.org/) with [Logback Classic](https://logback.qos.ch/) binding. The project also loads the [jcl-over-slf4j](https://www.slf4j.org/legacy.html) bridge for libraries that still use old [Apache Commons Logging](https://commons.apache.org/proper/commons-logging/).

There are example configuration files that you can use while developing:
- Copy `test/resources/logback-test.xml.example` to `test/resources/logback-test.xml`. This conf will be used when running tests.
- Copy `res/logback.xml.example` to `res/logback.xml`. This conf will be used when launching Briefcase on your machine.

### Logging tests vs development vs release
During the release process, we use a specific logback configuration which logs to a `briefcase.log` file in the folder Briefcase is launched from.

For testing and development purposes, customization of logback conf files is encouraged, especially to filter different levels of logging for specific packages. The following example sets the default level to `INFO` and `DEBUG` for components under `org.opendatakit`:

```xml

  
    
      %d [%thread] %-5level %logger{36} - %msg%n
    
  

  

  
    
  

```

More information on Logback configuration is available [here](https://logback.qos.ch/manual/configuration.html).

## Extended topics

There is a [`/docs`](./docs) directory in the repo with more documentation files that expand on certain topics:

- [Briefcase export CSV format](./docs/export-format.md)
- [How to release a Briefcase version](./docs/how-to-release.md)

## Contributing code
Any and all contributions to the project are welcome. ODK Briefcase is used across the world primarily by organizations with a social purpose so you can have real impact!

If you're ready to contribute code, see [the contribution guide](CONTRIBUTING.md).

## Contributing testing
All releases are verified on the following operating systems:
* Ubuntu 16.04
* Windows 10
* OS X 10.11.6

Testing checklists can be found on the [Briefcase testing plan](https://docs.google.com/spreadsheets/d/1H46G7OW21rk5skSyjpEx3dCZVv5Ly4WDK8LISmrz714/edit?usp=sharing).

If you have finished testing a pull request, please use a template from [Testing result templates](.github/TESTING_RESULT_TEMPLATES.md) to report your insights.

## Downloading builds
Per-commit debug builds can be found on [CircleCI](https://circleci.com/gh/getodk/briefcase). Login with your GitHub account, click the build you'd like, then find the JAR in the Artifacts tab under $CIRCLE_ARTIFACTS/libs.

Current and previous production builds can be found on the [ODK website](https://getodk.org/software/#odk-briefcase).
","'data-collection', 'global-development', 'global-health', 'java', 'mobile-data-collection', 'odk', 'social-impact', 'xforms'",2022-04-15T22:19:27Z,46,60,24,"('ggalmazor', 1163), ('yanokwa', 202), ('lognaturel', 91), ('mitchellsundt', 85), ('batkinson', 80), ('dcbriccetti', 61), ('acj', 21), ('jay4kelly', 13), ('mayank8318', 10), ('emajj', 10), ('poketim', 9), ('icemc', 9), ('huangyz0918', 8), ('nribeka', 8), ('shivam-tripathi', 7), ('dylanfprice', 6), ('rclakmal', 6), ('jpknox', 6), ('carstendev', 5), ('thomashhuang', 4), ('cletusajibade', 4), ('kkrawczyk123', 4), ('thedavidmccann', 3), ('kunalmulwani', 3), ('shobhitagarwal1612', 3), ('dmikheeva', 3), ('anakiou', 2), ('zstojanovic', 2), ('neoliteconsultant', 2), ('rockydcoder', 2), ('florian42', 2), ('Thodoris1999', 1), ('macdude357', 1), ('dsysme', 1), ('ukanga', 1), ('netrunnerX', 1), ('meletis', 1), ('macalac', 1), ('KiaFarhang', 1), ('jbeorse', 1), ('yougis', 1), ('grzesiek2010', 1), ('gilbva', 1), ('zakupower', 1), ('darshan934', 1), ('CicadasOutsideMyWindow', 1)","[17, 'Partnerships for the Goals']"
unicef/etools,,"ABOUT eTools
============

eTools is a platform to strengthen efficiency and results in UNICEF’s core work processes – work planning, partnership management, implementation monitoring – in development and humanitarian contexts.

The eTools platform has a modular suite of applications that enables the disaggregation of programme processes while still enabling data to be shared across modules.

eTools will enable UNICEF to:

*   Reduce staff time spent on administrative processes and increase staff time spent on achieving results
*   Modernize work processes under a single platform
*   Establish quality control on planning, agreements, and reporting
*   Link data between results monitoring, implementation monitoring and partnership management
*   Improve national civil society mapping and partnerships opportunities
*   Provide transparency in partner selection

EQUITRACK & eTOOLS
------------------

The predecessor to eTools was a system developed in the Lebanon Country Office (LCO)
called Equitrack. The success of Equitrack’s use in LCO drew other country programmes in MENA to also begin using it. With over 20 locally developed systems being used in country offices, FRG and EMOPS convened various stakeholders from the organization to design a universal platform for managing results for children. Equitrack was determined to be the system closest to addressing the common needs of this group. eTools manifested from the foundational elements of Equitrack, and has ever since been introduced with new features.

Today, the eTools team has made three notable restructuring in the way it functions moving forward: Core Project Team has 3 defined work streams for managing the platform from scoping to development to support. External software firms will begin to take on software development activities while the eTools Engineering is concentrating on technical project management and developer operations.

AGILE METHODOLOGY
-----------------

The development of eTools takes on a methodology known as Agile. This methodology takes into account shot, iterative software development cycles that incorporates user feedback.
Development strategy is similar to git flow approach.
New feature and bugfix are merged into development when PR have been approved and CI passes.
Once development is completed, changes are moved to staging for QA testing.
Adjustments and fixes should go direct to staging while new features should go in development.
Once QA is completed staging branch is merged to master.

MODULES
-------

eTools development follows a phased and modular approach to software development, with releases based on an agreed set of prioritized modules and features – new modules and features are released on a monthly basis.

These are modules currently in production for the eTools:
*   Partnership Management Portal (PMP)
*   Dashboard (DASH)
*   Trip Management (T2F)
*   Financial Assurance Module (FAM)
*   Third Party Monitoring (TPM)
*   Action Point Dashboard (APD)


DEVELOPMENT ROADMAP
-------------------

Along with introducing new features, eTools releases will also include refinements to existing features based on feedback received from users and business owners.

Links
-----

+--------------------+----------------+--------------+--------------------+
| Stable             |                | |master-cov| |                    |
+--------------------+----------------+--------------+--------------------+
| Development        |                | |dev-cov|    |                    |
+--------------------+----------------+--------------+--------------------+
| Source Code        |https://github.com/unicef/etools                    |
+--------------------+----------------+-----------------------------------+
| Issue tracker      |https://app.clubhouse.io/unicefetools/stories       |
+--------------------+----------------+-----------------------------------+


.. |master-cov| image:: https://circleci.com/gh/unicef/etools/tree/master.svg?style=svg
                    :target: https://circleci.com/gh/unicef/etools/tree/master


.. |dev-cov| image:: https://circleci.com/gh/unicef/etools/tree/develop.svg?style=svg
                    :target: https://circleci.com/gh/unicef/etools/tree/develop



Testing
-------------------

+---------------------------------+--------------------------------------------------------+
| tox                             | runs flake and checks there are not missing migrations |
+---------------------------------+--------------------------------------------------------+
| tox -r                          | in case you want to reuse the virtualenv               |
+---------------------------------+--------------------------------------------------------+
| python manage.py test  | run test related to a specific package                 |
+---------------------------------+--------------------------------------------------------+


Environments
--------------------
+----------------+---------------------------+-------------------------------------------------+
| Development    | etools-dev.unicef.org     | - Development environment for developers        |
|                |                           | - Potentially instable                          |
+----------------+---------------------------+-------------------------------------------------+
| Staging        | etools-staging.unicef.org | - Staging environment for QA testing            |
|                |                           | - Release candidate                             |
+----------------+---------------------------+-------------------------------------------------+
| Demo           | etools-demo.unicef.org    | - Demo environment                              |
|                |                           | - Same version of production                    |
|                |                           | - Used for demo, workshops and troubleshooting  |
+----------------+---------------------------+-------------------------------------------------+
| Test           | etools-test.unicef.org    | - Coming soon                                   |
+----------------+---------------------------+-------------------------------------------------+
| Production     | etools.unicef.org         | - Production environment                        |
+----------------+---------------------------+-------------------------------------------------+


Troubleshoot
--------------------
*  Exception are logged in Sentry: https://sentry.io/unicef-jk/
*  Each container in Rancher allows to access local logs
",,2024-04-26T15:36:06Z,27,9,12,"('robertavram', 4321), ('ntrncic', 2668), ('roman-karpovich', 2507), ('reinbach', 2177), ('jamescw', 1693), ('domdinicola', 1424), ('emaciupe', 812), ('csabadenes', 516), ('caktus-philip', 375), ('ilcsik', 261), ('czue', 219), ('vkurup', 207), ('maximlomakin', 198), ('n1207n', 160), ('dpoirier', 128), ('achamseddine', 102), ('nikita-gorodeckij', 70), ('ewheeler', 60), ('marko911', 55), ('foochris', 34), ('saxix', 34), ('simon-the-sloth', 27), ('tmoubarak', 11), ('itsjustbrian', 8), ('v-urbanovich', 2), ('acory', 1), ('ntrncic-babafi', 1)","[10, 'Reduced Inequalities']"
dhis2/dhis2-core,DHIS 2 Core. Written in Java. Contains the service layer and Web API.,"# DHIS 2

[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=dhis2_dhis2-core&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=dhis2_dhis2-core)
[![Tests](https://github.com/dhis2/dhis2-core/actions/workflows/run-tests.yml/badge.svg)](https://github.com/dhis2/dhis2-core/actions/workflows/run-tests.yml)
[![API tests](https://github.com/dhis2/dhis2-core/actions/workflows/run-api-tests.yml/badge.svg)](https://github.com/dhis2/dhis2-core/actions/workflows/run-api-tests.yml)

DHIS 2 is a flexible information system for data capture, management, validation, analytics and visualization. It allows for data capture through clients ranging from Web browsers, Android devices, Java feature phones and SMS. DHIS 2 features data visualization apps for dashboards, pivot tables, charting and GIS. It provides metadata management and configuration. The data model and services are exposed through a RESTful Web API.

## Overview

Issues can be reported and browsed in [JIRA](https://jira.dhis2.org).

For documentation visit the [documentation portal](https://docs.dhis2.org/).

You can download pre-built WAR files from the [continuous integration server](https://ci.dhis2.org/).

You can explore various demos of DHIS 2 in the [play environment](https://play.dhis2.org/).

For support and discussions visit the [community forum](https://community.dhis2.org/).

For general info visit the [project web page](https://www.dhis2.org/).

For OpenAPI documentation visit the [Stoplight workspace](https://dhis2.stoplight.io/).

For software developer resources visit the [developer portal](https://developers.dhis2.org/).

To contribute to the software read the [contributor guidelines](https://developers.dhis2.org/community/contribute).

The software is open source and released under the [BSD license](https://opensource.org/licenses/BSD-2-Clause).

## Run DHIS2 in Docker

The following guides use [Docker Compose](https://docs.docker.com/compose/install/) to run DHIS2
using Docker.

A DB dump is downloaded automatically the first time you start DHIS2. If you switch between
different DHIS2 versions and/or need to download a different DB dump you will need to remove the
shared volume `db-dump` using

```sh
docker compose down --volumes
```

### Pre-built Images

We push pre-built DHIS2 Docker images to Dockerhub. You can pick an `` from one of the following
repositories:

* [`dhis2/core`](https://hub.docker.com/r/dhis2/core) - images of the release and release-candidate DHIS2 versions. These images represent the **stable** DHIS2 versions, meaning they won't be rebuilt in the future.

* [`dhis2/core-dev`](https://hub.docker.com/r/dhis2/core-dev) - images of _the latest development_ DHIS2 versions - branches `master` (tagged as `latest`) and the previous 3 supported major versions. Image tags in this repository will be overwritten multiple times a day.

* [`dhis2/core-canary`](https://hub.docker.com/r/dhis2/core-canary) - images of _the latest daily development_ DHIS2 versions. We tag the last `core-dev` images for the day and add an extra tag with a ""yyyyMMdd""-formatted date, like `core-canary:latest-20230124`.

* [`dhis2/core-pr`](https://hub.docker.com/r/dhis2/core-pr) - images of PRs made from
  https://github.com/dhis2/dhis2-core/ and not from forks. As forks do not have access to our
  organizations/repos secrets.

To run DHIS2 from latest `master` branch (as it is on GitHub) run:

```sh
DHIS2_IMAGE=dhis2/core-dev:latest docker compose up
```

### Local Image

Build a DHIS2 Docker image first as described in [Docker image](#docker-image). Then execute

```sh
docker compose up
```

DHIS2 should become available at `http://localhost:8080` with the Sierra Leone Demo DB.

### Demo DB

If you want to start DHIS2 with a specific demo DB you can pass a URL like

```sh
DHIS2_DB_DUMP_URL=https://databases.dhis2.org/sierra-leone/2.39/dhis2-db-sierra-leone.sql.gz docker compose up
```

using versions we for example publish to https://databases.dhis2.org/

## Build process

This repository contains the source code for the server-side component of DHIS 2, which is developed in [Java](https://www.java.com/en/) and built with [Maven](https://maven.apache.org/). 

To build it you must first install the root `POM` file, navigate to the `dhis-web` directory and then build the web `POM` file.

See the [contributing](https://github.com/dhis2/dhis2-core/blob/master/CONTRIBUTING.md) page to learn how to run locally.

### Docker image

The DHIS2 Docker image is built using
[Jib](https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin). To build make sure
to build DHIS2 and the web project first

```sh
./dhis-2/build-dev.sh
```

Run the image using

```sh
docker compose up
```

It should now be available at `http://localhost:8080`.

#### Customizations

##### Docker tag

To build using a custom tag run

```sh
mvn -DskipTests -Dmaven.test.skip=true -f dhis-2/dhis-web-portal/pom.xml jib:dockerBuild -Djib.to.image=dhis2/core-dev:mytag
```

For more configuration options related to Jib or Docker go to the
[Jib documentation](https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin).

##### Context path

To deploy DHIS2 under a different context then root (`/`) configure the context path by setting the
environment variable

`CATALINA_OPTS: ""-Dcontext.path='/dhis2'""`

DHIS2 should be available at `http://localhost:8080/dhis2`.

##### Overriding default values

You can create a local file called `docker-compose.override.yml` and override values from the main `docker-compose.yml`
file. As an example, you might want to use a different version of the Postgres database and run it on a different port.
More extensive documentation of this feature is available [here](https://docs.docker.com/compose/extends/). Using the override
file you can easily customize values for your local situation.

```yaml
version: ""3.8""

services:
  db:
    image: postgis/postgis:14-3.3-alpine
    ports:
      - 127.0.0.1:6432:5432
```

##### DHIS2_HOME

[Previously](https://github.com/dhis2/dhis2-core/blob/b4d4242fb30d974254de2a72b86cc5511f70c9c0/docker/tomcat-debian/Dockerfile#L9),
the Docker image was built with environment variable `DHIS2_HOME` set to `/DHIS2_home`. This is not
the case anymore, instead `DHIS2_HOME` will [fallback to its default](https://github.com/dhis2/dhis2-core/blob/b4d4242fb30d974254de2a72b86cc5511f70c9c0/dhis-2/dhis-support/dhis-support-external/src/main/java/org/hisp/dhis/external/location/DefaultLocationManager.java#L58)
`/opt/dhis2`. You can still run the Docker image with the old behavior by setting the environment
variable `DHIS2_HOME` like

```yaml
    environment:
      DHIS2_HOME: /DHIS2_home
```

in a `docker-compose.override.yml` file. Alternatively, you can pass the system property `-Ddhis2.home` directly from the command line. You need to ensure that this `DHIS2_HOME` is writeable yourself!
","'hacktoberfest', 'hibernate', 'java', 'maven', 'spring', 'spring-mvc'",2024-05-03T14:08:07Z,79,280,53,"('larshelge', 3106), ('dependabotbot', 961), ('mortenoh', 779), ('zubaira', 758), ('vietnguyen', 558), ('abyot', 495), ('enricocolasante', 482), ('jbee', 475), ('teleivo', 422), ('stian-sandvold', 379), ('netroms', 322), ('dhis2-bot', 320), ('jimgrace', 306), ('maikelarabori', 288), ('ameenhere', 279), ('gnespolino', 220), ('vilkg', 163), ('luciano-fiandesio', 156), ('d-bernat', 150), ('DavidCKen', 133), ('lucaCambi77', 117), ('Bekkalizer', 103), ('jason-p-pickering', 101), ('henninghaakonsen', 100), ('muilpp', 70), ('david-mackessy', 68), ('janhenrikoverland', 60), ('Philip-Larsen-Donnelly', 50), ('radnov', 47), ('varl', 36), ('amcgee', 26), ('volsch', 17), ('lkwaerst', 17), ('tonsV2', 15), ('Birkbjo', 14), ('Markionium', 11), ('aamerm', 11), ('adeelshahid', 11), ('cjmamo', 10), ('tomzemp', 9), ('turban', 7), ('HendrikThePendric', 7), ('marcamsn', 6), ('bobjolliffe', 6), ('adrianq', 5), ('troyel', 5), ('kabaros', 4), ('mediremi', 4), ('tokland', 3), ('samaxes', 3), ('zepto5', 3), ('jenniferarnesen', 2), ('h4rikris', 2), ('martinstuga', 2), ('haase', 2), ('cooper-joe', 2), ('zewolfe', 2), ('swetha1992', 1), ('salihuM', 1), ('rgaudin', 1), ('abdak3', 1), ('vgarciabnz', 1), ('srikanthkatta2207', 1), ('RRGWhite', 1), ('Topener', 1), ('jimiolaniyan', 1), ('vanakenm', 1), ('MadhoolikaBulusu', 1), ('lukaswvd', 1), ('kjesvale', 1), ('KaiVandivier', 1), ('JLLeitschuh', 1), ('JoakimSM', 1), ('jhansireddy', 1), ('JasperTimm', 1), ('jturtler', 1), ('eternalgooner', 1), ('SferaDev', 1), ('alanivey', 1)","[3, 'Good Health and Well-Being']"
wikitongues/poly,"Open source, modern software to share and learn every language in the world.","

# Poly

![Poly](https://raw.githubusercontent.com/wikitongues/poly/master/repo/polyPoster.jpg)

> Poly is open source, modern software to share and learn every language in the world.

Poly streamlines the process of creating and sharing dictionaries between any two languages. Speakers of languages without a written standard, including the world's more than 200 sign languages, are supported by native video functionality.

# Table of Contents
1. [Background](#background)
  1. [About Wikitongues](#about-wikitongues)
1. [Set-up](#install)
  1. [Pre-requisites](#pre-requisits)
  1. [Install](#install)
  1. [Run](#run)
  1. [Test](#test)
    1. [Staging and Production](#staging-and-production)
    1. [Continuous Integration](#continuous-integration)
    1. [Browser testing](#browser-testing)
1. [Usage](#usage)
1. [Feature map](#feature-map)
1. [Security](#security)
1. [Maintainers](#maintainers)
1. [Contribute](#contribute)
  1. [Request features](#request-features)
  1. [PRs are accepted](#prs-are-accepted)
  1. [Best practices](#best-practices)
  1. [Code of conduct](#code-of-conduct)
1. [License](#license)

# Background
Poly was conceived in response to a [New York Times article](https://www.nytimes.com/2014/08/19/opinion/who-speaks-wukchumni.html) that identified one of the last living speakers of her native language Wukchumni manually composing documentation using pen, paper and a voice recorder. Through years of dedication, Marie Wilcox produced an exhaustive dictionary of her mother language, with supplemental recordings, to be used by future generations in their revitalization efforts.

Marie's reality is not unique, and over 5% of the languages spoken in the world today are expected to have fewer than 100 speakers. We at Wikitongues resolved to provide tools to facilitate the experience of documentation.

Initial development on Poly was made possible by record-breaking a [Kickstarter campaign](www.kck.st/poly). Thank you very much to all our backers.

## About [Wikitongues](www.wikitongues.org)

In the next eighty years, 3,000 languages are expected to disappear. We won't let that happen.

![Wikitongues](https://raw.githubusercontent.com/wikitongues/poly/master/repo/wikitonguesBanner.jpg)

Wikitongues is a platform for every language in the world. We publish oral histories and dictionaries in all of the world's 7,000 languages and develop open source technology for cultural exchange.



The [Oral Histories](https://youtube.com/wikitongues) project lives on YouTube.

# Set-up
Poly is written in [Ruby on Rails](http://rubyonrails.org/) and [React.js](https://facebook.github.io/react/), and uses [RecordRTC](recordrtc.org) for video recording. It uses a [PostgreSQL](https://www.postgresql.org/) database, is deployed to [Heroku](heroku.com) and uses [Amazon S3](https://aws.amazon.com/s3) as CDN. RecordRTC relies on [NPM](https://www.npmjs.com/), which in turn requires [Browserify](http://browserify.org/).

To install and set up this application locally, follow the steps below. For any specific questions, please email us at [poly@wikitongues.org](mailto:poly@wikitongues.org).

## Pre-requisites
You will need the following things properly installed on your computer:
* [Git](http://git-scm.com/)
* [Ruby 2.3.0](https://www.ruby-lang.org/en/downloads/) (check what version of ruby you have by running `ruby -v`)
* [NPM](npmjs.com) (Installed with [Node.js](https://nodejs.org/en/))
* [Bundler](http://bundler.io/) (run `$ gem install bundler`)

## Install
1. Clone or fork this repository:

  ```shell
  git clone https://github.com/wikitongues/poly.git
  ```

2. Change into the new directory.
3.  Install all dependencies:

  ```shell
  bundle install && npm install
  ```

4.  Create a Postgres database:

  ```shell
  rake db:setup
  ```

## Run
To start the application locally, run:
```shell
rails server
```
Then, visit the app at [http://localhost:3000](http://localhost:3000).

<!-- ## Alternative Workflow with Convox and Docker
An alternative to running poly that dispenses from installing prerequisites like ruby, Postgres, etc... locally is to use [docker](https://www.docker.com/) to run poly locally, namely via the [convox](https://convox.com/) cli.

* [Install docker](https://www.docker.com/products/docker) for your operating system
* [Install convox](https://dl.equinox.io/convox/convox/stable) for your operating system
* Clone or fork this repository with `git clone https://github.com/wikitongues/poly.git`
* Change into the new directory
* Run `bin/poly-setup` once to setup poly (will take a while the first time as docker images are downloaded and built)
* Run `bin/poly-start` to run poly locally (data will be persisted to `~/.convox/volumes/poly/database/var/lib/postgresql/`)
* Run `bin/poly-migrate` in case poly has some pending migrations
* Visit the app at [http://localhost:3000](http://localhost:3000). -->

## Test
Testing is implemented with RSpec, [FactoryGirl](https://github.com/thoughtbot/factory_girl_rails), [shouldamatchers](http://matchers.shoulda.io/) and [simplecov](https://github.com/colszowka/simplecov).

To run tests:

```shell
rspec
```

### Staging and Production
Poly has both a [Staging server](https://poly-staging.herokuapp.com/) and the [live production server](www.poly.wikitongues.org).

### Continuous Integration
Poly uses Continuous integration as provided by [CircleCI.com](https://circleci.com). Right out of the box, it provides automated testing and automatically deploys merged changes to the [staging server](https://poly-staging.herokuapp.com).

### Browser Testing
We're proud to do our live, web-based browser testing together with the awesome people at [BrowserStack](http://browserstack.com).

BrowserStack gives you instant access to all real mobile and desktop browsers. Say goodbye to your lab of devices and virtual machines. Go check them out!

[](http://browserstack.com)

# Usage
The app is intended to be used via the UI by any person with access to a modern browser. As of February 21st, 2017, anyone will be able to create an account and start creating content.

[Video Demo](https://youtu.be/rt-NigJJCgI)

[Live App](https://poly.wikitongues.org)


# Feature Map
We use [semantic versioning](http://semver.org/).

For an overview of planned features and future releases, refer to [ROADMAP.md](https://github.com/wikitongues/poly/blob/master/ROADMAP.md).

# Security
The app requires a number of secret keys to function correctly. Features that will not work locally without keys:
* Password reset
* Video recording

# Maintainers
||[Freddie Andrade](https://github.com/FredericoAndrade)|
|:---|:---|
||[Chris Voxland](https://github.com/ChrisVoxland)|

# Core contributors
||[Ben Arias](https://github.com/bjlaa)|
|---|---|
||[Amr Adel](https://github.com/AmrAdelKhalil)|
||[Scott Rohrer](https://github.com/smrohrer)|

# Contribute
We are actively looking for help developing Poly. If you're interested in getting involved, be sure to [let us know](mailto:poly@wikitongues.org)!

## Request features
Feature requests may be made by [opening new issues](https://github.com/wikitongues/poly/issues/new) and labeling them enhancements.

**Note on requesting features:**

>Before making a new request, please review our [roadmap](https://github.com/wikitongues/poly/blob/master/ROADMAP.md) and [issues list](https://github.com/wikitongues/poly/issues) to avoid duplicates. If a feature is already described, please feel free to add your support in the comments!

## PRs are accepted.
Make pull requests to have your contributions reviewed and deployed by the maintainers, or contact us directly by email at [poly@wikitongues.org](mailto:poly@wikitongues.org).

## Best Practices
Refer to the [Github Contribution guide](https://guides.github.com/activities/contributing-to-open-source/) or the more comprehensive [Open-Source Contribution Guide](http://www.contribution-guide.org/) for best practices in contributing to open source projects.



## Code of Conduct
All contributors will be held accountable to the [Contributor Covenant](https://github.com/wikitongues/poly/blob/master/CONDUCT.md).

TLDR: be nice. But go read it anyway.

# License
> Poly uses the GNU General Purpose License v3.

Read the license in detail [here](https://github.com/wikitongues/poly/blob/master/LICENSE.txt).
","'css', 'cultural-heritage', 'javascript', 'language', 'non-profit', 'open-source', 'rails', 'react', 'ruby', 'social-justice', 'web'",2020-08-15T19:48:22Z,11,81,16,"('FredericoAndrade', 920), ('bjlaa', 105), ('AmrAdelKhalil', 21), ('ChrisVoxland', 21), ('mapmeld', 5), ('fantasticole', 4), ('philj0st', 4), ('Thrillberg', 3), ('CestDiego', 1), ('emilyki', 1), ('levthedev', 1)","[11, 'Sustainable Cities and Communities']"
publiclab/infragram,A minimal core of the Infragram.org project in JavaScript,"Infragram
======

The Infragram project brings together a range of different efforts to make Do-It-Yourself plant health comparisons possible with infrared photography.

This project is a minimal core of the Infragram project in JavaScript and was made possible with support from Google and the AREN Project at NASA.

Check it here: https://publiclab.github.io/infragram/

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/publiclab/infragram)

This is ported out of the Node server application at https://github.com/p-v-o-s/infragram-js

Read more at https://publiclab.org/infragram




## Table of Contents

1. [Quick Installation](#quick-installation)
2. [Purpose](#purpose)
3. [Usage](#usage)
4. [Architecture](#architecture)
5. [Inputs](#inputs)
    - [File Upload](#file-upload)
    - [Video](#video)
6. [Converting](#converting)
    - [Infragrammar](#infragrammar)
7. [Processors](#processors)
8. [Presets](#presets)
9. [Colorizing](#colorizing)
10. [Outputs](#outputs)
    - [Export To Publiclab.org](#export-to-publiclaborg)
    - [Export to Image Sequencer](#export-to-image-sequencer)
11. [Interface](#interface)
12. [Legacy Code](#legacy-code)
13. [Pi and VR versions](#pi-and-vr-versions)
14. [Contributing](#contributing)
    - [Code of Conduct](#code-of-conduct)
15. [Developers](#developers)
16. [First Time?](#first-time)

## Quick Installation
1. [Fork](https://github.com/publiclab/infragram/fork) our repo.
2. In the console, download a copy of your forked repo with `git clone https://github.com/your_username/infragram` where `your_username` is your GitHub username.
3. Enter the new infagram directory with `cd infragram`.
4. `npm install` - this will install all the node packages into your local machine.
5. `npm install http-server -g && http-server` (this installs http-server globally and then runs it, next time you just need to do `http-server` to run it locally)


## Purpose

The purpose of this software is to convert a photo taken from an ""Infragram"" multispectral camera using NDVI or another technique, then colorize that image. It can then be downloaded or forwarded to another web app.




## Usage

See this tutorial for how to use Infragram.org, step by step:

https://publiclab.org/notes/warren/10-20-2017/getting-started-with-infrared-photography-on-infragram-org


## Architecture

This edition of Infragram is slimmed down and simplified vs. previous versions. The whole project runs using just JavaScript (and WebGL) from a single index.html file, in the browser. 

The basic flow is as shown here:

`Input` => `Convert` => `Colorize` => `Output`

1. Input: an image or video stream is chosen. 
2. Convert: The image gets modified/transformed using a given algorithm, like NDVI or another simple mathematic expression. This is handled by the processor (see below)
3. Colorize: It gets colorized (optionally), which is also handled by the processor (see below).
4. Output: the image can be downloaded or sent to another website.


## Inputs

Images can come from two sources: files or webcam video. 

### File upload

Handling for ""uploaded"" files (note that we don't send files anywhere, they're just used in the browser) is handled in [/src/ui/interface.js](https://github.com/publiclab/infragram/blob/80f3de4ddd96c2d5b452462d74076eab73ea0376/src/ui/interface.js#L75-L79).

Depending on which processor is selected (see below), the image is read, then sent to the `updateImage()` function of either the `webgl` (default) or `javascript` processor for conversion.

### Video

If the user presses the Webcam button, we use the WebRTC webcam API to stream video from the selected webcam, and we perform conversion and colorizing on each frame in real-time. This works faster with `webgl` mode. _In upcoming versions we would like to try accepting uploaded videos, which would be dragged in just like images, but would play on loop just like webcam video streams._




## Converting

Conversion is the step of changing the value of each pixel of the input image using a given mathematical expression. The default is NDVI or Normalized Difference Vegetation Index. The above diagram shows how one channel of the input image is used for infrared light, instead of its original color. NDVI represents the (normalized) difference between infrared and visible (red, green, or blue) light. 

Learn more about NDVI and the broader project and techniques at:

* https://publiclab.org/infragram
* https://publiclab.org/multispectral-imaging
* https://publiclab.org/wiki/ndvi

This step is labeled ""2. Analysis"" in the UI (although we could change that to Convert).




### Infragrammar

The mathematical expression which is run on each pixel follows a syntax we've called ""infragrammar"" (corny pun, sorry!). Basically, for each pixel of the input image, you get values of R, G, and B for the three color channels red, green, and blue. The expression (using any basic JavaScript math operations) uses these to calculate an ""output"" value for the pixel. The output in ""Grey"" or monochrome mode is just one value. That means the output of this step would be a monochrome image:




If you change to HSV mode, you can set a different expression for Hue, Saturation, and Value of the resulting pixel, and get a color image, but it's usually easier to just do that in the Colorize step (see below). Finally, in RGB mode, you can set a different expression for each channel of the resulting pixel, R, G, or B. This is useful for swapping channels, like displaying R as B.

Read more about Infragrammar and modes here:

* https://publiclab.org/infragrammar
* https://publiclab.org/notes/warren/08-24-2013/infragrammar-with-hsv-color-model

Conversion with Infragrammar is handled by the Processor, described below.

## Processors

Two separate systems are available for converting and colorizing: `javascript` and `webgl`, both using the [HTML Canvas API](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API). The [javascript](https://github.com/publiclab/infragram/blob/main/src/processors/javascript.js) processor is simpler to read, write, and debug but is slower. The [webgl](https://github.com/publiclab/infragram/blob/main/src/processors/webgl.js) processor is MUCH faster but very hard to code/modify/change.

The selected processor handles the Conversion and Colorizing steps.


## Presets

There are two types of input images, because there are two types of multispectral cameras people are making: red filtered (more common now) and blue filtered (older) cameras. You can read about the difference here:

https://publiclab.org/wiki/infragram#Conversion+types

As a result, we have a pop-up modal that guides users through the 2 most common workflows:

1. NDVI conversion for red filters + colorizing
2. NDVI conversion for blue filters + colorizing

Once these are selected, the pop-up disappears and the image is shown with these settings. The presets will change step 2 (Conversion/Analysis) and step 3 (Colorize) accordingly, although this might not be very obvious to new users.


## Colorizing

The processors also handle colorizing, which is also confusing because the `webgl` colorizer has to work very differently than the `javascript` version. 

Colorizing is just converting a black and white (monochrome) image to a color one, where a given spectrum or ""color map"" is used to convert each pixel's brightness value to a color. The most familiar would be where ""hot"" colors like red represent higher values, and ""cold"" colors like blue represent lower values. The range of colors is shown in a key at the bottom of the image. 



This gets a little more complex because the most common conversion, NDVI, outputs a value from -1 to 1, rather than 0 to 1. So the color mapping may represent values in that range.

The way NDVI works also means that it's useful to visually see what pixels are greater or less than 0, so some color maps are not smooth - they have a sharp color transition at 0, so you can visually see parts of the photo that are >0. 

Colorizing code for both processors can be found at https://github.com/publiclab/infragram/tree/main/src/color. While `javascript` code for colormaps has been cleaned and consolidated there, `webgl` code is contained in the fragment shader file at https://github.com/publiclab/infragram/blob/f15add6ae6057bd99b4b1b930449f75ca974270e/dist/shader.frag and has not been reconciled with the long list of colormaps available in JavaScript.


## Outputs

The output of the above is dependent on the quality of the input. When using an ""Infragram"" multispectral camera, careful white balancing is essential for good NDVI images. The following are two examples of well-white-balanced images.

| ![](https://storage.googleapis.com/publiclab-production/public/system/images/photos/000/018/533/thumb/Rosco_26_filtered.JPG) | ![](https://storage.googleapis.com/publiclab-production/public/system/images/photos/000/001/647/thumb/IMG_0025.JPG) |
|:--:| :--:| 
| *A red-filtered image by [@mathew](https://publiclab.org/profile/mathew) - try processing [this image](https://storage.googleapis.com/publiclab-production/public/system/images/photos/000/018/533/thumb/Rosco_26_filtered.JPG) on [Infragram](https://infragram.org/sandbox/index.html).* | *A blue-filtered image by [@warren](https://publiclab.org/profile/warren) - try processing [this image](https://storage.googleapis.com/publiclab-production/public/system/images/photos/000/001/647/thumb/IMG_0025.JPG) on [Infragram](https://infragram.org/sandbox/index.html).*|

Read more about white-balancing at [https://publiclab.org/wiki/infrablue-white-balance](https://publiclab.org/wiki/infrablue-white-balance).

### Final Results

Once the image is converted and (optionally) colorized, it can be downloaded. But there are two other options:

#### Export to PublicLab.org

The image is encoded as a data-url and a new tab is opened with the Public Lab Editor at https://publiclab.org/post, with the image ""sent"" to become the main image. This is convoluted but easier than sending the image separately; see the [code for this here](https://github.com/publiclab/infragram/blob/34d330001e3869da9caf34cb79d6dc7650c1db83/index.html#L235-L248). Images then appear with the tag `infragram-upload` on this page: https://publiclab.org/tag/infragram-upload

#### Export to Image Sequencer

Similarly, we can ""send"" the image to https://sequencer.publiclab.org, as a data-url although it may fail for very large images since we must [send it in a GET request](https://github.com/publiclab/infragram/blob/34d330001e3869da9caf34cb79d6dc7650c1db83/index.html#L250-L254). It is then run through a similar (but not identical, unfortunately) set of steps of conversion and colorizing, in the step-by-step interface of Image Sequencer, for fine-tuning. Learn more about this technique here: https://publiclab.org/notes/warren/08-02-2018/use-image-sequencer-for-ndvi-plant-analysis-with-a-modified-mini-sport-camera


## Interface

The interface (UI design) for Infragram is built using Bootstrap 3 and makes use of jQuery event handling. Most of the code can be found in https://github.com/publiclab/infragram/tree/main/src/ui although some listeners are still mixed into the rest of the code (we hope to improve this)


## Legacy code

Previous versions of Infragram had different workflows, technologies, and architectures. Some code remains which is no longer used.

See the deprecation label for more on this code: https://github.com/publiclab/infragram/labels/deprecation


## Pi and VR versions

Two extra folders `/pi/` and `/vr/` are for different variants of the project, designed to be run on a [Raspberry Pi-based camera](https://publiclab.org/infragram-pi) (live-streaming video through the converter from the Pi camera) and for use with a VR headset with a Pi camera attached to the front. These are experimental but if major breaking changes are implemented in the main `index.html` file, we would like them to be ported over to these files as well, especially the `/pi/` version, so the interfaces look and work the same.


## Contributing

We welcome contributions and are especially interested in welcoming [first-time contributors](#first-time). Read more about [how to contribute](#developers) below! We especially welcome contributions from people belonging to groups under-represented in free and open-source software!

### Code of Conduct

Please read and abide by our [Code of Conduct](https://publiclab.org/conduct); our community aspires to be a respectful place both during online and in-­personinteractions.


## Developers

Help improve Public Lab software!

* Join the chatroom at https://publiclab.org/chat
* Look for open issues at https://github.com/publiclab/infragram/labels/help-wanted
* We're specifically asking for help with issues labeled with the https://github.com/publiclab/infragram/labels/help%20wanted tag
* Find lots of info on contributing at http://publiclab.org/developers
* Review specific contributor guidelines at http://publiclab.org/contributing-to-public-lab-software
* Find the steps on how to make a Pull Request at https://github.com/publiclab/infragram/issues/300

## First Time?

New to open source/free software? Here is a selection of issues we've made **especially for first-timers**. We're here to help, so just ask if one looks interesting : https://code.publiclab.org

[Here](https://publiclab.org/notes/warren/11-22-2017/use-git-and-github-to-contribute-and-improve-public-lab-software) is a link to our Git workflow.

**Let the code be with you, happy open-sourcing :smile:**
",,2023-03-07T02:59:19Z,47,44,23,"('jywarren', 238), ('Forchapeatl', 99), ('pfoltyn', 46), ('bgamari', 42), ('TildaDares', 40), ('dependabotbot', 18), ('dependabot-previewbot', 13), ('dependabot-support', 12), ('shubhangi013', 9), ('adeolaadedeji', 5), ('stephaniequintana', 5), ('Deepthi562', 4), ('AbihaFatima', 3), ('Syed-Ansar', 3), ('Somya-Singhal', 2), ('faithngetich', 2), ('daluclemas', 2), ('ankittt20', 2), ('singhavs', 2), ('janvi01', 2), ('on2onyekachi', 2), ('wisdomekpotu', 2), ('akanbifatimah', 2), ('vikul1234', 1), ('select-case', 1), ('riyaku11', 1), ('marshatiisa', 1), ('komal3120', 1), ('Kemifrank', 1), ('dwblair', 1), ('danjkim21', 1), ('alicendeh', 1), ('reapedjuggler', 1), ('pydevsg', 1), ('sonaljain067', 1), ('ShrutiKoli19', 1), ('SaptarshiSarkar12', 1), ('y-h-v-h', 1), ('17neelam', 1), ('Julietadeboye', 1), ('ishumi007', 1), ('Hydriah', 1), ('HarrietMwanza', 1), ('govindgoel', 1), ('debck', 1), ('Ash-KODES', 1), ('ebeechsrc', 1)","[13, 'Climate Action']"
codeforboston/migrant_service_map,Refugees welcome,"# Migrant Service Map

### Migrant Service Map-MSM Project

(start date 9.13.18)

### by [Refugees Welcome](https://refugeeswelcomehome.org/)!

### [Live Demo](https://migrantservicemap.com)

 The Migrant Service Map is an application which allows service providers to more effectively network with one another in order to provide the best recommendations to their clients. 

### Partners: MapBox and Code for Boston

Thanks for choosing to support our initiative to support and strengthen migrant service provision in the greater Boston area. For more information about our organization, visit [Refugees Welcome](https://refugeeswelcomehome.org/).

### Project Founder:

- Charla M. Burnett ([Refugees Welcome](https://refugeeswelcomehome.org/)) charla.burnett89@gmail.com

### MapBox Support:

- Marena Brinkhurst marena.brinkhurst@mapbox.com
- Sam Fader sam.fader@mapbox.com
- Mall Wood mal.wood@mapbox.com
- Joe Clark joe.clark@mapbox.com

### Code for Boston Support:

- Liam Morley liam@codeforboston.org
- Matt Zagaja mattz@codeforboston.org

## Project Materials and Communications

- Slack Channel: #migrantservicemap
  - [Join Code for Boston's Slack Workplace](https://communityinviter.com/apps/cfb-public/code-for-boston)
- [Project Board](https://trello.com/invite/b/rVNLIxod/2c7ced88a95f6514f8ef49eb2e622cd8/migrant-service-map)
- [Design Google Drive](https://drive.google.com/drive/folders/12cSCKSf9IQT1nMJUomWFQNyG2PLCsmOw)
- [Database](https://docs.google.com/spreadsheets/d/e/2PACX-1vT4MST1klxwmBdfvMhOycfV5C-lxGe0_sidJnmGS8U42irBYhgazisd-OUjrI4V9l_GqnazklGhNjzJ/pub)
- [Planning Documentation](https://drive.google.com/drive/folders/1VnylhChsStemQu7hPk4bITtXmPQ1IAoY)

## Summary

The Migrant Service Map (MSM) was conceptualized as a part of Refugees Welcome!'s ongoing research on streamlining migrant services in Boston. The objective of the map is to act as a spatial guide for migrants and service providers to find specific types of services closest to them and their clients. The database includes a comprehensive list of all service providers and the services they offer. The data on service providers in Boston has already been collected by Refugees Welcome! and is geocoded.

## Ideal Product Vision
 
The migrant or service provider is able to click on the MSM link and be transferred to a questionnaire page where they would be asked whether they are service providers or migrants. The next page would ask the service provider if they have become a part of the Refugees Welcome! network and offer a separate link to apply to the RW! Network. This page would also ask what the legal status is of their client, the address of the client, and what types of services they are looking for. The legal status is important because undocumented and asylum seekers cannot receive all public services. None of this data would be collected or stored due to security concerns.

The types of services include housing, work placement, english, legal assistance, registration, health, mental health, community centers, and education. The map would conduct a spatial analysis to highlight each service provider within .5, 1, 3, and 5 miles of the address entered for whichever legal status and service applies to their search. The migrant or service provider will be able to click on the service provider points and is provided with a comprehensive overview of the services provided and contact information. This tool will be used by service providers and migrants to find and plan the integration of new arrivals to Boston. As a result, it would be helpful if the platform could keep a record of each point of interest that then could be easily printed. That way service providers can give clients an easy list of service providers to contact upon arrival.

## Phase One (The Product as of the December 2019 Refugees Welcome! Resource Forum)

Phase one of the Migrant Service Map is geared primarily toward service providers. When they arrive on the site, the service provider is able to select the type of provider(s) that they are looking for, specify a location to search near (such as their client's home or workplace), and choose a distance from that location (.5, 1, 3 and 5 miles) by which to filter their results by. 

The categories of providers which are currently included in the map are: Cash/Food Assistance, Community Center, Education, Health, Housing, Job Placement, Legal, Mental Health, and Resettlement. According to the other selections which the user makes (ie a location and/or proximity to it), a list of providers which meet those criteria populates the map and the ""Saved Providers"" tab. By selecting either entries in the map or from the list in the ""Saved Providers"" tab, users can see more details about organizations such as their mission statement, location, and contact information. Users can also sort the ""Saved Providers"" list to make it easier for them to find providers. 

Once a user has found organizations which they would like to recommend to a client, they may choose to save them, populating their ""Saved"" tab. In this tab, the user may reorder organizations manually to provide a list which is most logical to their client, then share this list via email or print it out.  

Service providers are also able to update their data or add themselves to the organizations included in the map. There is a form which allows users to provide feedback on the tool as well as complete help documentation.

## Next Steps

The next step in this process is soliciting user feedback about the tool. This is essential because the Migrant Service Map did not have substantial user feedback throughout its design or development stages. Getting feedback and input from service providers who are using the phase one of this product should primarily inform design decisions moving forward. 

With that said, however, some features were intended for future phases in the original product vision. Those include making the tool accesible for migrants (ie by making it mobile friendly and translating it), creating profiles for providers so that they may save lists or preferences across sessions or clients, and expanding to providers beyond the Boston area. For a complete list of features which were identified for future phases, visit https://refugeeswelcomehome.org/migmap-services/.


## Getting Started

### Prerequisites

#### Node

If you don't have nodejs installed, [install it](https://nodejs.org/en/download/). If you don't know, do `node --version` in your terminal/command prompt. If you don't get a number, you don't have it.

#### Git and Github

If you're new to github check out [Github Guide, Hello World](https://guides.github.com/activities/hello-world/) to make an account and get started with Github and [How to: fork a repo](https://help.github.com/articles/fork-a-repo/) to learn how to fork a repo.

### Setup

1. Fork the repository: On GitHub, navigate to the [repository](https://github.com/codeforboston/migrant_service_map). In the top-right corner of the page, click Fork.
2. On GitHub, navigate to your fork of the migrant_service_map repository. In the Clone with HTTPs section, click to copy the clone URL for the repository.
3. Clone your fork: In your terminal type `git clone`, paste the URL you copied and press enter. In your terminal/command prompt cd (change directory) into the new folder. Inside the directory:

```
git clone \
https://github.com/YOUR-USERNAME/migrant_service_map.git
cd migrant_service_map
```

4. Add the migrant_service_map repository as a remote to your fork:

```
git remote add upstream \
https://github.com/codeforboston/migrant_service_map.git
```

4. Installing: Checkout branch reboot and install

```
git checkout reboot
npm install
```

**Reboot** is the active development branch. Do not touch the `master` branch.

### Running the App

- In your terminal/command prompt run `npm start` to start the app. It will open automatically in a browser window.

- To stop the local server press ctrl + c in your terminal

### Updating

To learn more about keeping your fork up to date view this [article](https://help.github.com/articles/syncing-a-fork/),1. When there is an update, in your terminal inside your local repo:

```
git checkout reboot
git pull upstream
git pull upstream reboot
```

After running this command once, you may omit the `git pull upstream` step, and only enter `git pull upstream reboot`.

If there is a merge conflict that cannot be resolved automatically, the output from the `git pull` command will read: ""Automatic merge has failed; fix conflicts and then commit the result."" For more information, read this [article](https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/).

**REMEMBER:** If there are no merge conflicts, or after resolving any conflicts, run the following in terminal:

```
npm install
```

## Testing

Run unit tests with `npm start`. Tests will automatically re-run as you update files.

We use [Travis CI](https://docs.travis-ci.com/) for continuous integration and deployment. When a commit is added to a branch or a PR, travis runs tests, builds the app, and reports results in the PR conversation. Travis is configured in `.travis.yml` and builds are viewable in the [Travis dashboard](https://travis-ci.org/codeforboston/migrant_service_map). Travis should pass before merging PR's.

We use [Cypress](https://www.cypress.io) for functional (e2e) testing. Cypress test results from Travis branch builds are viewable in the [Cypress dashboard](https://dashboard.cypress.io/projects/wxgyq3/runs).

Running the Cypress test suite requires that a local version of the site is available at [http://localhost:3000/]. This can be done with the command `npm start`. Then in a separate terminal, run the following to open the Cypress interface:

```
$(npm bin)/cypress open
```

All tests can be run by clicking `Run all specs` in the top-right of the application window.

You can run Cypress tests from the command line using `npm run test:cy` against a running server, or `npm run test:cy-start-server` to start a server, run tests, and then shut everything down.

## Deployment

We use [Firebase](https://firebase.google.com/docs/hosting) to host the app at [migrant-service-map.web.app](https://migrant-service-map.web.app) and [migrant-service-map.firebaseapp.com](https://migrant-service-map.firebaseapp.com). Firebase is configured using `.firebaserc`, `firebase.json`, and the [migrant-service-map](https://console.firebase.google.com/project/migrant-service-map/hosting/main) project page. We use the free [Spark](https://firebase.google.com/pricing) plan, which should be more than enough for development, testing, and demos. If we go over the monthly limits, the site is disabled until the next month (or we upgrade to a paid plan).

We use Travis to automate the deployment process. Whenever the `prod` (production) branch is updated, Travis builds the app as for any other commit. Then, if the build succeeds, it uploads the site to Firebase using the token stored in `.travis.yml`. The token is set up using [these instructions](https://docs.travis-ci.com/user/deployment/firebase/#generating-your-firebase-token).

## Tech Stack

- [Reactjs](https://facebook.github.io/react/docs/react-api.html)
- [Mapbox](https://www.mapbox.com/)
- [Bootstrap](https://getbootstrap.com/)
","'boston', 'code-for-america', 'code-for-boston', 'mapbox', 'migrants', 'rapid-response', 'refugees', 'refugees-welcome', 'service-providers'",2021-09-30T18:47:45Z,26,25,12,"('sashamaryl', 211), ('alexjball', 177), ('p-somers', 99), ('ericjbowman', 40), ('bhinebaugh', 34), ('mwellman17', 30), ('Advencr', 25), ('charlaburnett', 24), ('mikeyavorsky', 13), ('alexandraleah', 12), ('maiam6242', 12), ('kststudio', 9), ('rivison', 7), ('endikaserra', 7), ('carpeliam', 4), ('grinyahoo', 4), ('dnmeyer06', 3), ('DavidNoftsier', 3), ('jonnyjohannes', 3), ('eric-lazarus', 2), ('spykins', 2), ('ecbypi', 1), ('jc-clark', 1), ('thadk', 1), ('tbfro', 1), ('Mattgoldman93', 1)","[11, 'Sustainable Cities and Communities']"
energywebfoundation/energyweb-ui,"Energy Web UI - Dapp shell, browser & launcher (Electron app)","# Energy Web Client UI

To quickly get started just download the binary for your OS and install the software.

[Windows](https://github.com/energywebfoundation/energyweb-ui/releases/download/v0.4.6/energyweb-ui-setup-0.4.6.exe)
          
[Mac](https://github.com/energywebfoundation/energyweb-ui/releases/download/v0.4.6/energyweb-ui-0.4.6.pkg)
      
[Linux](https://github.com/energywebfoundation/energyweb-ui/releases/download/v0.4.6/energyweb-ui_0.4.6_amd64.deb)

[Release history](https://github.com/energywebfoundation/energyweb-ui/releases)

[![GPLv3](https://img.shields.io/badge/license-GPL%20v3-green.svg)](https://www.gnu.org/licenses/gpl-3.0.en.html)


## About Energy Web Client UI

The Energy Web Client UI (EWF UI) is a User Interface desktop application for the [Parity Ethereum Client](https://github.com/paritytech/parity/blob/master/README.md) >=v1.10. It features a Wallet supporting Ether and ERC-20 Tokens, a contract development environment, and so much more. EWF UI will download and run [Parity Ethereum Client](https://github.com/paritytech/parity/blob/master/README.md) in the background if it is not found on the system. 
By default EWF UI will try connect to Tobalaba using Websocket port 8546. 

## Build from source

```bash
npm install
npm run electron
```

You should see the Electron app popping up.

### Build the binary (Optional)

One further, albeit optional step is to create an OS-spefific binary. This is done with the following command:

```bash
npm run release
```

This command may take some time. Once finished, you will see binaries for your OS in the `dist/` folder.

### EWF Client usage
Visit our [EWF Wiki](https://energyweb.atlassian.net/wiki/spaces/EWF/pages/544374788/Setting+Up+a+Node) for high level, or Parity's [Wiki](https://wiki.parity.io/) for in depth information.

### Any issues with the client itself?
Client related issues are best filed in the [Parity Repo](https://github.com/paritytech/parity-ethereum). 

### How to get in touch?
[Get in touch](https://energyweb.org/contact-us/) with EWF.

Get in touch with Parity on Gitter:
[![Gitter: Parity](https://img.shields.io/badge/gitter-parity-4AB495.svg)](https://gitter.im/paritytech/parity)

Or join the Parity community on Matrix:
[![Riot: +Parity](https://img.shields.io/badge/riot-%2Bparity%3Amatrix.parity.io-orange.svg)](https://riot.im/app/#/group/+parity:matrix.parity.io)
",,2019-12-08T12:55:03Z,12,45,14,"('amaury1093', 30), ('jacogr', 26), ('ngyam', 23), ('General-Beck', 16), ('axelchalon', 12), ('ewaldhesse', 10), ('ETHorHIL', 7), ('Tbaut', 5), ('devops-parity', 4), ('maciejhirsz', 3), ('manihagh', 2), ('alixeb', 1)","[13, 'Climate Action']"
hackoregon/civic,The frontend monorepo for the CIVIC platform.,"# Civic [![Build Status](https://travis-ci.org/hackoregon/civic.svg?branch=master)](https://travis-ci.org/hackoregon/civic)

This is the home of the front-end code for the [CIVIC Platform](https://civicplatform.org/). It's organized in a monorepo using Yarn Workspaces, but it's ok if you don't know what that means yet.

# ⚡ Rapid iteration in progress!

Currently, we're rapidly iterating to prepare improve the experience for future development! This might not be the best time to hop in as a first time contributor.

If you're interested in what we're doing and want to hear more when we're ready for more collaborative contributions, visit [Civic Software Foundation](https://civicsoftwarefoundation.org) and join our mailing list or apply to volunteer.

# Let's make this better, together!

Civic magic happens when we work together. We welcome your collaborative contributions. We also have a [more technical contribution guide](https://github.com/hackoregon/civic/blob/master/CONTRIBUTING.md).

🐧 **I see something that could be better:**
The first step is [open an issue](https://github.com/hackoregon/civic/issues/new/choose), and tell us what you see that could be better. Tell us about your vision so that we can see what you see and help to improve it.

🐦 **I want to work on making something specific better:**
If there's already a [documented issue](https://github.com/hackoregon/civic/issues) about what you want to work on, assign yourself to let others what you're working on. If there's not an issue, open one and assign yourself.

🐤 **I want to work on making something better, but I'm not sure where to start:**
Check out our [open issues with the good-first-issue label](https://github.com/hackoregon/civic/issues?q=is%3Aissue+is%3Aopen+label%3Agood-first-issue) for things to work on and [open pull requests with the good-first-review label](https://github.com/hackoregon/civic/pulls?q=is%3Aopen+is%3Apr+label%3Agood-first-review) to review and collaborate with others on existing efforts.

🦜 **I've done something towards making this better:**
Fantastic! Share it with us by [opening a pull request](https://github.com/hackoregon/civic/compare) with what you've done so far, and let's work together to make it even better and incorporate it into the CIVIC Platform!

🦚 **I want to explore more things:**

[CIVIC Platform](https://civicplatform.org/) 👏 [Components and Style Guide (Storybook)](https://master--5f55eec3d7d83100229d47fe.chromatic.com) 👏 [Platform Architecture Guide](https://github.com/hackoregon/civic/blob/master/ARCHITECTURE.md) 👏 [Redux Guide](https://github.com/hackoregon/civic/blob/master/WORKING_WITH_REDUX.md) 👏 [Contributing Guide](https://github.com/hackoregon/civic/blob/master/CONTRIBUTING.md)

# Setup

## Development environment

### Prerequisites

Before you install and build, you'll need the following.

1. **bash**

   You will need a Unix shell (bash). For Mac, this can be Terminal.app. For Windows, you'll need to use [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10) (see issue #[53](https://github.com/hackoregon/civic/issues/53)).

2. **nvm and yarn**

   You will need the following tools installed in your Unix shell:

   - [nvm](https://github.com/creationix/nvm)
   - [yarn](https://yarnpkg.com/) > 1.0

   These tools make sure every contributor has identical dependency versions, include node and node packages.

3. **git**

   You will need to have [Git](https://git-scm.com/) installed in your bash environment.

### Install and build

🐸**GENTLE WARNING**🐸: Make sure you have met the prerequisites ☝️

```bash
# Clone the repository using either https or ssh
# https
$ git clone https://github.com/hackoregon/civic.git
# OR ssh
$ git clone git@github.com:hackoregon/civic.git

$ cd civic

# Sets your Node.js version to match what the project uses. You may need to install the appropriate version per npm instructions
$ nvm use

# Installs all package dependencies and links cross-dependencies.
# Also builds the component packages and mock-wrapper
$ yarn bootstrap

# This can take a minute, so take a stretch break (mental or physical)!
```

## Text editor additions

In order to be the most productive, you’ll want to install some extensions or plug-ins for your text editor. These tools are already installed and configured project wide, so the only installation you’ll need is inside your text editor (don’t `npm install` or `yarn add` them). There are plug-ins or extensions available for the most commonly used editors (VS Code, Sublime Text, Vim, WebStorm, Atom, etc…)

- ESLint — to show linting in your editor as you’re coding
- Prettier — for code formatting in your editor as you’re coding
- EditorConfig — for consistency in settings like indentation line-endings

# Development

## Working on a single package other than a component package

Most developers working in this project will be contributing to one package at a time.

This is the command sequence that will allow you to build/run an individual package every time (for example, the `2019-template` package)
and work on it as if it was a standalone project:

```bash
$ cd packages/{package-name} # e.g. cd package/2019-template

# run local project
$ yarn start

# watch tests while working on them
$ yarn test:watch
```

## Working on component packages using Storybook

We are committed to using shared component packages. This is achieved using the `ui-*` packages and React Storybook.
Run Storybook with the following command or [view it here](https://hackoregon.github.io/civic/):

```bash
# run this command from project root (civic)
$ yarn storybook
```

> Note that component packages were formerly bundled in a single component library package

## Working on a component package and a project package simultaneuously

In separate terminals, run the commands in the **Working on a single package other than a component package** and **Working on a component package using Storybook** sections above. Project packages rely on the built version of the component packages, so if you have updated a component package, and want to see your changes in the project package you are working on, you'll need to rebuild the component package. Once the component package build has finished, your project package will reload with the update components.

```bash
# watch for changes across component packages and rebuild as necessary
$ yarn watch
```

## Testing across all packages

To run all tests for all packages, use the following command from the project root:

```bash
yarn test
```

Tests for individual packages can be run from within the individual package's directory. Running all tests is useful for continuous integration environments as well as verifying changes to common dependencies does not break packages.

For example, run the above command at the root of the project after making changes to a component in the component package to ensure that others packages are compatible with the changes made.

## Creating a new component

It's as easy as running `yarn component` from the root of the repo. You'll be prompted to name your component, and select the package where it will be created. When that's done, it will add new files as seen here:

```
civic
└───packages
    └───
        └───src
            └───YourNewComponent
                    YourNewComponent.js
                    YourNewComponent.stories.js
                    YourNewComponent.test.js
```

You can now run `yarn storybook` from the root and see YourNewComponent under Unsorted/Components/YourNewComponent. You can also run `yarn test` within `package-name/` and see that the YourNewComponent tests all pass. As you develop, make sure to update `YourNewComponent.stories.js` and `YourNewComponent.test.js` so that future developers can easily understand how to use your work and rest easy that it is well tested!

## Creating a new card

There are two options to create a card: from API data and from local json data. Our new card generator makes it easy to load data from the API so we recommend that approach for ease of creation and the most up to date data.

### Using API data (recommended)

At the top of the repo run:

```bash
$ yarn card
```

This will bring up the prompt:

```bash
$ Which existing package should this card be in?: >
```

Type the name of a package that already exists in the `civic/packages/` directory (example `2019-transportation`) and press enter.

You will now see this prompt:

```bash
$ What will be the title of the card? (Capitalized With Spaces): >
```

Type the title of what you want to show on the new card and press enter. You may always change it later. Be aware that this choice will determine the names of newly created directories, files, and the import of those files into existing files.

You should see something like this now:

```bash
$ yarn card
yarn run v1.16.0
$ yarn hygen api-story-card with-prompt
$ /Users/Awesome_Volunteer/civic/node_modules/.bin/hygen api-story-card with-prompt
✔ Which existing package should this card be in?: · 2019-transportation
✔ What will be the title of the card? (Capitalized With Spaces): · Ducks Taking Buses

Loaded templates: _templates
      MANY INJECT AND ADDED MESSAGES
✨  Done in 37.18s.
```

If the package you chose isn't set up quite the way the generator expected you may need to make some modifications to files where code was injected but this should get you most of the way there. Now you can customize and hook in your own data.

### Using local data (not recommended)

The process is very similar to genrating an API data based card. At the top of the repo run:

```bash
$ yarn card:local-data
```

You'll then follow the same prompts as above. When you've finished you'll see a similar output to the one above.

If the package you chose isn't set up quite the way the generator expected you may need to make some modifications to files where code was injected but this should get you most of the way there. Now you can customize and hook in your own data.

## Creating a new Package

At the top of the repo run:

```bash
$ yarn new-package
```

This will bring up the prompt:

```bash
$ What do you want to name the new package? (e.g. YYYY-package-name): >
```

Type the name of the new package that you want to create and press enter.
✨ DONE ✨
You can now add a card if you want. Try it out using the [Creating a new card](#creating-a-new-card)

# Packages & Deployment

## Project Layout

There are four types of packages right now:

1. **Project packages**: A React/Redux codebase that holds a collection of stories and API interactions for a single
   project in a Hack Oregon project cycle.
2. **Year package bundles**: A React/Redux codebase that bundles together all project packages for a given year. This
   is a unit that gets deployed to production.
3. **Component Packages**: Reuseable React components and Storybook documentation
4. **Utilities**: Common code that other projects depend on.

### Packages

Every package has its own README with further details on what the package is for and how it works. New packages in development for the 2019 project season.

- Year Packages
  - [2018](packages/2018/README.md) <- despite the name, this package powers [civicplatform.org](civicplatform.org) in 2019 and beyond.
  - [2017](packages/2017/README.md)
- Project Packages
  - [2018-neighborhood-development](packages/2018-neighborhood-development/README.md)
  - [2018-disaster-resilience](packages/2018-disaster-resilience/README.md)
  - [2018-transportation-systems](packages/2018-transportation-systems/README.md)
  - [2018-housing-affordability](packages/2018-housing-affordability/README.md)
  - [2018-local-elections](packages/2018-local-elections/README.md)
  - [2018-example-farmers-markets](packages/2018-example-farmers-markets/README.md)
  - [budget](packages/budget/README.md)
  - [emergency-response](packages/emergency-response/README.md)
  - [homelessness](packages/homelessness/README.md)
  - [housing](packages/housing/README.md)
  - [transportation](packages/transportation/README.md)
- Component Packages
  - [ui-brand](packages/ui-brand/README.md)
  - [ui-charts](packages/ui-charts/README.md)
  - [ui-core](packages/ui-core/README.md)
  - [ui-docs](packages/ui-docs/README.md)
  - [ui-maps](packages/ui-maps/README.md)
  - [ui-themes](packages/ui-themes/README.md)
- Utilities
  - [civic-sandbox](packages/civic-sandbox/README.md)
  - [civic-babel-presets](packages/civic-babel-presets/README.md)
  - [component-library](packages/component-library/README.md)
  - [dev-server](packages/dev-server/README.md)
  - [mock-wrapper](packages/mock-wrapper/README.md)
  - [webpack-common](packages/webpack-common/README.md)
  - [utils](packages/utils/README.md)

## Continuous Integration

Travis CI is configured to have a build pipeline for the component packages and one for each project year. Although most
commands are run using yarn scripts attached to a `package.json` file, due to the many steps required to run tests for
a specific set of packages, a Makefile is used instead.

## Continuous Delivery

Travis CI will deploy docker containers to ECS for each project year whenever the `master` branch builds successfully. [Storybook](https://master--5f55eec3d7d83100229d47fe.chromatic.com) also deploys via [Chromatic](https://www.chromatic.com/) when `master` builds successfully.

# Notes

**✨Demo Day⁇✨** _You may notice references to Demo Day! Our most recent Demo Day was September 11, 2019 in Portland, OR. [See highlights!](https://www.youtube.com/watch?v=99RFAwCJg-o)_
",,2022-12-14T17:15:08Z,68,60,38,"('jaronheard', 775), ('DingoEatingFuzz', 465), ('mendozaline', 410), ('DavideDaniel', 262), ('BJHunnicutt', 166), ('bradfordjanice', 142), ('flamingveggies', 93), ('paigewilliams', 72), ('lucastswick', 67), ('palmerev', 58), ('nmuldavin', 57), ('danieltomasku', 54), ('cosmolightfoot', 49), ('jasonleonhard', 45), ('meganmckissack', 44), ('JohnTasto', 37), ('worc', 34), ('cp', 33), ('sanjuroj', 29), ('drabelpdx', 25), ('MagWeaver', 21), ('nicrocs', 18), ('kdv24', 17), ('kodiakhqbot', 16), ('AlexanderNull', 15), ('oovg', 13), ('MikeTheCanuck', 13), ('accua', 12), ('clemf', 10), ('corydm92', 8), ('dsdemaria', 8), ('fstorr', 5), ('toyotathon', 4), ('Kodiraet', 4), ('ericgold', 4), ('lowesusannah', 4), ('jimtyhurst', 4), ('dorenproctor', 4), ('jos-h20', 3), ('iankerriganharris', 2), ('jaikamat', 2), ('tedbrunner', 2), ('sk3ske', 2), ('stellarluminary', 2), ('evandemaris', 2), ('pdxdiver', 2), ('pfarnach', 1), ('tyreer', 1), ('scholachoi', 1), ('sgbj', 1), ('TanviCodeLife', 1), ('tylerbuchea', 1), ('ynden', 1), ('abferris', 1), ('dependabotbot', 1), ('controlledburn', 1), ('github-actionsbot', 1), ('Niley', 1), ('mikaelacurrier', 1), ('waffle-iron', 1), ('hingham', 1), ('Foadm', 1), ('davidfloyd91', 1), ('daviccardoso', 1), ('ceci99pb', 1), ('BrianHGrant', 1), ('andrewbiang888', 1), ('zork4cyber', 1)","[17, 'Partnerships for the Goals']"
openpantry/open_pantry,A management system for pantry programs to help people eat healthy meals with dignity,"[![Build Status](https://travis-ci.org/openpantry/open_pantry.svg?branch=master)](https://travis-ci.org/openpantry/open_pantry)
# OpenPantry
## A management system for pantry programs to help people eat healthy meals with dignity

  * NOTES:
    * We are moving towards a SaaS like model, with one subdomain per facility, and facilities are managed by super-admins or by facility specific admins
    * Masbia has several locations, but we're trying to validate the scope and use-case for other organizations, please get in touch if you work with one 
    * Users are created/managed per facility on a user_selections page, and globally in a /manage/users page.
    * Food recipients are managed as users and can be logged in via magic login-links to be clicked on or emailed to them
    * We've been using ZenHub chrome extension for project management but this has caused some confusion, so we may move away from it...
    * Some mockups and UX flow for where we're trying to head are posted [here](https://invis.io/QPBK7WPB3).  
    * We initially attempted to make this multi lingual from the start but have largely deferred this work until things are more stable.
      * We have partial translations started for 9 languages
      * We have thousands of foods from USDA nutritional database ready for dynamic translation
      * We are using POEditor.com for static site translation, contribute to translations here: https://poeditor.com/join/project/wBfgEEUCht
      * We probably need to move to a database driven translation system for foods, given the quantity we are trying to manage, but we still need translation help.
      * Our curent best source for food images, facts and translations may be https://us.openfoodfacts.org/ but we started with USDA database and much work is needed to leverage/combine and rework the data model to pull photos and translations from openfoodfacts

  * Getting started with development:
    * Mac homebrew:
      * ensure homebrew is installed (instructions at https://brew.sh/ or paste `/usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""` into terminal)
      * brew bundle
        * This will install elixir, erlang, yarn, and download Postgres.app which has postgis preinstalled
        * It will cd into assets directory and install npm packages via yarn
        * You can use another postgres, but you may have to manually install postgis
      * run from open_pantry dir `mix do deps.get, ecto.create, ecto.migrate, run priv/repo/seeds.exs`
      * Start Phoenix endpoint with `mix phx.server`, or `iex -S mix phx.server` (this gives a server and REPL/console in one window)
    * Docker/docker-compose (fake, factory generated data but no dependencies)
      * Install docker (on most systems this also installs docker-compose)
      * Clone the OpenPantry repo and cd into directory
      * Run `docker-compose run setup`
      * Run `docker-compose up web`
    * Local/native development on Mac/Linux manually, without automation from `brew bundle` via Brewfile above (detailed instructions only for Mac at present, similar for linux)
      * Install Postgres (Mac)
          * Download and install the Postgresapp.com from [their site](https://postgresapp.com/documentation/install.html)
              * Execute the following command in Terminal to configure your $PATH, and then close & reopen the window:
              `sudo mkdir -p /etc/paths.d &&
                echo /Applications/Postgres.app/Contents/Versions/latest/bin | sudo tee /etc/paths.d/postgresapp`
          * Alternatively install Postgres with Homebrew
              * Execute (`brew install postgres`) in Terminal
      * if using Postgres.app you must initialize a data directory after installing, and follow instructions for adding CLI tools to your Terminal path...  `which psql` but succeed when done
        * (instructions defaulting to Mac below... for simplicity, linux users extrapolate, Windows, I have no idea, PR's with instructions for either/both welcome)
      * Install Elixir/Erlang (`brew install elixir`)
      * Install NPM and yarn (`brew install node && npm install -g yarn`)
      * Clone this repository locally, `git clone git@github.com:openpantry/open_pantry.git`
      * cd into the directory `cd open_pantry`
      * Download database from s3 via `wget https://s3.amazonaws.com/open-pantry-dev/openpantry_dev.dump`
        * Note we had problems with this dump being improperly generated recently.  I beleive the problem is fixed, but if you downloaded previously or have problems please contact someone for support, it's probably not your fault!
        * Dumps and restores are based on the method described here: https://devcenter.heroku.com/articles/heroku-postgres-import-export
      * Install Elixir package dependencies with `mix deps.get`
      * Create the database in Postgres with `mix ecto.create`, assuming default password etc in config works.
      * Migrate the database to add migrations since dump was created, via `mix ecto.migrate`
      * Import the dump to the database via `pg_restore --verbose --clean --no-acl --no-owner -h localhost -U postgres -d open_pantry_dev openpantry.dump`
      * Install Node.js dependencies with `yarn install`
      * Start Phoenix endpoint with `mix phx.server`, or `iex -S mix phx.server` (this gives a server and REPL/console in one window)
    * ALTERNATIVELY (and with much less detail), if you DON'T WANT TO USE the dump file referenced above/want to generate a dump from scratch, the above dump was generated with a complete USDA food/nutrient database approximately as below, along with non-dump steps above:
      * Create and migrate your database with `mix ecto.create && mix ecto.migrate`
      * Git clone https://github.com/openpantry/nutes locally and run make, modifying if necessary to point at your Postgres DB and the directory path to your local copy in imports.sql (requires golang to build data_cleanup tool)
      * Add seed data with `mix run priv/repo/seeds.exs` but modify to leave out foods/stocks as these are fakes generated by factories, you have real food from USDA

Now you can visit [`localhost:4000`](http://localhost:4000) from your browser.

## Learn more

  * Masbia Pantry: http://www.masbia.org/pantry
  * Volunteer to help develop this app, or other work with Masbia: http://www.masbia.org/volunteer_signup
","'food-banks', 'non-profit', 'nutrition', 'pantry-programs', 'social-good'",2018-11-20T20:46:21Z,8,63,10,"('bglusman', 211), ('komizutama', 48), ('fcapovilla', 41), ('cheerfulstoic', 13), ('davearonson', 8), ('net', 7), ('neilfulwiler', 5), ('keathley', 1)","[11, 'Sustainable Cities and Communities']"
codeforamerica/intake,A Django project behind the Clear My Record website,"# Intake
[![CircleCI](https://circleci.com/gh/codeforamerica/intake.svg?style=svg)](https://circleci.com/gh/codeforamerica/intake)  [![Code Climate](https://codeclimate.com/github/codeforamerica/intake/badges/gpa.svg)](https://codeclimate.com/github/codeforamerica/intake) 


[Tech Overview](https://github.com/codeforamerica/intake/wiki)

[Quickstart](https://github.com/codeforamerica/intake/wiki/Quickstart)

[Installation & Setup](https://github.com/codeforamerica/intake/wiki/Installation-&-Setup)

[Testing & Debugging](https://github.com/codeforamerica/intake/wiki/Testing-&-Debugging)

[Logging](https://github.com/codeforamerica/intake/wiki/Logging)

[Releasing & Deploying](https://github.com/codeforamerica/intake/wiki/Releasing-&-Deploying)

[Infrastructure](https://github.com/codeforamerica/intake/wiki/Infrastructure)

[Administrative Access](https://github.com/codeforamerica/intake/wiki/Administrative-Access)

## Contributing

We are not currently seeking contributions from the public.

If you have questions about the project, please contact clearmyrecord@codeforamerica.org.
",,2021-11-15T19:37:27Z,17,49,15,"('bengolder', 991), ('jennifermarie', 228), ('glassresistor', 162), ('hartsick', 45), ('kayline', 41), ('jazmynlatimer', 40), ('lkogler', 27), ('SymonneSingleton', 25), ('vrajmohan', 21), ('maxmcd', 16), ('loumoore', 10), ('spencerhitch', 6), ('rossettistone', 5), ('gitter-badger', 1), ('tmaybe', 1), ('tdooner', 1), ('awsonline', 1)","[16, 'Peace, Justice and Strong Institutions']"
CenterForOpenScience/SHARE,"SHARE is building a free, open, data set about research and scholarly activities across their life cycle.","# SHARE/Trove

SHARE is creating a free, open dataset of research (meta)data.

> **Note**: SHARE’s open API tools and services help bring together scholarship distributed across research ecosystems for the purpose of greater discoverability. However, SHARE does not guarantee a complete aggregation of searched outputs. For this reason, SHARE results should not be used for methodological analyses, such as systematic reviews.

[![Coverage Status](https://coveralls.io/repos/github/CenterForOpenScience/SHARE/badge.svg?branch=develop)](https://coveralls.io/github/CenterForOpenScience/SHARE?branch=develop)

## Documentation

### What is this?
see [WHAT-IS-THIS-EVEN.md](./WHAT-IS-THIS-EVEN.md)

### How can I use it?
see [how-to/use-the-api.md](./how-to/use-the-api.md)

### How do I navigate this codebase?
see [ARCHITECTURE.md](./ARCHITECTURE.md)

### How do I run a copy locally?
see [how-to/run-locally.md](./how-to/run-locally.md)


## Running Tests

### Unit test suite

  py.test

### BDD Suite

  behave

","'data', 'elasticsearch', 'harvest-data', 'metadata', 'openscience', 'python', 'scholarly-communication', 'science'",2024-02-13T21:31:26Z,36,98,23,"('chrisseto', 1167), ('aaxelb', 921), ('laurenbarker', 191), ('icereval', 138), ('erinspace', 137), ('fabianvf', 38), ('SSJohns', 34), ('efc', 33), ('JeffSpies', 18), ('sheriefvt', 14), ('zamattiac', 13), ('sloria', 11), ('fayetality', 10), ('AndrewSallans', 8), ('csheldonhess', 7), ('cwisecarver', 4), ('vpnagraj', 4), ('mattclark', 4), ('cmaimone', 3), ('bnosek', 3), ('aishanibhalla', 2), ('DanielSBrown', 2), ('felliott', 2), ('GaryKriebel', 2), ('mfraezz', 2), ('sf2ne', 2), ('adlius', 1), ('gitter-badger', 1), ('terroni', 1), ('Stevenholloway', 1), ('remram44', 1), ('PatrickEGorman', 1), ('hmoco', 1), ('huangginny', 1), ('ead12345', 1), ('evomellor', 1)","[17, 'Partnerships for the Goals']"
learningequality/kolibri,Kolibri Learning Platform: the offline app for universal education,"
# Kolibri

[![Python test status](https://github.com/learningequality/kolibri/actions/workflows/tox.yml/badge.svg?branch=develop)](https://github.com/learningequality/kolibri/actions/workflows/tox.yml)
[![JS test status](https://github.com/learningequality/kolibri/actions/workflows/yarn.yml/badge.svg?branch=develop)](https://github.com/learningequality/kolibri/actions/workflows/yarn.yml)
[![Developer docs](https://img.shields.io/badge/docs-dev-blue.svg)](http://kolibri-dev.readthedocs.org/en/develop/)
[![Developer chat](https://img.shields.io/badge/chat-dev-blue.svg)](http://webchat.freenode.net?channels=%23kolibri)
[![PyPI](https://img.shields.io/pypi/v/kolibri.svg?color=blue)](https://pypi.org/project/kolibri/)
[![Demo](https://img.shields.io/badge/demo-online-blue.svg)](http://kolibridemo.learningequality.org/)
[![User docs](https://img.shields.io/badge/docs-user-blue.svg)](http://kolibri.readthedocs.org/en/latest/)
[![Discourse topics](https://img.shields.io/discourse/https/community.learningequality.org/topics.svg?color=blue)](https://community.learningequality.org/)

This repository is for software developers wishing to contribute to Kolibri. If you are looking for help installing, configuring and using Kolibri, please refer to the [User Guide](https://kolibri.readthedocs.io/).


## What is Kolibri?

[Kolibri Learning Platform](https://learningequality.org/kolibri/): the offline-first platform for teaching and learning with technology without requiring the Internet.

Developed with and for the community by [Learning Equality](https://learningequality.org/).

## How can I use it?

Kolibri is [available for download](https://learningequality.org/download/) from our website.

## How do I get help or give feedback?

You can ask questions, make suggestions, and report issues in the [community forums](https://community.learningequality.org/).

If you have found a bug and are comfortable using Github and Markdown, you can create a [Github issue](https://github.com/learningequality/kolibri/issues) following the instructions in the issue template.


## How can I contribute?

1. 📙 **Skim through the [Developer documentation](https://kolibri-dev.readthedocs.io)** to understand where to refer later on.
2. 💻 **Follow the [Getting started](https://kolibri-dev.readthedocs.io/en/develop/getting_started.html) to set up your development server.** Some of the [How To Guides](https://kolibri-dev.readthedocs.io/en/develop/howtos/index.html#howtos) may be handy too.
3. 🔍 **Search for issues tagged as [help wanted](https://github.com/learningequality/kolibri/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22+no%3Aassignee) or [good first issue](https://github.com/learningequality/kolibri/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22+no%3Aassignee).**
4. 🗣️ **Ask us for an assignment in the comments of an issue you've chosen.** Please request assignment of a reasonable amount of issues at a time. Once you finish your current issue or two, you are welcome to ask for more.

**❓ Where to ask questions**

- For anything development related, refer to the [Developer documentation](https://kolibri-dev.readthedocs.io) at first. Some answers may already be there.
- For questions related to a specific issue or assignment requests, use the corresponding issue's comments section.
- Visit [GitHub Discussions](https://github.com/learningequality/kolibri/discussions) to ask about anything related to contributing or to troubleshoot development server issues.

**👥 How to connect**

- We encourage you to visit [GitHub Discussions](https://github.com/learningequality/kolibri/discussions) to connect with the Learning Equality team as well as with other contributors.
- If you'd like to contribute on a regular basis, we are happy to invite you to our open-source community Slack channel. Get in touch with us at info@learningequality.org to receive an invitation.

---

🕖 Please allow us a few days to reply to your comments. If you don't hear from us within a week, reach out via [GitHub Discussions](https://github.com/learningequality/kolibri/discussions).

As soon as you open a pull request, it may take us a week or two to review it as we're a small team. We appreciate your contribution and will provide feedback.

---

*Thank you for your interest in contributing! Learning Equality was founded by volunteers dedicated to helping make educational materials more accessible to those in need, and every contribution makes a difference.*
","'edtech', 'education', 'oer', 'offline-first'",2024-05-03T13:45:41Z,166,742,42,"('rtibbles', 7364), ('indirectlylit', 4903), ('jonboiser', 3256), ('christianmemije', 1663), ('nucleogenesis', 1581), ('DXCanas', 1093), ('marcellamaki', 1008), ('66eli77', 732), ('bjester', 687), ('jamalex', 547), ('dependabotbot', 491), ('ralphiee22', 475), ('jredrejo', 465), ('benjaoming', 451), ('MisRob', 395), ('aronasorman', 363), ('radinamatic', 348), ('AllanOXDi', 317), ('lyw07', 282), ('akolson', 231), ('whitzhu', 221), ('MCGallaspy', 197), ('sairina', 190), ('mrpau-richard', 144), ('vkWeb', 137), ('AlexVelezLl', 99), ('LianaHarris360', 98), ('pyup-bot', 89), ('jtamiace', 87), ('pcenov', 76), ('micahscopes', 72), ('kollivier', 69), ('ozer550', 62), ('thesujai', 59), ('thanksameeelian', 54), ('chaowentan', 48), ('GarvitSinghal47', 47), ('EshaanAgg', 45), ('cpauya', 39), ('magali-br', 38), ('dylanmccall', 34), ('Wck-iipi', 33), ('Jaspreet-singh-1032', 29), ('ShivangRawat30', 29), ('nick2432', 29), ('alexMet', 27), ('bonidjukic', 27), ('dbnicholson', 26), ('danigm', 25), ('ivanistheone', 24), ('pwithnall', 24), ('EliKlein', 21), ('KshitijThareja', 21), ('mzq592', 19), ('Evgeni998', 18), ('maxbrunet', 18), ('ThEditor', 18), ('mrpau-eugene', 17), ('AtKristijan', 16), ('iskipu', 15), ('alanchenz', 14), ('mdctleo', 14), ('MauHernandez', 14), ('geoffrich', 14), ('navdeepsingh92', 14), ('deepsource-autofixbot', 14), ('muditchoudhary', 14), ('kmrinal19', 13), ('GCodeON', 13), ('luna215', 12), ('rationality6', 12), ('akshaymahajans', 10), ('ashmeet13', 10), ('photon0205', 10), ('nikkuAg', 10), ('mrpau-eduard', 10), ('Tweniee', 9), ('im-NL', 9), ('Pursottam6003', 9), ('apurva-modi', 8), ('camperjett', 8), ('adviti-mishra', 8), ('HansGam', 8), ('crcastle', 7), ('a6ar55', 7), ('Kej-r03', 7), ('akash5100', 6), ('YashJipkate', 6), ('manuq', 6), ('bransgithub', 6), ('Tlazypanda', 6), ('k2avila', 6), ('arceduardvincent', 6), ('julianduque', 6), ('priyanka-choubey', 5), ('mhasbini', 5), ('Kalovelo', 5), ('PR4NJ41', 5), ('amitpanwar789', 4), ('FidalMathew', 4), ('katkuskris', 4), ('d0sadata', 4), ('abhimnc', 4), ('khangmach', 4), ('yash1378', 4), ('GeekGawd', 3), ('Ali-Kazmi', 3), ('reem-atalah', 3), ('kafukoM', 3), ('shrinishant', 3), ('NobleCactus', 3), ('nitheezkant', 3), ('birdcar', 3), ('br-kwon', 3), ('arky', 3), ('haldaranup', 3), ('Mohamedkhaled81', 2), ('cyberorg', 2), ('udithprabhu', 2), ('sakshampathak1508', 2), ('rayykim', 2), ('paulbusse', 2), ('abdelrahman725', 2), ('AdamStasiw', 2), ('willingc', 2), ('ConConRob', 2), ('Devanshu-Augusty', 2), ('hyprsyd', 2), ('Shashank245', 2), ('dolfandringa', 2), ('Lewiscowles1986', 2), ('mgamlem3', 2), ('inflrscns', 1), ('Priyaraj17', 1), ('shanavas786', 1), ('shivangtripathi', 1), ('uprab001', 1), ('zrg228', 1), ('VedderPradhan', 1), ('Ghat0tkach', 1), ('blackboxo', 1), ('Inder2421', 1), ('jose-hy', 1), ('manav1403', 1), ('oge1ata', 1), ('pre-commit-ci-litebot', 1), ('sharifmaryam', 1), ('sophianyberg', 1), ('swiftugandan', 1), ('Akila-I', 1), ('andreamisuraca', 1), ('andersan', 1), ('davidgarg20', 1), ('divad12', 1), ('deepsourcebot', 1), ('deepanshgoyal33', 1), ('drone-droid', 1), ('FireSuperior482', 1), ('GeorgesStavracas', 1), ('GSAprod', 1), ('hamzamunaf', 1), ('jayoshih', 1), ('mrpau-julius', 1), ('Aypak', 1), ('KushalBeniwal', 1), ('moweiss', 1)","[4, 'Quality Education']"
scratchfoundation/scratch-vm,"Virtual Machine used to represent, run, and maintain the state of programs for Scratch 3.0","## scratch-vm
#### Scratch VM is a library for representing, running, and maintaining the state of computer programs written using [Scratch Blocks](https://github.com/scratchfoundation/scratch-blocks).

[![CI/CD](https://github.com/scratchfoundation/scratch-vm/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/scratchfoundation/scratch-vm/actions/workflows/ci-cd.yml)

## Installation
This requires you to have Git and Node.js installed.

To install as a dependency for your own application:
```bash
npm install scratch-vm
```
To set up a development environment to edit scratch-vm yourself:
```bash
git clone https://github.com/scratchfoundation/scratch-vm.git
cd scratch-vm
npm install
```

## Development Server
This requires Node.js to be installed.

For convenience, we've included a development server with the VM. This is sometimes useful when running in an environment that's loading remote resources (e.g., SVGs from the Scratch server). If you would like to use your modified VM with the full Scratch 3.0 GUI, [follow the instructions to link the VM to the GUI](https://github.com/scratchfoundation/scratch-gui/wiki/Getting-Started).

## Running the Development Server
Open a Command Prompt or Terminal in the repository and run:
```bash
npm start
```

## Playground
To view the Playground, make sure the dev server's running and go to [http://localhost:8073/playground/](http://localhost:8073/playground/) - you will be directed to the playground, which demonstrates various tools and internal state.

![VM Playground Screenshot](https://i.imgur.com/nOCNqEc.gif)


## Standalone Build
```bash
npm run build
```

```html


    var vm = new window.VirtualMachine();
    // do things

```

## How to include in a Node.js App
For an extended setup example, check out the /src/playground directory, which includes a fully running VM instance.
```js
var VirtualMachine = require('scratch-vm');
var vm = new VirtualMachine();

// Block events
Scratch.workspace.addChangeListener(vm.blockListener);

// Run threads
vm.start();
```

## Abstract Syntax Tree

#### Overview
The Virtual Machine constructs and maintains the state of an [Abstract Syntax Tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree) (AST) by listening to events emitted by the [scratch-blocks](https://github.com/scratchfoundation/scratch-blocks) workspace via the `blockListener`. Each target (code-running object, for example, a sprite) keeps an AST for its blocks. At any time, the current state of an AST can be viewed by inspecting the `vm.runtime.targets[...].blocks` object.

#### Anatomy of a Block
The VM's block representation contains all the important information for execution and storage. Here's an example representing the ""when key pressed"" script on a workspace:
```json
{
  ""_blocks"": {
    ""Q]PK~yJ@BTV8Y~FfISeo"": {
      ""id"": ""Q]PK~yJ@BTV8Y~FfISeo"",
      ""opcode"": ""event_whenkeypressed"",
      ""inputs"": {
      },
      ""fields"": {
        ""KEY_OPTION"": {
          ""name"": ""KEY_OPTION"",
          ""value"": ""space""
        }
      },
      ""next"": null,
      ""topLevel"": true,
      ""parent"": null,
      ""shadow"": false,
      ""x"": -69.333333333333,
      ""y"": 174
    }
  },
  ""_scripts"": [
    ""Q]PK~yJ@BTV8Y~FfISeo""
  ]
}
```

## Testing
```bash
npm test
```

```bash
npm run coverage
```

## Publishing to GitHub Pages
```bash
npm run deploy
```

This will push the currently built playground to the gh-pages branch of the
currently tracked remote.  If you would like to change where to push to, add
a repo url argument:
```bash
npm run deploy -- -r 
```

## Donate
We provide [Scratch](https://scratch.mit.edu) free of charge, and want to keep it that way! Please consider making a [donation](https://secure.donationpay.org/scratchfoundation/) to support our continued engineering, design, community, and resource development efforts. Donations of any size are appreciated. Thank you!
",,2024-05-03T14:56:39Z,61,1173,99,"('renovatebot', 1721), ('semantic-release-bot', 657), ('ericrosenbaum', 477), ('kchadha', 455), ('paulkaplan', 385), ('tmickel', 360), ('renovate-bot', 328), ('evhan55', 238), ('dependabot-previewbot', 233), ('cwillisf', 188), ('thisandagain', 175), ('mzgoddard', 169), ('rschamp', 143), ('fsih', 130), ('knandersen', 82), ('picklesrus', 81), ('greenkeeperbot', 71), ('towerofnix', 47), ('griffpatch', 45), ('TheBrokenRail', 41), ('adroitwhiz', 36), ('gnarf', 36), ('joker314', 24), ('apple502j', 21), ('ktbee', 20), ('hyperobject', 19), ('chrisgarrity', 17), ('Kenny2github', 16), ('khanning', 11), ('SillyInventor', 9), ('sjhuang26', 6), ('morantsur', 6), ('greenkeeperio-bot', 6), ('technoboy10', 4), ('daikifukumori', 4), ('dekrain', 4), ('julescubtree', 4), ('stocktonkincade', 3), ('marisaleung', 2), ('vincentbriglia', 2), ('spectranaut', 2), ('LukeSchlangen', 2), ('colbygk', 2), ('yokobond', 1), ('u9g', 1), ('neurosie', 1), ('a49594a', 1), ('geppy', 1), ('AmazingMech2418', 1), ('spl237', 1), ('Scimonster', 1), ('KisaragiEffective', 1), ('foobartles', 1), ('terado0618', 1), ('IAP-Reloaded', 1), ('Affonso-Gui', 1), ('dlech', 1), ('davidaylaian', 1), ('CosmicWebServices', 1), ('boazsender', 1), ('aeons', 1)","[4, 'Quality Education']"
hotosm/OpenAerialMap,OpenAerialMap is an open service to provide access to a commons of openly licensed imagery and map layer services.,"OpenAerialMap (OAM)
===

## What is OpenAerialMap?
Imagery from satellites, unmanned aerial vehicles (UAVs) and other aircraft is becoming increasingly available after a disaster. It is often difficult to determine what is available and easily access it. OpenAerialMap (OAM) seeks to solve this by providing a **simple open way to host and provide access to imagery** for humanitarian response and disaster preparedness.

## Repositories 

The following repositories are part of the OAM project:

| | |
| --- | --- |
| [oam-api](https://github.com/hotosm/oam-api) | Catalog for indexing open imagery | 
| [oam-browser](https://github.com/hotosm/oam-browser) | Imagery browser for searching available imagery |
| [oam-uploader](https://github.com/hotosm/oam-uploader) | The web frontend to the OAM Uploader API |
| [oam-uploader-api](https://github.com/hotosm/oam-uploader-api) | The OAM Uploader API server |
| [oam-docs](https://github.com/hotosm/oam-docs) | OAM Documentation |
| [openaerialmap.org](https://github.com/hotosm/openaerialmap.org) | Code for the OpenAerialMap.org Website |
| [oam-design-system](https://github.com/hotosm/oam-design-system) | Style guide and UI components library |


Repositories maintained outside of the HOT Github:

| | |
| --- | --- |
| [marblecutter-openaerialmap](https://github.com/mojodna/marblecutter-openaerialmap) | Python, Flask and Lambda-based dynamic tiler for S3-hosted GeoTIFFs |
| [oam-qgis-plugin](https://github.com/yojiyojiyoji/oam_qgis3_express) | An experimental plugin for QGIS v3 to access OAM |


### Deprecated repositories

| | |
| --- | --- |
| [oam-server](https://github.com/hotosm/oam-server) | Main repository for imagery processing and tile service creation tools |
| [oam-server-tiler](https://github.com/hotosm/oam-server-tiler) | OAM Server tile engine |
| [oam-server-activities](https://github.com/hotosm/oam-server-activities) | SWFR Activities component for OAM Server |
| [oam-server-decider](https://github.com/hotosm/oam-server-decider) | SWF Decider component of OAM Server (using oam-server-tiler instead) |
| [oam-server-api](https://github.com/hotosm/oam-server-api) | OAM Server API |
| [oam-server-cli](https://github.com/hotosm/oam-server-cli) | A command line utility for interacting with the OAM Server API |
| [oam-server-deployment](https://github.com/hotosm/oam-server-deployment) | Amazon Web Services deployment tooling OAM Server |
| [oam-server-publisher](https://github.com/hotosm/oam-server-publisher) | Status publishing component of OAM Server |
| [oam-catalog-grid](https://github.com/hotosm/oam-catalog-grid) | Generate a vector tile grid from the OAM catalog |
| [oam-browser-filters](https://github.com/hotosm/oam-browser-filters) | The grid filters used by the oam-browser front end |
| [oam-uploader-admin](https://github.com/hotosm/oam-uploader-admin) | OAM uploader admin interface |
| [oam-status](https://github.com/hotosm/oam-status) | A simple status dashboard for oam-catalog |
| [oam-dynamic-tiler](https://github.com/hotosm/oam-dynamic-tiler) | Python, Flask and Lambda-based dynamic tiler for S3-hosted GeoTIFFs
| [oam-qgis-plugin](https://github.com/hotosm/oam-qgis-plugin) | An experimental plugin for QGIS to access OAM |

## Getting Involved

There are plenty of ways to get involved in OpenAerialMap! First of all, if
you're new to the project and want to learn a bit more about its current
design and architecture, head over to the
[docs](http://docs.openaerialmap.org/) for an overview.

### Ideas, Issue and Discussions

The best place to jump into discussions about OpenAerialMap is on the main [issue
tracker](https://github.com/hotosm/OpenAerialMap/issues) or individual ones for repos listed above.

Ongoing and past converations about the project take place in the project's
[Gitter](https://gitter.im/hotosm/OpenAerialMap?) chat room, and periodic
updates go out on the [OAM-Talk mailing
list](https://groups.google.com/a/hotosm.org/forum/#!forum/openaerialmap).)

Please come join the discussion and share your ideas!

[![Join the chat at https://gitter.im/hotosm/OpenAerialMap](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/hotosm/OpenAerialMap?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  
",,2024-01-04T15:43:57Z,14,236,46,"('cgiovando', 12), ('lossyrob', 6), ('dakotabenjamin', 3), ('wonderchook', 3), ('dodobas', 2), ('anandthakker', 1), ('bgirardot', 1), ('clkao', 1), ('oeon', 1), ('smit1678', 1), ('petya-kangalova', 1), ('ricardoduplos', 1), ('tombh', 1), ('tmcw', 1)","[17, 'Partnerships for the Goals']"
azavea/raster-vision,An open source library and framework for deep learning on satellite and aerial imagery.,"![Raster Vision Logo](docs/img/raster-vision-logo.png)
&nbsp;

[![Pypi](https://img.shields.io/pypi/v/rastervision.svg)](https://pypi.org/project/rastervision/)
[![Documentation Status](https://readthedocs.org/projects/raster-vision/badge/?version=latest)](https://docs.rastervision.io/en/stable/?badge=stable)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build Status](https://github.com/azavea/raster-vision/actions/workflows/release.yml/badge.svg)](https://github.com/azavea/raster-vision/actions/workflows/release.yml)
[![codecov](https://codecov.io/gh/azavea/raster-vision/branch/master/graph/badge.svg)](https://codecov.io/gh/azavea/raster-vision)

Raster Vision is an open source Python **library** and **framework** for building computer vision models on satellite, aerial, and other large imagery sets (including oblique drone imagery).

It has built-in support for chip classification, object detection, and semantic segmentation with backends using PyTorch.


    


**As a library**, Raster Vision provides a full suite of utilities for dealing with all aspects of a geospatial deep learning workflow: reading geo-referenced data, training models, making predictions, and writing out predictions in geo-referenced formats.

**As a low-code framework**, Raster Vision allows users (who don't need to be experts in deep learning!) to quickly and repeatably configure experiments that execute a machine learning pipeline including: analyzing training data, creating training chips, training models, creating predictions, evaluating models, and bundling the model files and configuration for easy deployment.
![Overview of Raster Vision workflow](docs/img/rv-pipeline-overview.png)

Raster Vision also has built-in support for running experiments in the cloud using [AWS Batch](https://github.com/azavea/raster-vision-aws).

See the [documentation](https://docs.rastervision.io/en/stable/) for more details.

## Installation

*For more details, see the [Setup documentation](https://docs.rastervision.io/en/stable/setup/)*.

### Install via `pip`

You can install Raster Vision directly via `pip`.

```sh
pip install rastervision
```

### Use Pre-built Docker Image

Alternatively, you may use a Docker image. Docker images are published to [quay.io](https://quay.io/repository/azavea/raster-vision) (see the *tags* tab).

We publish a new tag per merge into `master`, which is tagged with the first 7 characters of the commit hash. To use the latest version, pull the `latest` suffix, e.g. `raster-vision:pytorch-latest`. Git tags are also published, with the Github tag name as the Docker tag suffix.

### Build Docker Image

You can also build a Docker image from scratch yourself. After cloning this repo, run `docker/build`, and run then the container using `docker/run`.

## Usage Examples and Tutorials

**Non-developers** may find it easiest to use Raster Vision as a low-code framework where Raster Vision handles all the complexities and the user only has to configure a few parameters. The [*Quickstart guide*](https://docs.rastervision.io/en/stable/framework/quickstart.html) is a good entry-point into this. More advanced examples can be found on the [*Examples*](https://docs.rastervision.io/en/stable/framework/examples.html) page.

For **developers** and those looking to dive deeper or combine Raster Vision with their own code, the best starting point is [*Usage Overview*](https://docs.rastervision.io/en/stable/usage/overview.html), followed by [*Basic Concepts*](https://docs.rastervision.io/en/stable/usage/basics.html) and [*Tutorials*](https://docs.rastervision.io/en/stable/usage/tutorials/index.html).


## Contact and Support

You can ask questions and talk to developers (let us know what you're working on!) at:
* [Discussion Forum](https://github.com/azavea/raster-vision/discussions)
* [Mailing List](https://groups.google.com/forum/#!forum/raster-vision)

## Contributing

*For more information, see the [Contribution page](https://docs.rastervision.io/en/stable/CONTRIBUTING.html).*

We are happy to take contributions! It is best to get in touch with the maintainers
about larger features or design changes *before* starting the work,
as it will make the process of accepting changes smoother.

Everyone who contributes code to Raster Vision will be asked to sign the
Azavea CLA, which is based off of the Apache CLA.

1. Download a copy of the [Raster Vision Individual Contributor License
   Agreement](docs/_static/cla/2018_04_17-Raster-Vision-Open-Source-Contributor-Agreement-Individual.pdf)
   or the [Raster Vision Corporate Contributor License
   Agreement](docs/_static/cla/2018_04_18-Raster-Vision-Open-Source-Contributor-Agreement-Corporate.pdf)

2. Print out the CLAs and sign them, or use PDF software that allows placement of a signature image.

3. Send the CLAs to Azavea by one of:
  - Scanning and emailing the document to cla@azavea.com
  - Faxing a copy to +1-215-925-2600.
  - Mailing a hardcopy to:
    Azavea, 990 Spring Garden Street, 5th Floor, Philadelphia, PA 19107 USA

## Licenses

Raster Vision is licensed under the Apache 2 license. See license [here](./LICENSE).

3rd party licenses for all dependecies used by Raster Vision can be found [here](./THIRD_PARTY_LICENSES.txt).
","'classification', 'computer-vision', 'deep-learning', 'geospatial', 'machine-learning', 'object-detection', 'pytorch', 'remote-sensing', 'semantic-segmentation'",2024-05-02T13:49:44Z,30,2000,72,"('lewfish', 1432), ('AdeelH', 1099), ('lossyrob', 417), ('dependabotbot', 71), ('jamesmcclain', 48), ('citerana', 25), ('jpolchlo', 21), ('jmorrison1847', 13), ('lmbak', 10), ('jisantuc', 8), ('nholeman', 4), ('rbreslow', 3), ('tnation14', 3), ('theoway', 2), ('perliedman', 2), ('dustymugs', 2), ('ammarsdc', 1), ('ameier3', 1), ('notthatbreezy', 1), ('mccalluc', 1), ('colekettler', 1), ('echeipesh', 1), ('pomadchin', 1), ('jeromemaleski', 1), ('mbertrand', 1), ('NripeshN', 1), ('giswqs', 1), ('uribo', 1), ('mmcs-work', 1), ('simonkassel', 1)","[9, 'Industry, Innovation and Infrastructure']"
getodk/aggregate,"ODK Aggregate is a Java server that stores, analyzes, and presents survey data collected using ODK Collect. Contribute and make the world a better place! ✨🗄✨","## ⚠️ ODK Aggregate is no longer being updated. Please use [ODK Central](https://github.com/getodk/central) instead. ⚠️

## Overview
![Platform](https://img.shields.io/badge/platform-Java-blue.svg)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build status](https://circleci.com/gh/getodk/aggregate.svg?style=shield&circle-token=:circle-token)](https://circleci.com/gh/getodk/aggregate)
[![Slack](https://img.shields.io/badge/chat-on%20slack-brightgreen)](https://slack.getodk.org)

ODK Aggregate provides a ready-to-deploy server and database to:

- provide blank forms to ODK Collect (or other OpenRosa clients)
- accept submissions (finalized forms) from ODK Collect and manage collected data
- visualize the collected data using maps and simple graphs
- export submissions in CSV, KML, and JSON format
- publish submissions to external systems like Google Spreadsheets

ODK Aggregate can be deployed on an Apache Tomcat server, or any servlet 2.5-compatible (or higher) web container, backed with a PostgreSQL or a MySQL database server.

* ODK website: [https://getodk.org](https://getodk.org)
* ODK Aggregate usage instructions: [https://docs.getodk.org/aggregate-intro/](https://docs.getodk.org/aggregate-intro/)
* ODK forum: [https://forum.getodk.org](https://forum.getodk.org)
* ODK developer Slack chat: [https://slack.getodk.org](https://slack.getodk.org) 

## Getting the code

1. Fork the Aggregate project ([why and how to fork](https://help.github.com/articles/fork-a-repo/))

2. Install [Git LFS](https://git-lfs.github.com/)

3. Clone your fork of the project locally. At the command line:

    `git clone https://github.com/YOUR-GITHUB-USERNAME/aggregate`

## Setting up the database

Aggregate supports a variety of database engines, but we strongly recommend PostgreSQL. If you wish to use MySQL, see the [database configurations](docs/database-configurations.md) guide.

### PostgreSQL with Docker

1. Install [Docker](https://www.docker.com) and [Docker Compose](https://docs.docker.com/compose)

2. Start the development server with `./gradlew postgresqlComposeUp`

    Check that the port number **5432** is not used by any other service in your computer. You can change this editing the `ports` section of the `db/postgresql/docker-compose.yml` configuration file. Be sure to check the documentation: [Compose file version 3 reference - Ports section](https://docs.docker.com/compose/compose-file/#ports).

3. Stop the server with `./gradlew postgresqlComposeDown`

For more information see [here for Docker](docs/build-and-run-a-docker-image.md) and [here for Docker Compose]((docs/build-and-run-with-docker-compose.md).

### Local PostgreSQL server

1. Download and install [PostgreSQL 9](https://www.postgresql.org/download) or later

    - If you are a macOS user, we recommend [Postgres.app](http://postgresapp.com/)
    - If you are a Windows user, we recommend [BigSQL](https://www.openscg.com/bigsql/postgresql/installers.jsp)

2. In a command.line terminal, run the following commands to set up a database for Aggregate:

    (Linux and macOS)
    ```bash
    sudo su - postgres -c ""psql -c \""CREATE ROLE aggregate WITH LOGIN PASSWORD 'aggregate'\""""
    sudo su - postgres -c ""psql -c \""CREATE DATABASE aggregate WITH OWNER aggregate\""""
    sudo su - postgres -c ""psql -c \""GRANT ALL PRIVILEGES ON DATABASE aggregate TO aggregate\""""
    sudo su - postgres -c ""psql -c \""CREATE SCHEMA aggregate\"" aggregate""
    sudo su - postgres -c ""psql -c \""ALTER SCHEMA aggregate OWNER TO aggregate\"" aggregate""
    sudo su - postgres -c ""psql -c \""GRANT ALL PRIVILEGES ON SCHEMA aggregate TO aggregate\"" aggregate""
    ```

    (Windows)
    ```powershell
    psql.exe -c ""CREATE ROLE aggregate WITH LOGIN PASSWORD 'aggregate'""
    psql.exe -c ""CREATE DATABASE aggregate WITH OWNER aggregate""
    psql.exe -c ""GRANT ALL PRIVILEGES ON DATABASE aggregate TO aggregate""
    psql.exe -c ""CREATE SCHEMA aggregate"" aggregate
    psql.exe -c ""ALTER SCHEMA aggregate OWNER TO aggregate"" aggregate
    psql.exe -c ""GRANT ALL PRIVILEGES ON SCHEMA aggregate TO aggregate"" aggregate
    ```

## Building and Running the project

- Copy the `jdbc.properties.example`, `odk-settings.xml.example`, and `security.properties.example` files at `/src/main/resources` to the same location, removing the `.example` extension.

  If you have followed the database configuration steps above, you don't need to make any change in these files. Otherwise, head to the [Aggregate configuration guide](docs/aggregate-config.md) and make the required changes for your environment.

  If you are running the project in Docker, see [here](docs/build-and-run-a-docker-image.md) for the next steps.

- Start a local development Aggregate server with `./gradlew appRunWar`

  Gradle will compile the project and start the server, which can take some time.

  Eventually, you will see a ""Press any key to stop the server"" message. At this point, you can browse http://localhost:8080 to use Aggregate.

- Stop the server pressing any key in the terminal where you started the server

If you have more than one Java version installed in your computer, you can ensure that Java 8 will be used when running Gradle tasks from the command-line by adding `-Porg.gradle.java.home={PATH_TO_JAVA8_HOME}` to the task.

### Connect from an external device

By default, Gretty will launch a server using a `localhost` address which will not be accessible by external devices (e.g., ODK Collect in an emulator, ODK Briefcase on another computer). To set a non-localhost address, edit the following files:

- In `src/main/resources/security.properties`, change `security.server.hostname` to the address
- In `build.gradle`, inside the `gretty` block, change `host` to the same address

## Setting up your development environment

These instructions are for [IntelliJ IDEA Community edition](https://www.jetbrains.com/idea/), which is the (free) Java IDE we use for all the ODK toolsuite, but you don't really need any specific IDE to work with this codebase. Any Java IDE will support any of the steps we will be describing.

### Import

- On the welcome screen, click `Import Project`, navigate to your aggregate folder, and select the `build.gradle` file.

  Make sure you check `Use auto-import` option in the `Import Project from Gradle` dialog.

  Ignore any message about any detected GWT, Spring or web facets.

- Make sure you set Java 8 as the project's selected SDK

### Run

1. Show the Gradle tool window by selecting the menu option at **View** > **Tool Windows** > **Gradle**

    You will see a new panel on the right side with all the Gradle task groups

2. Double click the `appRunWar` Gradle task under the `gretty` task group

    A new `Run` bottom panel will pop up.

    Gradle will compile the project and start the server, which can take some time.

    Eventually, you will see a ""Press any key to stop the server"" message. At this point, you can browse http://localhost:8080 to use Aggregate.

You can stop the server by pressing any key in the `Run` panel.

### Debug

1. In the `Run` menu, select `Edit Configurations...`

2. Press the + button to add a `Remote` configuration

    - Name: `Debug Aggregate` (or whatever you'd like)
    - Host: `localhost`
    - Port: `5005`
    - Search sources using module's classpath: `aggregate`

3. Press `OK`

4. Run Aggregate with the `appRunWarDebug` task (double click it on the Gradle panel at the right side)

5. Run the `Debug Aggregate` run configuration you've created (use the debug button, not the play button, which should be disabled)

Eventually, the compilation will finish and the server will be ready for you to browse [http://localhost:8080](http://localhost:8080)

To stop the debugging session, press any key in the `Run` bottom panel. This will close your debug process in the `Debug` bottom panel as well.

## Extended topics

There is a [`/docs`](https://github.com/getodk/aggregate/tree/master/docs) directory in the repo with more documentation files that expand on certain topics:

- [Configuration files](./docs/aggregate-config.md)
- [Supported database configurations](./docs/database-configurations.md)
- [Build the Installer app](docs/build-the-installer-app.md)
- [Build and run a Docker image](docs/build-and-run-a-docker-image.md)
- [Build and run with Docker Compose](docs/build-and-run-with-docker-compose.md)
- [Build and run a Virtual Machine](docs/build-and-run-a-virtual-machine.md)

## Contributing

Any and all contributions to the project are welcome. ODK Aggregate is used across the world primarily by organizations with a social purpose so you can have real impact!

If you're ready to contribute code, see [the contribution guide](CONTRIBUTING.md).

The best way to help us test is to build from source! We are currently focusing on stabilizing the build process.

## Troubleshooting

* We enabled Git LFS on the Aggregate codebase and reduced the repo size from 700 MB to 34 MB. No code was changed, but if you cloned before December 11th, 2017, you'll need to reclone the project.

* If you get an **Invalid Gradle JDK configuration found** error importing the code, you might not have set the `JAVA_HOME` environment variable. Try [these solutions](https://stackoverflow.com/questions/32654016/).

* If you are having problems with hung Tomcat/Jetty processes, try running the `appStop` Gradle task to stop running all instances.

* If you're using Chrome and are seeing blank pages or refreshing not working, connect to Aggregate with the dev tools window open. Then in the `Network` tab, check `Disable cache`.
",,2021-04-29T16:40:18Z,21,74,27,"('mitchellsundt', 960), ('dylanfprice', 478), ('ggalmazor', 391), ('yanokwa', 124), ('srsudar', 62), ('wbrunette', 49), ('nick2049', 32), ('lognaturel', 23), ('brettneese', 10), ('dcbriccetti', 2), ('jbeorse', 2), ('ronna', 2), ('terencehonles', 2), ('trustbirungi', 2), ('Divya063', 1), ('florianm', 1), ('zwets', 1), ('shashvat-kedia', 1), ('ukanga', 1), ('clarlars', 1), ('krrun16', 1)","[17, 'Partnerships for the Goals']"
mysociety/fixmystreet,This is mySociety's popular map-based reporting platform: easy to install in new countries and regions,"# Welcome to FixMyStreet Platform

FixMyStreet Platform is an open source project to help people run websites for
reporting common street problems such as potholes and broken street lights to
the appropriate authority.

Users locate problems using a combination of address and sticking a pin
in a map without worrying about the correct authority to report it to.
FixMyStreet then works out the correct authority using the problem location and
type and sends a report, by email or using a web service such as Open311.
Reported problems are visible to everyone so they can see if something has
already been reported and leave updates. Users can also subscribe to email or
RSS alerts of problems in their area.

It was created in 2007 by [mySociety](https://www.mysociety.org/) for reporting
problems to UK councils and has been copied around the world. The FixMyStreet
Platform is now at version 5.0; see CHANGELOG.md for a version history.

## Installation

We've been working hard to make the FixMyStreet Platform easy to install and
re-use in other countries - please see our site at 
for help and documentation in installing the FixMyStreet Platform.

For development, if you have Vagrant installed, you can clone the repo and run
'vagrant up'. We use [Scripts to Rule Them All](https://githubengineering.com/scripts-to-rule-them-all/)
so `script/update` will update your checkout, `script/server` will run a dev
server, and `script/test` will run the tests.

## Contribution Guidelines

Whilst many contributions come as part of people setting up their own
installation for their area, we of course welcome stand-alone contributions as
well. The [*Suitable for
Volunteers*](https://github.com/mysociety/fixmystreet/labels/Suitable%20for%20Volunteers)
label in our GitHub issues hopefully labels some potential tasks that might be
suitable for that situation, though please do search through the other issues
to see if what you're after has been suggested or discussed - or feel free to
add your own issue if not.

## Mobile apps

We've extracted all of the mobile apps from this repository into the
[fixmystreet-mobile repository](https://github.com/mysociety/fixmystreet-mobile).

## Acknowledgements

Thanks to [Browserstack](https://www.browserstack.com/) who let us use their
web-based cross-browser testing tools for this project.

## Examples

* 
* 
* 
* 
","'civic-tech', 'civictech', 'councils', 'fixmystreet', 'international', 'map', 'mysociety', 'reporting'",2024-05-03T14:44:34Z,49,493,36,"('dracos', 7197), ('struan', 2047), ('davea', 1175), ('chrismytton', 408), ('crowbot', 322), ('zarino', 320), ('MorayMySoc', 233), ('nephila-nacrea', 162), ('osfameron', 161), ('lucascumsille', 153), ('neprune', 84), ('petterreinholdtsen', 74), ('joesiltberg', 54), ('mhl', 48), ('ludovic-tc', 38), ('pezholio', 37), ('sagepe', 30), ('wrightmartin', 28), ('Jedidiah', 27), ('Kagee', 20), ('sallybracegirdle', 12), ('jonkri', 11), ('rikardfroberg', 11), ('mySocietyClive', 10), ('jacksonj04', 9), ('unhammer', 9), ('gbp', 9), ('andrewblack', 5), ('andylolz', 5), ('annapowellsmith', 5), ('tmtmtmtm', 5), ('tomhukins', 4), ('MyfanwyNixon', 3), ('schlos', 2), ('klrkdekira', 2), ('timsk', 2), ('rbg-joe', 2), ('tobias-brunner', 2), ('mccheung', 1), ('danieldegroot2', 1), ('edent', 1), ('MySocAmelia', 1), ('mhalden', 1), ('lancew', 1), ('halkernel', 1), ('digitalfrost', 1), ('Gemmamysoc', 1), ('andrewperry', 1), ('altinukshini', 1)","[16, 'Peace, Justice and Strong Institutions']"
afinidata2019/afinidata-content-manager,Content manager ,"# Afinidata Content Manager


[![Build Status](https://travis-ci.com/afinidata2019/afinidata-content-manager.svg?branch=master)](https://travis-ci.com/afinidata2019/afinidata-content-manager)
[![Documentation Status](https://readthedocs.org/projects/afinidata-content-manager/badge/?version=latest)](https://afinidata-content-manager.readthedocs.io/en/latest/?badge=latest)
[![codecov](https://codecov.io/gh/afinidata2019/afinidata-content-manager/branch/master/graph/badge.svg)](https://codecov.io/gh/afinidata2019/afinidata-content-manager)


## Overview


The Afinidata Content Manager handles the content for Afinidata. This features Users who create Posts, which can be Reviewed by Special Users.

The Content Manager is built using Python3, Django, and a small set of extra dependencies. The directory layout is quite standard for Django projects. The main apps are: **content_manager**, **messenger_users**, **posts** and **upload**.

You can try Afini on [Afinidata.com](https://afinidata.com) or check out more of our development docs at our [Documentation](https://afinidata-content-manager.readthedocs.io/en/latest/)

## Installation / Running

1. Install Python 3.6 or more as suggested by your OS.
2. Install dependencies, a suggested way is to use virtualenv: ```virtualenv -p python3 venv/; source venv/bin/activate; pip install -r requirements.txt```
3. Run ```manage.py``` and build and execute db migrations. MySQL is used in production, while a stub config exists for running using SQLite. 
4. Ready to go! Use the WSGI app exposed as content_manager; or be lazy and use ```python manage.py runserver```.

## Contributing

The Afinidata Content Manager is a Free Software Product created by Afinidata and available under the AGPL Licence. 

To contribute, read our [Code of Conduct](CODE_OF_CONDUCT.md), our [Docs at Read The Docs](https://afinidata-content-manager.readthedocs.io/en/latest/) and code away.
Create a pull request and contact us in order to merge your suggested changes. We suggest the use of git flow in order to provide a better contributing experience.

",,2023-01-06T00:46:08Z,9,0,3,"('AleejandroReyna', 764), ('EstuarDiaz', 749), ('tian2992', 307), ('gabrielaHerreraPoggio', 126), ('dependabotbot', 2), ('Erlebnistage', 1), ('andreana27', 1), ('HaroldoLopez', 1), ('haroldolopezpruebas', 1)","[4, 'Quality Education']"
openMF/web-app,Mifos X Web App is the revamped version of the Mifos X Community App built on top of the Fineract Platform leveraging the popular Angular framework.,"# Mifos X Web App [![Build Status](https://travis-ci.com/openMF/web-app.svg?branch=master)](https://travis-ci.com/openMF/web-app) [![Gitter](https://badges.gitter.im/openMF/web-app.svg)](https://gitter.im/openMF/web-app?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

Mifos X Web App is the revamped version of the Mifos X Community App, an effective financial inclusion solution and the default web application built on top of the Mifos X platform for the Mifos User Community.

It is a Single-Page App (SPA) written in standard web technologies [HTML5](http://whatwg.org/html), [SCSS](http://sass-lang.com) and [TypeScript](http://www.typescriptlang.org). It leverages the popular [Angular](https://angular.io/) framework and [Angular Material](https://material.angular.io/) for material design components.


## Getting started using

The latest code is continuously deployed at https://openmf.github.io/web-app/ whenever a PR is merged into the master branch.


## Getting started developing

1. Ensure you have the following installed in your system:

    [`git`](https://git-scm.com/downloads)

    [`npm`](https://nodejs.org/en/download/)

2. Install [angular-cli](https://github.com/angular/angular-cli) globally.
```
npm install -g @angular/cli@14.2.12
```

3. Clone the project locally into your system.
```
git clone https://github.com/openMF/web-app.git
```

4. `cd` into project root directory and make sure you are on the master branch.

5. Install the dependencies.
```
npm install
```

6. To preview the app, run the following command and navigate to `http://localhost:4200/`.
```
ng serve
```

The application is using the development server with basic authentication by default. The credentials for the same are:
 
    Username - mifos
    Password - password

**Important Note:** Please do not make any alterations to these credentials.

### Development server

Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The app will automatically reload if you change any of the source files.

### Code scaffolding

Run `ng generate component component-name` to generate a new component. You can also use
`ng generate directive|pipe|service|class|guard|interface|enum|module`.

### Build

Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `--configuration production` flag for a production build.

Run `npm run build:prod` to build a production artifacts Instead.

### Further help

To get more help on the Angular CLI use `ng help` or go check out the
[Angular-CLI README](https://github.com/angular/angular-cli).


## Setting up a local server

Follow the given instructions for your operating system to setup a local server for the Mifos X platform.

[Windows](https://cwiki.apache.org/confluence/display/FINERACT/Fineract-platform+Installation+on+Windows)

[Ubuntu](https://cwiki.apache.org/confluence/display/FINERACT/Fineract+Installation+on+Ubuntu+Server)

For connecting to server running elsewhere update the base API URL and/or tenant identifier property in the `environments/environment.ts` file and `environments/environment.prod.ts` file for development and production use respectively.

By default OAuth2 is disabled. To enable it, change the value of oauth.enabled property to true in the `environments/environment.ts` file and `environments/environment.prod.ts` file for development and production use respectively.

### Docker


To locally build this Docker image from source (after `git clone` this repo), run:
```
docker build -t openmf/web-app:latest .
```
You can then run a Docker Container from the image above like this:
```
docker run -d -p 4200:80 openmf/web-app:latest
```

Access the webapp on http://localhost:4200 in your browser.

### Docker compose
It is possible to do a 'one-touch' installation of Mifos X Web App using containers (AKA ""Docker"").
Fineract now packs the Mifos community-app web UI in it's docker deploy.

As Prerequisites, you must have `docker` and `docker-compose` installed on your machine; see
[Docker Install](https://docs.docker.com/install/) and
[Docker Compose Install](https://docs.docker.com/compose/install/).

Now to run a new MifosX Web App instance you can simply:

1. `git clone https://github.com/openMF/web-app.git ; cd web-app`
1. for windows, use `git clone https://github.com/openMF/web-app.git --config core.autocrlf=input ; cd web-app`
1. `docker-compose up -d`
1. Access the webapp on http://localhost:4200 in your browser.

You can also setup different configurations for the MifosX Web App using environment variables:

1. Use environment variables (best choice if you run with Docker Compose):

Fineract backend settings
```
FINERACT_API_URLS
```
Value to set a Fineract server list (environments) to be used, Default value:
```
https://dev.mifos.io,https://demo.mifos.io,https://qa.mifos.io,https://staging.mifos.io,https://mobile.mifos.io,https://demo.fineract.dev,https://localhost:8443
```

```
FINERACT_API_URL
```
Default value used from the Fineract server list. Default value:
```
https://localhost:8443
```

```
FINERACT_PLATFORM_TENANT_IDENTIFIER
```
Fineract Tenant identifier to be used by default, It must be aligned with the Fineract `tenants` table. Default value:
```
default
```

```
FINERACT_PLATFORM_TENANTS_IDENTIFIER
```
Fineract Tenant identifier list to be used, Those must be aligned with the Fineract `tenants` table. 


Setting for Languages (i18n) still under development
```
MIFOS_DEFAULT_LANGUAGE=en-US
```
```
MIFOS_SUPPORTED_LANGUAGES=cs-CS,de-DE,en-US,es-MX,fr-FR,it-IT,ko-KO,lt-LT,lv-LV,ne-NE,pt-PT,sw-SW
```
These are the Language available now:

```
MIFOS_SESSION_IDLE_TIMEOUT=300000
```
Time in milliseconds for Session idle timeout, default 300000 seconds


|  Language  | Code |    File    |
|:----------:|:----:|:----------:|
| Czech      |  cs  | cs-CS.json |
| German     |  de  | de-DE.json |
| English    |  en  | en-US.json |
| Spanish    |  es  | es-MX.json |
| French     |  fr  | fr-FR.json |
| Italian    |  it  | it-IT.json |
| Korean     |  ko  | ko-KO.json |
| Lithuanian |  li  | li-LI.json |
| Latvian    |  lv  | lv-LV.json |
| Nepali     |  ne  | ne-NE.json |
| Portuguese |  pt  | pt-PT.json |
| Swahili    |  sw  | sw-SW.json |


Setting for applying the Client preload in the Clients view, Default true
```
MIFOS_PRELOAD_CLIENTS=false
```


Setting for exporting report table to CSV file using this field delimiter
```
MIFOS_DEFAULT_CHAR_DELIMITER=,
```


Setting for Wait time in seconds for reading the user notifications, Default 60 seconds
```
MIFOS_WAIT_TIME_FOR_NOTIFICATIONS=60
```

Setting for Wait time in seconds for reading the COB Catch-Up status, Default 30 seconds
```
MIFOS_WAIT_TIME_FOR_CATCHUP=30
```


For more information look the env.sample file in the root directory of the project

## Want to help? [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/openMF/web-app/issues)

Want to file a bug, request a feature, contribute some code, or improve documentation? Excellent! Read up on our guidelines for [contributing](.github/CONTRIBUTING.md) and then check out one of our [issues](https://github.com/openMF/web-app/issues). Make sure you follow the guidelines before sending a contribution!
","'angular-7', 'angular-material-7', 'hacktoberfest', 'mifosx-platform', 'scss', 'typescript'",2024-04-29T17:33:02Z,61,199,38,"('josehernandezfintecheandomx', 101), ('abhaychawla', 100), ('muskankhedia', 97), ('adamsaghy', 95), ('muddlebee', 50), ('BLasan', 36), ('PC-11-00', 34), ('ramvr1256', 29), ('Jov03', 27), ('punwai', 25), ('dependabotbot', 20), ('edcable', 17), ('luckyman20', 14), ('avikganguly01', 13), ('radhathakare', 13), ('gustavao', 12), ('jonathanzhang53', 12), ('galovics', 12), ('vikashsprem', 11), ('alberto-art3ch', 10), ('IOhacker', 10), ('spider0061', 7), ('shrunk3', 6), ('Abhirup-99', 5), ('abhishekkumar08', 5), ('OussEmaDevCode', 5), ('vorburger', 4), ('shobhi1310', 4), ('Onyx2406', 4), ('anika-001', 3), ('jagadeeshakn', 3), ('vidakovic', 3), ('agamvrinos', 3), ('abhi40308', 2), ('meetcric', 2), ('taskain7', 2), ('kunjgit', 2), ('nestorlazcano-fintecheando', 2), ('Omar-Nabil', 2), ('stableapple', 2), ('AshishkrGoyal', 2), ('LalanaChami', 1), ('onyx243', 1), ('rak108', 1), ('shan7030', 1), ('shubhamkorde', 1), ('zizzencs', 1), ('bharathcgowda', 1), ('Verdiane', 1), ('samerdar123', 1), ('sksDonni', 1), ('jonxuxu', 1), ('gagan2005', 1), ('fuch-fcuandonh', 1), ('Ddevendra', 1), ('TripleStarCodeHunter', 1), ('Austin-Mislevy', 1), ('kalsmic', 1), ('goel-aman', 1), ('abhiarrathore', 1), ('AashishChakravarty', 1)","[1, 'No Poverty']"
regardscitoyens/nosdeputes.fr,Repository of NosDéputés.fr : the french parliamentary monitoring website,"# NosDéputés.fr

Ce projet contient le code source des sites web [NosDéputés.fr](http://www.nosdeputes.fr) et [NosSénateurs.fr](https://www.nossenateurs.fr). Pour le télécharger :

 * NosDéputés.fr ``git clone https://github.com/regardscitoyens/nosdeputes.fr.git``
 * NosSénateurs.fr ``git clone https://github.com/regardscitoyens/nosdeputes.fr.git --branch nossenateurs.fr``

## Installation et déploiement

Voir les [instructions d'installation pour une machine GNU/Linux](doc/install.md).

Voir les [instructions d'installation avec Docker via Ansible](ansible/README.md).

## Données parlementaires & API

Toutes les données de NosDéputés.Fr et NosSénateurs.fr (à l'exception de celles concernant les utilisateurs) sont librement réutilisables en OpenData sous [licence ODbL](http://www.vvlibri.org/fr/licence/odbl/10/fr/legalcode), c'est-à-dire sous la condition d'en citer la source et de remettre à disposition des utilisateurs/lecteurs les données modifiées sous les mêmes conditions.

Plus de détails sur l'[accès à ces données et d'autres sur cette page](doc/opendata.md).

Une API a par ailleurs été développée pour offrir un accès direct aux données de NosDéputés.fr et NosSénateurs.fr aux formats XML, JSON et CSV. [Voir la documentation de l'API](doc/api.md).

## Licence

Copyleft 2009-... - [Regards Citoyens](https://RegardsCitoyens.org)

This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License, either version 3 of the License (as published by the Free Software Foundation and available [here](LICENSE), or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.
","'civic-tech', 'democracy', 'open-data', 'parliament', 'parliamentary-data', 'parliamentary-monitoring', 'politics'",2024-04-15T00:50:11Z,18,119,18,"('RouxRC', 2485), ('teymour', 1063), ('bjperson', 287), ('boogheta', 97), ('njoyard', 48), ('jbgabellieri', 45), ('mdamien', 35), ('kerneis', 34), ('DavidGayou', 15), ('yannguegan', 3), ('paulineleon', 3), ('nathanncohen', 3), ('Merinorus', 3), ('fmassot', 2), ('Stephanevg', 2), ('annelhote', 1), ('pmauduit', 1), ('lucgiffon', 1)","[16, 'Peace, Justice and Strong Institutions']"
openoakland/openbudgetoakland,"Visualizations of Oakland's budget data, and explanations about the budget process.","# Open Budget: Oakland

## Contributing

If you're looking for a starter development task to get your feet wet with our codebase, any of our Issues tagged [help wanted](https://github.com/openoakland/openbudgetoakland/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) might be a good fit.

Some of the other Issues are larger and require some deeper design or architectural work; if one of those catches your eye, you'll probably want to talk with us for some more context and background. Either comment on the Issue or — even better — catch up with us at one of [OpenOakland's weekly Hack Nights](https://www.openoakland.org).

## Developing Locally

### Quick Start Guide for Unix-based systems (Mac or Linux)

1. Sign into GitHub and fork this repo
1. Clone the fork onto your machine and navigate to the new folder
1. While still in the root directory of the repo, create a new folder called ""build"". This folder will be ignored by our version control system.
1. Navigate to the \_src/ folder, which is where all development work takes place.
1. Install dependencies with `npm install`
1. Serve the website by entering `npx @11ty/eleventy --serve --port=8011`

Congratulations! Your local copy of Open Budget Oakland's website should now be running at http://localhost:8011. That means you're ready to do the codez if you want to contribute to the codebase of Open Budget Oakland. You will probably want to open a new terminal window, though, to regain access to the command line.

- Please note that after editing a SASS file you should run `npm run build-css` from the \_src/ folder in order to incorporate your changes into the CSS

### Eleventy

This site is built with [Eleventy](https://11ty.dev), a JavaScript-based static site generator that parses Markdown, Pug, and other template languages and runs on Node.js. That means you can reproduce our site locally with minimal setup!

You'll need these installed globally:

- [Node](http://nodejs.org/download/) is a prerequisite for NPM
- [NPM](https://npmjs.com) or [Yarn](https://yarnpkg.com/en/)
- [NVM](https://github.com/nvm-sh/nvm/blob/master/README.md) is optional, but very handy for downloading, updating, and switching between versions of NPM

### Install & Run Eleventy in \_src/

Once you have the NPM package manager installed, you can install Eleventy and the other dependencies listed in **package.json**. Enter the following from the \_src/ folder, where the Eleventy configuration file **.eleventy.js** lives.

```
npm install
```

This command usually runs without a glitch, but if you run into trouble, check your version of node. The latest version of node that we can confirm works with our set-up is **v15.14.0**.

To start eleventy, simply enter the following. (You may choose any network port on your system that is available; 8011 is just a suggestion.)

```
npx @11ty/eleventy --serve --port=8011
```

## Frontend Stack

This project is coded with, among other things:

- [Bootstrap](http://getbootstrap.com/), a CSS framework
- [D3](https://d3js.org), a data visualization library for JavaScript
- [Pug](https://pugjs.org/api/getting-started.html), a JavaScript-friendly HTML templating language
- [React](https://facebook.github.io/react/), a rendering library for JavaScript
- [Sass](https://sass-lang.com/), a CSS preprocessor

## Creating & Editing Pages

- Please note that it is your responsibility to keep your fork of the repo up-to-date with changes made by others working on the project. Doing this diligently should go a long way towards protecting you from scary git merge conflicts.
- All development activity occurs in `_src/`. The root folder is only for compiled output for deployment.
- Page content is inserted into the `content` block. If you are updating data, be sure you understand how it will be consumed.
- In many cases you will simple create or update a `.pug` file, which Eleventy will turn into HTML. If you are making another type of change, you may need to read Pug documentation (which is excellent, by the way!).
- If your page uses custom page-specific css, add it to a new `.scss` partial and import it into the main stylesheet. (Make sure to namespace it the same way the others are.)

### Additional instructions for ""flow"" diagram pages

1. Flow pages are built off a template; copy one of the `*-budget-flow.pug` pages and update the content blocks as necessary.
1. Data files must be placed in the `data/flow` directory. Follow the naming convention seen there or your files won't load properly. You also will need to point your page at the appropriate files as seen in the `get_datafiles` content block.
1. the following columns are required in your datafile and their names should be normalized as seen here. Other columns should be removed to minimize the data download.
   - budget_year
   - department
   - fund_code
   - account_type (this should be the Expense/Revenue column, if there are duplicate names)
   - account_category
   - amount

### Additional instructions for treemap diagram pages

1. Treemap pages are built off a template; copy one of the `*-budget-tree.pug` pages and update the content blocks as necessary.
1. Instructions for generating the necessary data files can be found [here](_treemap/README.md). Add them to the `data/tree/` directory following the naming convention seen in the existing files.
1. Update the `datafiles` content block with the appropriate metadata and file path for the data files you generated.

### Additional instructions for the Compare page

1. The Compare page is a React application. The source files are in `_src/js/compare/` and are are bundled with [Webpack](https://webpack.js.org/).
1. When developing on the Compare page, run `yarn` to install all the necessary node dependencies and `yarn run watch` to watch the source files for changes and rebuild the asset bundles accordingly.
1. The Compare page communicates with a separately maintained API to fetch its data. Documentation for that API can be found [in our wiki](https://github.com/openoakland/openbudgetoakland/wiki/API-Documentation).

## Publishing Changes

Make changes and review them on your local development site. If everyting looks good, push your changes to your personal fork and merge the commit(s) into your master branch. Finally, issue a pull request and we'll take it from there!

### Issuing a pull request

Simply push your code changes to your repo in whatever branch you used locally, then merge into master. At this point you can either 1) push from your master to the **staging** branch of the upstream repo or 2) just tell an admin of the upstream repo that your work is ready for review. (Anyone with admin privileges on the original repo will be able to create a pull request from your repo). Your changes will then be reviewed, tested, and (if everything looks good) pushed into the master branch.

Starting in March 2020, code changes pushed to the master branch of the (original) repo will use GitHub Actions to trigger a continuous integration process that (among other things):

- runs WebPack;
- builds static files with Eleventy; and
- deploys the updated web files to GitHub Pages

## Generating the API

### Background

Oakland budget data are hosted in a special table that lives in the database of a WordPress site. This site exists primarily for the purpose of managing this data, and is not intended for public consumption. Should you need access to the backend of the site, please contact Felicia on Slack.

The API we have built is completely independent of the Open Budget Oakland site, and can be consumed by anyone. Thus far, we have not had to place any limits on traffic to the server, but that may change in the future. To learn how to use the API, please see the documentation in our GitHub wiki.

### Using the plugin to generate the API

The WordPress plugin (OBO Custom Routes) that generates our API can be installed and used on any WordPress site, providing a database table with the expected column names is present. Currently, the plugin is hard-coded to expect a table called `oakland_budget_items`. Obviously, that would be something you'd want to change if you were to use the plugin for another project. Additionally, database queries can easily be altered to fit a different table structure and to create different kinds of endpoints with a bit of PHP skill.

### Developing locally

To develop new features for the API, you may want to run Wordpress locally.
This repo includes a configuration file for doing so with [Docker Compose](https://docs.docker.com/compose/).
With Docker Compose installed, simply run `docker-compose up` in `wordpress plugin for custom API endpoints/`
to activate linked containers for Wordpress, MySQL, and PhpMyAdmin. The Wordpress container will
mount that directory as though it were Wordpress' `plugins/` directory, allowing your edits to
the plugin files in `obo_custom_routes/` to be reflected in your Wordpress instance. (Additional plugins that
are not part of this repository will appear in that directory; they should be ignored by git.)
","'budget', 'civic-hacking', 'civictech', 'civictechindex', 'code-for-america', 'data-visualization', 'openoakland'",2024-01-05T17:03:20Z,21,95,25,"('adstiles', 435), ('macfarlandian', 148), ('nydame', 91), ('ixley', 73), ('ppopp', 20), ('tdooner', 14), ('mikeubell', 9), ('daguar', 6), ('ckingbailey', 2), ('markbrough', 2), ('emilyperi', 2), ('tinyherocarrot', 1), ('asuth', 1), ('bitdeli-chef', 1), ('commahawk', 1), ('mwleeds', 1), ('notpeter', 1), ('saivishy', 1), ('dependabotbot', 1), ('villatrue', 1), ('wrought', 1)","[16, 'Peace, Justice and Strong Institutions']"
apache/climate,Mirror of Apache Open Climate Workbench,"Apache Open Climate Workbench
-----------------------------

|BuildStatus|_
|ImageLink|_
|pypi|_
|pythonbadge|_
|anacondainstaller|_
|anacondadownload|_
|anacondaversion|_

.. image:: ./docs/source/ocw-logo-variant-sm-01-01-new.png
   :width: 20px
   :height: 100px
   :scale: 50%
   :alt: alternate text
   :align: right


`Apache Open Climate Workbench`_ is an effort to develop software that
performs climate model evaluations using model outputs from a variety of
different sources (the Earth System Grid Federation, the Coordinated
Regional Downscaling Experiment, the U.S. National Climate Assessment
and the North American Regional Climate Change Assessment Program) and
temporal/spatial scales with remote sensing data from NASA, NOAA and
other agencies. The toolkit includes capabilities for rebinning, metrics
computation and visualization. For additional project information,
please check the `project website`_.

Getting Started
---------------

The `project’s wiki`_ is the best location for help and project
information. New users should check out the `Getting Started`_ and `Easy
OCW`_ pages for help getting the necessary dependencies installed. If
you would prefer to have an isolated environment set up in a virtual
machine you should read the `OCW VM`_ documentation. It will help you
get up and running quickly with a fresh VM image for OCW work.

There are a number of examples in the *examples* directory to help users
get started with the toolkit API. If you have questions, the best way to
get help is to email the project mailing lists which can be found on the
`project's community page`_


Development
---------------

OCW always welcomes pull request. Please check the `Developer Area`_ on the wiki for additional information on how to contribute. The `project's JIRA`_ is a great place to start looking for issues to solve. Make sure to stop by the mailing lists and introduce yourself as well!

Documentation
---------------

The project host the documentation built from the last release artifact on `the project website`_ 

If you would like to build the documentation for the project run the following command from the root of the repository:
::
         cd docs && make html


You will need to have installed the project dependencies first. Checkout the `Getting Started`_ and `Easy OCW`_ pages for help getting the necessary dependencies installed.


.. |ImageLink| image:: https://coveralls.io/repos/github/apache/climate/badge.svg?branch=master
.. _ImageLink: https://coveralls.io/github/apache/climate?branch=master

.. |BuildStatus| image:: https://api.travis-ci.org/apache/climate.svg?branch=master
.. _BuildStatus:  https://travis-ci.org/apache/climate

.. |pypi| image:: https://img.shields.io/pypi/v/ocw.svg?maxAge=2592000?style=plastic
.. _pypi:  https://pypi.python.org/pypi/ocw

.. |pythonbadge| image:: https://img.shields.io/badge/python-3-blue.svg
.. _pythonbadge: https://www.python.org/downloads/

.. |anacondainstaller| image:: https://anaconda.org/conda-forge/ocw/badges/installer/conda.svg
.. _anacondainstaller: https://anaconda.org/conda-forge/ocw

.. |anacondadownload| image:: https://anaconda.org/conda-forge/ocw/badges/downloads.svg
.. _anacondadownload: https://anaconda.org/conda-forge/ocw

.. |anacondaversion| image:: https://anaconda.org/conda-forge/ocw/badges/version.svg
.. _anacondaversion: https://anaconda.org/conda-forge/ocw


.. _Apache Open Climate Workbench: http://climate.apache.org
.. _project website: http://climate.apache.org/
.. _project’s wiki: https://cwiki.apache.org/confluence/display/CLIMATE/Home
.. _Getting Started: https://cwiki.apache.org/confluence/display/CLIMATE/Getting+Started
.. _Easy OCW: https://cwiki.apache.org/confluence/display/CLIMATE/Easy-OCW+-+A+Guide+to+Simplifying+OCW+Installation
.. _OCW VM: https://cwiki.apache.org/confluence/display/CLIMATE/OCW+VM+-+A+Self+Contained+OCW+Environment
.. _project's community page: http://climate.apache.org/community/mailing-li
.. _Developer Area: https://cwiki.apache.org/confluence/display/CLIMATE/Developer+Area
.. _project's JIRA: https://issues.apache.org/jira/browse/CLIMATE
.. _the project website: https://climate.apache.org/api/current/index.html
.. _Getting Started: https://cwiki.apache.org/confluence/display/CLIMATE/Getting+Started
.. _Easy OCW: https://cwiki.apache.org/confluence/display/CLIMATE/Easy-OCW+-+A+Guide+to+Simplifying+OCW+Installation
",'climate',2020-10-05T07:46:17Z,24,137,22,"('MJJoyce', 1008), ('lewismc', 65), ('agoodm', 64), ('jarifibrahim', 62), ('andrewfhart', 52), ('MichaelArthurAnderson', 33), ('Omkar20895', 30), ('kwhitehall', 20), ('rlaidlaw', 14), ('huikyole', 9), ('chrismattmann', 7), ('prateekiiest', 7), ('veduardo', 4), ('LucaCinquini', 4), ('smarru', 3), ('markschnitzer', 2), ('MBoustani', 2), ('WIZARDELF', 2), ('dmitri1357', 2), ('BrianWilson1', 1), ('justinlulejian', 1), ('mhajder', 1), ('Burrch3s', 1), ('wirelesswithbrain', 1)","[13, 'Climate Action']"
VRMakerTeam/VRMaker,,"# VRMaker
- The purpose of this project is to create your own vr content. You can import your own vr scenes in a few simple steps, or use the built-in assets to place your own design scene objects.  VRMaker supports the camera control, scene effects, music effects, and even in tandem between multiple scenes to form a complete vr navigation program.

- VRMaker enables children to think, design, and create content for virtual reality devices by visual programming in a real-time rendered  3D environment. Built with 3D art assets and 360-photos, children can express themselves with pictures and sound in a virtual world with links to create virtual reality stories where you can move and explore your surroundings. VRMaker makes the content creation process very simple, and the children can focus on creative storytelling.

- VRMaker includes a PC software called VRMaker, and a mobile application for Android-based phones called VRMaker Player. VRMaker is the creation tool, in which the 3D art assets,  360 photographs and other media such as music, sound, and text can be combined with simple programming using functional blocks, generating a virtual reality content. The completed content can then be viewed in the VRMaker Player on an Android-based phone, or the content can be uploaded to a private or public cloud, using our server-side publishing software, for easy access on other devices. The variety of digital assets that can be used and the block programming makes it very easy for the children to create everything from stories, education content to games.

## Features
- preview scene.
- import own assets.
- multi track for scene editor.
- deploy final result to cardboard.
- support 8 languages.

## Platforms
- Compatible with Windows Desktop, VPlayer compatible with Android 

## Dependencies
- [QT 5.9](https://download.qt.io/official_releases/qt/5.9/5.9.0/)
- [Unity 5.6.6](https://unity3d.com/get-unity/download/archive)

## Installing
```
- git clone https://github.com/VRMakerTeam/VRMaker
```

## Getting Started
- For detailed information on how to get started with this plugin visit the [Wiki](https://github.com/VRMakerTeam/VRMaker/wiki).

## Deployment
- Download release [install file](https://github.com/VRMakerTeam/VRMaker/releases/download/v1.0/VRMaker-UNICEF-v1.0.7z).
- ADB install -r vplayer.apk into android mobile phone.
- Start VRMaker and start edit your own project.

## Contributing
We welcome contributions to VRMaker!
- 📥 Pull requests and 🌟 Stars are always welcome.
- Read our [contributing guide](CONTRIBUTING.md) to get started, we're will take the time to guide you.

## Contributors
- [chimerakang](https://github.com/chimerakang)
- [crenovator](https://github.com/crenovator)
- [cyrilselasi](https://github.com/cyrilselasi)
- [easlee](https://github.com/easlee)

## Change Log
```
- 1.6 add multi language support
- 1.5 fix gaze bugs
- 1.4 add resources tags
- 1.3 change unity streaming assets path
- 1.2 support dynamic load scripts and assets from StreamingAssets
- 1.1 add model assets picker
- 1.0 New UI 
```

",,2019-08-06T07:32:36Z,4,4,4,"('chimerakang', 24), ('cyrilselasi', 6), ('crenovator', 1), ('simonkhc', 1)","[4, 'Quality Education']"
sugarlabs/turtleart-activity,A block-based Logo programming environment,"TurtleBlocks
============

Turtle Art, also known as Turtle Blocks, is an activity with a
Logo-inspired graphical ""turtle"" that draws colorful art based on
snap-together visual programming elements. Its ""low floor"" provides an
easy entry point for beginners. It also has ""high ceiling""
programming, graphics, mathematics, and Computer Science features
which will challenge the more adventurous student.

.. note ::

   There are two inter-compatible programs: Turtle Art and Turtle Blocks. Turtle Art, which closely parallels the Java version of Turtle Art maintained by Brian Silverman, offers a small subset of the functionality of Turtle Blocks. Turtle Blocks is the version included in the Sugar distribution. Sugar users probably want to use Turtle Blocks rather than Turtle Art. (Also see Turtle Confusion, a collection of programming challenges designed by Barry Newell.)

Using Turtle Art
----------------

Start by clicking on (or dragging) blocks from the Turtle palette. Use
multiple blocks to create drawings; as the turtle moves under your
control, colorful lines are drawn.

You add blocks to your program by clicking on or dragging them from
the palette to the main area. You can delete a block by dragging it
back onto the palette. Click anywhere on a ""stack"" of blocks to start
executing that stack or by clicking in the Rabbit (fast) or Turtle
(slow) on the Main Toolbar.

Dependencies
------------

* Python 3,
* PyGObject,
* GLib,
* GTK 3,
* Pango,
* Pango Cairo,
* Telepathy GLib,
* Sugar Toolkit,
* GStreamer,
* Csound,
* WebKit,

Copied Dependencies
-------------------

* CollabWrapper,
* sugariconify,

",,2024-04-28T20:50:35Z,32,17,9,"('walterbender', 2251), ('pootle-sugarlabs', 557), ('AlanJAS', 55), ('quozl', 52), ('outofthecave', 31), ('kiy4h', 13), ('cosimoc', 11), ('dsd', 11), ('Saumya-Mishra9129', 11), ('srevinsaju', 10), ('cscott', 9), ('i5o', 8), ('chimosky', 6), ('lfaraone', 4), ('vipulgupta2048', 3), ('icarito', 3), ('kipply', 3), ('picobrian', 2), ('rafaelcor', 2), ('rgs1', 2), ('rbuj', 2), ('aguirrea', 2), ('zeeshank95', 1), ('korakurider', 1), ('rhl-bthr', 1), ('mslg', 1), ('starnight', 1), ('Dimi20cen', 1), ('cynthiasolomon', 1), ('Clytie', 1), ('leonardcj', 1), ('Ark74', 1)","[4, 'Quality Education']"
Envirometrix/LandGISmaps,Processing of global environmental layers at various resolutions,"OpenLandMap — Open Land Data service
====================================




Migrated to: https://gitlab.com/openlandmap
",,2021-02-01T21:55:10Z,3,93,20,"('thengl', 58), ('jkrizan', 4), ('Nowosad', 1)","[15, 'Life On Land']"
HospitalRun/hospitalrun-server,Backend for HospitalRun,"# HospitalRun Server



![Status](https://img.shields.io/badge/Status-developing-brightgree) [![Release](https://img.shields.io/github/release/HospitalRun/hospitalrun-server.svg)](https://github.com/HospitalRun/hospitalrun-server/releases) [![Version](https://img.shields.io/github/package-json/v/hospitalrun/hospitalrun-server)](https://github.com/HospitalRun/hospitalrun-server/releases) [![GitHub CI](https://github.com/HospitalRun/server/workflows/GitHub%20CI/badge.svg)](https://github.com/HospitalRun/server/actions) [![Coverage Status](https://coveralls.io/repos/github/HospitalRun/hospitalrun-server/badge.svg?branch=master)](https://coveralls.io/github/HospitalRun/hospitalrun-server?branch=master) [![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/HospitalRun/hospitalrun-server.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/HospitalRun/hospitalrun-server/context:javascript) ![Code scanning](https://github.com/HospitalRun/hospitalrun-server/workflows/Code%20scanning/badge.svg?branch=master) [![Documentation Status](https://readthedocs.org/projects/hospitalrun-server/badge/?version=latest)](https://hospitalrun-server.readthedocs.io)
 [![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-server.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-server?ref=badge_shield) [![Commitizen friendly](https://img.shields.io/badge/commitizen-friendly-brightgreen.svg)](http://commitizen.github.io/cz-cli/) ![dependabot](https://api.dependabot.com/badges/status?host=github&repo=HospitalRun/hospitalrun-server) [![Slack](https://hospitalrun-slack.herokuapp.com/badge.svg)](https://hospitalrun-slack.herokuapp.com)



Node.js backend for [HospitalRun](http://hospitalrun.io/): free software for developing world hospitals. To contribute, follow the guidelines in this readme or alternatively ask for details on the community Slack channel: [#contributors](https://hospitalrun-slack.herokuapp.com).

---

**This repository is for the HospitalRun v2 and it is currently under heavy development. If you are searching for the no longer supported version 1.0.0-beta, you can find it [here](https://github.com/HospitalRun/hospitalrun-server/tree/1.0.0-beta).**

# Contributing

Contributions are always welcome. Before contributing please read our [contributor guide](https://github.com/HospitalRun/hospitalrun-server/blob/master/.github/CONTRIBUTING.md). Then follow these steps:

1. [Fork](https://github.com/HospitalRun/hospitalrun-server/fork) this repository to your own GitHub account
2. Clone it to your local machine
3. Navigate to the cloned folder: `cd hospitalrun-server`
4. Install the dependencies: `npm install`
5. Check that [env variables](https://github.com/HospitalRun/hospitalrun-server#environment) are set correctly 
6. Run `npm run dev` to build and watch for code changes:
   - a development database will start on http://localhost:5984
   - you can access its Admin interface on http://localhost:5984/_utils, `username: dev` and `password: dev`

## Working on Issues
In order to optimize the workflow and to prevent multiple contributors working on the same issue without interactions, a contributor must ask to be assigned to an issue by one of the core team members: it's enough to ask it inside the specific issue.

## Environment
In order to run `hospitalrun-server`  you need to set the correct environment variables. Since [dotenv](https://www.npmjs.com/package/dotenv) is already included, it is just matter of renaming `.env.example` file to `.env`: this file include all of the mandatory defaults.

## Development Database
This project uses [pouchdb-server](https://www.npmjs.com/package/pouchdb-server) for development and you, as contributor, don't need to provide your own CouchDB instance. Upon first run of the `dev` script (`npm run dev`), a new `data` folder will be created inside the `./db` folder. The database credentials are: `username: dev` and `password: dev`. The file `./db/config.json` contains the DB's configuration: you can change it if you want, but please don't commit any changes to it.

**Note: PouchDB-server is meant to be use only during development. Please don't deploy any production/testing HospitalRun instances on it. For production deployments please follow the deployment guide.**

## Tests
Every code additions or fixs on the existing code, has to be tested. This project uses [node-tap](https://node-tap.org/) as test runner. To run all tests use `npm run test`.

## How to commit

This repo uses [Conventional Commits](https://www.conventionalcommits.org/). [Commitizen](https://github.com/commitizen/cz-cli) is mandatory for making proper commits. Once you have staged your changes, can run `npm run commit` from the root directory in order to commit following our standards.

# Documentation
## DataBase
Read more at HospitalRun DataBase.

## Plugins
Read more at HospitalRun Plugins.

## Services
Read more at HospitalRun Services.

# License

Released under the [MIT license](LICENSE).
","'backend', 'fastify', 'hospitalrun', 'hospitalrun-server', 'nodejs'",2023-01-09T09:53:11Z,32,864,63,"('matteovivona', 330), ('dependabot-previewbot', 243), ('dependabotbot', 127), ('jkleinsc', 100), ('fox1t', 63), ('tangollama', 17), ('greenkeeperbot', 14), ('MatthewDorner', 13), ('hpierce1102', 6), ('stukalin', 5), ('jackcmeyer', 5), ('snyk-bot', 4), ('semantic-release-bot', 3), ('mofesola', 2), ('thorsteinsson', 2), ('schneems', 1), ('fossabot', 1), ('ariscris', 1), ('toshihidetagami', 1), ('serg123e', 1), ('nclBaz', 1), ('rwanyoike', 1), ('pgte', 1), ('drakemedic', 1), ('PhearZero', 1), ('turboMaCk', 1), ('KenyanGeek', 1), ('professorhaseeb', 1), ('fgiorlando', 1), ('donaldwasserman', 1), ('sericaia', 1), ('connorlurring', 1)","[3, 'Good Health and Well-Being']"
unep-grid/mapx,MapX ,"# [MapX](https://app.mapx.org/)

[MapX](https://app.mapx.org/) is an online platform for managing geospatial data on natural resources, developed by [UNEP/GRID-Geneva](https://unepgrid.ch/en) - a data centre resulting from the partnership between
[UN Environment Programme](https://www.unep.org/), the Swiss [Federal Office for the Environment](https://www.bafu.admin.ch/) and the [University of Geneva](https://unige.ch/).

Field applications of MapX are varied and include chemical management, disaster risk reduction, biodiversity planning, land-use planning, extractive industry, renewable energy and environmental security. 

MapX targets a wide community of users that are primarily UN Environment Programme and partners, the Secretariats of Multilateral Environmental Agreements (MEAs) and other UN agencies mandated to collect and use geospatial data in environmental decision-making. Civil society groups, non-governmental organizations, academia and citizens complement this set of users. 

MapX was designed in 2014 and since then continuously improved with wide international stakeholder consultations. 

MapX is fully integrated into the World Environment Situation Room, which is the UNEP data and knowledge platform.

![mapx preview](app/src/png/mapx-preview.png ""MapX"")


## Development 

Development servers are launched from within Docker containers, to match as closely as possible the environment found in production. Some commands, tools and config are still currently needed on your local computer.

### Requirement

__Mendatory__ 

- `docker` v20.10+

__Optional__  

- `node` v16.0+
- `g++`
- `npm`
- `yq`
- `git`


### Hosts

Some browsers require to modify your hosts file to link custom MapX local ""subdomains"". It could be as simple as adding those lines to `/etc/hosts/` and restarting your browser, if needed: 

```sh
127.0.0.1 app.mapx.localhost 
127.0.0.1 api.mapx.localhost
127.0.0.1 search.mapx.localhost
127.0.0.1 wsecho.mapx.localhost
127.0.0.1 probe.mapx.localhost
127.0.0.1 apidev.mapx.localhost
127.0.0.1 dev.mapx.localhost
127.0.0.1 geoserver.mapx.localhost
```

### Docker

The included `docker-compose.yml` allows to setup a development environment.

Trigger the following script which init some required directories and copy the default environment variable to `./mapx.dev.env` (if missing):

```sh
./mapx.dev.init.sh
```

Finally, launch the mapx stack:

```sh
# Pull the latest builds
docker compose pull

# Launch postgres first : in case of first launch, some tables and roles must be created
docker compose up pg

# Launch other services
docker compose up
```

The application should be available at  (curl -H Host:app.mapx.localhost 

An admin user is available as `admin@localhost` which can be used to login; get the password by browsing the web mail at 


### Docker : build for prod  

```sh
./build.sh -v 
```

### Docker : re-build individual docker images for local dev 

*app*

```sh
# Build app js code, update docker image
cd /app/ 
npm run docker 
```

*api/express*

```sh
# Build api js code, update docker image
cd /api/
npm run docker 
```

*geoserver*

```sh
# Update geoserver docker image 
cd /geoserver/
./build -a
```

*meili search*

```sh
# Update meili docker image 
cd /meili/
./build -a
```

#### Known issues

Postgis: `OperationalError: could not access file ""$libdir/postgis-X.X` _Solution:_ run `docker compose exec pg update-postgis.sh`


### Development session for the `app` service

Install all modules listed as dependencies in `package.json` for the `app` service, the `sdk` and the websocket handler `ws_handler` :

```sh
cd ./app
npm install

cd ./app/src/js/sdk/
npm install

cd ./app/src/js/ws_handler/
npm install
```

Optionally, if you want to develop submodules as `el`, `mx_valid` or rebuilding `sprites`: 

```sh 
cd ./app/src/js/el/
npm install

cd ./app/src/js/is_test/
npm install

# Note: could requires specific version of node
cd ./app/sprites/
npm install 
```

Start a development session for the `app` service:

- Automatically build all client side dependencies, included dictionaries and translation ( which needs some config, see below )

```sh
$ cd ./app
$ npm run dev
```

- Launch the server from within the running `app_dev` container. In another terminal window, launch the dev server :

```sh
docker compose exec app_dev R
> source('run.R') 

# OR, as a single line for a non-interactive session:
docker compose exec app_dev Rscript --vanilla run.R
```

Then, an instance of mapx should be available at  for which the source code from `./app/` is mounted as `/app/` in the container.

__Note for auto-translation__:
Automatic translation requires a valid Google cloud config file, which path should be refered in the host – not inside the docker container – as an environment variable named `GOOGLE_APPLICATION_CREDENTIALS`, accessible from your local node. You can test this with :

```sh
$ node -e 'console.log(process.env.GOOGLE_APPLICATION_CREDENTIALS)'
# Should return, as an example :
# > /home//.google_cloud_.json
```

### Development session for the `api` service

Setup the environmental variables for the `api` service in `mapx.dev.env` as follows:

```sh
API_HOST=api
API_PORT=3030
API_PORT_DEV=3333
API_PORT_PUBLIC=8880
API_HOST_PUBLIC=api.mapx.localhost
API_HOST_PUBLIC_DEV=apidev.mapx.localhost
```

Start the `Express.js` development server:

```sh
$ docker compose up -d
$ docker compose exec api_dev node inspect index.js port=3333
debug> c
```

The instance now should use the api service at  for which the source from `./api/` is mounted as `/api/` in the container.

If you want to use the prod version of the `api_dev` service, setup the environmental variables in `mapx.dev.env`as follows:

```sh
API_HOST=api
API_PORT=3030
API_PORT_DEV=3030
API_PORT_PUBLIC=8880
API_HOST_PUBLIC=api.mapx.localhost
API_HOST_PUBLIC_DEV=api.mapx.localhost
```

### `app` end-to-end tests 

Mapx use a custom end-to-end testing tool, which features the mapx's `sdk`. The testing coverage is partial, but should cover the largest part of all MapX features, while also tesing the `sdk`, as a all tests are written using common `sdk` async methods.  


```sh
cd app/src/js/sdk
npn run tests 
```

#### `api` tests

Run tests within the development container:

```sh
docker compose exec api sh
npm run test
```

### Development session for the `routines` service

```sh
docker compose exec routines node inspect routines.js
debug> c
```

# Countries boundaries layer

A sample dataset of countries boundaries (  polygons ) is included in this code repo, and will add a table named `mx_countries` in the database. The main purpose of this layer is croping dataset during exportation. 

## Citation

Administrative boundaries generalized by UNEP/GRID-Geneva (2019) based on the Global Administrative Unit Layers (GAUL) dataset (G2015_2014), implemented by FAO within the CountrySTAT and Agricultural Market Information System (AMIS) projects (2015).

## Generalization >= mapx 1.8.26 

Starting with version 1.8.26, a lightweight version of `mx_countries` is included.

It was generated with [mapshaper](https://mapshaper.org/) using the following parameters:

- import:
  - detect line intersections = true
  - snap vertices = true
- simplification:
  - prevent shape removal = true
  - method: Visvalingam / weighted area
  - percentage of removable points to retain: 3%

Once the simplification was done, the data was repaired in `mapshaper` and then in `QGIS 3.18` using the `Fix geometries` tool. All geometries are valid according to GEOS rules.


## Generalization < mapx 1.8.26 

The generalization was made using Feature Manipulation Engine (FME) with the following settings:

- Algorithm: Douglas (generalize)
- Generalization Tolerance: 0.02
- Preserve Shared Boundaries: Yes
- Shared Boundaries Tolerance: None
- Preserve Path Segment: No

Geometries obtained from FME have been repaired in PostGIS using ST_MakeValid() function.


## Restrictions

You are free to modify and/or adapt any Data provided for your own use, reproduction as well as unlimited use within your organization. The Data is licensed and distributed by UNEP/GRID-Geneva. Redistribution to a Third party or reseller is formerly prohibited at any stage whatsoever by UNEP/GRID-Geneva.

## Disclaimer

Due to the generalization process, the administrative boundaries of the countries have been modified. Therefore, this dataset can only be used for unofficial cartographic purposes for global mapping using a scale not higher than 1:25 million. It should not be used in any way as a reference for national boundaries. Territorial information from this dataset do not imply the expression of any opinion whatsoever on the part of the UNEP/GRID-Geneva concerning the legal status of any country, territory, city or area, or of its authorities, or concerning the delimitation of its frontiers or boundaries. The Data is being delivered to you “AS IS” and UNEP/GRID-Geneva makes no warranty as to its use or performance.

UNEP/GRID-Geneva cannot be held responsible for a misuse of this file and its consequences.

# PostgreSQL passwords update

Procedure to follow if PostgreSQL passwords need to be updated for security reason (or any other reasons).

1. Launch MapX stack with Docker Compose:

    ```sh
    docker compose up
    ```

2. Once your stack is up, update PostgreSQL passwords in the environment file:

    - `POSTGRES_PASSWORD`
    - `POSTGRES_USER_WRITE_PASSWORD`
    - `POSTGRES_USER_READ_PASSWORD`
    - `POSTGRES_USER_CUSTOM_PASSWORD`

3. Connect to PostgreSQL using psql:

    ```sh
    docker compose exec pg psql -U {POSTGRES_USER}
    ```

4. Queries to run in psql to update the passwords. Be careful to respect the order in which the queries are run.

    ```sql
    ALTER ROLE {POSTGRES_USER_READ} WITH PASSWORD '{POSTGRES_USER_READ_PASSWORD}';
    ALTER ROLE {POSTGRES_USER_WRITE} WITH PASSWORD '{POSTGRES_USER_WRITE_PASSWORD}';
    ALTER ROLE {POSTGRES_USER_CUSTOM} WITH PASSWORD '{POSTGRES_USER_CUSTOM_PASSWORD}';
    ALTER ROLE {POSTGRES_USER} WITH PASSWORD '{POSTGRES_PASSWORD}';
    \q
    ```

5. Force Compose to stop and recreate all containers to avoid any problems related to passwords update:

    ```sh
    docker compose up -d --force-recreate
    ```



&copy; 2014-present unepgrid.ch
","'dashboard', 'docker', 'geoserver', 'mapbox-gl-js', 'maps', 'nodejs', 'postgres', 'r', 'shiny-apps', 'spatial-data', 'storymaps', 'vector', 'webpack', 'wms'",2024-05-03T10:15:55Z,12,82,11,"('fxi', 3161), ('antobenve', 50), ('trepmag', 32), ('thomaspiller', 32), ('MarionPlanque', 11), ('giswt', 11), ('alejandraarango', 4), ('AlbertMase', 3), ('GianlucaGygax', 3), ('0xbase12', 1), ('PierreLacroix', 1), ('dependabotbot', 1)","[13, 'Climate Action']"
5calls/ios,iOS app for 5calls.org,"# 5Calls iOS App

This is the repository for the iOS app for [5Calls.org](https://5calls.org).

[![Build Status](https://app.bitrise.io/app/d786d837d94f6410/status.svg?token=BTL78uVY_9iE4XCx-iTekQ&branch=main)](https://app.bitrise.io/app/d786d837d94f6410)

## Requirements

- Xcode 13
- iOS 12

## Getting Started

Install the dependencies:

```
bundle install
```

## Using R.swift

R.swift removes the need to use ""stringly typed"" resources. Instead, you can reference your app's resources Android-style, which is strongly typed. Benefits are less casting, compile time checking for resources, and a little less code. [See examples for each type here.](https://github.com/mac-cain13/R.swift/blob/master/Documentation/Examples.md)

**Note**: Since 5Calls uses prototype cells instead of cell nibs, this is all you need to dequeue a cell:

```
let cell = tableView.dequeueReusableCell(withIdentifier: R.reuseIdentifier.setLocationCell, for: indexPath)!
```

Vendor the R.swift binary from the latest release (https://github.com/mac-cain13/R.swift/releases) into `vendor/rswift` if you're getting started with this project for the first time.

## Testflight Builds

> _This currently has to be done by Ben_

Install the dependencies:

```
bundle install
```

Make sure you have a `.env` file with the following keys defined:

- `APPLE_ID`
- `TEAM_ID`
- `ITUNES_CONNECT_TEAM_ID`
- `FASTLANE_APPLE_APP_SPECIFIC_PASSWORD`

Update the build number manually (for now).

Then run:

```
fastlane beta
```

## License

This project is released open source under the MIT License. See [LICENSE](https://raw.githubusercontent.com/5calls/ios/master/LICENSE) for more details.

## Contributors

See the complete list of contributors here: https://github.com/5calls/ios/graphs/contributors
",,2024-02-26T02:59:09Z,27,145,16,"('subdigital', 229), ('nickoneill', 147), ('mccarron', 81), ('bengottlieb', 52), ('Abizern', 51), ('cselin', 29), ('heathermh', 25), ('tomburns', 20), ('chrisbrandow', 18), ('designatednerd', 14), ('melville', 13), ('iainsmith', 10), ('ianyh', 8), ('musicisamiracle', 6), ('nikrad', 5), ('bjtitus', 5), ('chrisgoddard', 3), ('iosjillian', 3), ('indraShan', 3), ('adamwulf', 2), ('erikkrietsch', 2), ('mergesort', 2), ('keithgee', 2), ('dependabotbot', 2), ('alexargo', 1), ('alexanderjrobinson', 1), ('KaiqueDamato', 1)","[16, 'Peace, Justice and Strong Institutions']"
OpenConceptLab/oclapi,Deprecated. Please see https://github.com/OpenConceptLab/oclapi2,"# This project has been deprecated. Please see https://github.com/OpenConceptLab/oclapi2 for the latest version.

OCL API
======

## Contributing

We welcome all pull requests. Before starting any work, please check https://github.com/OpenConceptLab/ocl_issues/issues if the change you want to work on has been already reported there. If it's not there, create a new issue so that it can be discussed. We should triage the issue and get back to you in a few days.

All pull requests should contain a single commit (unless you have a very good reason to include multiple commits). A commit message should be in the following format: `OpenConceptLab/ocl_issues#id Short title`, where `id` is the issue number e.g. 170. Please always rebase your commit on the master branch and make sure all tests pass, before creating a pull request.

## What you'll need:
* git
* docker-compose

Source for the Open Concept Lab APIs

## Docker Environment Setup (preferred)

Fork the repo on github and clone your fork:
````sh
git clone https://github.com/{youruser}/oclapi
````

Add a remote repo to upstream in order to be able to fetch updates:
````sh
git remote add upstream https://github.com/OpenConceptLab/oclapi
````

Go to:
````sh
cd oclapi
````

Build containers explicitly (only the first time to go around oclapi:dev not found):
````sh
docker-compose build
````

Fire up containers:
````sh
docker-compose up
````

You can access the API at http://localhost:8000

The root password and the API token can be found in docker-compose.yml under api/environment.

### Docker Environment Settings

Docker `.env` file should be located under the root project folder. On development environment you don't need this file.

#### .env file details

`ENVIRONMENT=` Python module for environment, e.g. production, staging, local, qa

`AWS_ACCESS_KEY_ID=` Amazon Web Services access key.

`AWS_SECRET_ACCESS_KEY=` Amazon Web Services secret key.

`AWS_STORAGE_BUCKET_NAME=` Amazon Web Services bucket name.

`ROOT_PASSWORD=` API root user password.

`OCL_API_TOKEN=` API root token.

`SECRET_KEY=` DJANGO secret key.

`EMAIL_HOST_PASSWORD=` no-reply@openconceptlab.org password.

`SENTRY_DSN=` Sentry unique URL for the given environment.

`IMPORT_DEMO_DATA=` Set to 'true' to import ~2k concepts from the CIEL demo data.

`FLOWER_USER=` Flower user (Default value - floweruser).

`FLOWER_PWD=` Flower password (Default value - Flower123).

### Running commands in a container

You can run any command in a running container. Open up a new terminal and run for example:
````sh
docker-compose exec api python manage.py syncdb
````

### Running tests in a container
You can run tests in a container as any other command.

#### Unit Tests

````sh
docker-compose run --rm api python manage.py run_test --configuration=Dev
````

#### Integration Tests

See integration-tests/README.md

To run locally:
````sh
docker-comopse up -d
docker build integration-tests/. --network=""host"" --build-arg CACHEBUST=$(date +%s)
````

Deprecated integration tests can be run with:
````sh
docker-compose run --rm api python manage.py test integration_tests --configuration=Dev
````

#### Rebuilding SOLR index

If the SOLR index gets out of sync, you can run the following command:
````sh
docker-compose run --rm -d api python manage.py rebuild_index --batch-size 100 --workers 4 --verbosity 2 --noinput
````
It's asynchronous. To follow logs run:
````sh
docker logs -f oclapistg_api_run_1
````
, where oclapistg_api_run_1 is the container id returned by the `run` command.

### Backups

By default backups are taken every night at midnight. You can trigger a manual backup by running:
````sh
docker-compose run --rm backup bash backup.sh
````
Backups are saved in a backup directory configured via the BACKUP_DIR env property (./backups by default).
You can restore a particular backup by running:
````sh
docker-compose run --rm backup bash restore.sh 2017-09-27_00-00-01
````
### Connecting to mongo in container

````sh
docker-compose exec mongo mongo
````

### Debugging in container

To setup debugging PyCharm Professional Edition is required.

Docker-compose up starts the server in a development mode by default. It exposes all services on the host machine as well as enables SSH to the API service.

In Pycharm IDE open oclapi project and go to `Settings-> Project: oclapi -> Project Interpreter`

Click on gear icon and choose `Add Remote` option

Configure interpreter with SSH credentials as in the image (default password is `Root123`):

![alt](img/remote_interpreter_config.png)

There will be warnings about unknown host etc. but don't don't worry, just confirm.

Setup django debug configuration as in the image (Path mapping should be `absolute path to project directory=/code`):

![alt](img/docker_debug_config.png)

Run your configuration! Debugging server will run on [http://0.0.0.0:8001/](http://0.0.0.0:8001/)

In case of any problems with `.pycharm_helpers` just delete remote interpreter and create new with same configuration, it will write pycharm helpers in Your ocl container again.

## Continuous Integration

The project is built by CI at https://ci.openmrs.org/browse/OCL

You can see 3 plans there:
* OCL API
* OCL WEB
* OCL QA UI Tests

OCL API and OCL WEB are triggered by commits to respective repos. First docker images are built and pushed with a nightly tag to dockerhub at https://hub.docker.com/u/openconceptlab/dashboard/. Next unit and integration tests are being run. Finally a qa tag is being pushed to dockerhub and deployed to https://ocl-qa.openmrs.org/. On each deployment data is wiped out of the qa environment. You can login to the server using username 'admin' and password 'Admin123'.

### Deploying to staging and production

If you want to deploy to staging or production, you need to be logged in to Bamboo. Please request access via helpdesk@openmrs.org

1. Go to https://ci.openmrs.org/browse/OCL and click the cloud icon next to the project you want to deploy.
2. Click the related deployment plan.
3. Click the cloud icon next in the actions column for the chosen environment.
4. Choose whether to create a new release from build result or redeploy an existing release. You will choose the latter when promoting a release from staging to production, downgrading to a previous release or restarting services.
5. When creating a new release, choose the build result, which you want to deploy (usually the latest successful build). Leave the release title unchanged and click the Start deployment button.
6. Wait for the release to complete.

### Importing CIEL to staging and production

In order to import a newer version of the CIEL dictionary you need to have an SSH root access to staging.openconceptlab.org and openconceptlab.org.
Download the zip file with concepts and mappings in the OCL format and run the following commands for staging:
```sh
sudo -s
cd /root/docker/oclapi-stg 
unzip /path/to/zip/ciel_20180223.zip
docker-compose run -d --rm -v /root/docker/oclapi-stg:/ciel api python manage.py import_concepts_to_source --source 57cd60e2ba0d489c55039465 --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_concepts.json
docker logs -f oclapistg_api_run_1
docker-compose run -d --rm -v /root/docker/oclapi-stg:/ciel api python manage.py import_mappings_to_source --source 57cd60e2ba0d489c55039465 --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_mappings.json
docker logs -f oclapistg_api_run_2
```

Or for production:
```sh
sudo -s
cd /root/docker/oclapi-prd 
unzip /path/to/zip/ciel_20180223.zip
docker-compose run -d --rm -v /root/docker/oclapi-prd:/ciel api python manage.py import_concepts_to_source --source 5821b7a564d700001440f44a --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_concepts.json
docker logs -f oclapiprd_api_run_1
docker-compose run -d --rm -v /root/docker/oclapi-prd:/ciel api python manage.py import_mappings_to_source --source 5821b7a564d700001440f44a --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_mappings.json
docker logs -f oclapiprd_api_run_2
```

Imports run in background so you can disconnect from the server any time, but note that you must wait for concepts to be imported before importing mappings. You can get back to logs at any point by running: `docker logs -f CONTAINER_NAME`.

## Manual Environment Setup (on a Mac)

Follow this [guide](http://docs.python-guide.org/en/latest/starting/install/osx/) to install Python 2.7
and set up a virtual environment.  You may wish to name your virtual environment something more descriptive,
for example replace:

    virtualenv venv

With:

    virtualenv oclenv

And then run:

    source oclenv/bin/activate

### Mongo

The OCL API uses MongoDB as its backend datastore.  If you don't have it already, use Homebrew to install it:

    brew install mongodb

Once installed, use the `mongod` command to start a local instance of the MongoDB server.
Then, in a separate console window, run `mongo` to start the interactive command-line client.
Using the Mongo command-line, create a database named `ocl`:

     > use ocl

### Solr 4.9.0

Solr is used to support searching across OCL API entities.  To download Solr 4.9.0, visit the Solr [mirrors](http://www.apache.org/dyn/closer.cgi/lucene/solr/4.9.0) page and select a mirror.  Then download solr-4.9.0.tgz (NOT solr-4.9.0-src.tgz).

Choose an install directory (e.g. `~/third-party`, henceforth `$INSTALL_DIR`) and extract the tarball there.  You will then need to set 2 environment variables:

       export SOLR_ROOT=$INSTALL_DIR/solr-4.9.0
       export SOLR_HOME=$OCLAPI_ROOT/solr

`$OCLAPI_ROOT` refers to your Git project root (i.e. the location of this Readme file).

This should enable you to run `$OCLAPI_ROOT/run_solr.sh`, which starts Solr in a Jetty instance listening on port 8983.  Verify this by visiting:

     http://localhost:8983/solr

### The Django Project

Clone this repository, and `cd` into the `ocl` directory.
Before you can run the server, you will need to execute the following steps:

1. Install the project dependencies:

    pip install -r requirements.txt

2. Use `syncdb` to create your backing Mongo collections.

   ```sh
   ./manage.py syncdb
   ```

   If you are starting with a clean Mongo database, `syncdb` will prompt you to create a superuser.
   Follow that prompt.

   If you are not prompted to create a superuser, or wish to do so later, you can also use the command:

   ```sh
   ./manage.py createsuperuser
   ```
   
3. Verify your superuser and make note of your token.

   ```sh
   $ mongo
   > use ocl
   > db.auth_user.find({'is_superuser':true})
   ```

   This should revel the superuser you just created.  Note the user's _id (e.g. `ObjectId(""528927fb2f3e986be1627d6d"")`),
   and use it to locate your token:

   ```sh
   > db.authtoken_token.find({'user_id': ObjectId(""528927fb2f3e986be1627d6d"")})[0]
   ```

   Make note of the token `_id` (e.g. `""20e6ac8fe09129debac2929f4a20a56bea801165""`).  You will need this to access your endpoints
   once you start up your server.

4. Run the lightweight web server that ships with Django.

   ./manage.py runserver

   The OCL API should now be running at `http://localhost:8000`.

5. Test an endpoint.
   
   Remember, the API uses token-based authentication, so you can't just plug an endpoint into a browser and hit Return.
   You'll need to use a tool that allows you to specify a header with your request.  One simple example is `curl`:

   ```sh   
   curl -H ""Authorization: Token c1328d443285f2c933775574e83fe3abfe6d7c0d"" http://localhost:8000/users/
   ```

   I recommend using the [Advanced REST Client](https://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo?hl=en-US) app for Chrome.
   This provides you with a nice editor for passing parameters along with your `POST` and `PUT` requests.

6. Create an API user.
   
   Your superuser is not a valid API user, because it was not created via the `POST /users/` operation.
   However, you can use your superuser to access that endpoint and _create_ an API user:

   ```sh
   curl -H ""Authorization: Token c1328d443285f2c933775574e83fe3abfe6d7c0d"" -H ""Content-Type: application/json"" -d '{""username"":""test"",""email"":""test@test.com"", ""name"":""TestyMcTest""}' http://localhost:8000/users/   
   ```

7. (Optional) Make your API user an admin (staff) user.

   Log into the Django admin console with the superuser credentials you established in step 4:

   ```sh
   http://localhost:8000/admin/
   ```

   Then navigate to the user list:

   ```sh
   http://localhost:8000/admin/auth/user/
   ```

   Select the user you just created, and check the box next to ""staff status"".  Now your user is an admin within the context of the OCL API.
   
   

## Data Import Before Concept Creation
We need to have data before we go on creating a concept. 

The dropdowns that require preloaded data are Concept Class, Datatype, Name/Description Type, Locale, Map Type. 


### How to import Data
1. Create a new org `OCL`. 
2. Create a new user source `Classes` under org `OCL`. This will be be used for Concept Class dropdown.
3. Import the data as concepts in `Classes` from https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Classes/classes.json .

Follow https://github.com/OpenConceptLab/oclapi/wiki/Bulk-Importing#how-to-import to know how to import concepts in a source.

Proceed in same fashion for rest of the dropdown fields. Create sources `Datatypes`, `NameTypes`, `DescriptionTypes`, `Locales`, `MapTypes` under org `OCL`. 

Refer to following files for data: 

Datatypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Datatypes/datatypes_fixed.json

NameTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_NameTypes/nametypes_fixed.json

DescriptionTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_DescriptionTypes/description_types.json

Locales: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Locales/locales.json

MapTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_MapTypes/maptypes_fixed.json


---------------------------------------------------------------------
Copyright (C) 2016 Open Concept Lab. Use of this software is subject
to the terms of the Mozille Public License v2.0. Open Concept Lab is
also distributed under the terms the Healthcare Disclaimer
described at http://www.openconceptlab.com/license/.
---------------------------------------------------------------------
","'healthcare', 'python', 'terminology-management'",2021-08-11T07:01:23Z,29,24,16,"('rkorytkowski', 335), ('MisterNando', 295), ('snyaggarwal', 180), ('emrah-b', 111), ('kadiyan', 96), ('paynejd', 51), ('GauravButola', 36), ('heval', 35), ('karuhanga', 34), ('davetrig', 30), ('judywawira', 30), ('pkshiu', 26), ('AnshuAg', 25), ('ayseyo', 24), ('rohitggarg-tw', 20), ('rohitggarg', 19), ('burakince', 16), ('kavasoglu', 12), ('awadhwa1', 6), ('harpatel1', 6), ('molgun', 4), ('pgutkowski', 4), ('bhawnasingla', 2), ('selimober', 2), ('sgithens', 2), ('lhaber9', 1), ('tmarzeion', 1), ('fatmau', 1), ('mwaz', 1)","[3, 'Good Health and Well-Being']"
kobotoolbox/kobocat,Our (backend) server for providing blank forms to Collect and Enketo and for receiving and storing submissions. ,"# KoboCAT

## Important notice when upgrading from any release older than [`2.020.18`](https://github.com/kobotoolbox/kobocat/releases/tag/2.020.18)

Up to and including release [`2.020.18`](https://github.com/kobotoolbox/kobocat/releases/tag/2.020.18), this project (KoboCAT) and [KPI](https://github.com/kobotoolbox/kpi) both shared a common Postgres database. They now each have their own. **If you are upgrading an existing single-database installation, you must follow [these instructions](https://community.kobotoolbox.org/t/upgrading-to-separate-databases-for-kpi-and-kobocat/7202)** to migrate the KPI tables to a new database and adjust your configuration appropriately.

If you do not want to upgrade at this time, please use the [`shared-database-obsolete`](https://github.com/kobotoolbox/kobocat/tree/shared-database-obsolete) branch instead.

## Deprecation Notices

Much of the user-facing features of this application are being migrated
to . KoboCAT's data-access API and
OpenRosa functions will remain intact, and any plans to the contrary
will be announced well in advance. For more details and discussion,
please refer to
.

As features are migrated, we will list them here along with the last
release where each was present:

  - On 14 June 2021, the ability to upload forms directly to KoboCAT was
    removed, and it was announced that the legacy KoboCAT user interface would
    be preserved for ""a few more months"". After more than two years, we have
    removed the user interface and related endpoints entirely in release
    [2.023.37](https://github.com/kobotoolbox/kobocat/releases/tag/2.023.37).
    **This includes the ability to upload XLSForms via the legacy KoboCAT API.**
    Please use the KPI `v2` API for all form management. Other removed features
    should already be available in KPI as well. Please see
    [REMOVALS.md](REMOVALS.md) for a complete list.
  - To ensure security and stability, many endpoints that were already
    available in KPI, long-unsupported, or underutilized have been removed in
    release
    [2.020.40](https://github.com/kobotoolbox/kobocat/releases/tag/2.020.40).
    These were related to charts and stats, form cloning, form sharing, user
    profiles, organizations / projects / teams, bamboo, and ziggy. For a full
    list, please see [REMOVALS.md](REMOVALS.md). These endpoints were last
    available in the release
    [2.020.39](https://github.com/kobotoolbox/kobocat/releases/tag/2.020.39).
  - REST Services - an improved version [has been added to
    KPI](https://github.com/kobotoolbox/kpi/pull/1864). The last KoboCAT
    release to contain legacy REST services is
    [2.019.39](https://github.com/kobotoolbox/kobocat/releases/tag/2.019.39).

## About

kobocat is the data collection platform used in KoboToolbox. It is based
on the excellent [onadata](http://github.com/onaio/onadata) platform
developed by Ona LLC, which in itself is a redevelopment of the
[formhub](http://github.com/SEL-Columbia/formhub) platform developed by
the Sustainable Engineering Lab at Columbia University.

Please refer to
[kobo-install](https://github.com/kobotoolbox/kobo-install) for
instructions on how to install KoboToolbox.

## Code Structure

  - **logger** - This app serves XForms to and receives submissions from
    ODK Collect and Enketo.
  - **viewer** - This app provides a csv and xls export of the data
    stored in logger. This app uses a data dictionary as produced by
    pyxform. It also provides a map and single survey view.
  - **main** - This app is the glue that brings logger and viewer
    together.

## Localization

To generate a locale from scratch (ex. Spanish)

``` sh
$ django-admin.py makemessages -l es -e py,html,email,txt ;
$ for app in {main,viewer} ; do cd kobocat/apps/${app} && django-admin.py makemessages -d djangojs -l es && cd - ; done
```

To update PO files

``` sh
$ django-admin.py makemessages -a ;
$ for app in {main,viewer} ; do cd kobocat/apps/${app} && django-admin.py makemessages -d djangojs -a && cd - ; done
```

To compile MO files and update live translations

``` sh
$ django-admin.py compilemessages ;
$ for app in {main,viewer} ; do cd kobocat/apps/${app} && django-admin.py compilemessages && cd - ; done
```
## Testing in KoboCAT

For kobo-install users, enter the folder for kobo-install and run this command

```
./run.py -cf exec kobocat bash
```

For all other users, enter the container using this command

``` sh
$ docker exec -it {{kobocat container}} /bin/bash
```

Run pip install the development dependencies

``` sh
$ pip install -r dependencies/pip/dev.txt
```

Run pytest to run all automated tests

``` sh
$ pytest
```
",,2024-05-01T17:37:52Z,55,117,29,"('ukanga', 1744), ('larryweya', 1249), ('dorey', 1248), ('noliveleger', 1012), ('amarder', 815), ('jnm', 561), ('mejymejy', 281), ('pld', 215), ('rgaudin', 151), ('prabhasp', 140), ('ivermac', 123), ('modilabs-bumblebee', 108), ('prajjwol', 101), ('mberg', 96), ('TomCoder', 84), ('pmusaraj', 83), ('denniswambua', 70), ('royrutto', 69), ('JacquelineMorrissette', 68), ('bufke', 54), ('geoffreymuchai', 46), ('katembu', 46), ('modilabs-starscream', 44), ('LMNTL', 31), ('joshuaberetta', 29), ('mrmoje', 25), ('tinok', 23), ('Topol', 21), ('magicznyleszek', 20), ('ona-zebra', 18), ('codcrazi', 18), ('myf', 15), ('ksamuel', 13), ('duvld', 9), ('nstraub', 9), ('RuthShryock', 6), ('rhunwicks', 4), ('rowo', 4), ('jamesrkiger', 4), ('jeverling', 3), ('pitzer', 3), ('dpoirier', 2), ('dpapathanasiou', 2), ('hossein', 2), ('iwillig', 2), ('jeluxama', 2), ('kiorky', 2), ('okal', 2), ('cafootitt', 1), ('craigappl', 1), ('ggalmazor', 1), ('HaidarZ', 1), ('jalalmostafa', 1), ('rubenarslan', 1), ('simod', 1)","[17, 'Partnerships for the Goals']"
mosip/commons,This repository contains common utilities and services used by other MOSIP modules,"[![Maven Package upon a push](https://github.com/mosip/commons/actions/workflows/push-trigger.yml/badge.svg?branch=master)](https://github.com/mosip/commons/actions/workflows/push-trigger.yml)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=mosip_commons&metric=alert_status)](https://sonarcloud.io/dashboard?branch=master&id=mosip_commons)


# Commons

## Overview
As the name suggests, Commons refers to all the common services (also called ""kernel"") that are used by other modules of MOSIP. The Kernel services are listed below:

## Databases
Refer to [SQL scripts](db_scripts).

## Build & run (for developers)
The project requires JDK 1.11. 
1. Build and install:
    ```
    $ cd kernel
    $ mvn install -DskipTests=true -Dmaven.javadoc.skip=true -Dgpg.skip=true
    ```
1. Build Docker for a service:
    ```
    $ cd 
    $ docker build -f Dockerfile
    ```

## Deploy
To deploy Commons services on Kubernetes cluster using Dockers refer to [Sandbox Deployment](https://docs.mosip.io/1.2.0/deployment/sandbox-deployment).

## Test
Automated functaionl tests available in [Functional Tests repo](https://github.com/mosip/mosip-functional-tests).

## APIs
API documentation is available [here](https://mosip.github.io/documentation/).

## License
This project is licensed under the terms of [Mozilla Public License 2.0](LICENSE).


",,2024-05-03T09:56:25Z,128,10,13,"('Omsaieswar', 1084), ('LoganathanSekar7627', 1049), ('RajJhaJsr', 1048), ('urviljoshi', 928), ('manojsp12', 924), ('sowmya695', 744), ('MonobikashDas', 620), ('TaleevAalam', 558), ('MOSIPRajath', 422), ('Swatikp', 421), ('yaswanths2', 390), ('SanjayMurali7395', 386), ('dineshkaruppiaht7697', 386), ('srinivasanMT', 338), ('RamaduraiPandian1989', 336), ('Pranavk5', 310), ('AlokRanjan1106', 305), ('RaviBalaji6462', 301), ('NagalakshmiNithyanandan', 294), ('MaheshKumar1804', 290), ('gurpreet5991', 288), ('Sravya46559', 286), ('BALVIKASHSHARMA', 283), ('MeghaUttamTanga', 272), ('Shashank0707', 263), ('PremKumar1997', 242), ('rishabhjavad', 240), ('DineshAshokan7595', 229), ('abhishek-kumar3', 227), ('arun6368', 223), ('ReshamChugani', 216), ('gsasikumar', 204), ('balaji-sridharan', 199), ('sinhaneha', 189), ('HorteppaPujari', 183), ('RanjithaSiddegowda', 179), ('BalamuruganRamamoorthy', 179), ('tarunchawla2', 166), ('AshishRastogi-MOSIP', 161), ('Ajay1596', 150), ('kiranrajbs', 146), ('johnpsmindtree', 141), ('Mosip-Ritesh', 140), ('mosip-sagarmahapatra', 131), ('rakeshroshan1047543', 131), ('jyotikorimosip', 125), ('SaravanaKumar6564', 123), ('sadanandegowda', 123), ('ckm007', 120), ('sanjayzmindtree', 112), ('Sanober-Noor', 111), ('jainakshay510', 109), ('BrahamanadaReddyM1046112', 109), ('saumyamosip', 108), ('agneetra10', 97), ('akeer1', 97), ('SayeriMishra', 95), ('GurdayalDhillon2Mindtree', 81), ('jkori', 81), ('MOSIPTapaswini', 78), ('Code-Guna', 77), ('mandeepdhiman123', 68), ('RagavendranVenkatesan', 67), ('mahammedtaheer', 63), ('Shekhar6792', 62), ('rambhatt1591', 59), ('ArunakumarRati-MOSIP', 59), ('tenvsrini', 53), ('Aruna-Maddela', 50), ('amandeepk2403', 49), ('mosipkarthik', 49), ('ase-101', 49), ('nayakrounak', 46), ('vishwa-vyom', 44), ('Ravikant-M1044571', 44), ('sidhant-mindtree', 41), ('dhanendra06', 37), ('rijupjain', 33), ('vyasva', 31), ('akshayaMT', 30), ('tabishkhan7', 26), ('pjoshi751', 25), ('leonamarysusairaja', 24), ('hosurkrishnan', 21), ('shravanpoorigali', 19), ('gaurav-code', 17), ('kameshsr', 17), ('Mohanps', 16), ('nasirskhan-mt', 15), ('nehasinha1805', 14), ('saumyaprashar', 13), ('AyushSaxena28', 12), ('MOSIP-KISHAN', 12), ('hema2602', 12), ('as-ajitsingh', 11), ('NagarjunaKuchi', 10), ('mukul-puspam', 9), ('lalanamishra', 8), ('Sreekar0426', 7), ('GirishYarru', 7), ('Aswin-MN', 7), ('vjvignesh4', 6), ('HimajaDhanyamraju2', 6), ('Sivasankar1046113', 5), ('syedsalman3753', 5), ('Prafulrakhade', 5), ('Shilpa-Manjunath', 4), ('ssenmosip', 4), ('anadi', 3), ('rudra1in', 3), ('ramesh-n', 3), ('neosoftSatish', 2), ('Rakshithb1', 2), ('UshaAnandwade', 2), ('Srinivas1309', 2), ('Rakshitha650', 2), ('himajadhanyamraju', 2), ('ankitvaishnav', 2), ('akilalakshmanan', 1), ('Govindaraj-035', 1), ('harishmejari8496', 1), ('LavanyaRanganathan', 1), ('Lingam99777', 1), ('Mahesh-Binayak', 1), ('Sathishchandrasekaran', 1), ('dependabotbot', 1), ('MaheshKumarPanda', 1), ('mosip-guest', 1)","[17, 'Partnerships for the Goals']"
guardianproject/ripple,"A ""panic button"" app for triggering a ""ripple effect"" across apps that are set up to respond to panic events","Ripple
======

A ""panic button"" app for triggering a ""ripple effect"" across apps that are set up to respond to panic events.






Please visit our website [here](https://dev.guardianproject.info/projects/panic/wiki) to learn more about Ripple.

# How to use it?

You can find more informations on the purpose of Ripple, and how to use it, on our [blog post](https://guardianproject.info/2016/01/12/panickit-making-your-whole-phone-respond-to-a-panic-button/).

# Translating

Join the [Transifex page of Ripple](https://www.transifex.com/otf/rippleapp/) and help us to translate this app in your own language !
",,2023-11-01T15:27:35Z,5,201,24,"('eighthave', 139), ('weblate', 12), ('Poussinou', 2), ('8of', 1), ('grote', 1)","[11, 'Sustainable Cities and Communities']"
openstates/openstates-scrapers,source for Open States scrapers,"# Open States Scrapers

This repository contains the code responsible for scraping bills & votes for Open States.

## Links

* [Contributor's Guide](https://docs.openstates.org/contributing/)
* [Documentation](https://docs.openstates.org/contributing/scrapers/)
* [Open States Issues](https://github.com/openstates/issues/issues)
* [Open States Discussions](https://github.com/openstates/issues/discussions)
* [Code of Conduct](https://docs.openstates.org/code-of-conduct/)
","'government', 'hacktoberfest', 'python', 'scrapers', 'states', 'united-states'",2024-05-02T20:56:26Z,154,836,51,"('jamesturk', 5744), ('showerst', 2261), ('mikejs', 1793), ('twneale', 1589), ('NewAgeAirbender', 965), ('mileswwatkins', 767), ('paultag', 592), ('rshorey', 420), ('schneidy', 322), ('jmcarp', 269), ('chrisyamas', 254), ('hiteshgarg14', 139), ('judgejudes', 130), ('jessemortenson', 129), ('sroomf', 116), ('johnseekins', 93), ('EhtishamSabir', 92), ('BrandonLewis', 89), ('In-vincible', 84), ('braykuka', 55), ('divergentdave', 47), ('mattgrayson', 47), ('tamilyn', 40), ('csnardi', 34), ('estaub', 32), ('Shivansh-Bajaj', 32), ('bfossen-ce', 32), ('linzjax', 32), ('markolson', 30), ('ColbyReed', 27), ('mshenfield', 27), ('cweber', 27), ('cliftonmcintosh', 26), ('glydeb', 26), ('JoeGermuska', 22), ('giantryansaul', 22), ('samtregar', 22), ('doubleswirve', 21), ('fgregg', 21), ('elseagle', 21), ('rsimoes', 21), ('dcliner', 20), ('jballanc', 19), ('scichelli', 18), ('binaryechoes', 17), ('rmanocha', 15), ('nicklryan', 15), ('brymut', 13), ('stwlam', 13), ('gabriel4649', 13), ('dsummersl', 13), ('poliquin', 11), ('onyxfish', 11), ('a-cat-named-snowball', 10), ('abejburton', 10), ('srikant-ch5', 10), ('frenata', 9), ('azban', 9), ('crdunwel', 8), ('coopernurse', 8), ('hmoco', 8), ('browning', 8), ('dmc2015', 7), ('IanWhalen', 7), ('rmcarthur', 7), ('rmd6502', 7), ('rshapiro', 7), ('narendra36', 7), ('dschep', 6), ('erinspace', 6), ('gregoryfoster', 6), ('nickrecchi', 6), ('nloui', 6), ('AliAbstract', 5), ('nina-johnson', 5), ('zestyping', 5), ('gregjd', 5), ('eafisher', 5), ('Empact', 5), ('jalbertbowden', 5), ('shreyakupadhyay', 4), ('j-alder', 4), ('divij-sinha', 4), ('schedutron', 4), ('mbacchi', 4), ('mroswell', 4), ('kdmonroe', 4), ('julianedwards', 4), ('HarshCasper', 4), ('bhitov', 4), ('brjezierski', 4), ('Brant-Norris', 3), ('tcarobruce', 3), ('rhouse2', 3), ('mollycode', 3), ('mattkenney', 3), ('sarindipity', 3), ('JackBewley', 3), ('jgibson517', 3), ('redNixon', 3), ('LindsayYoung', 3), ('mattjmcnaughton', 2), ('konklone', 2), ('dougzor', 2), ('zstumgoren', 2), ('ryan-ricard', 2), ('rkiddy', 2), ('paulschreiber', 2), ('ninapsoncak', 2), ('nickoneill', 2), ('lazarus1331', 2), ('matthewbauer', 2), ('ltvolks', 2), ('adamderusha', 2), ('anelson-unfold', 2), ('annerajb', 2), ('BrantNorrisUFL', 2), ('arch-iver', 2), ('danhixon', 2), ('ianb', 2), ('blackbear', 2), ('jasonkb', 2), ('jdunck', 2), ('joemcl', 2), ('lmdragun', 2), ('kevinschaul', 1), ('MikeStall', 1), ('NixGD', 1), ('patgarcia', 1), ('pll33', 1), ('rezarzky', 1), ('RiqueingHavoc', 1), ('TheGRS', 1), ('Segovax', 1), ('shaunagm', 1), ('Nickardson', 1), ('TheRealFalcon', 1), ('timgates42', 1), ('tomschlick', 1), ('smcoll', 1), ('compwright', 1), ('mertonium', 1), ('JessePresnell', 1), ('grgcombs', 1), ('serixscorpio', 1), ('EricPaulson', 1), ('dsarhadian', 1), ('dlamoureaux', 1), ('PaulAnton', 1), ('chriszs', 1), ('carsonmcdonald', 1), ('bhrutledge', 1), ('trblae', 1), ('asentner', 1)","[16, 'Peace, Justice and Strong Institutions']"
code4romania/seismic-risc,Web app for keeping track of buildings in danger of collapsing in the event of an earthquake,"# Acasă în Siguranță

[![GitHub contributors](https://img.shields.io/github/contributors/code4romania/seismic-risc.svg)](https://github.com/code4romania/seismic-risc/graphs/contributors)
[![GitHub last commit](https://img.shields.io/github/last-commit/code4romania/seismic-risc.svg)](https://github.com/code4romania/seismic-risc/commits/develop)
[![License: MPL 2.0](https://img.shields.io/badge/license-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0)



[![code for romania twitter][1.1]][1]
[![code for romania facebook][2.1]][2]
[![code for romania instagram][3.1]][3]
[![code for romania linkedin][4.1]][4]



[1.1]: https://raw.githubusercontent.com/code4romania/.github/main/social_icons/social_img_twitter.png

[1]: https://twitter.com/Code4Romania

[2.1]: https://raw.githubusercontent.com/code4romania/.github/main/social_icons/social_img_facebook.png

[2]: https://facebook.com/code4romania/

[3.1]: https://raw.githubusercontent.com/code4romania/.github/main/social_icons/social_img_instagram.png

[3]: https://instagram.com/code4romania/

[4.1]: https://raw.githubusercontent.com/code4romania/.github/main/social_icons/social_img_linkedin.png

[4]: https://linkedin.com/in/code4romania/



:romania: Un cutremur în București nu este o situație ipotetică. Este o certitudine că acest lucru se va întâmpla. În
acest context, la mai bine de 40 de ani de la cutremurul din 1977, memoria colectivă a ascuns în profunzime amintirile
acelui dezastru în încercarea de a-și înăbuși teama. Dar realitatea este că, patru decenii mai târziu, Bucureștiul, la
fel ca restul orașelor cu risc seismic ridicat, nu ar face față unui asemenea eveniment, iar pierderile de vieți
omenești ar fi uriașe.
[Exercițiul Seism 2018](https://www.news.ro/social/exercitiul-seism-2018-cel-mai-recent-bilant-al-cutremurului-simulat-indica-peste-3-900-de-morti-peste-7-000-de-raniti-si-peste-2-300-de-persoane-disparute-1922405315222018102018579831)
derulat de DSU arată că cel puţin 4.587 persoane şi-ar pierde viaţa, iar 8.585 ar fost rănite, 6 spitale vor fi
distruse, 23 de unităţi spitaliceşti distruse parţial, iar 9 avariate, dar funcţionale. O estimare, am spune noi, destul
de optimistă.

Ce putem face pentru a deveni mai puțin vulnerabili? Să știm totul despre oraș, despre clădirile în care locuim astfel
încât să putem cere consolidarea lor. Acasă în Siguranță nu este doar ""un nou site de informare"", ci o platformă care
colectează și validează apoi cu experți date despre clădirile din România, la nivel național, ajută asociațiile de
proprietari să își consolideze clădirile, te ține la curent cu legislația și ți-o explică și are grijă să ai la îndemână
informații utile la orice moment.

:gb: An earthquake in Bucharest is not a hypothetical situation. It is certain that this will happen. In this context,
after more than 40 years from the 1977 earthquake, the collective memory has hidden deep the memories of that disaster
in its attempt of stifling its fear. The reality is that, four decades later, Bucharest, as well as the rest of the
cities with a high seismic risk, would not stand up tu such an event, and the loss of life would be tremendous.
[The Earthquake 2018 Exercise](https://www.romania-insider.com/seism-2018-exercise-bucharest) conducted by the DSU shows
that at least 4,587 people would have died and 8,585 would have been injured, 6 hospitals would be destroyed, 23 more
would be partially destroyed, and 9 would be damaged, though still functional. An estimation that we would consider
quite optimistic.

What can we do to become less vulnerable? Find out everything about the city, about the buildings in which we live so
that we can ask for their consolidation. Home Safe is not just ""a new information site"", but a platform that collects
and then validates with the help of experts data about the buildings in Romania, at a national level, it helps owners
associations to consolidate their buildings, it keeps you in touch with the current legislation and explains it to you,
and it makes sure that you have useful information at your disposal at all times.

**Let's save lives together.**


    TABLE OF CONTENTS (click to expand) 


* [Acasă în Siguranță](#acasă-în-siguranță)
  * [Contributing](#contributing)
  * [Built With](#built-with)
    * [Programming languages](#programming-languages)
    * [Frameworks](#frameworks)
    * [Package managers](#package-managers)
    * [Code styling](#code-styling)
    * [Database technology & provider](#database-technology--provider)
  * [Getting started](#getting-started)
    * [Pre-requisites](#pre-requisites)
    * [Initial set-up](#initial-set-up)
      * [Initialising all the services](#initialising-all-the-services)
      * [Backing up and restoring the database in case of an upgrade](#backing-up-and-restoring-the-database-in-case-of-an-upgrade)
    * [Environment variables](#environment-variables)
      * [Deployment variables](#deployment-variables)
      * [External services API keys](#external-services-api-keys)
        * [HERE Maps API Key](#here-maps-api-key)
        * [hCAPTCHA API Key](#hcaptcha-api-key)
    * [Starting the project](#starting-the-project)
    * [Starting the project without docker](#starting-the-project-without-docker)
      * [Windows platform](#windows-platform)
        * [Prerequisites](#prerequisites)
        * [Steps to set your environment](#steps-to-set-your-environment)
      * [Steps needed to start development servers](#steps-needed-to-start-development-servers)
    * [Development](#development)
    * [Known Issues](#known-issues)
      * [Client hot-reload on Windows Docker is not working](#client-hot-reload-on-windows-docker-is-not-working)
      * [In VS Code, ESLint fails to load the Prettier plugin](#in-vs-code-eslint-fails-to-load-the-prettier-plugin)
  * [Management Commands](#management-commands)
  * [Testing](#testing)
  * [Production](#production)
  * [Client Deployment](#client-deployment)
  * [Feedback](#feedback)
  * [License](#license)
  * [About Code4Ro](#about-code4ro)




## Contributing

If you would like to contribute to one of our repositories, first identify the scale of what you would like to
contribute. If it is small (grammar/spelling, or a bug fix) feel free to start working on a fix. If you are submitting a
feature or substantial code contribution, please discuss it with the team and ensure it follows the product roadmap.

Our collaboration model [is described here](https://github.com/code4romania/.github/blob/main/CONTRIBUTING.md).
**And make sure you check the [workflow document](https://github.com/code4romania/.github/blob/main/WORKFLOW.md)**;
it helps you keep your environment in a good shape, and it helps everyone move faster with code reviews.
If you want to make any change to this repository, please **make a fork first**.

We don't have a specific set of coding guidelines, so just follow the way the code was written until now, if in doubt,
you can use [Google's style guide](http://google.github.io/styleguide/pyguide.html).

## Built With

### Programming languages

[Python 3](https://www.python.org)
[JavaScript](https://developer.mozilla.org/en-US/docs/Web/JavaScript)

### Frameworks

**Backend:** [Django](https://www.djangoproject.com)
**Client:** [React](https://reactjs.org/)

### Package managers

**Backend:** [pip](https://pypi.org/)
**Client:** [npm](https://www.npmjs.com/)

### Code styling

**Backend:** [Black](https://black.readthedocs.io/en/stable/)
**Client:** [Prettier](https://prettier.io/) + [ESLint](https://eslint.org/) + [Airbnb style guide](https://github.com/airbnb/javascript)

### Database technology & provider

[PostgreSQL](https://www.postgresql.org)

## Getting started

Risc Seismic backend is a Django application, built on top of Python 3.9+ with a PostgreSQL database. The Client is a React
single page application.

### Pre-requisites

In order to run the project locally, you need to have [Docker](https://docs.docker.com/install/)
and [docker-compose](https://docs.docker.com/compose/overview/) installed.

You can install the above-mentioned packages manually, or you can use our helper commands.

On `Ubuntu` run:

```shell
make install-docker-ubuntu
```

On `MacOS` run:

```shell
make install-docker-osx
```

On other platforms please follow the instructions described here:

- 
- 

### Initial set-up

#### Initialising all the services

Make sure to check the [Environment variables](#environment-variables)
section for info on how to set up the keys before you run the following commands:

```shell
cp .env.example.dev .env.dev
# build the development container
make build-dev
```

If you didn't set up the `RUN_LOAD_INITIAL_DATA` variable, you can add dummy data to the database with the following command:

```shell
make build-dev
```

If the `RUN_LOAD_INITIAL_DATA` was `yes`, then you should have dummy data but will have to create a superuser:

```shell
docker-compose exec api ./manage.py createsuperuser
```

#### Backing up and restoring the database in case of an upgrade

1. Create a back-up of the data
   (the `build` folder is ignored by git)

    ```shell
    docker-compose exec db pg_dumpall -U postgres > ./build/backup.sql
    ```

2. Run the database upgrade or just get the latest version of the code from git if it upgrades the database

    ```shell
    git pull upstream develop
    ```

3. Remove the current database and start-up the environment
   (remove the `-dev` part if you don't want the development mode)

    ```shell
    make drop-db && make build-dev
    ```

4. Restore the backed-up data to the new database

    ```shell
    docker-compose exec db psql -U postgres < ./build/backup.sql
    ```

5. Check the API endpoints and that you can log in the admin interface with the same users as before

### Environment variables

#### Deployment variables

The following variables change the way the backend is deployed.

`RUN_MIGRATIONS`
Run the initial migrations (sets up the data models from the database).

`RUN_LOAD_INITIAL_DATA`
Adds real & dummy data to the database (adds buildings, datafiles, and statistics).

`RUN_COLLECT_STATIC`
Collects static files so that they can be easily served to production.

`RUN_DEV_SERVER`
Runs the application in the development mode.

#### External services API keys

To have a fully functional project, you have to get two API keys: HERE Maps API Key and hCAPTCHA API Key.

##### HERE Maps API Key

Tutorial: [https://developer.here.com/tutorials/getting-here-credentials/](https://developer.here.com/tutorials/getting-here-credentials/)

Keys added to the `.env` file:

```shell
# the same key can be used for both variables
HERE_MAPS_API_KEY
REACT_APP_HERE_MAPS_API_KEY
```

##### hCAPTCHA API Key

1. [Create a hCAPTCHA account](https://dashboard.hcaptcha.com/signup)
2. Go to [your settings page](https://dashboard.hcaptcha.com/settings)
3. Create a New Site, copy the Site Key and add it to the environment variables list

Keys added to the `.env` file:

```shell
REACT_APP_CAPTCHA_API_KEY
```

### Starting the project

First check the `.env` file created by the init command and see if there are any environment variables that you might
need to provide or change. This file is used by `docker-compose` to pass the environment variables to the container it
creates.

Get the project up and running:

```shell
docker-compose up
```

You should be able to access the local environment site and admin at the following URLs:

- 
- 

If you have problems starting the project, first check out
the [FAQ](https://github.com/code4romania/seismic-risc/wiki/FAQ) and if that doesn't work, ask someone from the
project's channel. Maybe the issue you just had is worth adding to
the [FAQ](https://github.com/code4romania/seismic-risc/wiki/FAQ), wouldn't it?

To work on running containers that were started using `docker-compose up`, open another terminal and:

```shell
cd path/to/repo
docker-compose exec api some_container_command
# or
docker-compose exec client some_container_command
```

To see all available commands, run:

```shell
make help
```

### Starting the project without Docker

#### Windows platform

##### Prerequisites

1. [PostgreSQL](https://www.enterprisedb.com/postgresql-tutorial-resources-training?cid=55)
2. [Python 3.9](https://www.python.org/downloads/release/python-399/)
3. [Node.js](https://nodejs.org/dist/v14.16.0/node-v14.16.0-x64.msi)

##### Steps to set your environment

1. In project directory run:

    ```shell
    python -m venv .venv
    .venv\Scripts\activate.bat
    pip install -r ./backend/requirements-dev.txt
    copy .env.dev .env
    ```

2. Check the .env file created by the copy command and see if there are any environment variables that you might need to
   provide or change. Double check database config line in .env. It has to follow this
   pattern: `postgres://USER:PASSWORD@HOST:PORT/NAME`

3. Run following to set the needed environment variables:

    ```shell
    activate_dev_env.bat
    ```

4. Check database connection. If this fails double check database configuration.

    ```shell
    python backend/wait_for_db.py
    ```

5. Run migrations:

    ```shell
    python backend/manage.py migrate --no-input
    ```

6. Create admin user (user to login into admin panel):

    ```shell
    python backend/manage.py createsuperuser
    ```

7. Load dummy data in database:

    ```shell
    python backend/manage.py loaddata statistics
    python backend/manage.py loaddata buildings
    python backend/manage.py loaddata pages
    ```

8. Install node modules.

    ```shell
    cd client
    npm install
    ```

#### Steps needed to start development servers

*1. Start backend server.*

Open terminal in the project directory and run environment activation script, then start the server.

```shell
.venv\Scripts\activate.bat
activate_dev_env.bat
python backend\manage.py runserver 0.0.0.0:8030
```

Check functionality at http://localhost:8030 you should get a 404 page.

*2. Start front-end server.*

Open terminal in the project directory and run environment activation script, then start the server.

```shell
activate_dev_env.bat
cd client
npm start
```

Check functionality at http://localhost:3000.

### Development

When creating new models in Django, to make sure they are generated in a clean environment, it is recommended
to generate the migration files using the `make` command:

```shell
make makemigrations && make migrate
```

When you need to add/remove requirements or restrict the version of a requirement, edit the `requirements.in` (prod) and
the `requirements-dev.in` (dev) files accordingly. After doing this run:

```shell
make update-requirements
```

This will create a clean environment where it uses the [pip-tools](https://github.com/jazzband/pip-tools/) library to
compile the corresponding `requirements.txt` files with the versions of the packages pinned. This is important as it
guarantees that every environment this service runs in, has the same dependencies installed and minimizes the risk
of `works on my machine`.

### Known Issues

#### Client hot-reload on Windows Docker is not working

Try following these steps:

1. open up a terminal in **seismic-risc_client** container
2. `cd ./node_modules/react-scripts/config/`
3. `vi webpackDevServer.config.js`
4. on the exported config object, update the value of `watchOptions` to include the following properties:

    ```shell
    aggregateTimeout: 100,
    poll: 500
    ```

5. save the file and restart the client container

This way, webpack-dev-server should be watching files in polling mode, instead of listening for file change events.

#### In VS Code, ESLint fails to load the Prettier plugin

Add the following option to user settings in VS Code if ESLint fails to load the Prettier plugin.

```json
{
    ""eslint.workingDirectories"": [
        {
            ""mode"": ""auto""
        }
    ]
}
```

## Management Commands

The new custom command can be called using
`python manage.py buildings `
required arguments:

- --delete
- --create

```shell
cd path/to/repo
docker-compose exec api bash
root@ba4fd81f9023:/code# python manage.py buildings 30 --create
100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 37.89it/s]
Successfully created 30 buildings.
root@ba4fd81f9023:/code# python manage.py buildings 25 --delete
Successfully deleted 25 buildings.
```

## Testing

Local development testing:

```shell
cd path/to/repo
docker-compose exec api bash
root@3c5df91778ad:/code# pytest
```

Pipeline testing:

```shell
make test
```

## Production

To get the container ready for production use, we need to first build it:

```shell
docker build -t seismic-risc:latest ./backend
```

Use the `prod.env.dist` template file and create a `prod.env` file with the correct environment variables and run like
so:

```shell
docker run --env-file prod.env -p HOST_PORT:GUNICORN_PORT seismic-risc:latest
```

Or, you can provide all the environment variables at runtime:

```shell
docker run -e DJANGO_CONFIGURATION=Prod -e DJANGO_SECRET_KEY= -e DATABASE_URL=postgres://USER:PASSWORD@HOST:PORT/NAME -e GUNICORN_PORT=5000 -e GUNICORN_WORKERS_COUNT=2 -p HOST_PORT:GUNICORN_PORT seismic-risc:latest
```

After testing the container runs properly, tag and upload the self to Docker hub:

```shell
docker tag seismic-risc:latest code4romania/seismic-risc:latest
docker push code4romania/seismic-risc:latest
```

## Client Deployment

- Change directory to `./client`
- Build the solution `npm install`
- Start a development server `npm start`
- Run the tests `npm test`
- Build the solution `npm run build`

## Feedback

- Request a new feature on GitHub.
- Vote for popular feature requests.
- File a bug in GitHub Issues.
- Email us with other feedback contact@code4.ro

## License

This project is licensed under the MPL 2.0 License — see the [LICENSE](LICENSE) file for details

## About Code4Ro

Started in 2016, Code for Romania is a civic tech NGO, official member of the Code for All network. We have a community
of over 500 volunteers (developers, UX/UI, communications, data scientists, graphic designers, devops, IT security, and
more) who work pro bono for developing digital solutions to solve social problems. #techforsocialgood. If you want to
learn more details about our projects [visit our site](https://www.code4.ro/en/) or if you want to talk to one of our
staff members, please e-mail us at contact@code4.ro.

Last, but not least, we rely on donations to ensure the infrastructure, logistics and management of our community that
is widely spread across 11 timezones, coding for social change to make Romania and the world a better place. If you want
to support us, [you can do it here](https://code4.ro/en/donate/).
","'civic-tech', 'code4ro', 'crowdsourcing', 'django', 'earthquakes', 'javascript', 'python', 'react'",2024-05-02T10:52:28Z,38,34,9,"('tudoramariei', 149), ('dependabot-previewbot', 82), ('dependabotbot', 62), ('vladplesu', 31), ('danniel', 21), ('costibleotu', 12), ('myshy93', 10), ('kata-kas', 10), ('cipick', 9), ('aramboi', 9), ('alinaMihai', 7), ('iuliux', 7), ('catileptic', 7), ('calindragomir', 6), ('cnstlungu', 6), ('mateesville93', 6), ('sk-anna', 5), ('JustBeYou', 4), ('aniri', 4), ('danmic94', 4), ('danichim', 3), ('juliageek', 3), ('RaduCStefanescu', 3), ('francisc-czobor', 2), ('GabrielMajeri', 2), ('ImgBotApp', 2), ('imgbotbot', 2), ('razvanpavel', 2), ('geozaur', 2), ('emanuelbesliu', 1), ('dmfarcas', 1), ('damianr13', 1), ('lorenamitrea', 1), ('eyarz', 1), ('tuanthanh2067', 1), ('bogdanhopu', 1), ('chalx', 1), ('AlexandruBuhai', 1)","[11, 'Sustainable Cities and Communities']"
OSU-Battelle-Center/DRC-Ebola-Conflict,"Situational awareness platform for the ongoing Ebola outbreak in North Kivu, Democratic Republic of the Congo. A project of the Battelle Center for Science, Engineering and Public Policy at The Ohio State University.","## DRC-Ebola-Conflict
Open-source situational awareness portal for the ongoing Ebola outbreak in North Kivu, Democratic Republic of the Congo. A project of the Battelle Center for Science, Engineering and Public Policy at The Ohio State University. Read about the problem context and project rationale [here](https://medium.com/the-battelle-center-for-science-engineering-and/an-open-source-geospatial-application-to-support-the-ebola-response-d53b81fad9a4).

![](https://github.com/OSU-Battelle-Center/DRC-Ebola-Response/blob/master/Images/most_recent_master.png)

### Documentation
See our project [wiki](https://github.com/OSU-Battelle-Center/DRC-Ebola-Response/wiki) for more information about this project.

## How to Load
Open `index.html` file in Web Browser (Google Chrome or Firefox preferred)

Step-by-step instructions:

Go to the DRC-Ebola-Response Repository

Click the green Clone or download button then select Download Zip

Uncompress the Zip file on your desktop

Open the index.html file in your Web Browser (Google Chrome or Firefox preferred)
",,2019-07-10T17:56:14Z,3,15,3,"('malloysamuel', 72), ('EarthAdam', 61), ('l-ferguson', 5)","[3, 'Good Health and Well-Being']"
catalyst-cooperative/pudl,"The Public Utility Data Liberation Project provides analysis-ready energy system data to climate advocates, researchers, policymakers, and journalists.","===============================================================================
The Public Utility Data Liberation Project (PUDL)
===============================================================================

.. readme-intro

.. image:: https://www.repostatus.org/badges/latest/active.svg
   :target: https://www.repostatus.org/#active
   :alt: Project Status: Active

.. image:: https://github.com/catalyst-cooperative/pudl/workflows/pytest/badge.svg
   :target: https://github.com/catalyst-cooperative/pudl/actions?query=workflow%3Apytest
   :alt: PyTest Status

.. image:: https://img.shields.io/codecov/c/github/catalyst-cooperative/pudl?style=flat&logo=codecov
   :target: https://codecov.io/gh/catalyst-cooperative/pudl
   :alt: Codecov Test Coverage

.. image:: https://img.shields.io/readthedocs/catalystcoop-pudl?style=flat&logo=readthedocs
   :target: https://catalystcoop-pudl.readthedocs.io/en/latest/
   :alt: Read the Docs Build Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/psf/black
   :alt: Any color you want, so long as it's black.

.. image:: https://results.pre-commit.ci/badge/github/catalyst-cooperative/pudl/main.svg
   :target: https://results.pre-commit.ci/latest/github/catalyst-cooperative/pudl/main
   :alt: pre-commit CI

.. image:: https://zenodo.org/badge/80646423.svg
   :target: https://zenodo.org/badge/latestdoi/80646423
   :alt: Zenodo DOI

.. image:: https://img.shields.io/badge/calend.ly-officehours-darkgreen
   :target: https://calend.ly/catalyst-cooperative/pudl-office-hours
   :alt: Schedule a 1-on-1 chat with us about PUDL.

What is PUDL?
-------------

The `PUDL `__ Project is an open source data processing
pipeline that makes US energy data easier to access and use programmatically.

Hundreds of gigabytes of valuable data are published by US government agencies, but it's
often difficult to work with. PUDL takes the original spreadsheets, CSV files, and
databases and turns them into a unified resource. This allows users to spend more time
on novel analysis and less time on data preparation.

The project is focused on serving researchers, activists, journalists, policy makers,
and small businesses that might not otherwise be able to afford access to this data from
commercial sources and who may not have the time or expertise to do all the data
processing themselves from scratch.

We want to make this data accessible and easy to work with for as wide an audience as
possible: anyone from a grassroots youth climate organizers working with Google sheets
to university researchers with access to scalable cloud computing resources and everyone
in between!

PUDL is comprised of three core components:

Raw Data Archives
^^^^^^^^^^^^^^^^^
PUDL `archives `__ all our raw
inputs on `Zenodo
`__ to ensure
permanent, versioned access to the data. In the event that an agency changes how they
publish data or deletes old files, the data processing pipeline will still have access
to the original inputs. Each of the data inputs may have several different versions
archived, and all are assigned a unique DOI (digital object identifier) and made
available through Zenodo's REST API.  You can read more about the Raw Data Archives in
the `docs `__.

Data Pipeline
^^^^^^^^^^^^^
The data pipeline (this repo) ingests raw data from the archives, cleans and integrates
it, and writes the resulting tables to `SQLite `__ and `Apache
Parquet `__ files, with some acompanying metadata stored as
JSON.  Each release of the PUDL software contains a set of of DOIs indicating which
versions of the raw inputs it processes. This helps ensure that the outputs are
replicable. You can read more about our ETL (extract, transform, load) process in the
`PUDL documentation `__.

Data Warehouse
^^^^^^^^^^^^^^
The SQLite, Parquet, and JSON outputs from the data pipeline, sometimes called ""PUDL
outputs"", are updated each night by an automated build process, and periodically
archived so that users can access the data without having to install and run our data
processing system. These outputs contain hundreds of tables and comprise a small
file-based data warehouse that can be used for a variety of energy system analyses.
Learn more about `how to access the PUDL data
`__.

What data is available?
-----------------------

PUDL currently integrates data from:

* **EIA Form 860**: 2001-2022
  - `Source Docs `__
  - `PUDL Docs `__
* **EIA Form 860m**: 2023-12
  - `Source Docs `__
* **EIA Form 861**: 2001-2022
  - `Source Docs `__
  - `PUDL Docs `__
* **EIA Form 923**: 2001-2023
  - `Source Docs `__
  - `PUDL Docs `__
* **EPA Continuous Emissions Monitoring System (CEMS)**: 1995Q1-2023Q4
  - `Source Docs `__
  - `PUDL Docs `__
* **FERC Form 1**: 1994-2022
  - `Source Docs `__
  - `PUDL Docs `__
* **FERC Form 714**: 2006-2022 (mostly raw)
  - `Source Docs `__
  - `PUDL Docs `__
* **FERC Form 2**: 1996-2022 (raw only)
  - `Source Docs `__
* **FERC Form 6**: 2000-2022 (raw only)
  - `Source Docs `__
* **FERC Form 60**: 2006-2022 (raw only)
  - `Source Docs `__
* **US Census Demographic Profile 1 Geodatabase**: 2010
  - `Source Docs `__

Thanks to support from the `Alfred P. Sloan Foundation Energy & Environment
Program `__, from
2021 to 2024 we will be cleaning and integrating the following data as well:

* `EIA Form 176 `__
  (The Annual Report of Natural Gas Supply and Disposition)
* `FERC Electric Quarterly Reports (EQR) `__
* `FERC Form 2 `__
  (Annual Report of Major Natural Gas Companies)
* `PHMSA Natural Gas Annual Report `__
* Machine Readable Specifications of State Clean Energy Standards

How do I access the data?
-------------------------

For details on how to access PUDL data, see the `data access documentation
`__. A quick
summary:

* `Datasette `__
  provides browsable and queryable data from our nightly builds on the web:
  https://data.catalyst.coop
* `Kaggle `__
  provides easy Jupyter notebook access to the PUDL data, updated weekly:
  https://www.kaggle.com/datasets/catalystcooperative/pudl-project
* `Zenodo `__
  provides stable long-term access to our versioned data releases with a citeable DOI:
  https://doi.org/10.5281/zenodo.3653158
* `Nightly Data Builds `__
  push their outputs to the AWS Open Data Registry:
  https://registry.opendata.aws/catalyst-cooperative-pudl/
  See `the nightly build docs `__
  for direct download links.
* `The PUDL Development Environment `__
  lets you run the PUDL data processing pipeline locally.

Contributing to PUDL
--------------------

Find PUDL useful? Want to help make it better? There are lots of ways to help!

* Check out our `contribution guide `__
  including our `Code of Conduct `__.
* You can file a bug report, make a feature request, or ask questions in the
  `Github issue tracker `__.
* Feel free to fork the project and make a pull request with new code, better
  documentation, or example notebooks.
* `Make a recurring financial contribution `__
  to support our work liberating public energy data.
* `Hire us to do some custom analysis `__ and
  allow us to integrate the resulting code into PUDL.

Licensing
---------

In general, our code, data, and other work are permissively licensed for use by anybody,
for any purpose, so long as you give us credit for the work we've done.

* The PUDL software is released under
  `the MIT License `__.
* The PUDL data and documentation are published under the
  `Creative Commons Attribution License v4.0 `__
  (CC-BY-4.0).

Contact Us
----------

* For bug reports, feature requests, and other software or data issues please make a
  `GitHub Issue `__.
* For more general support, questions, or other conversations around the project
  that might be of interest to others, check out the
  `GitHub Discussions `__
* If you'd like to get occasional updates about the project
  `sign up for our email list `__.
* Want to schedule a time to chat with us one-on-one about your PUDL use case, ideas
  for improvement, or get some personalized support? Join us for
  `Office Hours `__
* `Follow us here on GitHub `__
* Follow us on Mastodon: `@CatalystCoop@mastodon.energy `__
* Follow us on BlueSky:  `@catalyst.coop `__
* `Follow us on LinkedIn `__
* `Follow us on HuggingFace `__
* Follow us on Twitter: `@CatalystCoop `__
* `Follow us on Kaggle `__
* More info on our website: https://catalyst.coop
* Email us if you'd like to hire us to provide customized data extraction and analysis:
  `hello@catalyst.coop `__

About Catalyst Cooperative
--------------------------

`Catalyst Cooperative `__ is a small group of data wranglers
and policy wonks organized as a worker-owned cooperative consultancy. Our goal is a
more just, livable, and sustainable world. We integrate public data and perform
custom analyses to inform public policy
(`Hire us! `__). Our focus is primarily on
mitigating climate change and improving electric utility regulation in the United
States.
","'cems', 'climate', 'coal', 'ddj', 'eia', 'eia860', 'eia923', 'electricity', 'emissions', 'energy', 'epa', 'etl', 'ferc', 'ghg', 'natural-gas', 'open-data', 'pudl', 'python', 'sqlite', 'utility'",2024-05-03T15:13:14Z,35,445,18,"('zaneselvans', 4549), ('cmgosnell', 1478), ('aesharpe', 758), ('e-belfer', 547), ('katie-lamb', 358), ('zschira', 345), ('dependabotbot', 332), ('bendnorman', 322), ('rousik', 262), ('ezwelty', 178), ('jdangerx', 140), ('alanawlsn', 136), ('pre-commit-cibot', 122), ('TrentonBush', 99), ('stevenbwinter', 98), ('karldw', 85), ('swinter2011', 72), ('ptvirgo', 50), ('arengel', 49), ('gschivley', 37), ('yashkumar1803', 36), ('grgmiller', 34), ('pudlbot', 15), ('dstansby', 10), ('knordback', 7), ('AppTrain', 4), ('robertozanchi', 3), ('katherinelamb', 3), ('davidmudrauskas', 2), ('Wheelspawn', 2), ('erictleung', 1), ('kyleries', 1), ('nelsonauner', 1), ('codacy-badger', 1), ('gitter-badger', 1)","[13, 'Climate Action']"
digidem/mapeo-desktop,Local-first mapping and monitoring in remote environments,"# Mapeo Desktop

[![Build Status](https://github.com/digidem/mapeo-desktop/workflows/Node%20CD/badge.svg)](https://github.com/digidem/mapeo-desktop/actions)

An offline map editing application for indigenous territory mapping in remote
environments. It uses [mapeo-core](https://github.com/digidem/mapeo-core) for
offline peer-to-peer synchronization of an OpenStreetMap database, without any
server. The map editor is based on [iDEditor](https://github.com/openstreetmap/iD/),
a simple and easy to use editor for OpenStreetMap. The app is built with
[Electron](http://electron.atom.io).

This project is considered stable and used by over 150 communities.

![screenshot](static/screenshot.png)

For a mobile application that is compatible with Mapeo Desktop, see [Mapeo Mobile](https://github.com/digidem/mapeo-mobile).

## Guide

Read the [online user guide](https://digital-democracy.gitbook.io/mapeo/) for
information on how to install aerial imagery and tiles, custom configurations,
and more. 

![architecture](docs/desktop-architecture.png)

## Getting Started

To clone and install all dependencies and start a process to re-build the app whenever you change a file:

```sh
git clone git@github.com:digidem/mapeo-desktop.git
cd mapeo-desktop
npm install
npm run build:translations
npm run watch
```

Before proceeding, you will have to wait until some background processes have culminated. You will know this has taken place when you see terminal output indicating that your app (and assets such as translations) have been built.

Then, in another terminal, run the app in development mode:

```sh
npm run dev
```

Running `npm run dev` will run the background process in an electron window
that can be stepped through similarly to the front-end code.

To see log messages in real-time while in debug mode, run `tail` in another
terminal window:

```sh
tail -f USERDATA/Mapeo/logs/$DATE.debug.log
```

## Contributing

See [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) for more details.


## Community

Connect with the Mapeo community for support & to contribute!

- [**Discord**](https://discord.gg/KWRFDh3v73)

## License

GPLv3
",,2023-11-23T20:54:36Z,22,255,18,"('okdistribute', 587), ('gmaclennan', 416), ('hackergrrl', 151), ('Karissa', 60), ('achou11', 28), ('noffle', 23), ('arky', 9), ('rudokemper', 9), ('ErikSin', 8), ('lightlii', 7), ('digidem-bot', 4), ('thibautRe', 3), ('louh', 2), ('RainbowGiantSquid', 2), ('ScottBrenner', 2), ('aliya-ryan', 1), ('netsgnut', 1), ('ralphtheninja', 1), ('Nmargolis', 1), ('ungoldman', 1), ('poga', 1), ('sethvincent', 1)","[11, 'Sustainable Cities and Communities']"
openMF/mifos-mobile,Repository for the Mifos Mobile Banking App for clients,"
  




# Mifos-Mobile Android Application for MifosX

An Android Application built on top of the MifosX Self-Service platform for end-user customers to view/transact on the accounts and loans they hold. Data visible to customers will be a sub-set of what staff can see. This is a native Android Application written in Kotlin.

## Notice

:warning: We are fully committed to implement [Jetpack Compose](https://developer.android.com/jetpack/compose) and moving ourself to support 
`kotlin multi-platform`. **If you are sending any PR regarding `XML changes` we will `not` consider at this moment but converting XML to jetpack compose are most welcome.** If you sending any PR regarding logical changes in Activity/Fragment you are most welcome. 

### Status

| Master | Development | Chat |
|------------|-----------------|-----------------|
| ![Mifos-Mobile CI[Master]](https://github.com/openMF/mifos-mobile/workflows/Workflow%20for%20master/development%20branches/badge.svg?branch=master) | ![Mifos-Mobile CI[Development]](https://github.com/openMF/mifos-mobile/workflows/Workflow%20for%20master/development%20branches/badge.svg?branch=development) |[![Join the chat at https://mifos.slack.com/](https://img.shields.io/badge/Join%20Our%20Community-Slack-blue)](https://mifos.slack.com/)|


## Join Us on Slack

Mifos boasts an active and vibrant contributor community, Please join us on [slack](https://join.slack.com/t/mifos/shared_invite/zt-2f4nr6tk3-ZJlHMi1lc0R19FFEHxdvng). Once you've joined the mifos slack community, please join the `#mifos-mobile` channel to engage with mifos-mobile development. If you encounter any difficulties joining our Slack channel, please don't hesitate to open an issue. This will allow us to assist you promptly or send you an invitation.



## How to Contribute

This is an OpenSource project and we would be happy to see new contributors. The issues should be raised via the GitHub issue tracker.
For Issue tracker guidelines please click here. All fixes should be proposed via pull requests.
For pull request guidelines please click here. For commit style guidelines please click here.

### Branch Policy

We have the following branches :

* **development**
  All the contributions should be pushed to this branch. If you're making a contribution,
  you are supposed to make a pull request to _development_.
  Please make sure it passes a build check on Github Workflows CI.

  It is advisable to clone only the development branch using the following command:

  `git clone -b  `

  With Git 1.7.10 and later, add --single-branch to prevent fetching of all branches. Example, with development branch:

  `git clone -b development --single-branch https://github.com/username/mifos-mobile.git`

* **ui-redesign**
  All the contributions related to redesigning of the app should be pushed to this branch. If you're making a contribution,
  you are supposed to make a pull request to _ui-redesign_.
  Please make sure it passes a build check on Github Workflows CI.

  This branch will be merged with the development branch once the redesign is complete.

* **master**
  The master branch contains all the stable and bug-free working code. The development branch once complete will be merged with this branch.

### Demo credentials
Fineract Instance: gsoc.mifos.community

Username: `mifos`

Password: `password`

### Instruction to get the latest APK

To get the latest apk of master/development branch from Github Artifacts, follow these steps:
1. Go to to the [Actions](https://github.com/openMF/mifos-mobile/actions?query=workflow%3A%22Workflow+for+master%2Fdevelopment+branches%22+event%3Apush) tab of this repository.
2. Select the latest workflow for master/development branch.
3. Click on hyperlink 'mifos-mobile' in Artifacts section.
4. Extract the downloaded file and get the apk.

## Development Setup

Before you begin, you should have already downloaded the Android Studio SDK and set it up correctly. You can find a guide on how to do this here: [Setting up Android Studio](http://developer.android.com/sdk/installing/index.html?pkg=studio).

## Building the Code

1. Clone the repository using HTTP: git clone https://github.com/openMF/mifos-mobile.git

2. Open Android Studio.

3. Click on 'Open an existing Android Studio project'

4. Browse to the directory where you cloned the mifos-mobile repo and click OK.

5. Let Android Studio import the project.

6. Build the application in your device by clicking run button.

## Wiki

View the [wiki](https://github.com/openMF/self-service-app/wiki) to see pages that provide details on the project.

## Specification

See the [requirements](https://github.com/openMF/self-service-app/wiki/Design-&-Requirements) for an initial design mockup and documentation on the Fineract API.

## PaymentHub Usecases

For Payment Hub usecases, check this [documentation](https://mifos.gitbook.io/docs/payment-hub-ee/overview/payment-hub-apis). Mifos Mobile utilises medium connector of Payment Hub.

## Note

The UI design is currently being revamped. New design can be found [here](https://docs.google.com/presentation/d/1yFR19vGlKW-amxzGms8TgPzd1jWkrALPFcaC85EyYpw/edit#slide=id.g6c6ccd991d_0_42)

## Contributors

Special thanks to the incredible code contributors who continue to drive this project forward.


  


","'android-application', 'hilt-dependency-injection', 'jetpack-compose', 'kotlin', 'kotlin-flows', 'mifosx', 'mvvm-architecture', 'okhttp3', 'retrofit2'",2024-04-24T21:13:59Z,104,259,25,"('therajanmaurya', 227), ('jawidMuhammadi', 173), ('PratyushSingh07', 146), ('dilpreet2028', 128), ('miPlodder', 88), ('gururani-abhishek', 68), ('renovatebot', 43), ('dependabotbot', 30), ('ashwinkey04', 25), ('prashantkh19', 20), ('AkshGautam', 19), ('AvneetSingh2001', 15), ('shiv07tiwari', 14), ('vjs3', 14), ('dependabot-previewbot', 12), ('devansh-299', 12), ('droidchef', 12), ('ivary43', 11), ('letelete', 11), ('satyan', 10), ('SekhGulamMainuddin', 10), ('rutvik-panchal', 10), ('mohak1712', 9), ('Rish02Sharma', 7), ('rahul-gill', 7), ('mayank8318', 7), ('garvit984', 7), ('UduakUmanah10', 6), ('ShivangiSingh17', 5), ('MigDinny', 5), ('ritish099', 5), ('Sparsh1212', 5), ('sridharjajoo', 5), ('m-sameer', 5), ('etlhsu', 5), ('itsamitgoel', 5), ('itsPronay', 4), ('rahul-jha98', 4), ('prashanthkomuravelli', 4), ('IOhacker', 4), ('sosnickm', 3), ('Vikashgathala', 3), ('yashraj-01', 3), ('NiranjanNlc', 3), ('kingbass01', 3), ('dubeyaayush07', 3), ('AbrahamOsmondE', 3), ('renovate-bot', 3), ('Pritom14', 3), ('lssarao', 3), ('avikganguly01', 3), ('aj019', 3), ('jsuyash1514', 2), ('m-murad', 2), ('geekanamika', 2), ('kmanikanta335', 2), ('divyank00', 2), ('afk11', 2), ('Thanush66', 2), ('Rishabhk07', 2), ('rchtgpt', 2), ('naman14', 2), ('lonewolf2208', 2), ('ishan1604', 2), ('keshriAyushh', 2), ('ankurs287', 2), ('ashu-dadhich', 2), ('AbhilashG97', 2), ('amanjeetsingh150', 1), ('aayushsingla', 1), ('vyukti13', 1), ('vladimirfomene', 1), ('utkarsh006', 1), ('srishti-R', 1), ('0xsiddharthks', 1), ('code-dagger', 1), ('edcable', 1), ('harrycode007', 1), ('harshitbansal05', 1), ('intrigus', 1), ('kalpesh30', 1), ('mofetej', 1), ('nikit19', 1), ('doomers', 1), ('rahulmangla28', 1), ('superav', 1), ('ReCodee', 1), ('Aditya-gupta99', 1), ('arinmodi', 1), ('luckyman20', 1), ('deekshatw', 1), ('elirehema', 1), ('oyehanif', 1), ('laxyapahuja', 1), ('manki11', 1), ('vorburger', 1), ('OussEmaDevCode', 1), ('puneetkohli521', 1), ('rahulbabbar1', 1), ('RajaVamsi11', 1), ('kienme', 1), ('Sagar0-0', 1), ('sagar15795', 1), ('se-ke', 1)","[1, 'No Poverty']"
Gapminder/gapminder-offline,,"# Gapminder Offline  

With Gapminder Offline you can show animated statistics from your own laptop!

• You can use it without internet access  
• You can visualise your own data with it or even combine it with the datasets provided (examples & instructions)  
• The software and the datasets will update automatically when connection is available  
• It’s free  

![app screenshot](https://s3-eu-west-1.amazonaws.com/static.gapminder.org/GapminderMedia/wp-uploads/20170113171243/Gapminder-Offline-Tools.png)

## Using the app

Download the stable version from here http://www.gapminder.org/tools-offline/

## Release routines
1. `npm run changelog` - generates content for `CHANGELOG.md` file with changes that have happened since last release
2. `npm version` - this one is a bit more complicated. Let's start with what it needs in order to run.
  - `CONVENTIONAL_GITHUB_RELEASER_TOKEN` environment variable should be set up for this command:

    Example: `CONVENTIONAL_GITHUB_RELEASER_TOKEN=aaaaaaaaaabbbbbbbbbbccccccccccffffffffff npm version minor`

  - this command understands following parameters:
    - `major` (having initially version **0.0.0** by applying this option it will be changed to **1.0.0**).

        Example:
        ```
          CONVENTIONAL_GITHUB_RELEASER_TOKEN=aaaaaaaaaabbbbbbbbbbccccccccccffffffffff npm version major
        ```

    - `minor` (having initially version **0.0.0** by applying this option it will be changed to **0.1.0**)

        Example:
        ```
          CONVENTIONAL_GITHUB_RELEASER_TOKEN=aaaaaaaaaabbbbbbbbbbccccccccccffffffffff npm version minor
        ```

    - `patch` (having initially version **0.0.0** by applying this option it will be changed to **0.0.1**)

        Example:
        ```
          CONVENTIONAL_GITHUB_RELEASER_TOKEN=aaaaaaaaaabbbbbbbbbbccccccccccffffffffff npm version patch
        ```

    During the release process two files will be changed and pushed to github:
      1. CHANGELOG.md - because of added history.
      2. package.json - because of bumped version.

    **Note:** `aaaaaaaaaabbbbbbbbbbccccccccccffffffffff` - is the fake token. In order to generate proper one you need to do following: [github tutorial](https://help.github.com/articles/creating-an-access-token-for-command-line-use)

    **Important note:** you should merge `development` branch into `master` and **performing `npm verison` on `master`** branch according to our [gitflow](https://github.com/valor-software/valor-style-guides/tree/master/gitflow)

    **Even more important note:** while generating token (using tutorial given above) you need to choose which permissions should be granted to it. For our *release purposes* you need to choose all permissions under the section `repo`

## Contributing
We use Angular 6, Electron & chrome app featuring Webpack

### Building the app

Mac
```
npm i
npm run electron:mac
```

Win 64 *(should be done under Win 64 environment)*
```
npm i
electron:windows
```

Win 32 *(should be done under Win 32 environment)*
```
npm i
npm run electron:windows32
```

Linux
```
npm i
./build-dev
```

### Running the app with debugger open  

```
""./Gapminder Offline"" dev
```

Example on Mac, since apps are really just folders:  
```
./Gapminder\ Offline.app/Contents/MacOS/Gapminder\ Offline dev
```

Important note! The main goal of building the app in developer mode is testing only, 
because `auto update` functionality will be unavailable in this case. 

[Read how to build the application with code signing and stuff](https://github.com/Gapminder/gapminder-offline/blob/master/docs/build.md)  

[Read about auto-update functionality](https://github.com/Gapminder/gapminder-offline/blob/master/docs/auto-update.md)



## License

[MIT](http://markdalgleish.mit-license.org)
",,2023-06-02T15:41:15Z,8,46,33,"('buchslava', 289), ('angiehjort', 102), ('dab2000', 91), ('dmitry-zhemchugov', 6), ('SergeyKuryatnick', 4), ('omfgnuts', 2), ('Legionivo', 1), ('korel-san', 1)","[17, 'Partnerships for the Goals']"
datosgcba/obras-abiertas,Proyecto de visualización y análisis de la información de las obras del Gobierno de la Ciudad de Buenos Aires.,"# [Obras Abiertas](https://obras.buenosaires.gob.ar/)

[![Build Status](https://travis-ci.com/datosgcba/ba_obras.svg?branch=master)](https://travis-ci.com/datosgcba/ba_obras)

[English documentation](./README_en.MD)

Proyecto de visualización y análisis de la información de las obras públicas basada en la iniciativa [BA Obras](https://obras.buenosaires.gob.ar/) del Gobierno de la Ciudad de Buenos Aires.

  - [Acerca de Obras Abiertas](#acerca-de-obras-abiertas)
  - [Requisitos de datos](#requisitos-de-datos)
    - [Esquema de datos](#esquema-de-datos)
  - [Archivos necesarios para el correcto funcionamiento del sitio](#archivos-necesarios-para-el-correcto-funcionamiento-del-sitio)
    - [Archivos geoespaciales](#archivos-geoespaciales)
    - [Imagenes del sitio](#imagenes-del-sitio)
    - [Imagenes de obras](#imagenes-de-obras)
  - [Instrucciones para desarrolladores](#instrucciones-para-desarrolladores)
    - [Adaptación estetica](#adaptaci%C3%B3n-estetica)
  - [Levantar el proyecto en un ambiente productivo](#levantar-el-proyecto-en-un-ambiente-productivo)
    - [Requerimientos](#requerimientos)
    - [Configuración](#configuraci%C3%B3n)
    - [Instalación por primera vez](#instalaci%C3%B3n-por-primera-vez)
    - [Actualizaciones](#actualizaciones)
  - [Archivo de Configuración](#archivo-de-configuraci%C3%B3n)
    - [i18n](#i18n)

## Acerca de Obras Abiertas 

Obras Abiertas es un sitio web que busca informar a la ciudadanía acerca del avance de todas las obras de la jurisdicción.
El sistema es una aplicación frontend que toma información de un csv y crea visualizaciones de datos sobre el 
cumplimiento de las obras. Se genera una vista para cada obra y una vista general donde pueden verse todas. 
La misma tiene la capacidad de ser embebida dentro otras páginas correspondientes a organismos relacionados.

El proyecto original puede verse en [obras.buenosaires.gob.ar](https://obras.buenosaires.gob.ar/)

Se trata de una aplicación enteramente de frontend (todos archivos estáticos), con diferentes vistas. 
El proyecto es exclusivamente frontend. Todos los datos son obtenidos desde un unico CSV estáticos y los representa. 
No tiene formularios, submits a backend con información ni acceso a base de datos.
Está construído como Single Page Application basado en Angular JS.

## Requisitos de datos

La fuente de datos del sitio consta de un archivo csv que puede estar alojado en un servicio externo o puede ser 
un archivo que forme parte del proyecto. Para conocer detalles sobre la conexion entre el sitio y el csv, ver 
la sección relativa al archivo de configuración.

### Esquema de datos

| Nombre de la columna          | Columna obligatoria / opcional | Tipo de dato      | Detalle                                              |
| ----------------------------- |------------------------------- | ----------------- | ---------------------------------------------------- |
| `nombre`                      | Obligatoria                    | Texto             | Nombre con el cual se identifica a la obra           |
| `lat`                         | Obligatoria                    | Numerico¹         | Latittud de la ubicación de la obra                  |
| `lng`                         | Obligatoria                    | Numerico¹         | Longitud de la ubicación de la obra                  |
| `descripcion`                 | Obligatoria                    | Texto             | Descripción la obra                                  |
| `entorno`                     | Obligatoria                    | Categoria (Texto) | Clasificación territorial de la obra                 |
| `monto_contrato`              | Obligatoria                    | Numerico¹         | Monto presupuestado para la obra                     |
| `etapa`                       | Obligatoria                    | Categoria (Texto). Uno de los siguientes valores: `En proyecto`, `En licitacion`, `En ejecucion`, `Finalizada` | Etapa de la obra |
| `tipo`                        | Obligatoria                    | Categoria (Texto) | Tipo de la obra. Los valores deben coincidir con la nomenclatura actual o, en caso contrario, deben cargarse nuevos iconos al proyecto² |
| `comuna`                      | Opcional³                      | Numerico¹         | Número de la comuna                                  |
| `jurisdiccion`                | Opcional³                      | Categoria (Texto) | Jurisdicción a la cual pertenece la obra             |
| `mano_obra`                   | Opcional                       | Numerico¹         | Cantidad de obreros trabajando en la obra            |
| `porcentaje_avance`           | Opcional                       | Numerico¹         | Porcentaje de avance entre 0 y 100                   |
| `id`                          | Opcional                       | Numerico¹         | Identificador de la obra                             |
| `etapa_detalle`               | Opcional                       | Texto             | Información extra de la etapa                        |
| `area_responsable`            | Opcional                       | Categoria (Texto) | Ministerio o área responsable                        |
| `barrio`                      | Opcional                       | Texto             | Nombre del barrio                                    |
| `calle_1`                     | Opcional                       | Texto             | Calle                                                |
| `seccion`                     | Opcional                       | Numerico¹         | Número de sección                                    |
| `manzana`                     | Opcional                       | Numerico¹         | Número de manzana                                    |
| `parcela`                     | Opcional                       | Numerico¹         | Número de parcela                                    |
| `direccion`                   | Opcional                       | Texto             | Dirección y número                                   |
| `fecha_inicio`                | Opcional                       | Fecha             | Fecha de inicio de la obra                           |
| `fecha_fin_inicial`           | Opcional                       | Fecha             | Fecha de fin de la obra                              |
| `plazo_meses`                 | Opcional                       | Numerico¹         | Cantidad de meses                                    |
| `imagen_1`                    | Opcional                       | Url⁴              | Url de una imagen representando la obra              |
| `imagen_2`                    | Opcional                       | Url⁴              | Url de una imagen secundaria representando la obra   |
| `imagen_3`                    | Opcional                       | Url⁴              | Url de una imagen secundaria representando la obra   |
| `imagen_4`                    | Opcional                       | Url⁴              | Url de una imagen secundaria representando la obra   |
| `licitacion_oferta_empresa`   | Opcional                       | Texto             | Nombre de la empresa                                 |
| `licitacion_anio`             | Opcional                       | Numerico¹         | Año de la licitación                                 |
| `benficiarios`                | Opcional                       | Numerico¹         | Cantidad de beneficiarios                            |
| `compromiso`                  | Opcional                       | Texto             | Nombre del compromiso                                |
| `link_interno`                | Opcional                       | Url               | Link para mas información sobre la obra              |
| `pliego_descarga`             | Opcional                       | Url               | Link absoluto al pliego de la obra                   |

¹ Tener en cuenta que es importante formatear los numeros de forma tal que el separador decimal sea `.` y no deben 
contar con separadores de miles, comillas, caracter de moneda, ni otros caracteres especiales. 

² Los tipos de obras que actualmente cuentan con iconos son: `Arquitectura`, `Escuelas`, `Espacio publico`, 
`Hidraulica e infraestructura`, `Hidraulica`, `Salud`, `Transporte` y `Vivienda`. En caso de contar con tipos de obras
que no se encuentren entre las anteriores, es necesario agregar los iconos correspondientes a la carpeta 
`app/images/iconos`. Los nombres de archivo deben estar en formato `svg` y su nombre debe estar en minuscula y 
reemplazando los espacios por guiones `-`, por ej: el icono correspondiente a `Espacio publico` tiene nombre de 
archivo `espacio-publico.svg`. Luego de agregar los iconos, es necesario buildear el proyecto (ver instrucciones para
desarrolladores en este mismo documento).

³ Si bien las columnas de `comuna` y `jurisdiccion` se presentan como opcionales, es requisito que una de las dos
columnas esté presente.

⁴ En caso de no contar con urls para las imagenes, es posible incorporarlas cómo parte del sitio y servirlas junto
al resto del contenido del sitio. Ver sección de [Imagenes de obras](#imagenes-de-obras)

## Archivos necesarios para el correcto funcionamiento del sitio

### Archivos geoespaciales

La vista de las obras distribuidas espacialmente sobre el mapa de la jurisdiccion requiere de un archivo geojson con 
información geografica de la geometria de la jurisdicción. Este archivo geojson debe tener la siguiente estructura
```
{
  ""type"": ""FeatureCollection"",
  ""features"": [
    {
      ""type"": ""Feature"",
      ""properties"": {
        ""id"": ""First jurisdiction"",
        ""comuna"": 1
      },
      ""geometry"": {
        ""type"": ""Polygon"",
        ""coordinates"": [
          [
            [lat, long],
            ...
            [lat, long]
          ]
        ]
      }
    },
    ...
    {
      ""type"": ""Feature"",
      ""geometry"": {
        ""properties"": {
          ""id"": ""Last jurisdiction"",
          ""comuna"": 15
        },
        ""type"": ""Polygon"",
        ""coordinates"": [
          [
            [lat, long],
            ...
            [lat, long]
          ]
        ]
      }
    }
    
  ],
  ""properties"": {
    ""center"": [0, 0],
    ""zoomLevel"": 100
  }
}
``` 

Cada feature corresponde a una subdivisión del area geografica. Debe haber por lo menos un feature con un poligono.

La propiedad `properties.id` y `properties.comuna` hacen referencia a las columnas `jurisdiccion` y `comuna`, 
respecticamente, del csv de datos del sitio y deben coincidir con los valores asignados a esas columnas. Como solo
una de estas columnas es obligatoria para el archivo de datos, entonces solo una propiedad tambien lo es para el
archivo geomtry.geojson (ver nota 3 del [esquema de datos](#esquema-de-datos))

La propiedad `properties.center` es un par de `[lat, long]` que debe ser definido como el punto central del 
mapa, y la propiedad `properties.zoomLevel` es el nivel de zoom al cual debe estar el mapa.

Dentro del repositorio hay dos copias de este archivo geojson. La primera ubicada en `app/geo/geometry.geojson`, usada
para cuando el sitio se levanta en modo desarrollo, y otra copia ubicada en `dist/geo/geometry.geojson`, usada para
cuando el sitio se levanta en modo productivo. Para leer más sobre estas dos formas de levantar el sitio, ver la 
sección de [Instrucciones para desarrolladores](#instrucciones-para-desarrolladores)

### Imagenes del sitio

El selector de la seccion de ""Mapa"" usa una imagen en formato svg. La imagen default del proyecto es la del mapa de la
ciudad de Buenos Aires. Para reemplazar esta imagen, pisar el archivo ubicado en `app/images/selectores/mapa.svg`. 

Para modificar el favicon del sitio, pisar el archivo `app/favicon.ico` con el favicon que se quiera utilizar.

Es necesario buildear el proyecto luego de modificar estas imagenes. Ver la siguiente seccion.

### Imagenes de obras

En caso de contar con imagenes de obras con urls accesibles publicamente, se pueden agregar al csv cómo celdas en las
columnas correspondientes (ver sección de [esquema de datos](#esquema-de-datos)). 

En caso de contar con imagenes cómo archivos que no tienen una url pública, es posible agregarlos a los archivos del 
sitio para que sean servidos de la misma forma que el resto de los archivos estáticos (html, js, css, etc).

Como se especifica en la sección de [Instalación por primera vez](#instalaci%C3%B3n-por-primera-vez), la carpeta 
`dist` se utiliza como root del servidor, con lo cual todos los archivos contenidos dentro serán servidos con un path
relativo a esta carpeta. Si posicionan archivos de imagenes en la carpeta `dist/images` entonces estarán 
disponibles públicamente de la misma forma que el resto de las imagenes dentro de dicha carpeta. 
Por ej, si la url base del sitio es `http://sitio-de-obras.com` y se posiciona una imagen `obra.jpg` en `dist/images`
entonces la url final de la imagen será `http://sitio-de-obras.com/images/obra.jpg`. 
Tambien es posible crear carpetas para organizar las imagenes. En caso de ubicar `obra.jpg` en una carpeta 
`obras-imagenes` dentro de `dist/images`, entonces la url final será 
`http://sitio-de-obras.com/images/obras-imagenes/obra.jpg`

## Instrucciones para desarrolladores

Para realizar modificaciones al sitio actual, realizar los siguientes pasos

* Clonar el proyecto usando git
* Instalar NodeJS. Recomendamos usar [nvm](https://github.com/creationix/nvm). Una vez instalado `nvm` ejecutar 
`nvm use` en la carpeta raiz del proyecto para asegurarse de usar la misma version de node.
* Instalar las dependencias de npm via `npm install` en la carpeta raiz del proyecto 
* Instalar las dependencias de bower via `bower install` en la carpeta raiz del proyecto
* Archivo de configuración: En /app duplicar el archivo config.js.example con el nombre config.js y configurar 
acorde a la [seccion correspondiente al archivo de configuración](#archivo-de-configuraci%C3%B3n)
* En caso de que el archivo de datos del sitio sea parte del sitio, ubicarlo dentro de la carpeta `app`
* Levantar el servidor de desarrollo via `grunt serve` en la carpeta raiz del proyecto
* Hacer los cambios en /app y con live reloading se actualizará en http://localhost:10000
* Una vez que se cuenta con los cambios deseados, compilar assets via `grunt build` en la carpeta raiz del proyecto

### Adaptación estetica

En caso de necesitar agregar cambios al css del sitio, ubicarlos en el archivo `app/styles/custom.css`. Este archivo
tiene precedencia por sobre los estilos base del sitio. 

Una vez modificado este archivo, ejecutar el comando `grunt build` para empaquetar los cambios y disponibilizarlos
en la carpeta `dist` con el resto de los assets productivos.


## Levantar el proyecto en un ambiente productivo

### Requerimientos

Sólo requiere un servidor web que pueda servir los contenidos. 
La aplicación contiene su código y las librerías que necesita, ya compiladas en la carpeta `/dist`.

* Apache o NGINX
* Varnish u otro caché de estáticos (opcional, ideal para mejorar la performance)
* No se requiere hacer build de ningún tipo.
* No se requiere salida a internet desde el servidor.
* No se requieren instalación de dependencias.
* Todo lo necesario está compilado y minificado en estáticos (html, css, js e imágenes), dentro de la carpeta `/dist`.

### Configuración

* La única configuración requerida es la creación del ARCHIVO de configuración y al archivo `csv` de datos 
(en caso de no cargarlo via un servicio externo)
* No requiere parámetros de ambiente del servidor, ni de proceso. Es todo web y js client – side.

### Instalación por primera vez

1. Crear un subdominio o definir la url donde vivirá la aplicación, podría ser: 
`https://obras-site.buenosaires.gob.ar/`.
2. Definir un servidor con nginx o apache y clonar el proyecto usando 
`git clone https://github.com/datosgcba/ba_obras.git`.
3. Crear el [archivo de configuración](#archivo-de-configuraci%C3%B3n)
4. En caso de que el archivo de datos del sitio sea parte del sitio, ubicarlo dentro de la carpeta `dist`
5. Apuntar las configuraciones del web server y subdominio a la carpeta `dist`, donde se encuentran los archivos 
finales y compilados.

### Actualizaciones

En caso de querer actualizar a la ultima version disponible, ubicarse en la carpeta raiz del proyecto y ejecutar
`git pull`.

Si no se han realizado cambios en el proyecto respecto a la version de github, entonces `git pull` deberá exitosamente
actualizar el proyecto a su ultima version.

Si en cambio se han realizado cambios en el proyecto y el comando `git pull` devuelve algun error de conflicto de
versiones de archivos, debera procederse a mergear manualmente los archivos correspondientes.
- Realizar copias de resguardo de los archivos que no son parte del proyecto original o que han sido modificados. 
Por ej, el archivo csv con los datos de las obras del lugar actual, el archivo geometry.geojson, el archivo config.js, 
el archivo custom.css en caso de haberlo modificado y cualquier otro archivo (como por ej iconos, favicon, etc) que 
se hayan agregado o reemplazado respecto a la version de github de BA Obras
- Revertir los cambios de la version local del repositorio para que quede acorde a la version de github. Esto puede
realizarse mediante `git checkout .` para revertir cambios a archivos registrados en git, y mediante 
`git clean -d -f .` para eliminar archivos no registrados en git.
- Ejecutar de vuelta `git pull`. Esta vez no deberia tener conflictos
- Volver a copiar los archivos resguardados en el primer punto
- Realizar un `grunt build` para compilar nuevamente el sitio. Este ultimo paso deberia dejar en la carpeta `dist` 
la version actualizada del sitio productivo.

#### Cambios entre versiones

El dia 3 de junio de 2019 se agregó un requisito al archivo `geomtry.geojson`, siendo ahora necesario que cada 
componente de primer nivel del mapa disponga de un atributo `""properties"": { ""id"": ""Nombre de la subdivision"" }`.
Para mas información, ver la sección sobre [Archivos geoespaciales](#archivos-geoespaciales).

## Archivo de Configuración

El archivo de configuración `config.js` deberá configurarse acordemente para poder levantar el sitio en modo de
desarrollo o producción. Existen dos ejemplos del archivo de configuración `config.js.example` en las carpetas `app`
y `dist` para servir de base al archivo de configuración de desarrollo `app/config.js` y al archivo de configuración
de producción `dist/config.js`.

Hay dos formas a traves de las cuales puede cargarse el archivo de datos para el sitio: mediante un request `get` a un
archivo dentro del sitio o mediante un request `jsonp` a una url externa. En caso de que el archivo sea parte del sitio
es necesario setear `DATA_PATH` como el nombre de archivo. Por ej, si el archivo de datos se llama `data.csv` y está 
ubicado en `app/data.csv` (para desarrollo) y `dist/data.csv` (para produccion) entonces `DATA_PATH` deberá tener
valor `data.csv`.

| Config                | Valor                 | Descripción                                                                                                           |
| --------------------- |---------------------- | --------------------------------------------------------------------------------------------------------------------- |
| `LOAD_USING_JSONP`    | `true\|false`         | Indica si el archivo de datos deberá ser cargado utilizando un request via jsonp o si deberá ser cargado localmente.  |
| `DATA_PATH`           | String                | Url o nombre de archivo                                                                                               |
| `USE_USIG_MAP_TILES`  | `true\|false`         | Indica si los mapas deberan usar tilemaps de USIG (soporte unicamente para Buenos Aires)                              |
| `CITY_NAME`           | String                | Nombre de la jurisdicción correspondiente al sitio                                                                    |
| `DATA_ORIGIN`         | String                | Valor por defecto: `undefined`. Único otro valor soportado: `andino-json-api`                                         |

### i18n

Se encuentra disponible un archivo con la traduccion de todos los textos presentados al usuario. Este archivo está 
pensado para ser modificado en caso de querer levantar el sitio en un idioma distinto al español o en caso de tener
que modificar la nomenclatura actual del sitio.

El archivo se encuentra en `app/scripts/i18n.js`. En caso de modificar las traducciones, es necesario re-buildear el
sitio (ver seccion de instrucciones para desarrolladores).   
",,2023-11-09T15:28:46Z,9,19,6,"('palamago', 159), ('iheredia', 102), ('tinchoforever', 36), ('pieroenrico', 4), ('martoalalu', 3), ('ddecampos', 2), ('manupaiva', 2), ('poligarcia', 1), ('cavallof', 1)","[16, 'Peace, Justice and Strong Institutions']"
HospitalRun/hospitalrun-server-routes,Express server routes for HospitalRun,"# hospitalrun-server-routes
Express server routes for HospitalRun.
These routes are used by the [HosptialRun server](https://github.com/HospitalRun/hospitalrun-server) for HospitalRun deployments.  They are also used by [HosptialRun frontend](https://github.com/HospitalRun/hospitalrun-frontend) when using [Ember CLI](http://ember-cli.com/) in a development environment.
",,2019-07-24T14:42:17Z,4,23,12,"('jkleinsc', 31), ('tangollama', 2), ('emadehsan', 1), ('greenkeeperbot', 1)","[3, 'Good Health and Well-Being']"
ushahidi/platform,Ushahidi Platform API version 3+,"[client]: https://github.com/ushahidi/platform-client
[download]: https://github.com/ushahidi/platform-release/releases
[setup-guides]: https://docs.ushahidi.com/platform-developer-documentation/development-and-code/setup_alternatives
[support]: https://www.ushahidi.com/support
[rest-api-docs]: https://docs.ushahidi.com/platform-developer-documentation/tech-stack/api-documentation
[getin]: https://www.ushahidi.com/support/get-involved
[issues]: https://github.com/ushahidi/platform/issues
[ush2]: https://github.com/ushahidi/Ushahidi_Web
[ushahidi]: http://ushahidi.com

Ushahidi Platform
=================

## What is Ushahidi Platform?

Ushahidi Platform is an open source web application for information collection, visualization and interactive mapping. It helps you to collect info from: SMS, Twitter, RSS feeds, Email. It helps you to process that information, categorize it, geo-locate it and publish it on a map.

This repository contains the backend code with the REST API implementation.

Head over to the [Platform Client repository][client] for the browser app code.

## Setup essentials

The shortest path to get up and running is:

- Install Docker Engine
- Install Make command (parses Makefile)
- Run `make start`

The backend will be listening on localhost:8080.

> **What about the browser client application?**

> Once your Platform backend is running, head over to the [platform-client-mzima](https://github.com/ushahidi/platform-client-mzima) repository to get the in-browser Platform experience!

### Other helpful commands

You may use `make start` to restart the containers (does a full container build).

You may use `make apply` to apply dependency and migration changes to containers (without full container build). **Note:** this requires containers to be up.
​
To stop Docker containers run `make stop`

To take everything down (including deleting the database) `make down` will do that for you.



**WIP**: to run the automated tests ...

## Manuals and documentation

### A note for grassroots organizations
If you are starting a deployment for a grassroots organization, you can apply for a free social-impact responder account [here](https://www.ushahidi.com/pricing/apply-for-free) after verifying that you meet the criteria.


### Platform User Manual

The official reference on how to use the Platform. Create surveys, configure data sources... it's all in there!
[Platform User Manual](https://docs.ushahidi.com/platform-user-manual/)

### Platform Developer Documentation

Key pointers on installing and developing on the Platform.

[Platform Developer Documentation](https://docs.ushahidi.com/platform-developer-documentation/)

## Credits

## Contributors ✨

Thanks goes to the wonderful people who [[Contribute](CONTRIBUTING.md)]! See the list of contributors at [all-contributors](docs/contributors-to-ushahidi.md)
This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

## Useful Links
- [Code of Conduct](https://docs.ushahidi.com/platform-developer-documentation/code-of-conduct)
- [Download][download]
- [Installation guides][setup-guides]
- [Developer and User Support][support]
- [REST API docs][rest-api-docs]
- [Get Involved][getin]
- [Bug tracker][issues]
- [About Ushahidi][ushahidi]
- [Ushahidi Platform v2][ush2]
",'hacktoberfest',2024-04-29T08:29:00Z,66,664,73,"('rjmackay', 2590), ('rowasc', 818), ('willdoran', 658), ('tuxpiper', 409), ('Mh-Asmi', 325), ('webong', 282), ('eyedol', 205), ('allcontributorsbot', 190), ('jasonmule', 184), ('Angamanga', 138), ('crcommons', 61), ('aMoniker', 51), ('kamaulynder', 50), ('kubabialy', 32), ('kinstella', 32), ('AmTryingMyBest', 21), ('kinstelli', 13), ('wino', 8), ('ptandler', 6), ('ushahidlee', 6), ('jrtricafort', 5), ('individual-it', 5), ('AriannaLanz', 5), ('Jenniline', 4), ('UdokaVrede', 4), ('monkeywithacupcake', 4), ('jemaltieri', 4), ('bkplaravel2018', 3), ('ps-19', 3), ('StaicyG', 3), ('kiranzack', 3), ('zhalloran', 3), ('shadowhand', 3), ('cvele', 3), ('hollycorbett', 3), ('miraxes', 2), ('ymohii', 2), ('gausam', 2), ('mohsin', 2), ('MigDinny', 2), ('muchirijane', 2), ('arelsirin-tk', 2), ('aoduor', 2), ('aitorres', 2), ('lexoyo', 2), ('renujain31', 1), ('ryanchristo', 1), ('laravel-shift', 1), ('TomC-Codes', 1), ('virgilio', 1), ('dariatsvetkova', 1), ('dependabotbot', 1), ('helen-ndip', 1), ('johnmurray4', 1), ('ljvarghese', 1), ('noone0212', 1), ('m-sameer', 1), ('leninpaulino', 1), ('jcbashdown', 1), ('gvarisco', 1), ('evansims', 1), ('tobiasziegler', 1), ('caharding', 1), ('CeciliaHinga', 1), ('ozdemirburak', 1), ('brylie', 1)","[11, 'Sustainable Cities and Communities']"
code4romania/redirectioneaza,Helping NGOs collect the 230 tax form,"# Redirectioneaza

[![GitHub contributors][ico-contributors]][link-contributors]
[![GitHub last commit][ico-last-commit]][link-last-commit]
[![License: MPL 2.0][ico-license]][link-license]

- tax form #230 made easy
- digital solution for an offline process
- as simple and as efficient as possible
- helps you compare and choose who to support
- helps NGOs reach their public and keep track of their supporters

[See the project live][link-production]

Give a short introduction of your project. Let this section explain the objectives or the motivation behind this
project.

[Contributing](#contributing) | [Built with](#built-with) | [Deployment](#deployment) | [Feedback](#feedback) | [License](#license) | [About Code for Romania](#about-code-for-romania)

## Contributing

This project is built by amazing volunteers, and you can be one of them. Here's a list of ways
in [which you can contribute to this project][link-contributing]. If you want to make any change to this repository,
please **make a fork first**.

Help us out by testing this project in the [staging environment][link-staging]. If you see something that doesn't quite
work the way you expect it to, open an Issue. Make sure to describe what you _expect to happen_ and _what is actually
happening_ in detail.

If you would like to suggest new functionality, open an Issue and mark it as a __[Feature request]__. Please be specific
about why you think this functionality will be of use. If you can, please include some visual description of what you
would like the UI to look like if you are suggesting new UI elements.

## Built With

### Programming languages

- Backend: [Python3.11](https://www.python.org/)

### Platforms

- [AWS](https://aws.amazon.com/) through [Terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)

### Frontend framework

- HTML + CSS + JS + Bootstrap

### Package managers

- [pip-tools](https://pip-tools.rtfd.io/)
- [npm](https://www.npmjs.com/)

### Database technology & provider

The project has been configured to work with the following databases:

- [PostgreSQL](https://www.postgresql.org/)
- [MySQL](https://www.mysql.com/)
- [SQLite](https://www.sqlite.org/index.html)

## Development

### Compiling the CSS with less

Go to the `backend/` folder and run the following commands:

```bash
nvm use --lts || nvm install --lts

npm i -g less@2.7.3 less-plugin-clean-css@1.5.1

cp -r bower_components/  static_extras/

pushd static_extras/

lessc css/main.less > css/main.css --clean-css=""--s1 --advanced --compatibility=ie8""

rm -rf bower_components/

popd
```

or, in a one-liner:

```bash
pushd backend/ && nvm use --lts || nvm install --lts && npm i -g less@2.7.3 less-plugin-clean-css@1.5.1 && cp -r bower_components/  static_extras/ && pushd static_extras/ && lessc css/main.less > css/main.css --clean-css=""--s1 --advanced --compatibility=ie8"" && rm -rf bower_components/ && popd && popd
```

## Deployment

### With Docker

1. Go to the root of the project
2. Run `cp .env.example .env` to create the environment file
3. Run `make run` to start the containers with an SQLite database
4. Open http://localhost:8080 in your browser

## Feedback

* Request a new feature on GitHub.
* Vote for popular feature requests.
* File a bug in GitHub Issues.
* Email us with other feedback [contact@code4.ro](mailto:contact@code4.ro) or
  on [redirectioneaza@code4.ro](mailto:redirectioneaza@code4.ro).

## License

This project is licensed under the MPL 2.0 License — see the [LICENSE](LICENSE) file for details

## About Code for Romania

Started in 2016, Code for Romania is a civic tech NGO, official member of the Code for All network. We have a community
of around 2.000 volunteers (developers, ux/ui, communications, data scientists, graphic designers, devops, it security
and more) who work pro bono for developing digital solutions to solve social problems. #techforsocialgood. If you want
to learn more details about our projects [visit our site][link-code4] or if you want to talk to one of our staff
members, please e-mail us at contact@code4.ro.

Last, but not least, we rely on donations to ensure the infrastructure, logistics and management of our community that
is widely spread across 11 timezones, coding for social change to make Romania and the world a better place. If you want
to support us, [you can do it here][link-donate].


[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job.)

[ico-contributors]: https://img.shields.io/github/contributors/code4romania/redirectioneaza.svg?style=for-the-badge
[ico-last-commit]: https://img.shields.io/github/last-commit/code4romania/redirectioneaza.svg?style=for-the-badge
[ico-license]: https://img.shields.io/badge/license-MPL%202.0-brightgreen.svg?style=for-the-badge

[link-contributors]: https://github.com/code4romania/redirectioneaza/graphs/contributors
[link-last-commit]: https://github.com/code4romania/redirectioneaza/commits/main
[link-license]: https://opensource.org/licenses/MPL-2.0
[link-contributing]: https://github.com/code4romania/.github/blob/main/CONTRIBUTING.md

[link-production]: https://redirectioneaza.ro
[link-staging]: https://redirectioneaza.staging.heroesof.tech/

[link-code4]: https://www.code4.ro/en/
[link-donate]: https://code4.ro/en/donate/
","'civic-hacking', 'civic-tech', 'civictech', 'code4ro', 'django', 'python'",2024-04-17T13:05:17Z,14,21,8,"('tudoramariei', 319), ('onel', 169), ('danniel', 79), ('andreiio', 54), ('catileptic', 46), ('ccorneliu', 22), ('andreisocrative', 19), ('aniri', 8), ('bvizureanu', 6), ('code4ro-github-bot', 3), ('bogdanivanel', 1), ('gabrielcramer', 1), ('RaduCStefanescu', 1), ('bertearazvan', 1)","[17, 'Partnerships for the Goals']"
codeforamerica/michigan-benefits,Digital Assister for Michigan,"# Michigan Benefits

Code repository for MichiganBenefits.org, a multi-benefit assistance application from
[Code for America](https://www.codeforamerica.org).

Try it at [demo.michiganbenefits.org](https://demo.michiganbenefits.org).

[![CircleCI](https://circleci.com/gh/codeforamerica/michigan-benefits.svg?style=svg)](https://circleci.com/gh/codeforamerica/michigan-benefits)
[![Code Climate](https://codeclimate.com/github/codeforamerica/michigan-benefits/badges/gpa.svg)](https://codeclimate.com/github/codeforamerica/michigan-benefits)
[![Test Coverage](https://codeclimate.com/github/codeforamerica/michigan-benefits/badges/coverage.svg)](https://codeclimate.com/github/codeforamerica/michigan-benefits/coverage)

* [GitHub](https://github.com/codeforamerica/michigan-benefits)
* [Pivotal Tracker](https://www.pivotaltracker.com/n/projects/2123705) (private)
* [InVision](https://projects.invisionapp.com/d/main#/projects/prototypes/10425326) (private)
* [CircleCI](https://circleci.com/gh/codeforamerica/michigan-benefits)
* [Staging](https://staging.michiganbenefits.org)
* [Demo](https://demo.michiganbenefits.org) - Try it out! Doesn't submit real applications.
* [Production](https://michiganbenefits.org) - Submits real applications to Michigan. Use with caution.

## Documentation

Documentation is contained in Markdown files within the repository

| File          | Purpose |
| ------------- | -----------|
| [README.md](README.md) | This root document |
| [CONTRIBUTING.md](CONTRIBUTING.md) | How we write and test code for this application |
| [DEPLOYING.md](DEPLOYING.md) | How to deploy code to production servers |
| [LICENSE.md](LICENSE.md) | Our software license |
",,2018-11-16T19:48:38Z,27,27,21,"('hartsick', 399), ('jessieay', 289), ('luigi', 203), ('jayroh', 191), ('bengolder', 87), ('rachelcope', 63), ('alltom', 42), ('dependabot-support', 40), ('zspencer', 34), ('adarsh', 29), ('wschaefer-cfa', 11), ('eahanson', 9), ('mainej', 8), ('disambiguator', 8), ('aquabu', 8), ('robhanlon22', 7), ('dbalatero', 7), ('claytonmeador', 7), ('bion', 5), ('heatherm', 5), ('dukejones', 4), ('wjjwo', 3), ('harlantwood', 3), ('alanjosephwilliams', 2), ('alexch', 2), ('racheledelman', 2), ('gleenn', 1)","[16, 'Peace, Justice and Strong Institutions']"
get-alex/alex,"Catch insensitive, inconsiderate writing","


  
  
  


> 📝 **alex** — Catch insensitive, inconsiderate writing.

[![Build][build-badge]][build]
[![Coverage][coverage-badge]][coverage]
[![First timers friendly][first-timers-badge]][first-timers]

Whether your own or someone else’s writing, **alex** helps you find gender
favoring, polarizing, race related, or other **unequal** phrasing in text.

For example, when `We’ve confirmed his identity` is given, **alex** will warn
you and suggest using `their` instead of `his`.

Give **alex** a spin on the [Online demo »][demo].

## Why

*   [x] Helps to get better at considerate writing
*   [x] Catches many possible offences
*   [x] Suggests helpful alternatives
*   [x] Reads plain text, HTML, MDX, or markdown as input
*   [x] Stylish

## Install

Using [npm][] (with [Node.js][node]):

```sh
$ npm install alex --global
```

Using [yarn][]:

```sh
$ yarn global add alex
```

Or you can follow this step-by-step tutorial:
[Setting up alex in your project][setup-tutorial]



## Contents

*   [Checks](#checks)
*   [Integrations](#integrations)
*   [Ignoring files](#ignoring-files)
    *   [`.alexignore`](#alexignore)
*   [Control](#control)
*   [Configuration](#configuration)
*   [CLI](#cli)
*   [API](#api)
    *   [`markdown(value, config)`](#markdownvalue-config)
    *   [`mdx(value, config)`](#mdxvalue-config)
    *   [`html(value, config)`](#htmlvalue-config)
    *   [`text(value, config)`](#textvalue-config)
*   [Workflow](#workflow)
*   [FAQ](#faq)
    *   [This is stupid!](#this-is-stupid)
    *   [alex didn’t check “X”!](#alex-didnt-check-x)
    *   [Why is this named alex?](#why-is-this-named-alex)
*   [Further reading](#further-reading)
*   [Contribute](#contribute)
*   [Origin story](#origin-story)
*   [Acknowledgments](#acknowledgments)
*   [License](#license)

## Checks

**alex** checks things such as:

*   Gendered work-titles (if you write `garbageman` alex suggests `garbage
    collector`; if you write `landlord` alex suggests `proprietor`)
*   Gendered proverbs (if you write `like a man` alex suggests `bravely`; if you
    write `ladylike` alex suggests `courteous`)
*   Ableist language (if you write `learning disabled` alex suggests `person
    with learning disabilities`)
*   Condescending language (if you write `obviously` or `everyone knows` alex
    warns about it)
*   Intolerant phrasing (if you write `master` and `slave` alex suggests
    `primary` and `replica`)
*   Profanities (if you write `butt` 🍑 alex warns about it)

…and much more!

Note: alex assumes good intent: that you don’t mean to offend!

See [`retext-equality`][equality] and [`retext-profanities`][profanities] for
all rules.

**alex** ignores words meant literally, so `“he”`, `He — ...`, and [the
like][literals] are not warned about.

## Integrations

*   Sublime — [`sindresorhus/SublimeLinter-contrib-alex`](https://github.com/sindresorhus/SublimeLinter-contrib-alex)
*   Gulp — [`dustinspecker/gulp-alex`](https://github.com/dustinspecker/gulp-alex)
*   Slack — [`keoghpe/alex-slack`](https://github.com/keoghpe/alex-slack)
*   Ember — [`yohanmishkin/ember-cli-alex`](https://github.com/yohanmishkin/ember-cli-alex)
*   Probot — [`swinton/linter-alex`](https://github.com/swinton/linter-alex)
*   GitHub Actions — [`brown-ccv/alex-recommends`](https://github.com/marketplace/actions/alex-recommends)
*   GitHub Actions (reviewdog) — [`reviewdog/action-alex`](https://github.com/marketplace/actions/run-alex-with-reviewdog)
*   Vim — [`w0rp/ale`](https://github.com/w0rp/ale)
*   Browser extension — [`skn0tt/alex-browser-extension`](https://github.com/skn0tt/alex-browser-extension)
*   Contentful - [`stefanjudis/alex-js-contentful-ui-extension`](https://github.com/stefanjudis/alex-js-contentful-ui-extension)
*   Figma - [`nickradford/figma-plugin-alex`](https://github.com/nickradford/figma-plugin-alex)
*   VSCode - [`tlahmann/vscode-alex`](https://github.com/tlahmann/vscode-alex)

## Ignoring files

The CLI searches for files with a markdown or text extension when given
directories (so `$ alex .` will find `readme.md` and `path/to/file.txt`).
To prevent files from being found, create an [`.alexignore`][alexignore] file.

### `.alexignore`

The CLI will sometimes [search for files][ignoring-files].
To prevent files from being found, add a file named `.alexignore` in one of the
directories above the current working directory (the place you run `alex` from).
The format of these files is similar to [`.eslintignore`][eslintignore] (which
in turn is similar to `.gitignore` files).

For example, when working in `~/path/to/place`, the ignore file can be in
`to`, `place`, or `~`.

The ignore file for [this project itself][.alexignore] looks like this:

```txt
# `node_modules` is ignored by default.
example.md
```

## Control

Sometimes **alex** makes mistakes:

```markdown
A message for this sentence will pop up.
```

Yields:

```txt
readme.md
  1:15-1:18  warning  `pop` may be insensitive, use `parent` instead  dad-mom  retext-equality

⚠ 1 warning
```

HTML comments in Markdown can be used to ignore them:

```markdown


A message for this sentence will **not** pop up.
```

Yields:

```txt
readme.md: no issues found
```

`ignore` turns off messages for the thing after the comment (in this case, the
paragraph).
It’s also possible to turn off messages after a comment by using `disable`, and,
turn those messages back on using `enable`:

```markdown


A message for this sentence will **not** pop up.

A message for this sentence will also **not** pop up.

Yet another sentence where a message will **not** pop up.



A message for this sentence will pop up.
```

Yields:

```txt
readme.md
  9:15-9:18  warning  `pop` may be insensitive, use `parent` instead  dad-mom  retext-equality

⚠ 1 warning
```

Multiple messages can be controlled in one go:

```md

```

…and all messages can be controlled by omitting all rule identifiers:

```md

```

## Configuration

You can control **alex** through `.alexrc` configuration files:

```json
{
  ""allow"": [""boogeyman-boogeywoman""]
}
```

…you can use YAML if the file is named `.alexrc.yml` or `.alexrc.yaml`:

```yml
allow:
  - dad-mom
```

…you can also use JavaScript if the file is named `.alexrc.js`:

```js
// But making it random like this is a bad idea!
exports.profanitySureness = Math.floor(Math.random() * 3)
```

…and finally it is possible to use an `alex` field in `package.json`:

```txt
{
  …
  ""alex"": {
    ""noBinary"": true
  },
  …
}
```

The `allow` field should be an array of rules or `undefined` (the default is
`undefined`).  When provided, the rules specified are skipped and not reported.

The `deny` field should be an array of rules or `undefined` (the default is
`undefined`).  When provided, *only* the rules specified are reported.

You cannot use both `allow` and `deny` at the same time.

The `noBinary` field should be a boolean (the default is `false`).
When turned on (`true`), pairs such as `he and she` and `garbageman or
garbagewoman` are seen as errors.
When turned off (`false`, the default), such pairs are okay.

The `profanitySureness` field is a number (the default is `0`).
We use [`cuss`][cuss], which has a dictionary of words that have a rating
between 0 and 2 of how likely it is that a word or phrase is a profanity (not
how “bad” it is):

| Rating | Use as a profanity | Use in clean text | Example  |
| ------ | ------------------ | ----------------- | -------- |
| 2      | likely             | unlikely          | `asshat` |
| 1      | maybe              | maybe             | `addict` |
| 0      | unlikely           | likely            | `beaver` |

The `profanitySureness` field is the minimum rating (including) that you want to
check for.
If you set it to `1` (maybe) then it will warn for level `1` *and* `2` (likely)
profanities, but not for level `0` (unlikely).

## CLI



![][screenshot]

Let’s say `example.md` looks as follows:

```markdown
The boogeyman wrote all changes to the **master server**. Thus, the slaves
were read-only copies of master. But not to worry, he was a cripple.
```

Now, run **alex** on `example.md`:

```sh
$ alex example.md
```

Yields:

```txt
example.md
   1:5-1:14  warning  `boogeyman` may be insensitive, use `boogeymonster` instead                boogeyman-boogeywoman  retext-equality
  1:42-1:48  warning  `master` / `slaves` may be insensitive, use `primary` / `replica` instead  master-slave           retext-equality
  1:69-1:75  warning  Don’t use `slaves`, it’s profane                                           slaves                 retext-profanities
  2:52-2:54  warning  `he` may be insensitive, use `they`, `it` instead                          he-she                 retext-equality
  2:61-2:68  warning  `cripple` may be insensitive, use `person with a limp` instead             gimp                   retext-equality

⚠ 5 warnings
```

See `$ alex --help` for more information.

> When no input files are given to **alex**, it searches for files in the
> current directory, `doc`, and `docs`.
> If `--mdx` is given, it searches for `mdx` extensions.
> If `--html` is given, it searches for `htm` and `html` extensions.
> Otherwise, it searches for `txt`, `text`, `md`, `mkd`, `mkdn`, `mkdown`,
> `ron`, and `markdown` extensions.

## API

This package is [ESM only](https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c):
Node 14+ is needed to use it and it must be `import`ed instead of `require`d.

[npm][]:

```sh
$ npm install alex --save
```

This package exports the identifiers `markdown`, `mdx`, `html`, and `text`.
The default export is `markdown`.

### `markdown(value, config)`

Check Markdown (ignoring syntax).

###### Parameters

*   `value` ([`VFile`][vfile] or `string`) — Markdown document
*   `config` (`Object`, optional) — See the [Configuration][] section

###### Returns

[`VFile`][vfile].
You are probably interested in its [`messages`][vfile-message] property, as
shown in the example below, because it holds the possible violations.

###### Example

```js
import alex from 'alex'

alex('We’ve confirmed his identity.').messages
```

Yields:

```js
[
  [1:17-1:20: `his` may be insensitive, when referring to a person, use `their`, `theirs`, `them` instead] {
    message: '`his` may be insensitive, when referring to a ' +
      'person, use `their`, `theirs`, `them` instead',
    name: '1:17-1:20',
    reason: '`his` may be insensitive, when referring to a ' +
      'person, use `their`, `theirs`, `them` instead',
    line: 1,
    column: 17,
    location: { start: [Object], end: [Object] },
    source: 'retext-equality',
    ruleId: 'her-him',
    fatal: false,
    actual: 'his',
    expected: [ 'their', 'theirs', 'them' ]
  }
]
```

### `mdx(value, config)`

Check [MDX][] (ignoring syntax).

> Note: the syntax for [MDX@2][mdx-next], while currently in beta, is used in
> alex.

###### Parameters

*   `value` ([`VFile`][vfile] or `string`) — MDX document
*   `config` (`Object`, optional) — See the [Configuration][] section

###### Returns

[`VFile`][vfile].

###### Example

```js
import {mdx} from 'alex'

mdx('He walked to class.').messages
```

Yields:

```js
[
  [1:12-1:14: `He` may be insensitive, use `They`, `It` instead] {
    reason: '`He` may be insensitive, use `They`, `It` instead',
    line: 1,
    column: 12,
    location: { start: [Object], end: [Object] },
    source: 'retext-equality',
    ruleId: 'he-she',
    fatal: false,
    actual: 'He',
    expected: [ 'They', 'It' ]
  }
]
```

### `html(value, config)`

Check HTML (ignoring syntax).

###### Parameters

*   `value` ([`VFile`][vfile] or `string`) — HTML document
*   `config` (`Object`, optional) — See the [Configuration][] section

###### Returns

[`VFile`][vfile].

###### Example

```js
import {html} from 'alex'

html('He walked to class.').messages
```

Yields:

```js
[
  [1:18-1:20: `He` may be insensitive, use `They`, `It` instead] {
    message: '`He` may be insensitive, use `They`, `It` instead',
    name: '1:18-1:20',
    reason: '`He` may be insensitive, use `They`, `It` instead',
    line: 1,
    column: 18,
    location: { start: [Object], end: [Object] },
    source: 'retext-equality',
    ruleId: 'he-she',
    fatal: false,
    actual: 'He',
    expected: [ 'They', 'It' ]
  }
]
```

### `text(value, config)`

Check plain text (as in, syntax is checked).

###### Parameters

*   `value` ([`VFile`][vfile] or `string`) — Text document
*   `config` (`Object`, optional) — See the [Configuration][] section

###### Returns

[`VFile`][vfile].

###### Example

```js
import {markdown, text} from 'alex'

markdown('The `boogeyman`.').messages // => []

text('The `boogeyman`.').messages
```

Yields:

```js
[
  [1:6-1:15: `boogeyman` may be insensitive, use `boogeymonster` instead] {
    message: '`boogeyman` may be insensitive, use `boogeymonster` instead',
    name: '1:6-1:15',
    reason: '`boogeyman` may be insensitive, use `boogeymonster` instead',
    line: 1,
    column: 6,
    location: Position { start: [Object], end: [Object] },
    source: 'retext-equality',
    ruleId: 'boogeyman-boogeywoman',
    fatal: false,
    actual: 'boogeyman',
    expected: [ 'boogeymonster' ]
  }
]
```

## Workflow

The recommended workflow is to add **alex** to `package.json` and to run it with
your tests in Travis.

You can opt to ignore warnings through [alexrc][configuration] files and
[control comments][control].

A `package.json` file with [npm scripts][npm-scripts], and additionally using
[AVA][] for unit tests, could look like so:

```json
{
  ""scripts"": {
    ""test-api"": ""ava"",
    ""test-doc"": ""alex"",
    ""test"": ""npm run test-api && npm run test-doc""
  },
  ""devDependencies"": {
    ""alex"": ""^1.0.0"",
    ""ava"": ""^0.1.0""
  }
}
```

If you’re using Travis for continuous integration, set up something like the
following in your `.travis.yml`:

```diff
 script:
 - npm test
+- alex --diff
```

Make sure to still install alex though!

If the `--diff` flag is used, and Travis is detected, lines that are not changes
in this push are ignored.
Using this workflow, you can merge PRs if it has warnings, and then if someone
edits an entirely different file, they won’t be bothered about existing
warnings, only about the things they added!

## FAQ





### This is stupid!

Not a question.
And yeah, alex isn’t very smart.
People are much better at this.
But people make mistakes, and alex is there to help.

### alex didn’t check “X”!

See [`contributing.md`][contributing] on how to get “X” checked by alex.

### Why is this named alex?

It’s a nice unisex name, it was free on npm, I like it!  :smile:



## Further reading

No automated tool can replace studying inclusive communication and listening to
the lived experiences of others.
An error from `alex` can be an invitation to learn more.
These resources are a launch point for deepening your own understanding and
editorial skills beyond what `alex` can offer:

*   The [18F Content Guide](https://content-guide.18f.gov/our-style/inclusive-language/)
    has a helpful list of links to other inclusive language guides used in
    journalism and academic writing.
*   The [Conscious Style Guide](https://consciousstyleguide.com/articles/) has
    articles on many nuanced topics of language.  For example, the terms race
    and ethnicity mean different things, and choosing the right word is up to
    you.
    Likewise, a sentence that overgeneralizes about a group of people
    (e.g. “Developers love to code all day”) may not be noticed by `alex`, but
    it is not inclusive.  A good human editor can step up to the challenge and
    find a better way to phrase things.
*   Sometimes, the only way to know what is inclusive is to ask.
    In [Disability is a nuanced thing](https://incl.ca/disability-language-is-a-nuanced-thing/),
    Nicolas Steenhout writes about how person-first language, such as
    “a person with a disability,” is not always the right choice.
*   Language is always evolving.  A term that is neutral one year ago can be
    problematic today.  Projects like the
    [Self-Defined Dictionary](https://github.com/selfdefined/web-app) aim to
    collect the words that we use to define ourselves and others, and connect
    them with the history and some helpful advice.
*   Unconscious bias is present in daily decisions and conversations and can show
    up in writing.
    [Textio](https://textio.com/blog/4-overlooked-types-of-bias-in-business-writing/27521593662)

    offers some examples of how descriptive adjective choice and tone can push
    some people away, and how regional language differences can cause confusion.
*   Using complex sentences and uncommon vocabulary can lead to less inclusive
    content.  This is described as literacy exclusion in
    [this article by Harver](https://harver.com/blog/inclusive-job-descriptions/).
    This is critical to be aware of if your content has a global audience,
    where a reader’s strongest language may not be the language you are writing
    in.

## Contribute

See [`contributing.md`][contributing] in [`get-alex/.github`][health] for ways
to get started.
See [`support.md`][support] for ways to get help.

This project has a [Code of conduct][coc].
By interacting with this repository, organization, or community you agree to
abide by its terms.

## Origin story

Thanks to [**@iheanyi**][iheany] for [raising the problem][tweet] and
[**@sindresorhus**][sindre] for inspiring me ([**@wooorm**][wooorm]) to do
something about it.

When alex launched, it got some traction on [twitter][] and [producthunt][].
Then there was a [lot][tnw] [of][dailydot] [press][vice] [coverage][bustle].

## Acknowledgments

Preliminary work for alex was done [in 2015][preliminary].
The project was authored by [**@wooorm**][wooorm].

Lots of [people helped since][contributors]!

## License

[MIT][license] © [Titus Wormer][author]



[build]: https://github.com/get-alex/alex/actions

[build-badge]: https://github.com/get-alex/alex/workflows/main/badge.svg

[coverage]: https://codecov.io/github/get-alex/alex

[coverage-badge]: https://img.shields.io/codecov/c/github/get-alex/alex.svg

[first-timers]: https://www.firsttimersonly.com/

[first-timers-badge]: https://img.shields.io/badge/first--timers--only-friendly-blue.svg

[node]: https://nodejs.org/en/download/

[npm]: https://docs.npmjs.com/cli/install

[yarn]: https://yarnpkg.com/

[setup-tutorial]: https://dev.to/meeshkan/setting-up-the-alex-js-language-linter-in-your-project-3bpl

[demo]: http://alexjs.com/#demo

[screenshot]: screenshot.png

[vfile]: https://github.com/vfile/vfile

[profanities]: https://github.com/retextjs/retext-profanities/blob/main/rules.md

[equality]: https://github.com/retextjs/retext-equality/blob/main/rules.md

[vfile-message]: https://github.com/vfile/vfile#vfilemessages

[literals]: https://github.com/syntax-tree/nlcst-is-literal#isliteralparent-index

[eslintignore]: http://eslint.org/docs/user-guide/configuring.html#ignoring-files-and-directories

[cuss]: https://github.com/words/cuss

[npm-scripts]: https://docs.npmjs.com/misc/scripts

[ava]: http://ava.li

[author]: http://wooorm.com

[health]: https://github.com/get-alex/.github

[contributing]: https://github.com/get-alex/.github/blob/main/contributing.md

[support]: https://github.com/get-alex/.github/blob/main/support.md

[coc]: https://github.com/get-alex/.github/blob/main/code-of-conduct.md

[tweet]: https://twitter.com/kwuchu/status/618799087006130176

[twitter]: https://twitter.com/wooorm/status/639123753490907136

[producthunt]: https://www.producthunt.com/posts/alex

[tnw]: http://thenextweb.com/apps/2015/09/11/alex-stops-you-from-publishing-inconsiderate-content/

[vice]: https://www.vice.com/en_us/article/nzeawx/meet-alex-the-javascript-tool-to-make-your-code-less-offensive

[bustle]: https://www.bustle.com/articles/108684-alex-javascript-tool-corrects-harmful-language-in-your-writing-because-there-are-some-mistakes-spell-check

[dailydot]: https://www.dailydot.com/debug/alex-coding-tool-offensive/

[iheany]: https://github.com/iheanyi

[sindre]: https://github.com/sindresorhus

[wooorm]: https://github.com/wooorm

[preliminary]: https://github.com/get-alex/alex/commit/3621b0a

[contributors]: https://github.com/get-alex/alex/graphs/contributors

[.alexignore]: .alexignore

[license]: license

[control]: #control

[configuration]: #configuration

[ignoring-files]: #ignoring-files

[alexignore]: #alexignore

[mdx]: https://mdxjs.com

[mdx-next]: https://github.com/mdx-js/mdx/issues/1041
","'alex', 'hacktoberfest2018', 'linter', 'writing'",2023-12-05T21:51:24Z,37,4754,70,"('wooorm', 235), ('sindresorhus', 8), ('greenkeeperbot', 7), ('greenkeeperio-bot', 4), ('shinnn', 3), ('jenweber', 2), ('riley-martine', 2), ('LMulvey', 1), ('mcmcgrath13', 1), ('mitchstewart08', 1), ('nickradford', 1), ('rickhanlonii', 1), ('edorado93', 1), ('SheetJSDev', 1), ('Skn0tt', 1), ('taylorreece', 1), ('tlahmann', 1), ('yohanmishkin', 1), ('vaishnavi-janardhan', 1), ('a1346054', 1), ('haya14busa', 1), ('nikewall', 1), ('k-yle', 1), ('kjacobsen', 1), ('kanadgupta', 1), ('julienw', 1), ('jdalton', 1), ('erbridge', 1), ('danoszz', 1), ('coliff', 1), ('mansona', 1), ('bpugh', 1), ('MrBenJ', 1), ('anseljh', 1), ('alexwoollam', 1), ('alexgleason', 1), ('gotham13', 1)","[11, 'Sustainable Cities and Communities']"
rapidpro/ureport,Dashboard module for UReport working against RapidPro data,"# U-Report 

[![Build Status](https://github.com/rapidpro/ureport/workflows/CI/badge.svg)](https://github.com/rapidpro/ureport/actions?query=workflow%3ACI) 
[![codecov](https://codecov.io/gh/rapidpro/ureport/branch/main/graph/badge.svg)](https://codecov.io/gh/rapidpro/ureport)

This is the U-Report dashboard built on data collected by RapidPro.

Built for UNICEF by Nyaruka - http://nyaruka.com

Getting Started
================

Install dependencies
```
% pip install --upgrade pip poetry
% poetry install --no-root
% poetry shell
```

Link up a settings file (you'll need to create the postgres db first, username: 'ureport' password: 'nyaruka')
```
% ln -s ureport/settings.py.postgres ureport/settings.py
```

Sync the database, add all our models and create our superuser
```
% python manage.py syncdb
% python manage.py migrate
% python manage createsuper
% python manage collectstatic
```

At this point everything should be good to go, you can start with:

```
% python manage.py runserver
```

Note that the endpoint called for API calls is by default 'localhost:8001', you can uncomment the RAPIDPRO_API line in settings.py.postgres to go against production servers.
",,2024-05-02T13:14:08Z,23,23,14,"('norkans7', 3668), ('nicpottier', 159), ('teehamaral', 122), ('johncordeiro', 73), ('transifex-integrationbot', 67), ('musamusa', 52), ('hudsonbrendon', 41), ('xkmato', 30), ('matmsa27', 29), ('dependabotbot', 19), ('edudouglas', 16), ('paulobernardoaf', 13), ('rowanseymour', 10), ('alviriseup', 10), ('brunocavalcantelima', 7), ('WilliamTales', 7), ('ericnewcomer', 5), ('elitonzky', 5), ('bslindoso', 4), ('rafaelbreno', 3), ('Sandro-Meireles', 3), ('evansmurithi', 1), ('pauloabreu', 1)","[16, 'Peace, Justice and Strong Institutions']"
WildMeOrg/Wildbook,"Wild Me's first product, Wildbook supports researchers by allowing collaboration across the globe and automation of photo ID matching","# Wildbook

Wildbook is an open source software framework to support mark-recapture, molecular ecology, and social ecology studies. The biological and statistical communities already support a number of excellent tools, such as Program MARK,GenAlEx, and SOCPROG for use in analyzing wildlife data. Wildbook is a complementary software application that:

- provides a scalable and collaborative platform for intelligent wildlife data storage and management, including advanced, consolidated searching

- provides an easy-to-use software suite of functionality that can be extended to meet the needs of wildlife projects, especially where individual identification is used

- provides an API to support the easy export of data to cross-disciplinary analysis applications (e.g., GenePop ) and other software (e.g., Google Earth)

- provides a platform that supports the exposure of data in biodiversity databases (e.g., GBIF and OBIS)

- provides a platform for animal biometrics that supports easy data access and facilitates matching application deployment for multiple species

## Getting Started with Wildbook
Wildbook is a long-standing tool that support a wide variety of researchers and species. The Wild Me team is working on revamping the tool as a true open source project, so if you have ideas and are excited to help, reach out to us on the [Wild Me Development Discord](https://discord.gg/zw4tr3RE4R)!

## Pull Request Workflow
All contributions should be made from a fork off of the Wildbook repo. While there are a number of repositories for specific Wildbook communities, large scale development is driven from the main repository. 

### Fork Wildbook
To start, you will need to be signed in to your GitHub account, have admin access to your OS's terminal, and have Git installed.
1. From your browser, in the top right corner of the [Wildbook repo](https://github.com/WildMeOrg/Wildbook), click the **Fork** button. Confirm to be redirected to your own fork (check the url for your USERNAME in the namespace).
1. In your terminal, enter the command `git clone https://github.com/USERNAME/Wildbook`
1. Once the Wildbook directory becomes available in your working directory, move to it with the command `cd Wildbook`
1. Add a reference to the original repo, denoting it as the upstream repo.
```
git remote add upstream https://github.com/WildMeOrg/Wildbook
git fetch upstream
```

### Create Local Branch
You will want to work in a branch when doing any feature development you want to provide to the original project.
1. Verify you are on the main branch. The branch you have checked out will be used as the base for your new branch, so you typically want to start from main.
`git checkout main`
1. Create your feature branch. It can be helpful to include the issue number (ISSUENUMBER) you are working to address.
`git branch ISSUENUMBER-FEATUREBRANCHNAME`
1. Change to your feature branch so your changes are grouped together.
`git checkout ISSUENUMBER-FEATUREBRANCHNAME`
1. Update your branch (this is not needed if you just created new branch, but is a good habit to get into).
` git pull upstream main`

### Set Up Development Environment with Docker
For easiest development, you will need to set up your development environment to work with Docker. See `devops/development/README.md` for detailed instructions.

### Deploy drontend
To setup frontend, we need to deploy the React build to Wildbook, please follow the detailed instructions provided in the `frontend/README.md` file within the project directory.

### Making Local Changes
Make the code changes necessary for the issue you're working on. The following git commands may prove useful.

* `git log`: lastest commits of current branch
* `git status`: current staged and unstaged modifications
* `git diff --staged`:  the differences between the staging area and the last commit
* `git add : add files that have changes to staging in preparation for commit
* `git commit`: commits the stagged files, opens a text editor for you to write a commit log

### Submit PR
Up to this point, all changes have been done to your local copy of Wildbook. You need to push the new commits to a remote branch to start the PR process.

1. Now's the time clean up your PR if you choose to squash commits, but this is not required. If you're looking for more information on these practices, see this [pull request tutorial](https://yangsu.github.io/pull-request-tutorial/).
1. Push to the remote version of your branch ` git push  `
`git push origin ISSUENUMBER-FEATUREBRANCHNAME`
1. When prompted, provide your username and GitHub Personal Access Token. If you do not have a GitHub Personal Access Token, or do not have one with the correct permissions for your newly forked repository, you will need to [create a Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).
1. Check the fork's page on GitHub to verify that you see a new branch with your added commits. You should see a line saying ""This branch is X commits ahead"" and a **Pull request** link. 
1. Click the **Pull request** link to open a form that says ""Able to merge"". (If it says there are merge conflicts, go the  for help).
1. Use an explicit title for the PR and provide details in the comment area. Details can include text, or images, and should provide details as to what was done and why design decisions were made.
1. Click **Create a pull request**. 
 
### Respond to feedback
At this point, it's on us to get you feedback on your submission! Someone from the Wild Me team will review the project and provide any feedback that may be necessary. If changes are recommended, you'll need to checkout the branch you were working from, update the branch, and make these changes locally.

1. `git checkout ISSUENUMBER-FEATUREBRANCHNAME`
1. `git pull upstream main`
1. Make required changes
1. `git add ` for all files impacted by changes
1. Determine which method would be most appropriate for updating your PR  
  * `git commit --ammend` if the changes are small stylistic changes
  * `git commit` if the changes involved significant rework and require additional details

## Machine Learning in Wildbook

Wildbook leverages [Wildbook IA (WBIA)](https://github.com/WildbookOrg/wildbook-ia) as the machine learning engine, which pulls data from Wildbook servers to detect features in images and identify individual animals. WBIA brings massive-scale computer vision to wildlife research.

## Need direct help?

Wild Me (wildme.org) engineering staff provide support for Wildbook. You can contact us at: opensource@wildme.org

We provide support during regular office hours on Mondays and Tuesdays.

## Support resources
* User documentation is available at [Wild Me Documentation](http://docs.wildme.org)
* For user support, visit the [Wild Me Community Forum](https://community.wildme.org)
* For developer support, visit the [Wild Me Development Discord](https://discord.gg/zw4tr3RE4R)
* Email the team at opensource@wildme.org

## Contribution Guidelines

### Variable naming conventions
* Camel case
* Don’t use single-letter variable names (no matter how temporary you think the code is)
* Code should be clear enough to speak for itself without comments, but use your judgement on if a comment is necessary
* Code for clarity rather than for efficiency (one-liners are cool, but not at the expense of future obfuscation)

### Overall outline of code framework<
Spell out how .jsp files relate to servlet files relate to java files, etc. Someone new to the codebase should be able to orient themselves based on your notes.

### Java/jsp style
Initialize variables and type signatures at the abstract/interface level when possible.

Instead of:

```
ArrayList encounters = new ArrayList();
...
public int getMax(ArrayList numbers) {
```

Try:

```
List encounters = new ArrayList();
...
public int getMax(Collection numbers) {
```

It’s easier to read and more intuitive for a function to take a Map or List than a HashMap or ArrayList.

The List interface defines how we want that variable to behave, and whether it’s an ArrayList or LinkedList is incidental. Keeping the variable and method signatures abstract means we can change the implementation later (eg swapping ArrayList->LinkedList) without changing the rest of our code.
https://stackoverflow.com/questions/2279030/type-list-vs-type-arraylist-in-java

Related: when writing utility methods, making the input type as abstract as possible makes the method versatile. See Util.asSortedList in Wildbook: since the input is an abstract Collection, it can accept a List, Set, PriorityQueue, or Vector as input, and return a sorted List.

Runtime (not style): Use Sets (not Lists or arrays) if you’re only keeping track of collection membership / item uniqueness. 

Instead of:

```
    	List uniqueIndividuals = new ArrayList();
    	for(Encounter currentEncounter: encounters){
		MarkedIndividual currentInd = enc.getIndividual();
		if !(uniqueIndividuals.contains(currentInd) {
			uniqueIndividuals.add(currentInd);
			doStuff();
```
      			
Try:

```
Set uniqueIndividuals = new HashSet();	
    	for(Encounter currentEncounter: encounters){
		MarkedIndividual currentInd = enc.getIndividual();
		if !(uniqueIndividuals.contains(currentInd) {
			uniqueIndividuals.add(currentInd);
			doStuff();
```

The reason is a little deep in the data types. Sets are defined as unordered collections of unique elements; and Lists/arrays are ordered collections with no bearing on element-uniqueness. If the order of a collection doesn’t matter and you’re just checking membership, you’ll have faster runtime using a Set.

Sets implement contains, add, and remove methods much faster than lists [contains is O(log(n)) vs O(n) runtime]. A list has to iterate through the entire list every time it runs contains (it checks each item once at a time) while a set (especially a HashSet) keeps track of an item index for quick lookup.


Use for-each loops aka “enhanced for loops” to make loops more concise and readable.

Instead of:

```
for (int i=0; i<encounters.length(); i++) {
	Encounter enc = encounters.get(i)
	doStuff();
```

try:

```
for (Encounter enc: encounters) {
	doStuff();
```

Note that in both cases you might want to check if `encounters == null` if relevant, but you rarely need to check if `encounters.length()>0` because the for-loops take care of that.

Also note that if you want access to the `i` variable for logging or otherwise, the classic for-loop is best.


`Util.stringExists` is shorthand for a common string check:

Instead of:

```
	if (str!=Null && !str.equals("""")) {
		doStuff();
```
 
Try:

```
	if (Util.stringExists(str)) {
		doStuff();
```

This method also checks for the strings “none” and “unknown” which have given us trouble in displays in the past.

## History
Wildbook started as a collaborative software platform for globally-coordinated whale shark (Rhincodon typus ) research as deployed in the Wildbook for Whale Sharks (now part of http://www.sharkbook.ai). After many requests to use our software outside of whale shark research, it is now an open source, community-maintained standard for mark-recapture studies.


Wildbook is a registered trademark of [Conservation X Labs](https://conservationxlabs.com/), a 501(c)(3) non-profit organization, and is supported by the [Wild Me](https://wildme.org) team.
","'cloud', 'conservation', 'java', 'nonprofit', 'tomcat'",2024-05-03T15:28:46Z,36,94,17,"('holmbergius', 2295), ('naknomum', 2276), ('RichardJune', 996), ('colinwkingen', 942), ('drewblount', 875), ('Atticus29', 586), ('JasonWildMe', 565), ('gpoautomation', 395), ('gwinstanley', 272), ('schibel21', 179), ('annarbecker', 131), ('Katacka', 114), ('erinz2020', 96), ('cwainner', 74), ('marcial21', 70), ('TanyaStere42', 61), ('coverbeck', 46), ('crowmagnumb', 41), ('mbrecunier', 40), ('uturunku1', 30), ('mattmcla', 28), ('sufwankhalid1', 26), ('Starstew', 19), ('SeanGillespie', 18), ('ecostats', 16), ('dependabotbot', 8), ('hoangj21', 7), ('fjoyce', 5), ('bluemellophone', 5), ('doinkythederp', 3), ('goddesswarship', 3), ('sebrecht20', 3), ('brmscheiner', 2), ('EstiShay', 2), ('Veda-Gogineni', 2), ('NNattoji', 1)","[15, 'Life On Land']"
farmOS/farmOS,farmOS: A web-based farm record keeping application.,"# farmOS

[![Licence](https://img.shields.io/badge/Licence-GPL%202.0-blue.svg)](https://opensource.org/licenses/GPL-2.0/)
[![Release](https://img.shields.io/github/release/farmOS/farmOS.svg?style=flat)](https://github.com/farmOS/farmOS/releases)
[![Last commit](https://img.shields.io/github/last-commit/farmOS/farmOS.svg?style=flat)](https://github.com/farmOS/farmOS/commits)
[![Docker](https://img.shields.io/docker/pulls/farmos/farmos.svg)](https://hub.docker.com/r/farmos/farmos/)
[![Chat](https://img.shields.io/matrix/farmOS:matrix.org.svg)](https://app.element.io/#/room/#farmOS:matrix.org)
[![Backers on Open Collective](https://opencollective.com/farmOS/backers/badge.svg)](#backers)
[![Sponsors on Open Collective](https://opencollective.com/farmOS/sponsors/badge.svg)](#sponsors)

farmOS is a web-based application for farm management, planning, and record
keeping. It is developed by a community of volunteers and aims to provide a
standard platform for farmers, developers, and researchers to build upon.

Official website: [farmOS.org](https://farmOS.org)

## GETTING STARTED

If you would like to install and host farmOS yourself, see the official
documentation on farmOS.org: https://farmOS.org/hosting/install

If you would like to pay for hosting, [Farmier](https://farmier.com) provides
affordable options for individual farms and organizations.

## MAINTAINERS

Current maintainers:
 * Michael Stenta (m.stenta) - https://drupal.org/user/581414

This project has been sponsored by:
 * [Farmier](http://farmier.com)
 * [Cornell University](http://www.cornell.edu)
 * [Vermont Agency of Agriculture Food & Markets](http://agriculture.vermont.gov)
 * [Vermont Housing & Conservation Board](http://www.vhcb.org)
 * [UVM Extension](https://www.uvm.edu/extension)
 * [Globetrotter Foundation](http://globetrotterfoundation.org)
 * [Vermont Vegetable and Berry Growers Association](http://www.uvm.edu/vtvegandberry)
 * [Pennsylvania Association for Sustainable Agriculture](https://pasafarming.org)
 * [Natural Resources Conservation Service](https://www.nrcs.usda.gov)
 * [The United States Forest Service - International Programs](https://www.fs.fed.us/about-agency/international-programs)
 * [The National Forestry Authority of Uganda](https://www.nfa.org.ug/)
 * [Our Sci](http://our-sci.net)
 * [Bionutrient Food Association](https://bionutrient.org)
 * [Foundation for Food and Agriculture Research](https://foundationfar.org/)
 * [PVAMU College of Agriculture and Human Sciences](https://www.pvamu.edu/cahs/)
 * [Rothamsted Research](https://www.rothamsted.ac.uk/)
 * [OpenTEAM](https://openteam.community)
 * [Wolfe's Neck Center for Agriculture and the Environment](https://www.wolfesneck.org)

## CONTRIBUTORS

This project exists thanks to all the people who contribute.


## OPENCOLLECTIVE BACKERS

Thank you to all our OpenCollective backers! [[Become a backer](https://opencollective.com/farmOS#backer)]



## OPENCOLLECTIVE SPONSORS

Support this project by becoming an OpenCollective sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/farmOS#sponsor)]











","'agriculture', 'crops', 'drupal', 'equipment', 'farm', 'livestock', 'mapping', 'sensors', 'soil'",2024-05-02T19:12:34Z,37,914,78,"('mstenta', 6777), ('paul121', 1179), ('jgaehring', 48), ('symbioquine', 44), ('pcambra', 13), ('wotnak', 8), ('AlMaVizca', 6), ('jgOhYeah', 6), ('Skipper-is', 5), ('calbasi', 4), ('Farmer-Eds-Shed', 3), ('s33a', 3), ('rjcorwin', 3), ('tangix', 2), ('chrowe', 2), ('and712', 2), ('SirSundays', 2), ('erlarese', 2), ('anstosa', 2), ('svenn71', 1), ('komatek21', 1), ('dwblair', 1), ('cavebeat', 1), ('botlfarm', 1), ('beagaliana', 1), ('sokoow', 1), ('ppetru', 1), ('thedude459', 1), ('almostengr', 1), ('julianfoad', 1), ('monkeywithacupcake', 1), ('hasorli', 1), ('braughtg', 1), ('Fosten', 1), ('penyaskito', 1), ('Sidiox', 1), ('buckaroogeek', 1)","[9, 'Industry, Innovation and Infrastructure']"
openmrs/openmrs-contrib-android-client,Android client for OpenMRS,"[![Logo](http://i.imgur.com/fpVkTZk.png)](http://www.openmrs.org)

OpenMRS Android Client and SDK
==============================

[![Build Status Travis](https://travis-ci.org/openmrs/openmrs-contrib-android-client.svg?branch=master)](https://travis-ci.org/openmrs/openmrs-contrib-android-client) [![Build Status AppVeyor](https://ci.appveyor.com/api/projects/status/github/openmrs/openmrs-contrib-android-client?branch=master&svg=true)](https://ci.appveyor.com/project/AvijitGhosh82/openmrs-contrib-android-client) [![Demo Server](https://img.shields.io/badge/demo-online-green.svg)](http://devtest04.openmrs.org:8080/openmrs) [![GitHub version](https://d25lcipzij17d.cloudfront.net/badge.svg?id=gh&type=6&v=2.8.1&x2=0)](https://github.com/openmrs/openmrs-contrib-android-client/releases/latest) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/37fa8e86a3cb4256a3b7ffcc644f13c6)](https://www.codacy.com/app/marzeion-tomasz/openmrs-contrib-android-client?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=openmrs/openmrs-contrib-android-client&amp;utm_campaign=Badge_Grade) [![codecov](https://codecov.io/gh/openmrs/openmrs-contrib-android-client/branch/master/graph/badge.svg)](https://codecov.io/gh/f4ww4z/openmrs-contrib-android-client) [![IRC](https://img.shields.io/badge/IRC-%23openmrs-1e72ff.svg?style=flat)](http://irc.openmrs.org)

## Table of Contents
- [OpenMRS Android Client](#OpenMRS-Android-Client)
   - [Table of Contents](#Table-of-Contents)
- [Description](#Description)
         - [Key Features](#Key-Features)
- [Screenshots](#Screenshots)
- [Demo Server](#Demo-Server)
            - [Demo Username: admin](#Demo-Username-admin)
            - [Demo Password: Admin123](#Demo-Password-Admin123)
- [Releasing [Collaborators only]](#Releasing-Collaborators-only)
- [License](#License)
- [Resources](#Resources)

# Description
* The purpose of this project is to provide an OpenMRS client for Android devices. The app is designed to cover most of the functionality currently on the web application.
The app communicates with OpenMRS instances using REST. It supports working offline (without a network connection). The database on the device is encrypted and password-protected to secure patient data.
* In addition to the OpenMRS-Android-Client, this repository also has openmrs-android-sdk package which serves as an sdk which helps to build custom UI on top of the core functionality it provides.

# Development
* We use JIRA to track issues and monitor project development. Refer to this link to view all issues and project summary: [Android Client JIRA](https://issues.openmrs.org/browse/AC). 
To get started contributing, try working on [introductory issues](https://issues.openmrs.org/issues/?filter=17165) in JIRA and check out [OpenMRS Pull Request Tips](https://wiki.openmrs.org/display/docs/Pull+Request+Tips).
Also, before creating a pull request, please run code review tools (Lint) and all tests.

* There is a detailed guide for setting up the OpenMRS-Android-Client locally, before starting to contribute to the project [here](CONTRIBUTING.md).

#### 1. openmrs-android-sdk package
* There was a need to make the app extendable without forking it out, instead just adding it as a dependency in any android application which wants to use the functionality but with a custom UI on top of it.
* We can add the implementation    
``` 
    dependencies 
    {
        implementation 'com.github.openmrs:openmrs-contrib-android-client:deploy-android-sdk-SNAPSHOT'
    } 
```
as a dependency in the app module build.gradle to get the functionality provided by openmrs-android-sdk.
* The openmrs-android-sdk exposes the functionality through, methods present in various repository classes which just need to be plugged in with the UI and view-Model.
* An simple example of the usage would be [this demo application](https://github.com/LuGO0/Test-Application), a more complex application depicting the usage would be the openmrs-client package itself. There is a confluence article [here](
https://wiki.openmrs.org/display/docs/Getting+Started+with+openmrs-android-sdk) which will take you through the creation and usage of the test Application mentioned above.
* The JavaDocs for the openmrs-android-sdk can be generated by running the Gradle command ./gradle dokkaHtml which will generate an HTML interface with documentation inside the openmrs-android-sdk/build directory.

#### 2. openmrs-client package
* This package was earlier used as the sole package containing all the code for the OpenMRS-Android-Client now a part of it has been encapsulated in the form of openmrs-android-sdk and published on Jitpack from where it can simply be added as a dependency to any other app.
* This package uses the methods exposed by the openmrs-android-sdk and builds UI on top of it, which can be used as an example to implement UI on top of openmrs-android-sdk.
* The app is also published on PlayStore just to get used to the functionalities it provides and can be tested on local OpenMRS server or Demo OpenMRS Server.

#### Key Features
- Connect to OpenMRS server and sync all data
- Register and Edit patients
- Record Visits and Encounters
- View patient data (Details, Diagnoses, Visits, and Vitals)
- Allergies Module
- Provider Module
- Offline access (specific modules like provider, patient and some functionalities of allergy module)

# Screenshots
   
 
# Demo Server

The demo test server dedicated to the client is (https://demo.openmrs.org/openmrs/).
In case the demo server fails to respond, you can use other alternate servers provided [here.](https://wiki.openmrs.org/display/ISM/OpenMRS+environments)
##### Demo Username: admin
##### Password: Admin123

 
 
 
# Releasing [Collaborators only]

### 1. OpenMRS-Android-Client to PlayStore
We follow the sprint model for development. Read more about it here: [OpenMRS Sprints](https://wiki.openmrs.org/display/RES/Development+Sprints).
To release the application, make sure to do these steps **in order**:

1. Update the [version variable in versions.gradle](https://github.com/openmrs/openmrs-contrib-android-client/blob/master/openmrs-client/versions.gradle#L6) prior to the release.
3. Update the [Release notes](releaseNotes.md) file.
4. Update the [release notes text file](https://github.com/openmrs/openmrs-contrib-android-client/blob/master/openmrs-client/src/main/play/release-notes/en-US/default.txt) to publish in the Play store. Ideally, change the wording so that normal end users understand.
5. Now commit with the title `Release ` to the master branch.
6. Tag the commit, using the version as the tag name. Make sure CI is green!
7. Go to [the releases page](https://github.com/openmrs/openmrs-contrib-android-client/releases) and click the [Draft a new release](https://github.com/openmrs/openmrs-contrib-android-client/releases/new) button. It will create a new version tag in the repository and build the app. The tag name will be used as the version number for this. Be sure to bump unfinished issues to the next due version.
8. Go to [JIRA's releases page](https://issues.openmrs.org/projects/AC?selectedItem=com.atlassian.jira.jira-projects-plugin:release-page), click on the three-dots on the right, and hit **Release**.
9. Post a new Talk thread and describe what is changed or improved in the release.

### 2. openmrs-android-sdk to jitpack
1. The Openmrs-Android-sdk gets published to the Jitpack library so that it can be added as a dependency in various projects.
2. Due to some issues with the release build configuration of the app is explained in detail [here](https://stackoverflow.com/questions/68420822/handling-release-keystore-while-uploading-android-library-to-jitpack) we are not able to do jitpack releases from the master branch.
3. There is a dedicated branch for this purpose that has got only the debug variant of the android application. So that the jitpack build passes.
4. For now until the issue gets resolved or we write a script for it, To release the latest code improvements in the openmrs-android-sdk package to the jitpack, we need to copy the whole package from master to branch deploy-android-sdk.
5. After getting the latest code changes to branch deploy-android-sdk, we publish a snapshot of the branch to jitpack. So that the tag of jitpack published looks exactly like `deploy-android-sdk-SNAPSHOT`.

# License
This project is licensed under the OpenMRS Public License, see the [copyright](copyright/copyright) file for details.

# Resources
- [User Guide](https://github.com/openmrs/openmrs-android-client-user-guide)
- [Contribution Guidelines](https://github.com/openmrs/openmrs-contrib-android-client/blob/master/CONTRIBUTING.md)
- [JIRA](https://issues.openmrs.org/browse/AC/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel)
- [Sprint board](https://issues.openmrs.org/secure/RapidBoard.jspa?rapidView=60)
- [Dashboard](https://issues.openmrs.org/secure/Dashboard.jspa?selectPageId=12851)
- [CI](https://travis-ci.org/openmrs/openmrs-contrib-android-client)
- [Google Play](https://play.google.com/store/apps/details?id=org.openmrs.mobile)
- [Release Notes](releaseNotes.md)
","'android', 'android-client', 'jira', 'openmrs'",2024-03-28T10:37:55Z,89,172,84,"('evijit', 196), ('tmarzeion', 139), ('rishabh-997', 98), ('smalecki', 73), ('AdamGrzybkowski', 73), ('rkorytkowski', 64), ('LuGO0', 63), ('shubhamsgit', 60), ('shivtej1505', 44), ('f4ww4z', 41), ('amrsalah3', 34), ('deepak140596', 27), ('DefCon-007', 20), ('ehaligowska', 18), ('kwitczak', 14), ('VibhorChinda', 13), ('VANKINEENITAWRUN', 12), ('gautamp8', 12), ('shivamsawlani02', 9), ('Rohit-2602', 8), ('Abdelaty', 7), ('Rimjhim28', 7), ('anuar2k', 5), ('ivary43', 5), ('ribhavsharma', 5), ('vansha10', 5), ('Akshatji800', 4), ('codepoet2017390', 4), ('hao555sky', 4), ('zjmeow', 3), ('yatna', 3), ('sviho', 3), ('henziger', 3), ('AryanGanotra07', 3), ('HelioStrike', 2), ('csmuthukuda', 2), ('cypherop', 2), ('MasterBlaster99', 2), ('sanjulamadurapperuma', 2), ('Romit-Mohanty-1', 2), ('PermissionError', 2), ('rdodiya', 2), ('its-snorlax', 2), ('prathamesh-mutkure', 2), ('Sub6Resources', 2), ('Rec0iL99', 2), ('kant', 2), ('anonymous-ME', 2), ('VaishSiddharth', 1), ('ShivanshGoel221B', 1), ('satvikshri', 1), ('sachin-0102', 1), ('Siddhant9868', 1), ('RSM55712', 1), ('rafalsliwinski', 1), ('aggarwalpulkit596', 1), ('prateek27das', 1), ('Bisht13', 1), ('rishujam', 1), ('TarekAlabd', 1), ('Veeshal', 1), ('vish198910', 1), ('vjs3', 1), ('ykarim', 1), ('dkayiwa', 1), ('haripriya999', 1), ('csnardi', 1), ('karannaik', 1), ('surangak', 1), ('tbagz104', 1), ('wongalvis', 1), ('Anshul-9923', 1), ('Asgardian8740', 1), ('BenjaminGHo', 1), ('bmamlin', 1), ('chaityabshah', 1), ('merovingienne', 1), ('NullByte08', 1), ('debarghya472', 1), ('Devanshk9', 1), ('Gelassen', 1), ('JLLeitschuh', 1), ('JyothsnaAshok', 1), ('lahirujayathilake', 1), ('madhurgupta10', 1), ('manjotsidhu', 1), ('K1RA-16', 1), ('oorjitchowdhary', 1), ('pranoppal', 1)","[3, 'Good Health and Well-Being']"
openelisglobal/openelisglobal-core,Global OpenELIS (Enterprise Laboratory Information System) is an initiative to provide laboratory information systems for resource-constrained international clinical and reference laboratories.,"openelisglobal-core
===================

Global OpenELIS (Enterprise Laboratory Information System) is an initiative to provide laboratory information systems for resource-constrained international clinical and reference laboratories.

This is OpenELIS 1.x and is end of life, please find OpenELIS Global 2.x [Here](https://github.com/I-TECH-UW/OpenELIS-Global-2)
",,2021-03-15T21:21:29Z,13,35,18,"('pfschwartz', 1177), ('jfurlong', 227), ('phassoa', 220), ('caseyi', 136), ('rossumg', 59), ('CalebSLane', 36), ('koneconstant', 30), ('ykw0', 17), ('echiteri', 15), ('nathaelf', 9), ('kkalain123', 4), ('kkalain', 2), ('caseynth', 2)","[3, 'Good Health and Well-Being']"
OperationCode/operationcode-pyback,"A microservice supporting PyBot, Resources API, and other features that the official back-end does not manage.","
  
  
    <img
      alt=""Operation Code logo""
      src=""https://s3.amazonaws.com/operationcode-assets/branding/logos/large-blue-logo.png""
      width=""75%""
    >
  
  
  


# IMPORTANT
This repo has been archived and its functionality merged into the new Python Backend located at https://github.com/OperationCode/back-end


[![Build Status](https://travis-ci.org/OperationCode/operationcode-pyback.svg?branch=master)](https://travis-ci.org/OperationCode/operationcode-pyback)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Twitter Follow](https://img.shields.io/twitter/follow/operation_code.svg?style=social&label=Follow&style=social)](https://twitter.com/operation_code)


# [OperationCode-Pyback](https://github.com/OperationCode/operationcode-Pyback)



## Contributing
Bug reports and pull requests are welcome on [Github](https://github.com/OperationCode/operationcode-pybot). This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [Contributor Covenant](http://contributor-covenant.org) code of conduct. If you wish to assist, join the [\#new-team-rewrite](https://operation-code.slack.com/messages/C7NJLCCMB/) rewrite to learn how to contribute.

## License
This package is available as open source under the terms of the [MIT License](http://opensource.org/licenses/MIT).
",,2019-06-01T19:11:10Z,3,15,4,"('AllenAnthes', 24), ('kylemh', 2), ('vyaspranjal33', 1)","[10, 'Reduced Inequalities']"
codeforamerica/cfapi,The Code for America API. Tracks and motivates activity and participation across the civic technology movement.,"[![Stories in Ready](https://badge.waffle.io/codeforamerica/cfapi.png?label=ready&title=Ready)](https://waffle.io/codeforamerica/cfapi)
[![Build Status](https://travis-ci.org/codeforamerica/cfapi.svg?branch=master)](https://travis-ci.org/codeforamerica/cfapi)

# The Code for America API

### What the CFAPI is
Code for America has developed this API to track all the activity across the civic technology movement. Our goal is to measure and motivate the movement by recognizing participation. The CFAPI describes an organization's projects, stories, and events.

The tools that the Brigades and other groups use to do their fine deeds are all different. The CFAPI does the difficult job of being able to track these activities no matter what tools an organization is using. The participants don't need to change their activities to be included.

### How it works
To get the information for the CfAPI, Code for America maintains a [list of civic tech organizations](https://github.com/codeforamerica/brigade-information/blob/master/organizations.json) and once an hour checks their activity on Meetup.com, their blog, and their GitHub projects. Other services and support for noncode projects are slowly being added. More technical details [below](https://github.com/codeforamerica/cfapi#installation).

### Projects powered by the CFAPI
* The Code for America Brigade website


* The Brigade Projects Page


* The Civic Tech Issue Finder


* The Civic Issue Twitter Bot


* Lots of different Brigades websites


### Example Response
See the full documentation at http://codeforamerica.org/api

Response for `http://codeforamerica.org/api/organizations/Code-for-San-Francisco`
```
{
  ""all_events"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/events"",
  ""all_issues"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/issues"",
  ""all_projects"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/projects"",
  ""all_stories"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/stories"",
  ""api_url"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco"",
  ""city"": ""San Francisco, CA"",
  ""current_events"": [
    {
      ""api_url"": ""http://codeforamerica.org/api/events/710"",
      ""created_at"": ""2014-02-26 21:05:21"",
      ""description"": null,
      ""end_time"": null,
      ""event_url"": ""http://www.meetup.com/Code-for-San-Francisco-Civic-Hack-Night/events/193535742/"",
      ""id"": 710,
      ""location"": null,
      ""name"": ""Weekly Civic Hack Night"",
      ""organization_name"": ""Code for San Francisco"",
      ""start_time"": ""2014-08-27 18:30:00 -0700""
    },
    ...
  ],
  ""current_projects"": [
    {
      api_url: ""http://codeforamerica.org/api/projects/122"",
      categories: null,
      code_url: ""https://github.com/sfbrigade/localfreeweb.org"",
      description: ""Front end for the Local Free Web project"",
      github_details: { ... },
      id: 122,
      issues: [ ... ],
      last_updated: ""Thu, 24 Jul 2014 22:01:17 GMT"",
      link_url: null,
      name: ""localfreeweb.org"",
      organization: {},
      organization_name: ""Code for San Francisco"",
      tags: [""digital access"",""bus stops""],
      type: null,
      status: ""Official""
      commit_status: ""success""
    },
    ...
  ],
  ""current_stories"": [
    {
      ""api_url"": ""http://codeforamerica.org/api/stories/10"",
      ""id"": 10,
      ""link"": ""https://groups.google.com/d/msg/code-for-san-francisco/9OewkHV-D1M/0UW_ye9UXc8J"",
      ""organization_name"": ""Code for San Francisco"",
      ""title"": ""Hack Night Project Pick List"",
      ""type"": ""blog""
    },
    ...
  ],
  ""id"" : ""Code-for-San-Francisco"",
  ""events_url"": ""http://www.meetup.com/Code-for-San-Francisco-Civic-Hack-Night/"",
  ""last_updated"": 1409087294,
  ""latitude"": 37.7749,
  ""longitude"": -122.4194,
  ""name"": ""Code for San Francisco"",
  ""past_events"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/past_events"",
  ""projects_list_url"": ""https://docs.google.com/spreadsheet/pub?key=0ArHmv-6U1drqdDVGZzdiMVlkMnRJLXp2cm1ZTUhMOFE&output=csv"",
  ""rss"": """",
  ""started_on"": ""2014-07-30"",
  ""type"": ""Brigade"",
  ""upcoming_events"": ""http://codeforamerica.org/api/organizations/Code-for-San-Francisco/upcoming_events"",
  ""website"": ""http://codeforsanfrancisco.org/""
}
```

### History
The need for a way to show off good civic tech projects was apparent. Several Brigades had all started working on ways to track their projects. They were working separately on the same idea at the same time. The CFAPI is a generalization of the great work done by:

 Open City 

 Beta NYC 

 Code for Boston 

*For the full story behind this API, [read this](https://hackpad.com/Civic.json-planning-meeting-EusFEMPgMio#:h=Chicago's-Open-Gov-Hack-Night-).

This repository is forked from [Open City's Civic Json Worker](https://github.com/open-city/civic-json-worker)

### Future
We hope that this experiment of tracking activity within a community is useful for other groups besides the civic technology movement. We will begin working with other groups to see if an instance of the CfAPI is useful for them.

We also want to add support for many more services to be included, such as events from Eventbrite. Our goal is for any organization to use any tool to do their work and we will integrate with them.

### How to add your Brigade to the API

Submit a Pull Request with your brigade's information to the [Brigade Information repository](https://github.com/codeforamerica/brigade-information). Instructions are included in that repo's [README](https://github.com/codeforamerica/brigade-information/blob/master/README.md).

### Civic.json
To add extra data about your projects to the CfAPI, include a `civic.json` file in the top level of your repo.

Currently we accept `status` and `tags` as fields in the civic.json.

An example civic.json file
```
{
    ""status"": ""Production"",
    ""tags"": [""slack"", ""bot"", ""integration"", ""python"", ""flask"", ""glossary"", ""dictionary""]
}
```

This project could then be easily found by searching the CfAPI like
[http://codeforamerica.org/api/projects?q=production,slack,bot](http://codeforamerica.org/api/projects?q=production,slack,bot)

The `civic.json` idea comes from BetaNYC and still has an [active discussion](https://github.com/BetaNYC/civic.json/issues) about its spec.


### Civic Tech Issue Finder
Once you've got your organization's GitHub projects on the API, all of your groups open GitHub Issues will be seen in the [Civic Tech Issue Finder](http://www.codeforamerica.org/geeks/civicissues). Use the label ""help wanted"" to get the most exposure. More info on that [project's README](https://github.com/codeforamerica/civic-issue-finder#civic-issue-finder).

## Installation

The CFAPI is built on [Flask](http://flask.pocoo.org/) and Python. The `app.py` file describes the models and routes. The `run_update.py` file runs once an hour and collects all the data about the different Brigades. Both `tests.py` and `run_update_test.py` are automatically run by [Travis-CI](https://travis-ci.org/codeforamerica/cfapi) whenever a new commit is made. The production service lives on Heroku. Please contact [us](https://github.com/codeforamerica/cfapi#contacts) with any questions.

### Development setup

#### Requirements

* PostgreSQL Database - [How To](https://github.com/codeforamerica/howto/blob/master/PostgreSQL.md)

#### Environmental variables

* `DATABASE_URL=[db connection string]` — My local example is `postgres:///cfapi`
* `GITHUB_TOKEN=[GitHub API token]` — Read about setting that up here: http://developer.github.com/v3/oauth/
* `MEETUP_KEY=[Meetup API Key]` — Read about setting that up here: https://secure.meetup.com/meetup_api/key/

Set these up in a local `.env` file.

#### Project setup

* Set up a [virtual environment](https://github.com/codeforamerica/howto/blob/master/Python-Virtualenv.md)

* Install the required libraries

```
$ pip install -r requirements.txt
```

* Set up a new database

```
createdb cfapi
python app.py createdb
```

* Run the updater

The `run_update.py` script will be run on Heroku once an hour and populate the database. To run locally, try:

```
python run_update.py
```

You can update just one organization if you need by using:

```
python run_update.py --name ""Beta NYC""
```

For quicker update testing, use a shorter list of orgs by calling run_update.py with the `--test` flag:

```
python run_update.py --test
```

* Start the API

```
env `cat .env` python app.py runserver
```

* or use [foreman](http://theforeman.org/) to mimic how the CfAPI runs on Heroku.

```
foreman start
```

* Visit `localhost:5000` in your browser to see the results
```
http://localhost:5000/api/organizations/Code-for-America
```

### Deployment

Deployment is typically on Heroku. Follow [this tutorial](https://devcenter.heroku.com/articles/getting-started-with-python) for basic information on how to setup the project.

#### Environmental variables

These must be set:

* `GITHUB_TOKEN`
* `MEETUP_KEY` (if used)

`DATABASE_URL` will be handled by Heroku.

#### Project setup

* Initialize the database

```
heroku run bash
python -c 'from app import db; db.create_all()'
```

### Tests
* Set up a new database

```
createdb civic_json_worker_test
python -c 'from app import db; db.create_all()'

createdb peopledbtest
psql peopledbtest < test/peopledbtest.pgsql
```

`green -vvv --run-coverage` to run everything at once.

`green test/updater -vvv` to test the run_update process.

`green test/updater -vvv --run-coverage` to test the run_update process with coverage.

`green test/integration -vvv` to test the API.

`green test/integration -vvv --run-coverage` to test the API with code coverage.


### Codestyle (PEP8 and co.)

The project ships with flake8 to track style, perform a flake8 check by calling

`flake8 . --exclude=migrations,test --ignore=E501,E711,E712`




### Migrations
Migrations are handled through [flask-migrate](https://github.com/miguelgrinberg/Flask-Migrate#flask-migrate)

Contacts
--------

* Andrew Hyder ([ondrae](https://github.com/ondrae))
* Michal Migurski ([migurski](https://github.com/migurski))
* Tomas Apodaca ([tmaybe](https://github.com/tmaybe))

Contributing
------------

Here are some ways *you* can contribute:

* by reporting bugs
* by suggesting new features
* by translating to a new language
* by writing or editing documentation
* by writing code (**no patch is too small**: fix typos, add comments, clean up
  inconsistent whitespace)
* by refactoring code
* by closing [issues][]
* by reviewing patches
* [financially][]

[issues]: https://github.com/codeforamerica/cfapi/issues
[financially]: https://secure.codeforamerica.org/page/contribute


Submitting an Issue
-------------------

We use the [GitHub issue tracker][issues] to track bugs and features. Before
submitting a bug report or feature request, check to make sure it hasn't
already been submitted. You can indicate support for an existing issue by
voting it up. When submitting a bug report, please include a [Gist][] that
includes a stack trace and any details that may be necessary to reproduce the
bug.

[gist]: https://gist.github.com/

Submitting a Pull Request
-------------------------

1. Fork the project.
2. Create a topic branch.
3. Implement your feature or bug fix.
4. Write tests!
5. Run a migration if needed.
6. Commit and push your changes.
7. Submit a pull request.


Copyright
---------

Copyright (c) 2015 Code for America.
",,2021-04-30T17:49:56Z,23,113,31,"('ondrae', 350), ('tmaybe', 277), ('migurski', 157), ('tdooner', 59), ('chrisrodz', 56), ('volkanunsal', 26), ('MaxPresman', 6), ('jpvelez', 5), ('daguar', 4), ('derekeder', 4), ('smalley', 4), ('cweems', 3), ('pui', 2), ('carpeliam', 2), ('pmackay', 2), ('hlprmnky', 1), ('waffle-iron', 1), ('maya', 1), ('RandyMoore', 1), ('ralren', 1), ('ycombinator', 1), ('junosuarez', 1), ('loumoore', 1)","[16, 'Peace, Justice and Strong Institutions']"
ushahidi/tenfour,API For TenFour,"## What is TenFour?

TenFour is an emergency check-in application for teams that unites multiple communication channels: SMS, email, desktop, mobile and chat. It's a team check-in system that gets answers when you need them most.

Responses can be collected from anyone, anytime, anywhere by SMS, email, the TenFour app and slack so there is no device type dependency.

It can securely store team info with multiple points of contact, all synced securely via encrypted transport.

It will support multiple admin and user roles, so that your team only sees what you want.

Users can Add contacts to a group so one can check-in with multiple people with the push of a button.

TenFour has modules that work with telecom carriers around the world (SMS currently in US, UK, CA, KE).

[View our TenFour product slides here](https://drive.google.com/file/d/1B80_hcdWD-7SQFdssUzzRegfChtbwjdH/view)

## How to use TenFour

TenFour step by step
* [Create an organisation URL](https://www.tenfour.org/support/how-to-sign-up)
* [Go through 'onboarding'](https://www.tenfour.org/support/onboarding)
* Onboarding should prompt you to:
    + [Add people](https://www.tenfour.org/support/adding-users) 
    + [Send a test check-in](https://www.tenfour.org/support/compose-a-checkin) 
    + [Create a group](https://www.tenfour.org/support/creating-groups)
* After onboarding you will be able to use all the functions in TenFour freely.
    
Make note of your URL (example: https://myorganisation.tenfour.org/) and the password associated with yoru URL/Account or store in a secure password vault.

An admin can login to the dashboard online or on their phone and easily send a message.

Authoring a message is easy, you just type in your question and select the answers you want back. Example: “There was just an earthquake in SF, are you ok?”

Users then receive messages from all different channels. They can respond, and their response is routed back to TenFour

You can then go back to the app and see the status of check-ins.

More info on how to use TenFour is included on the [TenFour support page](https://www.tenfour.org/support/what-is-tenfour)

## Why do we need TenFour?

Crises are growing in the world.

In the last two decades, there has been an increase of human-made attacks globally, while the majority are concentrated in just a few countries, there were attacks in 106 countries in 2017. Though overall mortality is low from these attacks, from our experience, they still require tools to coordinate to maintain duty of care for your team.

Extreme weather related events are increasing quickly, causing significant economic damage and casualties. All science shows that these extreme events will only increase in the coming years.

Crises are a big deal for organizations, as in the US, even with a robust 1st responder and insurance ecosystem, 25% of small businesses don’t reopen, and it is worse elsewhere in the world where SMEs have lower insurance rates for their assets.


## TenFour test case

We’ve tested this in real life with a handful of organizations, community projects, volunteer groups and field staff and we’ve already seen it has been absolutely necessary.

[AkiraChix](http://akirachix.com/) is one of our sister organizations in Kenya that teaches women in Nairobi’s slums how to code and places them into tech jobs.
During the election in Kenya this last summer, violence broke out in the slums. The manager of AkiraChix immediately checked-in with her students over TenFour. The 25 girls texted, emailed and responded to check-in notifications. In just a matter of minutes, AkiraChix knew that their students were safe.


## TenFour API

[Get started](docs/getting_started.md)

## Official Documentation

[Docs](docs)

## Contributing Code

TenFour became Open Source in August 2019. It was previous a closed repository and not an Open Source tool due to SMS vendor information being tied to [Ushahidi](ushahidi.com) who built the tool over the last 4 years.

As such the process for contributing is a work in progress and will be updated here. 

### Labels and how to use them to find development/technical issues

**back-end issues** - Issues to do with TenFour's Back-end 'API' or 'Client'.

**bug** - An issue that is not functioning as expected or not in the 'ideal' way for a large part of the TenFour users.

**devops** - An issue that looks at the development operation of the feature/product as a whole. Could be server, connection, reliability based.

**documentation** - An issue that needs supporting documentation created and logged for the feature/issue.

**duplicate** - A label to flag that there is a duplicate or very similar issue to the one with this label and the two issues could potentiall be condensed into one.

**enhancement** - A label to show that the issue is an improvement on an existing feature.

**epic** - A label to indicate a collection of issues under one theme or type. This is called an 'epic'.

**Feature: Check-In** - An issue that looks at the TenFour check-ins screens/process.

**Feature: Groups** - An issue that looks at the TenFour groups screens/process.

**Feature: Notifications** - An issue that looks at the TenFour notifications screens/process.

**Feature: Onboarding** - An issue that looks at the TenFour onboarding screens/process.

**Feature: People** - An issue that looks at the TenFour people screens/process.

**Feature: Profile** - An issue that looks at the TenFour profile screens/process.

**Feature: Settings** - An issue that looks at the TenFour settings screens/process.

**feature request** - A request specifically come from a user of TenFour that does not alreayd exist in the product.

**front-end** - An issue which needs front-end development coding.

**good first issue** - An issue that is suspected to be a good first issue for a new contributor to TenFour.

**grant requirement** - An issue that is connected to a requirement detailed in a grant/funding proposal.

**low priority** - An issue that is not at all critical to the functioning/improvement of the product.

**medium priority** - An issue that is somewhat critical to the functioning/improvement of the product.

**high priority** - An issue that is very critical to the functioning/improvement of the product.

**marketing** - An issue for marketing materials and/or promotional materials for TenFour. Can include presentation templates, merchandise etc.

**mobile** - This is an issue which affects and/or is most important on mobile.

**P0 - Unbreak now!** - This is a critical issue for developer/s to fix as it stops TenFour from functioning as intended.

**P1 - Immediate** - This is an important but not critical issue for developer to fix. It make reduce TenFour from functioning as intended

**P2 - Normal** - An issue which will be useful but not critical to have in the product.

**P1 - Low** - An issue that is somewhat useful but not critical to have in the product.

**P4 - Wishlist** - An issue which is a very nice to have in the product.

**PWA** - An issue that is specific to the PWA or [Progressive Web App](https://ionicframework.com/docs/v3/developer-resources/progressive-web-apps/).

**QA by dev** - An issue that needs quality assurance testing by someone with development/coding experience.

**question** - An issue which raises a question from the community.

**Support** - An issue that needs work from a support function like tutorials, guides, documentation or customer/user support.

**Tech debt** - An issue which is as a result of [technical debt](https://en.wikipedia.org/wiki/Technical_debt)

**TenFour.org** - An issue that affects [Tenfour.org website](https://tenfour.org/)

**wontfix** - An issue that won't be looked at until further notice.

## Contributing Design

The process to contribute design to TenFour is captured by [Open Design](https://opendesign.ushahidi.com/), a project between [Ushahidi](ushahidi.com), [Adobe](https://www.adobe.com/) & [Designit](https://www.designit.com/).

You can find details on how to contribute design to TenFour on the [Design contribution page](https://github.com/ushahidi/tenfour/blob/develop/design-contributions.md) 

Documentation on how to contribute design work will be added to a documentation environment specifically for TenFour but for now please see [Ushahidi's design documentation on Gitbooks](https://app.gitbook.com/@ushahidi/s/platform-developer-documentation/design/design-process)

### Labels and how to use them to find design issues

**Open Design issue** - An issue that has been identified for an [Open design](http://opendesign.ushahidi.com/) workshop. These workshops bring together designers wanting to contribute in groups (or individuals) to issues in TenFour.

**Design: Interaction** - An issue which needs attention to how users interaction with it. Could be UI connected, voice, or gesture. Particularly useful for Human Computer Interaction/Design (HCI/D) people.

**Design: UI** - An issue which needs attention to the interface that users use. Can be a visual interface but also a conversation interface (voice) etc.

**Design: Usability + Inclusion** - An issue which needs attention to how users of all abilities and needs are considered. Inclusion and accessibility focus needed to inform how the feature functions.

**Design: User Research** - An issue which needs further user research done to fully discover and validate the problem/hypothesis presented in the issue.

**Design: UX** - An issue which needs attention on user experince of the feature or issue. Could be prototypes, flows, inclusion of other features, interactions and tasks that the user performs as part of this issue.

**Design: Visual + Graphic** - An issue which needs visual design and/or graphics. Could be to help communicate a feature visually with animations, supporting graphics and illustration or logo, colours and brand changes.

**QA by Design** - This is an issue that after having gone through development, needs a designer to approve the functionailty.

### License

TenFour is under the license [AGPL](https://www.gnu.org/licenses/agpl-3.0.en.html)
",,2023-02-02T01:25:58Z,2,7,5,"('Erioldoesdesign', 20), ('mackers', 3)","[17, 'Partnerships for the Goals']"
scratchfoundation/scratch-gui,Graphical User Interface for creating and running Scratch 3.0 projects.,"# scratch-gui

Scratch GUI is a set of React components that comprise the interface for creating and running Scratch 3.0 projects

To open the current build in your browser on Github Pages:

https://scratchfoundation.github.io/scratch-gui/

## Installation

This requires you to have Git and Node.js installed.

In your own node environment/application:

```bash
npm install https://github.com/scratchfoundation/scratch-gui.git
```

If you want to edit/play yourself:

```bash
git clone https://github.com/scratchfoundation/scratch-gui.git
cd scratch-gui
npm install
```

**You may want to add `--depth=1` to the `git clone` command because there are some [large files in the git repository
history](https://github.com/scratchfoundation/scratch-gui/issues/5140).**

## Getting started

Running the project requires Node.js to be installed.

## Running

Open a Command Prompt or Terminal in the repository and run:

```bash
npm start
```

Then go to [http://localhost:8601/](http://localhost:8601/) - the playground outputs the default GUI component

## Developing alongside other Scratch repositories

### Getting another repo to point to this code


If you wish to develop `scratch-gui` alongside other scratch repositories that depend on it, you may wish
to have the other repositories use your local `scratch-gui` build instead of fetching the current production
version of the scratch-gui that is found by default using `npm install`.

Here's how to link your local `scratch-gui` code to another project's `node_modules/scratch-gui`.

#### Configuration

1. In your local `scratch-gui` repository's top level:
    1. Make sure you have run `npm install`
    2. Build the `dist` directory by running `BUILD_MODE=dist npm run build`
    3. Establish a link to this repository by running `npm link`

2. From the top level of each repository (such as `scratch-www`) that depends on `scratch-gui`:
    1. Make sure you have run `npm install`
    2. Run `npm link scratch-gui`
    3. Build or run the repository

#### Using `npm run watch`

Instead of `BUILD_MODE=dist npm run build`, you can use `BUILD_MODE=dist npm run watch` instead. This will watch for
changes to your `scratch-gui` code, and automatically rebuild when there are changes. Sometimes this has been
unreliable; if you are having problems, try going back to `BUILD_MODE=dist npm run build` until you resolve them.

#### Oh no! It didn't work!

If you can't get linking to work right, try:

* Follow the recipe above step by step and don't change the order. It is especially important to run `npm install`
  _before_ `npm link` as installing after the linking will reset the linking.
* Make sure the repositories are siblings on your machine's file tree, like
  `.../.../MY_SCRATCH_DEV_DIRECTORY/scratch-gui/` and `.../.../MY_SCRATCH_DEV_DIRECTORY/scratch-www/`.
* Consistent node.js version: If you have multiple Terminal tabs or windows open for the different Scratch
  repositories, make sure to use the same node version in all of them.
* If nothing else works, unlink the repositories by running `npm unlink` in both, and start over.

## Testing

### Documentation

You may want to review the documentation for [Jest](https://facebook.github.io/jest/docs/en/api.html) and
[Enzyme](http://airbnb.io/enzyme/docs/api/) as you write your tests.

See [jest cli docs](https://facebook.github.io/jest/docs/en/cli.html#content) for more options.

### Running tests

*NOTE: If you're a Windows user, please run these scripts in Windows `cmd.exe`  instead of Git Bash/MINGW64.*

Before running any tests, make sure you have run `npm install` from this (scratch-gui) repository's top level.

#### Main testing command

To run linter, unit tests, build, and integration tests, all at once:

```bash
npm test
```

#### Running unit tests

To run unit tests in isolation:

```bash
npm run test:unit
```

To run unit tests in watch mode (watches for code changes and continuously runs tests):

```bash
npm run test:unit -- --watch
```

You can run a single file of integration tests (in this example, the `button` tests):

```bash
$(npm bin)/jest --runInBand test/unit/components/button.test.jsx
```

#### Running integration tests

Integration tests use a headless browser to manipulate the actual HTML and javascript that the repo
produces. You will not see this activity (though you can hear it when sounds are played!).

To run the integration tests, you'll first need to install Chrome, Chromium, or a variant, along with Chromedriver.

Note that integration tests require you to first create a build that can be loaded in a browser:

```bash
npm run build
```

Then, you can run all integration tests:

```bash
npm run test:integration
```

Or, you can run a single file of integration tests (in this example, the `backpack` tests):

```bash
$(npm bin)/jest --runInBand test/integration/backpack.test.js
```

If you want to watch the browser as it runs the test, rather than running headless, use:

```bash
USE_HEADLESS=no $(npm bin)/jest --runInBand test/integration/backpack.test.js
```

## Troubleshooting

### Ignoring optional dependencies

When running `npm install`, you can get warnings about optional dependencies:

```text
npm WARN optional Skipping failed optional dependency /chokidar/fsevents:
npm WARN notsup Not compatible with your operating system or architecture: fsevents@1.2.7
```

You can suppress them by adding the `no-optional` switch:

```bash
npm install --no-optional
```

Further reading: [Stack Overflow](https://stackoverflow.com/questions/36725181/not-compatible-with-your-operating-system-or-architecture-fsevents1-0-11)

### Resolving dependencies

When installing for the first time, you can get warnings that need to be resolved:

```text
npm WARN eslint-config-scratch@5.0.0 requires a peer of babel-eslint@^8.0.1 but none was installed.
npm WARN eslint-config-scratch@5.0.0 requires a peer of eslint@^4.0 but none was installed.
npm WARN scratch-paint@0.2.0-prerelease.20190318170811 requires a peer of react-intl-redux@^0.7 but none was installed.
npm WARN scratch-paint@0.2.0-prerelease.20190318170811 requires a peer of react-responsive@^4 but none was installed.
```

You can check which versions are available:

```bash
npm view react-intl-redux@0.* version
```

You will need to install the required version:

```bash
npm install  --no-optional --save-dev react-intl-redux@^0.7
```

The dependency itself might have more missing dependencies, which will show up like this:

```bash
user@machine:~/sources/scratch/scratch-gui (491-translatable-library-objects)$ npm install  --no-optional --save-dev react-intl-redux@^0.7
scratch-gui@0.1.0 /media/cuideigin/Linux/sources/scratch/scratch-gui
├── react-intl-redux@0.7.0
└── UNMET PEER DEPENDENCY react-responsive@5.0.0
```

You will need to install those as well:

```bash
npm install  --no-optional --save-dev react-responsive@^5.0.0
```

Further reading: [Stack Overflow](https://stackoverflow.com/questions/46602286/npm-requires-a-peer-of-but-all-peers-are-in-package-json-and-node-modules)

## Troubleshooting

If you run into npm install errors, try these steps:

1. run `npm cache clean --force`
2. Delete the node_modules directory
3. Delete package-lock.json
4. run `npm install` again

## Publishing to GitHub Pages

You can publish the GUI to github.io so that others on the Internet can view it.
[Read the wiki for a step-by-step guide.](https://github.com/scratchfoundation/scratch-gui/wiki/Publishing-to-GitHub-Pages)

## Understanding the project state machine

Since so much code throughout scratch-gui depends on the state of the project, which goes through many different
phases of loading, displaying and saving, we created a ""finite state machine"" to make it clear which state it is in at
any moment. This is contained in the file src/reducers/project-state.js .

It can be hard to understand the code in src/reducers/project-state.js . There are several types of data and functions
used, which relate to each other:

### Loading states

These include state constant strings like:

* `NOT_LOADED` (the default state),
* `ERROR`,
* `FETCHING_WITH_ID`,
* `LOADING_VM_WITH_ID`,
* `REMIXING`,
* `SHOWING_WITH_ID`,
* `SHOWING_WITHOUT_ID`,
* etc.

### Transitions

These are names for the action which causes a state change. Some examples are:

* `START_FETCHING_NEW`,
* `DONE_FETCHING_WITH_ID`,
* `DONE_LOADING_VM_WITH_ID`,
* `SET_PROJECT_ID`,
* `START_AUTO_UPDATING`,

### How transitions relate to loading states

Like this diagram of the project state machine shows, various transition actions can move us from one loading state to
another:

![Project state diagram](docs/project_state_diagram.svg)

_Note: for clarity, the diagram above excludes states and transitions relating to error handling._

#### Example

Here's an example of how states transition.

Suppose a user clicks on a project, and the page starts to load with URL `https://scratch.mit.edu/projects/123456`.

Here's what will happen in the project state machine:

![Project state example](docs/project_state_example.png)

1. When the app first mounts, the project state is `NOT_LOADED`.
2. The `SET_PROJECT_ID` redux action is dispatched (from src/lib/project-fetcher-hoc.jsx), with `projectId` set to
   `123456`. This transitions the state from `NOT_LOADED` to `FETCHING_WITH_ID`.
3. The `FETCHING_WITH_ID` state. In src/lib/project-fetcher-hoc.jsx, the `projectId` value `123456` is used to request
   the data for that project from the server.
4. When the server responds with the data, src/lib/project-fetcher-hoc.jsx dispatches the `DONE_FETCHING_WITH_ID`
   action, with `projectData` set. This transitions the state from `FETCHING_WITH_ID` to `LOADING_VM_WITH_ID`.
5. The `LOADING_VM_WITH_ID` state. In src/lib/vm-manager-hoc.jsx, we load the `projectData` into Scratch's virtual
   machine (""the vm"").
6. When loading is done, src/lib/vm-manager-hoc.jsx dispatches the `DONE_LOADING_VM_WITH_ID` action. This transitions
   the state from `LOADING_VM_WITH_ID` to `SHOWING_WITH_ID`.
7. The `SHOWING_WITH_ID` state. Now the project appears normally and is playable and editable.

## Donate

We provide [Scratch](https://scratch.mit.edu) free of charge, and want to keep it that way! Please consider making a
[donation](https://www.scratchfoundation.org/donate) to support our continued engineering, design, community, and
resource development efforts. Donations of any size are appreciated. Thank you!
",,2024-05-03T02:07:41Z,80,4253,216,"('renovatebot', 2482), ('paulkaplan', 2057), ('dependabot-previewbot', 1615), ('greenkeeperbot', 906), ('renovate-bot', 715), ('semantic-release-bot', 629), ('chrisgarrity', 392), ('kchadha', 384), ('benjiwheeler', 366), ('ericrosenbaum', 346), ('rschamp', 295), ('fsih', 256), ('thisandagain', 244), ('cwillisf', 199), ('evhan55', 160), ('picklesrus', 89), ('apple502j', 57), ('zoebentley', 52), ('adroitwhiz', 40), ('quachtina96', 30), ('towerofnix', 27), ('sjhuang26', 25), ('josiahneuberger', 22), ('mzgoddard', 21), ('bogusred', 19), ('tmickel', 19), ('nikhiljha', 16), ('LiFaytheGoblin', 15), ('lifeinchords', 13), ('foobartles', 13), ('Kenny2github', 12), ('gnarf', 11), ('LukeSchlangen', 8), ('kosiecki', 7), ('TheBrokenRail', 7), ('carljbowman', 7), ('kyleplo', 7), ('tomlum', 6), ('mewtaylor', 5), ('davidaylaian', 5), ('dgnball', 5), ('hyperobject', 5), ('colbygk', 5), ('as-com', 3), ('Cqoicebordel', 3), ('watilde', 3), ('gunchleoc', 3), ('ktbee', 3), ('JinXJinX', 3), ('stdio2016', 2), ('williamjallen', 2), ('SillyInventor', 2), ('mxmou', 2), ('helioh2', 2), ('CosmicWebServices', 2), ('Catalufa', 2), ('tutacat', 1), ('retronbv', 1), ('mrmeku', 1), ('jwzimmer-zz', 1), ('ivanixgames', 1), ('bwestover', 1), ('nimiri', 1), ('pyz1989', 1), ('Wgil', 1), ('sehgalvibhor', 1), ('noamraph', 1), ('nterray', 1), ('niccokunzmann', 1), ('skripted-io', 1), ('setoyama60jp', 1), ('ianobermiller', 1), ('GrahamSH-LLK', 1), ('glenpike', 1), ('gengshenghong', 1), ('SpeakkVisually', 1), ('technoboy10', 1), ('taybenlor', 1), ('epicfaace', 1), ('aoneill01', 1)","[4, 'Quality Education']"
radicallyopensecurity/pentext,"PenText system: Easily create beautifully looking penetration test quotes, reports, and documents in many formats (PDF, text, JSON, CSV, ...)","# PenText

The PenText XML documentation project is a collection of XML templates, XML
schemas and XSLT code, which combined provide an easy way to generate IT
security documents including test reports (for penetration tests, load tests,
code audits, etc), offers (to companies requesting these tests), and invoices.

### How it Works

The OWASP PenText project is based on XML. A PenText Report, Quote, Invoice or
Generic Document is in fact a (modular) XML document, conforming to an XML
Schema. The XML Schema ensures that the documents are structured correctly, so
that they can then be transformed into other formats like PDF, CSV and JSON
using XSLT and the SAXON XSLT processor.

To produce a PDF document, the report, offer, invoice or generic document XML is
first transformed into XSL-FO (XSL Formatting Objects), which is then converted
to PDF using Apache FOP.

It is also possible to export e.g. all findings of a report to JSON, XML or
plaintext format.

### The Structure

- The framework itself consists of files in the `dtd` and `xslt` directory
- A report or quote will go into `source`
- Graphics, including screenshots and e.g. a company logo, will go into
  `graphics`
- Findings and non-findings of penetration test reports can go into `findings`
  and `non-findings`

## Getting Started

What do you need ?

1. Clone this repository

2. Install the toolchain

3. Edit the content

Listo! That's all you need. Now you can build PDF reports using the content.

### Toolchain

To convert the XML content into PDF files the tools the _Apache FOP_ library and
the _Java_ library _Saxon_ will be used. A separate repository will contain
these tools on a handy Docker container.

To edit (and view) the content you'll need a XML editor - which could be any
text editor like _JEdit_, to a full IDE- for editing of course ;). Preferably
something that can check XML file validity. To view the resulting PDF files a
PDF viewer is necessary.

### Building PDFs

Manually compiling a quotation, report or other document can be done using the
supplied Makefile:

`make report`

This performs
`java -jar path-to-Saxon-jar -s:name-of-xml-file -xsl:name-of-xsl-file-in-xsl-directory -o:name-for-pdf-output`

See for more detailed information the [tools manual](/doc/tools-manual.md)

### Building CSVs

To export all findings as CSV file run:

```sh
make export-csv
```

This will output all columns as separate fields as well as a ""Jira Formatted
Description"" field that can be used in Jira.

#### Building CSVs using CI/CD

Copy over the latest `xslt/findings2csv` to your project.

Make sure the build step in `gitlab-ci.yml` saves `.csv` files:

```yml
build:
  tags:
    - docbuilder
  stage: build
  script:
    - echo ""Building documents!""
  artifacts:
    paths:
      - target/*.pdf
      - target/*.csv
```

CSVs will now be generated alongside every PDF.

## Adding and Modifying Content

### Guidelines

- There is a guide for
  [report writing](doc/report/Report%20Writing%20-%20Procedure.md)
- There is also a guide for
  [quotation writing](doc/offerte/Offerte%20Writing%20Procedure.md)

### Example documents

Besides the reports and quotations, generic documents can also be created. Those
can be found [here](doc/examples)

## Important note

From version 2.0 onwards, the structure of the PenText repository has been
simplified. Please see the [CHANGELOG](CHANGELOG.md) for more information.
","'documentation-generator', 'pdf', 'pentest-report', 'pentesting'",2024-04-18T14:18:27Z,12,114,16,"('skyanth', 540), ('PeterMosmans', 232), ('Einzelganger', 38), ('Synchro', 25), ('gronke', 19), ('melanierieback', 11), ('sinteur', 9), ('syrosh', 9), ('khorben', 7), ('ChrisMacNaughton', 3), ('4akk1', 2), ('ParalL4X', 2)","[17, 'Partnerships for the Goals']"
OpenLMIS/openlmis-ref-distro,OpenLMIS v3+ Reference Distribution. Last mile health commodity information system.,"# OpenLMIS Reference Distribution
Location for the OpenLMIS v3+ Reference Distribution

The Reference Distribution utilizes Docker Compose to gather the published OpenLMIS Docker Images together and
launch a running application.  These official OpenLMIS images are updated frequently and published to
[our](https://hub.docker.com/u/openlmis/) Docker Hub. These images cover all aspects of OpenLMIS: from server-side
Services and infrastructure to the reference UI modules that a client's browser will consume.

The docker-compose files within this repository should be considered the authoritative OpenLMIS Reference Distribution,
as well as a template for how OpenLMIS' services and UI modules should be put together in a deployed instance of
OpenLMIS following our architecture.

## Starting the Reference Distribution

## Tech Requirements

* Docker Engine: 1.12+
* Docker Compose: 1.8+

Note that Docker on Mac and Windows hasn't always been as native as it is now with [Docker for Mac](https://www.docker.com/products/docker#/mac)
and [Docker for Windows](https://www.docker.com/products/docker#/windows).  If you're using one of these, please note that there are some known issues:

* docker compose on Windows hasn't supported our development environment setup, so you _can_ use Docker for Windows to run the Reference Distribution, but not to develop
* if you're on a Virtual Machine, finding your correct IP may have some caveats - esp for development


### Quick Setup

1. Copy and configure your settings, edit `VIRTUAL_HOST` and `BASE_URL` to be your IP address
(if you're behind a NAT, then don't mistakenly use the router's address), You __should only need
to do this once__, though as this is an actively developed application, you may need to check the
environment file template for new additions.
  ```
  $ cp settings-sample.env settings.env
  ```

  Note that 'localhost' will not work here—-it must be an actual IP address (like aaa.bbb.yyy.zzz) or domain name.
  This is because localhost would be interpreted relative to each container, but providing your
  workstation's IP address or domain name gives an absolute outside location that is reachable from each container.
  Also note that your BASE_URL will not need the port "":8080"" that may be in the environment file
  template.

2. Update api access configs in https://github.com/OpenLMIS/openlmis-ref-distro/blob/master/reporting/.env

3. Pull all the services, and bring the Reference Distribution up.  Since this is actively
developed, you __should pull the services frequently__.
  ```
  $ docker-compose pull
  $ docker-compose up -d  # drop the -d here to see console messages
  ```

4. When the application is up and running, you should be able to access the Reference Distribution at:

	```
	http:///
	```

	_note if_ you get a `HTTP 502: Bad Gateway`, it is probably still starting up all of the
	microservice containers.  You can wait a few minutes for everything to start.  You can also
	run `docker stats` to watch each container using CPU and memory while starting.

  By default the demo configuration (facilities, geographies, users, etc) is loaded on startup. To use that demo you may start
  with a demo account:

  ```
  Username:  administrator
  Password: password
  ```

  If you opted not to load the demo data, and instead need a bare-bones account to configure your system, de-activate the
  demo data and use the bootstrap account:

  ```
  Username: admin
  Password: password
  ```

  If you are configuring a production instance, be sure to secure these accounts ASAP and refer to the Configuration Guide
  for more about the OpenLMIS setup process.

5. To stop the application & cleanup:

	* if you ran `docker-compose up -d`, stop the application with `docker-compose down -v`
	* if you ran `docker-compose up` _note_ the absence of `-d`, then interupt the application with `Ctrl-C`, and perform cleanup by removing containers.  See
	our [docker cheat sheet](https://openlmis.atlassian.net/wiki/x/PwBIAw) for help on manually removing containers.
	
6. To enable unskipping previously skipped requisition line items during approval add this flag in settings.env file
   
   ```
   UNSKIP_REQUISITION_ITEM_WHEN_APPROVING=true
   
   ```

## Demo Data

It's possible to load demo data using an environment variable. This variable is
called `spring.profiles.active`.  When this environment has as one of it's 
values `demo-data`, then the demo data for the service will be loaded.  This 
variable may be set in the settings.env file or in your shell with:

```shell
$ export spring_profiles_active=demo-data
$ docker-compose up -d
```

## Performance data

Performance data may also be optionally loaded and is defined by some Services. If you'd like to 
start a demo system with a lot of data, run this script instead of executing step #2 of the Quick 
Setup.

```shell
$ export spring_profiles_active=demo-data
$ ./demo-data-start.sh
```

See http://docs.openlmis.org/en/latest/conventions/performanceData.html for
more.

## Refresh Database (Profile)

This deployment profile is used by a few services to help ensure that the database they're working against is
in a good state.  This profile should be set when:

* Manual updates to the database have been made (INSERT, UPDATE, DELETE) through SQL or another tool other
than the HTTP REST API each service exposes.
* When the Release Notes call for it to be run in an upgrade.

Using this profile means that extra checks and updates are performed.  This uses extra resources such as
memory, cpu, etc.  When set, Services will start slower, sometimes significantly slower.

Usually this profile only needs to be set before the service(s) starts once.  If no further upgrades or manual
database changes are made, the profile may be removed before subsequent starting of the service(s) to quicken
startup time.

```
spring_profiles_active=refresh-db
```

## Docker Compose configuration

The docker-compose.yml file may be customized to change:

* Versions of Services that should be deployed.
* Host ports that should be used for specific Services.

This may be configured in the included [.env](.env) file or overridden by setting the same variable in the shell.

For example to set the HTTP port to 8080 instead of the default 80:

```
export OL_HTTP_PORT=8080
./start-local.sh
```

A couple conventions:

1. The .env file has service versions.  See the [.env](.env) file for more.
1. Port mappings have defaults in the [docker-compose.yml](docker-compose.yml):
  * OL_HTTP_PORT - Host port that the application will be made available.
  * OL_FTP_PORT_20 - Host port that the included FTP's port 20 is mapped to.
  * OL_FTP_PORT_21 - Host port that the included FTP's port 21 is mapped to.


## Configuring Services

When a container needs configuration via a file (as opposed to an environment variable for example), then
there is a special Docker image that's built as part of this Reference Distribution from the Dockerfile of
the `config/` directory.  This image, which will also be deployed as a container, is only a vessel for
providing a named volume from which each container may mount the `/config` directory in order to self-configure.

To add configuration:

1. Create a new directory under `config/`.  Use a unique and clear name. e.g. *kannel*.
2. Add the configuration files in this directory.  e.g. `config/kannel/kannel.config`.
3. Add a COPY statement to `config/Dockerfile` which copies the configuration file to the container's `/config`.
e.g. `COPY kannel/kannel.config /config/kanel/kannel.config`.
4. Ensure that the container which will use this configuration file mounts the named-volume `service-config` to
`/config`.  e.g.

  ```shell
  kannel:
    image: ...
    volumes:
      - 'service-config:/config'
  ```
5. Ensure the container uses/copies the configuration file from `/config/...`.
6. When you add new configuration, or change it, ensure you bring this Reference Distribution with the `--build`
flag.  e.g. `docker-compose up --build`.

The logging configuration utilizes this method.

_NOTE:_ that the configuration container that's built here doesn't _run_.  It is normal for it's Status to be
Exited.

## Logging

Logging configuration is ""passed"" to each service as a file (logback.config) through a named docker volume:
`service-config`.  To change the logging configuration:

1. update `config/log/logback.xml`
2. bring the application up with `docker-compose up --build`.  The `--build` option will re-build the
configuration image.

Most logging is collected by way of rsyslog (in the `log` container) which writes to the named volume: `log`.
However not every docker container logs via rsyslog to this named volume.  For these services they log either
via docker logging or to a file for which a named-volume approach works well.

### Log container. How-to log methods in services when working with Ref-Distro - step-by-step

The `log` container runs rsyslog which Services running in their own containers may forward their logging
messages to.  This helps centralize all the various Service logging into one location.  This container writes
all of these messages to the file `/var/log/messages` of the named volume `syslog`.

The steps below work for default settings so you don't have to edit any logback.xml files.

1. Log methods in a service with ""DEBUG"" level
2. Build the code with `sudo docker-compose run --service-ports ` followed by `gradle clean build integrationTest`
2. Build an image of the service you're working on with `docker-compose -f docker-compose.builder.yml build image`
3. Change the service's version to the recently built one in .env file, for example: `OL_REFERENCEDATA_VERSION=latest`
4. Bring the application up with `docker-compose -f docker-compose.yml up`
5. Check what is the version of your openlmis/dev image 
6. To read the file with logs, mount this filesystem via:
```shell
docker run -it --rm -v openlmis-ref-distro_syslog:/var/log openlmis/dev: bash
> tail /var/log/messages
```
Different versions of docker and different deployment configurations can result in different names of the syslog volume. If `openlmis-ref-distro_syslog` doesn't work, run `docker volume ls` to see all volume names.

#### Log format for Services

The default log format for the Services is below:

* `     `

The format from the thread ID onwards can be changed in the `config/log/logback.xml` file.

### Nginx container

The `nginx` container runs the nginx and consul-template processes.  These two log to the named volumes:

* `nginx-log` under `/var/log/nginx/log`
* `consul-template-log` under `/var/log/consul/template`

e.g to see Nginx's access log:

```shell
$ docker run -it --rm -v openlmis-ref-distro_nginx-log:/var/log/nginx/log openlmis/dev:3 bash
> tail /var/log/nginx/log/access.log
```

Different versions of docker and different deployment configurations can result in different names of the syslog volume. If `openlmis-ref-distro_nginx-log` doesn't work, run `docker volume ls` to see all volume names.

With Nginx it's also possible to use Docker's logging so that both logs are accessible via `docker logs `.  
This is owed to the configuration of the official Nginx image.  To use this configuration, change the environment
variable `NGINX_LOG_DIR` to `NGINX_LOG_DIR=/var/log/nginx`.

### Postgres container

If using the postgres container, the logging is accessible via:  `docker logs openlmisrefdistro_db_1`.

## Cleaning the Database

Sometimes it's useful to drop the database completely, for this there is a script included that
is able to do just that.

*Note this should never be used in production, nor should it ever be deployed*

To run this script, you'll first need the name of the Docker network that the database is using.
If you're using this repository, it's usually the name `openlmisrefdistro_default`.  With this run
the command:

 ```shell
 docker run -it --rm --env-file=.env --network=openlmisrefdistro_default -v $(pwd)/cleanDb.sh:/cleanDb.sh openlmis/dev:3 /cleanDb.sh
 ```
 Replace `openlmisrefdistro_default` with the proper network name if yours has changed.

 *Note that using this script against a remote Docker host is possible, though not advised*

## Production

When deploying the Reference Distribution as a production instance, you'll need to remember to set the following
environment variable so the production database isn't first wiped when starting:

```
export spring_profiles_active=""production""
docker-compose up --build -d
```

## Documentation
Documentation is built using Sphinx. Documents from other OpenLMIS repositories are collected and published on readthedocs.org nightly.

Documentation is available at:
http://openlmis.readthedocs.io

## Troubleshooting

When connecting locally to the `UAT` database on some networks connections were being cut after short amount of time. In order to 
resolve it we have added following code to the `.env` file:

```
spring.datasource.hikari.maxLifetime=180000
spring.datasource.hikari.idleTimeout=90000
```

When some requests are throwing `404` errors it is possible that the NGINX_TIMEOUT value has to be adjusted in the `.env` file.

","'docker', 'global-health', 'healthcare', 'lmis', 'logistics', 'microservices', 'open-source', 'openlmis', 'supply-chain'",2024-04-19T18:51:13Z,50,62,37,"('joshzamor', 201), ('chunky56', 85), ('antonatem', 82), ('brandonbowersox', 51), ('kpalkowska', 50), ('Wambere', 36), ('pbuzderewicz', 34), ('nickdotreid', 34), ('craigappl', 26), ('mkwiatkowskisoldevelo', 26), ('koch0142', 25), ('akmad', 22), ('saleksandra', 20), ('samimvr', 15), ('NikolaLaskowska', 14), ('jasonrogena', 14), ('sebbrudzinski', 13), ('pgesek', 12), ('alebiedzinski', 12), ('lukaslew', 12), ('HusnaHariz', 12), ('ngraczewski', 11), ('pld', 11), ('morrismukiri', 10), ('anawrotsoldevelo', 10), ('dariapionk', 9), ('mgrochalskisoldevelo', 9), ('pwargulak', 9), ('oskarhinc', 8), ('jkondrat', 7), ('eliasmu', 6), ('MagdaToczek', 6), ('dbienkowska', 6), ('cuipengfei', 6), ('krzysiekSolD', 5), ('pmironiuk', 4), ('pmuchowski', 4), ('aswiszcz', 4), ('sniedzielski', 3), ('benleibert', 3), ('mwedel', 2), ('P-Pinker', 2), ('pnaw94', 2), ('ellymakuba', 2), ('nlaskowska', 2), ('msomierick', 2), ('geekeren', 1), ('pgarrison', 1), ('sradziszewski', 1), ('adugnaworku', 1)","[9, 'Industry, Innovation and Infrastructure']"
pixframe/towi,Towi was born as a platform to evaluate and develop cognitive abilities in children between 6 and 12 years old. http://www.pixframestudios.com,"# Towi Portal Backend

Towi was born as a platform to evaluate and develop cognitive abilities in children between 6 and 12 years old.

Now, thanks to the support of the UNICEF INNOVATION FUND, it is also a research tool that incorporates elements of big data and artificial intelligence to obtain information about children´s cognitive abilities and the learning difficulties that they can present, among other topics of scientific, psychological and educative interest.

Towi platform and its data base are available for research for free. To start using it you just follow the next instructions.

## Getting Started

These instructions will get you a copy of Towi up and running on your local machine for development and testing purposes.

### Prerequisites

This are the prerequisites to install Towi on your local machine.

```
Python 3.5 >= 3.7
MySQL
Virtual Environment
Sendgrid Account
Openpay Account
```

## Installing

Please clone this repository
```
git clone https://github.com/pixframe/towi_portal
```

Create a new python environment and install the requirements

```
virtualenv -p python3.5 YOUR-ENVIRONMENT-NAME

workon YOUR-ENVIRONMENT-NAME/bin/activate

pip install -r requirements/develop.txt
```

Install the test dump for MySQL DB

```
mysqldump -u YOUR-USER -p YOUR_DATABASE < towi_portal/db/towi_test_dump.sql
```
After set the dump db please configure your config/settings/local.py file:
```
from .base import *

DEBUG = False or True

ALLOWED_HOSTS = [""ALLOWED_HOSTS""]

DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.mysql"",
        'NAME': ""DATABASE-NAME"",
        'USER': ""DB-USERNAME"",
        'PASSWORD': ""DB-USERNAME-PASSWORD"",
        'HOST': ""HOST"",
        'PORT': ""PORT""
       }
   }

STATIC_URL = ""/NAME-FOR-THE-STATIC-ROOT/""

STATIC_ROOT = ""PROJECT-ROOT""


```
After configuring your config/settings/local.py
```
python manage.py migrate
python manage.py collectstatic
python manage.py runserver
```

You will receive this in the console:
```
System check identified no issues (0 silenced).
--DATE OF RUNSERVER--
Django version 1.11.7, using settings ""config.settings.local""
Starting development server at http://localhost/
Quit the server with CONTROL-C.
```

Please read the system documentation in:

```
http://localhost/docs/
```

## Running the tests

Go to:

```
http://localhost/inicio/
```

Register or login with a test account to see the game reports and more.


## Built With

* [Django](https://www.djangoproject.com/) - The web framework used.
* [Django Rest Framework](www.django-rest-framework.org/) - Used to generate the web API's.
* [Celery](www.celeryproject.org) - Used to manage tasks.
* [drf-yasg](https://rometools.github.io/rome/) - Used to generate project documentation.

## Contributing

Please read [CONTRIBUTING.md](https://github.com/pixframe/towi/blob/master/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.


## Authors

* **Pixframe Studios SAPI de CV**  - [Pixframe Studios](https://www.pixframestudios.com)

See also the list of [contributors](https://github.com/pixframe/towi/blob/master/CONTRIBUTORS.md) who participated in this project.

## License

This project is licensed under the GNU License - see the [LICENSE.md](https://github.com/pixframe/towi/blob/master/LICENSE) file for details

## Acknowledgments

* Unicef
* Publimetro
* Nyx Technologies
",,2022-12-08T02:33:27Z,3,2,0,"('icasillas-nyx', 2), ('IvanMSP', 1), ('crifa', 1)","[4, 'Quality Education']"
somleng/somleng-scfm,Open Source Contact Center Platform and Call Flow Manager for Somleng (or Twilio),"# Somleng SCFM

![Build](https://github.com/somleng/somleng-scfm/workflows/Build/badge.svg)
[![View performance data on Skylight](https://badges.skylight.io/status/YxPzpqwXsqPx.svg)](https://oss.skylight.io/app/applications/YxPzpqwXsqPx)

Somleng Simple Call Flow Manager (Somleng SCFM) (part of [The Somleng Project](https://github.com/somleng/somleng-project)) is an Open Source Contact Management System and Call Flow manager. It can manage both inbound and outbound calls through [Somleng](https://github.com/somleng/somleng) or [Twilio](https://www.twilio.com/).

Somleng SCFM is best used via the [REST API](https://www.somleng.org/docs/scfm). There is also a dashboard available so you can play around without writing any code.

This repository includes the following core features:

* [REST API](https://www.somleng.org/docs/scfm)
* [Terraform infrastructure as code](https://github.com/somleng/somleng-scfm/tree/develop/infrastructure) for deployment to AWS
* Dashboard

## Concepts

SCFM contains the following core concepts:

* Contact - A person with a phone number and optional custom metadata.
* Callout - A callout to a group of people with a call flow logic defined as code.
* Callout Participation - A participation of a contact in a callout.
* Phone Call - A phone call to the participation.

Unlike [RapidPro](https://community.rapidpro.io/), which provides a graphical user interface for designing call flows, SCFM is powered by programmable call-flow logic.

## Documentation

* [SCFM REST API](https://www.somleng.org/docs/scfm)

## Usage

Somleng SCFM can be run independent of Somleng, so it's not a requirement to have to full Somleng stack up and running. If you want actual phone calls to be delivered/answered then you can either run the full Somleng stack on your development machine by following the [GETTING STARTED](https://github.com/somleng/somleng-project/blob/master/docs/GETTING_STARTED.md) guide, or you can connect it to Twilio. Both options can be configured via the Dashboard.

## Deployment

The [infrastructure directory](https://github.com/somleng/somleng-scfm/tree/develop/infrastructure) contains [Terraform](https://www.terraform.io/) configuration files in order to deploy Somleng SCFM to AWS.

The infrastructure in this repository depends on some shared core infrastructure. This core infrastructure can be found in [The Somleng Project](https://github.com/somleng/somleng-project/tree/master/infrastructure) repository.

## License

The software is available as open source under the terms of the [MIT License](http://opensource.org/licenses/MIT).
",,2024-04-30T20:23:22Z,10,26,3,"('dependabot-previewbot', 1015), ('dependabotbot', 544), ('dwilkie', 436), ('dependabot-support', 301), ('github-actionsbot', 242), ('samnang', 62), ('simleap', 22), ('depfubot', 2), ('lukechurch', 1), ('lacabra', 1)","[9, 'Industry, Innovation and Infrastructure']"
openimis/web_app_vb,openIMIS Web Application,"# openIMIS Web Application

The openIMIS Web Application is the main component of the openIMIS infrastructure.
It is used to manage all openIMIS entities from any browser.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

In order to use and develop the openIMIS Web Application on your local machine, you first need to install:

* SQL Server instance (follow [database installation](https://github.com/openimis/database_ms_sqlserver))


### Installing

To make a copy of this project on your local machine, please clone the repository.

```
git clone https://github.com/openimis/web_app_vb
```

Restore the NuGet packages needed by the application using VS or [nuget CLI](https://www.nuget.org/downloads).

```
nuget restore
```

From the [IMIS](./IMIS/) folder, remove the .dist extension from web.debug.config.dist, or web.release.config.dist
(depending on which configuration you need). In the chosen file, change the connection string to connect to the database.

```

```

Then, build the application via Visual Studio; this action will also generate the web.config file. The latter is not
included in the sources for privacy purpose.

<!--## Running the tests

Explain how to run the automated tests for this system

### Break down into end to end tests

Explain what these tests test and why

```
Give an example
```

### And coding style tests

Explain what these tests test and why

```
Give an example
```-->

## Deployment

For deployment please read the [installation manual](https://openimis.atlassian.net/wiki/spaces/OP/pages/906952742/WA2.2+Web+Application+installation).

<!--## Built With

* [Visual Studio](https://visualstudio.microsoft.com/) - The web framework used
* [Dropwizard](http://www.dropwizard.io/1.0.2/docs/) - The web framework used
* [Maven](https://maven.apache.org/) - Dependency Management
* [ROME](https://rometools.github.io/rome/) - Used to generate RSS Feeds
-->

<!--## Contributing

Please read [CONTRIBUTING.md](https://gist.github.com/PurpleBooth/b24679402957c63ec426) for details on our code of conduct, and the process for submitting pull requests to us.
-->

## Versioning

We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/openimis/web_app_vb/tags). 


<!--## Authors

* **Billie Thompson** - *Initial work* - [PurpleBooth](https://github.com/PurpleBooth)

See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.
-->

## User Manual 

The user manual can be read on [openimis.readthedocs.io](http://openimis.readthedocs.io/en/latest/user_manual.html).

## Issues

To report a bug, request a new features or asking questions about openIMIS, please use the [openIMIS Service Desk](https://openimis.atlassian.net/servicedesk/customer/portal/1). 

## License

Copyright (c) Swiss Agency for Development and Cooperation (SDC)

This project is licensed under the GNU AGPL v3 License - see the [LICENSE.md](LICENSE.md) file for details.

<!--## Acknowledgments

* Hat tip to anyone whose code was used
* Inspiration
* etc
-->
",,2022-12-08T05:31:27Z,13,6,12,"('dragos-dobre', 226), ('hirensoni913', 73), ('dborowiecki', 61), ('delcroip', 34), ('malinowskikam', 30), ('areklalo', 24), ('sniedzielski', 24), ('wzglinieckisoldevelo', 22), ('druchniewicz', 15), ('DragosDobre', 9), ('avanobberghen', 5), ('edarchis', 2), ('purusap', 1)","[9, 'Industry, Innovation and Infrastructure']"
marytts/marytts,"MARY TTS -- an open-source, multilingual text-to-speech synthesis system written in pure java","[![CI](https://github.com/marytts/marytts/actions/workflows/main.yml/badge.svg)](https://github.com/marytts/marytts/actions/workflows/main.yml)

# MaryTTS

This is the source code repository for the multilingual open-source MARY text-to-speech platform (MaryTTS).
MaryTTS is a client-server system written in pure Java, so it runs on many platforms.

**For a downloadable package ready for use, see [the releases page](https://github.com/marytts/marytts/releases).**

Older documentation can also be found at https://github.com/marytts/marytts-wiki, http://mary.dfki.de and https://mary.opendfki.de.

This README is part of the the MaryTTS source code repository.
It contains information about compiling and developing the MaryTTS sources.

The code comes under the Lesser General Public License LGPL version 3 -- see LICENSE.md for details.


## Running MaryTTS

Run `./gradlew run`  (or `gradlew.bat run` on Windows) to start a MaryTTS server.
Then access it at http://localhost:59125 using your web browser.

If you want to start a MaryTTS on a different address and port, you can use the following options:

```sh
./gradlew run -Dsocket.port=5920 -Dsocket.addr=0.0.0.0 --info
```
where 5920 is the new port and 0.0.0.0 the new address. In case of the address being 0.0.0.0, all the interfaces will be listened.

By using the option `--info`, you set the logger of `gradle` *AND* MaryTTS at the level INFO. By using `--debug`, you set the level to DEBUG.

It is also possible to set the MaryTTS logger level to `INFO` or `DEBUG` by defining the system variable `log4j.logger.marytts`.

## Downloading and installing voices

Run `./gradlew runInstallerGui` to start an installer GUI to download and install more voices.
A running MaryTTS server needs to be restarted before the new voices can be used.


## Building MaryTTS

Run `./gradlew build`.
This will compile and test all modules, and create the output for each under `build/`.

Note that previously, MaryTTS v5.x was built with Maven. Please refer to the [**5.x branch**](https://github.com/marytts/marytts/tree/5.x).


## Packaging MaryTTS

Run `./gradlew distZip` or `./gradlew distTar` to build a distribution package under `build/distributions`.
You can also ""install"" an unpacked distribution directly into `build/install` by running `./gradlew installDist`.

The distribution contains all the files required to run a standalone MaryTTS server instance, or to download and install more voices.
The scripts to run the server or installer GUI can be found inside the distribution in the `bin/` directory.


##  Using MaryTTS in your own Java projects

The easiest way to use MaryTTS in your own Java projects is to declare a dependency on a relevant MaryTTS artifact, such as the default US English HSMM voice:

### Maven

Add to your `pom.xml`:
```xml

  
    https://mlt.jfrog.io/artifactory/mlt-mvn-releases-local
  



  
    de.dfki.mary
    voice-cmu-slt-hsmm
    5.2.1
    
      
        com.twmacinta
        fast-md5
      
      
         gov.nist.math
         Jampack
      
    
  

```

### Gradle

Add to your `build.gradle`:
```groovy
repositories {
   mavenCentral()

   exclusiveContent {
      forRepository {
         maven {
            url 'https://mlt.jfrog.io/artifactory/mlt-mvn-releases-local'
         }
      }
      filter {
         includeGroup 'de.dfki.lt.jtok'
      }
   }
}

dependencies {
   implementation group: 'de.dfki.mary', name: 'voice-cmu-slt-hsmm', version: '5.2.1', {
      exclude group: 'com.twmacinta', module: 'fast-md5'
      exclude group: 'gov.nist.math', module: 'Jampack'
   }
}
```


## Synthesizing speech

Text to wav basic examples are proposed in this repository
- Maven: https://github.com/marytts/marytts-txt2wav/tree/maven
- Gradle: https://github.com/marytts/marytts-txt2wav/tree/gradle


## Using MaryTTS for other programming languages

If you want to use MaryTTS for other programming languages (like python for example), you need to achieve 3 steps

1. compiling marytts
2. starting the server
3. query synthesis on the server


### Synthesize speech using the server

Synthesizing speech, using the server, is pretty easy.
You need to generate proper HTTP queries and deal with the associated HTTP responses.
Examples are proposed :
- python 3: https://github.com/marytts/marytts-txt2wav/tree/python
- shell: https://github.com/marytts/marytts-txt2wav/tree/sh

## Extra documentation

### Server as service (Linux specific)

An example of how to define marytts server as service is proposed [here](./src/main/dist/misc/marytts.server).

### User dictionaries

You can extend the dictionaries by adding a user dictionary. The documentation of how to do it is [here](./src/main/dist/user-dictionaries/README.md).

## Contributing

The recommended workflow for making contributions to the MaryTTS source code is to follow the GitHub model:

1. fork the MaryTTS repository into your own profile on GitHub, by navigating to https://github.com/marytts/marytts and clicking ""fork"" (of course you need a GitHub account);

2. use the `git clone`, `commit`, and `push` commands to make modifications on your own marytts repository;
   in this process, make sure to `git pull upstream master` regularly to stay in sync with latest developments on the master repo;

3. when you think a reusable contribution is ready, open a ""pull request"" on GitHub to allow for easy merging into the master repository.

Have a look at the [GitHub documentation](http://help.github.com/) for further details.


### IDE configuration

Wiki pages are available to help you to configure your IDE to develop MaryTTS.
The following IDEs have been tested and documented:

- IntelliJ IDEA
- Eclipse: https://github.com/marytts/marytts/wiki/Eclipse
","'java', 'speech-synthesis', 'text-to-speech', 'tts'",2023-04-14T15:17:31Z,39,2231,138,"('psibre', 1219), ('marc1s', 1198), ('marcelach1', 374), ('seblemaguer', 93), ('ftesser', 57), ('aitorme', 50), ('HaraldBerthelsen', 24), ('timobaumann', 16), ('Rootex', 14), ('wholder', 10), ('giuliopaci', 8), ('Munzey', 6), ('insa-k', 6), ('gsommavilla', 5), ('PeterGilles', 3), ('fhennig', 2), ('dargmuesli', 2), ('qwertologe', 2), ('schnelle', 2), ('murtraja', 1), ('sathishpc', 1), ('kubikrubikvkube', 1), ('yuripourre', 1), ('alishah-ahmed', 1), ('entenbein', 1), ('jcrumpton', 1), ('kilida', 1), ('kwatters', 1), ('x3a', 1), ('moitreebasu1990', 1), ('Krrishdhaneja', 1), ('jart', 1), ('jarekczek', 1), ('hzeller', 1), ('5str', 1), ('candrews', 1), ('carldea', 1), ('attacomsian', 1), ('Mechaniquake', 1)","[17, 'Partnerships for the Goals']"
Terrastories/terrastories,"Terrastories is a geostorytelling application for mapping, managing and sharing place-based stories.","![Terrastories](documentation/logo.png)


 
 
















## About Terrastories

**Terrastories** is an open-source geostorytelling application for mapping, managing and sharing place-based stories. The application is being co-created with Indigenous and other local communities to collectively manage their oral histories and other cultural knowledge, but it can be used by anyone to create a map of their stories.

Terrastories is a Dockerized Rails and React app that uses [**Mapbox GL JS**](https://mapbox.com) / [**MapLibre GL JS**](https://maplibre.com/) to help users locate place-based media content or narrative stories on an interactive map. As a local-first application, Terrastories is designed to work entirely offline, so that remote communities can access the application entirely without needing internet connectivity.

The main Terrastories interface is principally composed of an interactive map and a sidebar with media content. Users can explore the map and click on activated points to see the stories associated with those points. Alternatively, users can interact with the sidebar and click on stories to see where in the landscape these narratives took place. 

By means of a content management system, users with the right level of access can also explore, add, edit, remove, and import stories, or set them as restricted so that they are viewable only with a special login. Users can design and customize the content of the interactive map entirely.

There is also [Explore Terrastories](https://github.com/terrastories/explore-terrastories), a separate React app that allows public exploration of unrestricted stories that communities have opted into sharing. Explore Terrastories queries the API of the main Terrastories application provided in this repository.

Learn more about Terrastories on [our website](https://terrastories.app/). 

> ❗️ *The remainder of the documentation on Github is **for developers**. For documentation on using Terrastories, or setting up Terrastories on an online, offline ""Field Kit"", or mesh network server, please visit the Terrastories Support Materials at **[https://docs.terrastories.app/](https://docs.terrastories.app/)***

![](documentation/terrastories.gif)
###### *Terrastories: Matawai Konde 1.0 (October 2018)*

## Install Terrastories

Terrastories can be set up for different hosting environments, including online, local (development), mesh network, or offline ""field kit"". For local or offline hosting, there is a convenience script that walks you through all of the steps, or you can choose to follow the more granular guides for the various environments and operating systems.

### Prerequisites

#### Docker
Local development and offline mode both require Docker to be installed.

Download and install [Docker](https://www.docker.com/products/docker-desktop/) for your platform.

> NOTE: Windows requires WSL 2.0 or virtualization in order to work. Additionally, it is possible that you may need to configure some additional settings for Terrastories to properly work on Windows.

#### Tileserver (Offline ""Field Kit"" Mode)

If you plan on running Terrastories offline, you'll need to configure local tiles for offline use.

A default, open-license map for using offline with Terrastories is available at https://github.com/terrastories/default-offline/tiles. You will have the option of downloading these using the setup script below. Alternatively, you can manually download these files and place them in the `tileserver/data` directory, and they should work when you load Terrastories in Field Kit mode.

### Setup

1. Clone this repository
1. Run
   ```sh
   $ ./bin/setup
   ```
   and follow the prompts.

Once you have set up Terrastories, you can log in to the super admin console, or the sample Terrastories community, using login information found in `rails/db/seeds.rb`.

If you are developing with an online (Mapbox) map, you will need to provide an access token. Copy the contents of the `.env.example` file into a newly created file called `.env` (Do not change .env.example!). In the `.env` file, replace where it says `pk.set-your-key-here` (after `DEFAULT_MAPBOX_TOKEN=`) with your mapbox access token. 

### Issues?

Review more granular setup options in the [Setup](documentation/SETUP.md) documentation.
## Developing with Terrastories

To find out how to develop with the Terrastories app, read our [developer guide](documentation/DEVELOPMENT.md) and check out our [Developer Community](https://terrastories.app/community/) pages on the Terrastories website.

For a Vision statement and Roadmap, please see our [Wiki](https://github.com/Terrastories/terrastories/wiki).

## Contributing

Please visit our [contributing](CONTRIBUTING.md) page for more details.
","'community-project', 'docker', 'hacktoberfest', 'indigenous-languages', 'indigenous-peoples', 'indigenous-territories', 'mapbox-gl-js', 'maplibre-gl-js', 'mapping-tools', 'non-profit', 'open-source', 'oral-histories', 'react', 'ruby-on-rails', 'terrastories', 'tileserver-gl'",2024-04-15T16:51:59Z,79,306,12,"('mirandawang', 141), ('lauramosher', 133), ('rudokemper', 103), ('Bootjack', 63), ('dependabotbot', 36), ('mkmckenzie', 23), ('julinvictus', 22), ('MxOliver', 21), ('rgenchev', 19), ('nickmjones', 16), ('albertchae', 15), ('marc', 15), ('cmhnk', 14), ('ssoonniia', 13), ('robbkidd', 11), ('seanlinsley', 11), ('agustinariq', 10), ('bkjohnson', 10), ('kalimar', 10), ('csexton', 10), ('tundal45', 9), ('witcheswhocode', 9), ('kayhide', 9), ('baweaver', 9), ('leosoaivan', 9), ('GQuirino', 8), ('willeyen', 8), ('ohamuy', 7), ('noelledusahel', 6), ('AliTopal89', 6), ('lsfernandes92', 6), ('midhunkrishna', 6), ('hereje', 5), ('astojanowski', 4), ('sudo-bmitch', 4), ('FeminismIsAwesome', 4), ('Kell-Stack', 4), ('slaloggia', 4), ('jaimevelaz', 4), ('kev-kev', 3), ('EitanFuturo', 3), ('ConnieJChi', 3), ('Aliciawyse', 3), ('sebastiancaso', 3), ('asheerrizvi', 3), ('henriquepjv', 3), ('jtu0', 3), ('Lukmin1999', 3), ('ruan-brandao', 3), ('ice1080', 2), ('andersan', 2), ('seanmarcia', 2), ('neckenth', 2), ('kathenry', 2), ('shekibobo', 2), ('gkop', 2), ('everly-gif', 2), ('jensaxena', 1), ('Julien-Bidar', 1), ('jmk-198xtsuga', 1), ('KrunkZhou', 1), ('namel3ss', 1), ('maxkadel', 1), ('nikhilbhatt', 1), ('oriane212', 1), ('spencerldixon', 1), ('tauhir', 1), ('cheta-nyadav', 1), ('IlinDmitry', 1), ('larymak', 1), ('gVirtu', 1), ('domlet', 1), ('carlosfrodrigues', 1), ('bperlik', 1), ('ashwini-seshadri', 1), ('AshishSharma1203', 1), ('arefathi', 1), ('anupeshverma', 1), ('alindeman', 1)","[11, 'Sustainable Cities and Communities']"
tidepool-org/blip,"Blip is the internal name for Tidepool for Web, a tool for seeing diabetes data in one place.","# Blip

[![Build Status](https://img.shields.io/travis/com/tidepool-org/blip.svg)](https://travis-ci.com/tidepool-org/blip)

Blip is a web app for type 1 diabetes (T1D) built on top of the [Tidepool](http://tidepool.org/) platform. It allows patients and their ""care team"" (family, doctors) to visualize their diabetes device data (from insulin pumps, BGMs, and/or CGMs) and message each other.

This README is focused on just the details of getting blip running locally. For more detailed information aimed at those working on the development of blip, please see the [developer guide](docs/StartHere.md).

* * * * *

### Table of contents

- [Install](#install)
- [Running locally](#running-locally)
- [Debugging](#debugging)
- [Running the tests](#running-the-tests)
- [Build and deployment](#build-and-deployment)
- [Using Storybook](#storybook)

* * * * *
## Install

Requirements:

- [Node.js](http://nodejs.org/ 'Node.js') version 20.x
- [Yarn](https://yarnpkg.com/ 'Yarn') version 3.6.4 or higher

*Note for Mac users:* we suggest first uninstalling any old version of Yarn installed via Homebrew.

Clone this repo [from GitHub](https://github.com/tidepool-org/blip 'GitHub: blip'), then install the dependencies:

After cloning this repository to your local machine, first make sure that you have node `20.x` and yarn `3.6.4` or higher installed. If you have a different major version of node installed, consider using [nvm](https://github.com/creationix/nvm 'GitHub: Node Version Manager') to manage and switch between multiple node (& npm/yarn) installations.

Once your environment is setup with node `20.x` and yarn `3.6.4` or higher, install the dependencies with Yarn:

```bash
$ yarn install
```

## Running locally

While blip can be run locally using a local kubernetes deployment similar to our remote environments (see [tidepool-org/development](https://github.com/tidepool-org/development/)), it's recommended that you run this locally with the built-in webpack dev server, and point to one of our remote environments

To do this, copy `config/local.example.js` to `config/local.js` and update as needed:

Uncomment any `linkedPackages` as desired to link them for local development.

These will be resolved as aliases in the webpack config. Note that you will need to ensure that the packages are installed (via `yarn install`) in each respective folder

It's recommended to use the `yarn startLocal` script to run the app, as it will automatically start the webpack development server for the `viz` repo when needed.

You may add as other modules to this list as well.

```bash
$ yarn startLocal
```

Open your web browser and navigate to `http://localhost:3000/`.

### Redux dev tools

Blip includes several Redux developer tools: the original time-travel dev tools UI, a console action logger, and a mutation tracker for catching mutations to the state tree (which should be immutable). The last of these in particular is a performance killer (though *none* of them could even be said to have a *negligible* effect on performance). By default when running for local development with `npm start` (which means `NODE_ENV` is `development`), the `DEV_TOOLS` flag will be `true`, and all of these dev tools will be active. Because they affect performance profoundly, this may not always be desirable. To turn *off* the dev tools in development, kill the Webpack dev server (i.e, the `npm start` process), run `export DEV_TOOLS=false`, then start up blip again with `npm start`.

**NB:** Due to differences in the `development` versus `production` builds of React itself (most notably PropTypes validation), performance of the app whenever `NODE_ENV` is `development` will *never* be as good as it is in the production build under a `NODE_ENV` of `production`. If you're concerned about the performance of a particular feature, the only way to test with good fidelity is with the production build, which you can do locally according to [these instructions below](#testing-the-production-build-locally).

## Debugging

The app uses the [bows](http://latentflip.com/bows/) library to log debugging messages to the browser's console. It is disabled by default (which makes it production-friendly). To see the messages type `localStorage.debug = true` in the browser console and refresh the page. Create a logger for a particular app module by giving it a name, such as:

```javascript
app.foo = {
  log: bows('Foo'),
  bar: function() {
    this.log('Walked into a bar');
  }
};
```

## Running the tests

We use [Mocha](https://mochajs.org/) with [Chai](http://chaijs.com/) for our test framework inside [Karma](https://karma-runner.github.io/) as our test runner, as well as [Sinon.JS](http://sinonjs.org/) and [Sinon-Chai](https://github.com/domenic/sinon-chai) for spies and stubs. Our tests currently run on headless Chrome.

To run the unit tests, use:

```bash
$ yarn test
```

To run the unit tests in watch mode, use:

```bash
$ yarn run test-watch
```

### Testing the production build locally

You can also build everything at once locally to test the production build by simply running:

```bash
$ yarn build
$ yarn server
```


## Storybook

To run storybook, use:

```bash
$ yarn storybook
```

## Stylelint

To run stylelint

```bash
$ npm run lint:css
```
",,2024-04-27T00:29:42Z,37,105,37,"('clintonium-119', 4714), ('krystophv', 1063), ('jebeck', 1012), ('jh-bate', 589), ('nicolashery', 527), ('GordyD', 436), ('ianjorgensen', 108), ('hntrdglss', 84), ('cheddar', 82), ('courtenayhuffman', 80), ('kentquirk', 76), ('ursooperduper', 54), ('dependabotbot', 43), ('darinkrauss', 41), ('coyotte508', 31), ('ginnyyadav', 26), ('gcharest', 23), ('jehernandezrodriguez', 13), ('snyk-bot', 12), ('gniezen', 12), ('jpreillymb', 11), ('pazaan', 9), ('derrickburns', 9), ('bcecilio', 6), ('bewest', 4), ('HowardLook', 3), ('mrinnetmaki', 3), ('dotMR', 3), ('anderspitman', 3), ('rspier', 2), ('andrew-dixon', 1), ('mortonfox', 1), ('PeterSampson', 1), ('creswick', 1), ('toddkazakov', 1), ('franck-fourel', 1), ('pkparimi', 1)","[3, 'Good Health and Well-Being']"
LoopKit/CGMBLEKit,Make your G5/G6 data truly mobile.,"# CGMBLEKit

[![CI Status](http://img.shields.io/travis/LoopKit/CGMBLEKit.svg?style=flat)](https://travis-ci.org/LoopKit/CGMBLEKit)
[![Carthage compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage)

A iOS framework providing an interface for communicating with the G5 and G6 glucose transmitters over Bluetooth.

*Please note this project is neither created nor backed by Dexcom, Inc. This software is not intended for use in therapy.*

## Requirements

This framework connects to a G5 or G6 Mobile Transmitter via Bluetooth LE. It does not connect to the G4 Share Receiver or any earlier CGM products.

## Frameworks Installation

### Carthage

CGMBLEKit is available through [Carthage](https://github.com/Carthage/Carthage). To install it, add the following line to your Cartfile:

```ruby
github ""LoopKit/CGMBLEKit""
```

Note that you'll need to confgure your target to link against `CommonCrypto.framework` in addition to `CGMBLEKit.framework`

## Usage

If you plan to run your app alongside the G5 Mobile application, make sure to set `passiveModeEnabled` to true.

### Examples

[glucose-badge](https://github.com/dennisgove/glucose-badge) – Display the latest glucose values as an app icon badge

## ResetTransmitter App Installation

Download the CGMBLEKit code by clicking on the green `Clone or Download` button (scroll up on this page and you'll find it), then select `Download Zip`

![ResetTransmitter help](https://github.com/Kdisimone/images/blob/master/resetTransmitter-first.png)

Then navigate to the `CGMBLEKit` folder that just downloaded to your computer.  Double-click on the `CGMBLEKit.xcodeproj` file to open the project in Xcode.

![ResetTransmitter help](https://github.com/Kdisimone/images/blob/master/resetTransmitter-download.png)

To install the ResetTransmitter App on your iPhone, simply make sure to sign the ResetTransmitter target and then select just the `ResetTransmitter` scheme in the build area.  Make sure your iPhone is plugged into the computer, select your iPhone from the top of the `Devices` in the 4th circled area, screenshot below.  Note: You do not have to change bundle IDs or anything beyond the steps listed.

![ResetTransmitter help](https://github.com/Kdisimone/images/blob/master/resetTransmitter.png)


## Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](https://github.com/LoopKit/LoopKit/blob/master/CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## License

CGMBLEKit is available under the MIT license. See the LICENSE file for more info.
",,2024-04-12T14:45:12Z,18,140,39,"('ps2', 187), ('loudnate', 100), ('nhamming', 12), ('darinkrauss', 7), ('Kdisimone', 4), ('cfaagaard', 3), ('thebookins', 3), ('Camji55', 2), ('hummelstrand', 2), ('mylma', 2), ('IsThisPaul', 2), ('aranasaurus', 2), ('ktomy', 1), ('whiten', 1), ('ReadmeCritic', 1), ('rickpasetto', 1), ('achkars', 1), ('pi4ohhi7', 1)","[3, 'Good Health and Well-Being']"
OperationCode/operationcode_backend,This is the backend repo for the Operation Code website,"
  
  
    <img
      alt=""Operation Code Hacktoberfest Banner""
      src=""https://s3.amazonaws.com/operationcode-assets/branding/logos/large-blue-logo.png""
    >
  
  
  


[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Twitter Follow](https://img.shields.io/twitter/follow/operation_code.svg?style=social&label=Follow&style=social)](https://twitter.com/operation_code)
[![GitHub contributors](https://img.shields.io/github/contributors/cdnjs/cdnjs.svg)](https://github.com/OperationCode/operationcode_backend)


[![Build Status](https://travis-ci.org/OperationCode/operationcode_backend.svg?branch=master)](https://travis-ci.org/OperationCode/operationcode_backend)
[![View performance data on Skylight](https://badges.skylight.io/status/0iQU6bEW8ha1.svg)](https://oss.skylight.io/app/applications/0iQU6bEW8ha1)
[![Good First Issue](https://img.shields.io/github/issues/OperationCode/front-end/good%20first%20issue.svg)](https://github.com/OperationCode/operationcode_backend/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)

# Welcome!

This is the back-end application for [OperationCode](https://operationcode.org). We highly recommend [joining our organization](https://operationcode.org/join) to receive an invite to our Slack team. From there, you'll want to join the `#oc-projects` and `#oc-ruby-projects` channels. You can get help from multiple professional developers.

Before contributing, please review our [Contributing Guide](CONTRIBUTING.md)

## Maintainers

For information about the maintainers of the project, check out [MAINTAINERS.md](MAINTAINERS.md).

## Quick Start

If you're unsure of how to start this app or code for it, don't worry! You're our target audience!
Please read our [Contributing Guide](CONTRIBUTING.md) to learn everything you need to be able to ask the right questions on our Slack team.

**In continuing with the quick start instructions, it is assumed that you are no stranger to Rails Applications, and standard GitHub workflows such as forking, cloning, and branching.**

Recommended versions of tools used within the repo:

* Ruby: See [.ruby-version](/.ruby-version)
* `ruby@2.3.3`
* `git@2.17.0` or greater


  Expand To View Quickstart

 Install Dependencies (git, gnu make, docker) 

  
   git 
     make 
     docker 
  

  Retrieve Codebase 
  Setup Database 
  Setup Codebase 
  Interact with Codebase 





## Description

If you'd like to learn more about what this project is what a Backend is you can read about this in our [Introduction Guide](CONTRIBUTING.md#explanations).

## Wiki

We also have a [wiki](https://github.com/OperationCode/operationcode_backend/wiki) that contains common tutorials, tips, workflows and documentation around how to successfully contribute to, and work within, the operationcode_backend repository.

## Postmortem Reports



Click to expand 


As incidents happen that require a great deal of troubleshooting and reveal a certain lack of domain knowledge, we wish to document these issues in way that can be used for contributors to learn. When such an event happens and a Root Cause Analysis is performed these issues will be added here.

For the purpose of security we need to ensure that no data is added to AAR that would compromise any operation code asset. These reports are intended to document issues and reasoning for future contributors and assist future investigations. In these reports we keep the details blameless and focus on how we can improve and iterate in a better manner.


 Slack User invites Stopped link



## License

MIT Licensed , see [LICENSE](LICENSE) for details
",,2019-11-15T20:49:00Z,56,62,12,"('hpjaj', 378), ('apex-omontgomery', 66), ('nagano564', 66), ('PixelWitch', 63), ('alexbeeken', 45), ('kylemh', 37), ('leenyburger', 36), ('hollomancer', 33), ('cmar', 31), ('rickr', 27), ('igor-starostenko', 19), ('nellshamrell', 17), ('jmayergit', 14), ('robbkidd', 13), ('Hiren-Mehta', 12), ('juliantrueflynn', 10), ('jjhampton', 7), ('jaydorsey', 6), ('kirbyfern', 6), ('Brantron', 5), ('ohaiwalt', 5), ('PSkaggs', 5), ('Cooperbuilt', 5), ('JYoung217', 4), ('ramillim', 3), ('yosefbennywidyo', 3), ('ianroberts131', 3), ('dmarchante', 3), ('BRIMIL01', 3), ('robotscissors', 2), ('mkmckenzie', 2), ('mattierocks', 2), ('luminousbeam', 2), ('vyaspranjal33', 2), ('martinzugnoni', 2), ('jvillama', 2), ('benjervis', 2), ('alexspence', 2), ('whuang8', 1), ('vaidehijoshi', 1), ('shawnlknight', 1), ('markchernov', 1), ('EvansJWang', 1), ('colto', 1), ('cmd-space', 1), ('Linette695', 1), ('kafoster11', 1), ('coloradojay', 1), ('iorme1', 1), ('henry-doan', 1), ('heliocola', 1), ('gy741', 1), ('chancancode', 1), ('GaryBaconJr', 1), ('garyray-k', 1), ('charlesjkwanin', 1)","[10, 'Reduced Inequalities']"
OpenUpSA/pmg-cms-2,Content Management System for the Parliamentary Monitoring Group · https://www.pivotaltracker.com/n/projects/1367366,"Parliamentary Monitoring Group website
======================================

Parliamentary monitoring application for use by the Parliamentary Monitoring Group in Cape Town, South Africa.
See: https://www.pmg.org.za.

## What does this project do

Allow citizens and other interested parties to monitor what's going on in the South African parliament. With specific
focus on tracking the progress of legislation as it moves through the various phases: from being introduced for the
first time to finally being approved and signed into law.

The purpose of the project is to improve parliamentary oversight, make the parliamentary process more accessible
and improve transparency surrounding the activities of parliament.

## How it works

The project consists of the following major components:

  * User-facing website, including free and paid-for content (built using Flask, Jinja2 templates, Bootstrap and jQuery)
    * https://pmg.org.za
  * Database (PostgreSQL)
  * Search engine (Elastic Search)
  * Admin interface (Flask-Admin, integration with Mandrill for email notifications)
    * https://pmg.org.za/admin
  * API (Flask)
    * https://api.pmg.org.za
    * When this web app connects to its own API, it always  connects to 127.0.0.1:5000 and sends the Host header of the `api.` subdomain of `SERVER_HOST` to avoid routing ""dogfooding"" traffic to the outside internet.

## Making use of the API

All of the data that is displayed through the frontend website, is served
through an API at https://api.pmg.org.za which is freely accessible.  However,
please note that access to some content on the frontend website is restricted,
and the same restrictions apply for the API.

* [More details on the API and what it contains are in API.md](API.md)

## Contributing to the project

This project is open-source, and anyone is welcome to contribute. If you just want to make us aware of a bug / make
a feature request, then please add a new GitHub Issue (if a similar one does not already exist).

**NOTE:** On 2015-07-05 we removed some very large files from the repo and its history, reducing the size of the repo from over 100MB to 30MB.
This required re-writing the history of the repo. You **must** [pull and rebase your changes](https://www.kernel.org/pub/software/scm/git/docs/git-rebase.html#_recovering_from_upstream_rebase).

If you want to contribute to the code, please fork the repository, make your changes, and create a pull request.

### Local setup
Build the necessary services:

    docker compose build

Setup the database:

    docker compose run web python setup_dev_database.py
    docker compose run web python app.py db stamp head
    docker compose run web python bin/search.py --reindex all

Add the following lines to your .hosts file:

    127.0.0.1 pmg.test
    127.0.0.1 api.pmg.test

Start the server:

    docker compose up

You should now see it running at [http://pmg.test:5000/](http://pmg.test:5000/) and [http://api.pmg.test:5000/](http://api.pmg.test:5000/).

You can login with:

    user : admin
    password : admin

Each time you pull in changes that might contain database changes:

    docker compose run web python app.py db migrate
    docker compose run web python app.py db upgrade

To delete the database for a completely fresh setup, run:

    docker compose down --volumes

To start the task scheduler, run:

    docker compose run web python app.py start_scheduler

### Developing email features

Run [a local mock SMTP server](http://nilhcem.com/FakeSMTP/index.html) on port 2525

Set the SMTP environment variables

```
source env.localmail
```

### Running tests

    docker compose -f docker-compose.yml -f docker-compose-test.yml run --rm web nosetests tests --exe -v

### Code formatting

We use [Black](https://github.com/psf/black) to format our code. You can install it using
`pip install black` and run it with:

    black app.py bin config pmg tests


### Deployment instructions

Deployment is to dokku, a Heroku-like environment. To deploy, simply push to the git remote:

    git push dokku

Sensitive or environment-specific configuration variables are set as environment variables using `dokku config:set`, the important ones are:

* SERVER_NAME - Flask uses this as the base hostname and port for the server - Flask Blueprint subdomains base from this. If it can't match the Host header in requests to this, it serves a 404 response.
  - `pmg.org.za` in production
  - `pmg.test:5000` in development
  - Flask seems to use this for generating absolute URLs, except when the `X-Forwarded-Host` header is provided, in which case that hostname is used for absolute URLs.
* FRONTEND_HOST - It's not currently clear if this is used anywhere
  - `https://pmg.org.za/` in production
  - `http://pmg.test:5000/` in development
* SESSION_COOKIE_DOMAIN
  - `pmg.org.za` in production
  - `pmg.test` in dev
* SQLALCHEMY_DATABASE_URI
* FLASK_ENV=production
* AWS_ACCESS_KEY_ID
* AWS_SECRET_ACCESS_KEY
* SENDGRID_API_KEY
* MAIL_PASSWORD
* SECURITY_PASSWORD_SALT
* RUN_PERIODIC_TASKS=true
* SOUNDCLOUD_APP_KEY_ID
* SOUNDCLOUD_APP_KEY_SECRET
* SOUNDCLOUD_USERNAME
* SOUNDCLOUD_PASSWORD
* SOUNDCLOUD_PERIOD_HOURS=6
* MAX_SOUNDCLOUD_BATCH=10
* S3_BUCKET=pmg-assets
* STATIC_HOST=https://static.pmg.org.za/ or http://pmg-assets.s3-website-eu-west-1.amazonaws.com/


### Reindexing for Search

To re-index all content for search, run:

    ssh dokku@dokku.code4sa.org run python bin/search.py --reindex all

This isn't normally necessary as the search index is updated as items are created, updated and deleted.
It can be useful when the index has become out of date. Search functionality will fail while the indexing
is in progress. Re-indexing takes about 10 minutes.

## Task scheduler

Dokku won't automatically start the task scheduler process. To start it, run:

    dokku ps:scale pmg worker=1

### Database migration

We use [Flask-Migrate](https://flask-migrate.readthedocs.org/en/latest/) and [Alembic](https://alembic.readthedocs.org/en/latest/) for applying changes to the data model. To setup a migration script:

    python app.py db migrate -m """"

Then to run the script on your local machine:

    python app.py db upgrade

### Updating parliamentary days

PMG needs to know the individual days in which Parliament sat, for each year. It uses this information
to calculate the number of parliamentary days that it took for bills to be adopted. It reads these days
from the file `data/parliament-sitting-days.txt`.

Updating this information is a two-step process:

1. Update the spreadsheet `data/parliament-sitting-days.xlsx` that lists the days parliament sits
2. Run `python bin/load_parliamentary_days --pm-days data/parliament-sitting-days.xlsx` to update `data/parliament-sitting-days.txt`
3. Run `git diff` to sanity check the changes
3. Commit the changes

### Caching

Application-level caching is used for certain views, initially based on which views the server spends most time on as seen in NewRelic Transaction overview.

To add caching to a view, add the following decorator - it must be the decorator closest to the view method so that it caches the view result, and not the result from other decorators:

```python
from pmg import cache, cache_key, should_skip_cache
...
@cache.memoize(make_name=lambda fname: cache_key(request),
               unless=lambda: should_skip_cache(request, current_user))
```

Arguments:

- `unless` must be true when the cache should ***not*** be used. Frontend (views.py) views must always use this because the view shows them as logged in, even on pages where the rest of the data is the same. API views that don't serve subscription data or have any user-specific data don't need it.
- `make_name` must be the cache key for the view. It's very important that query strings are taken into consideration for the cache key.
","'open-data', 'parliament', 'parliamentary-monitoring', 'python'",2024-05-03T14:28:22Z,17,4,14,"('longhotsummer', 1151), ('petrus-jvrensburg', 491), ('delenamalan', 341), ('jbothma', 305), ('j-norwood-young', 265), ('guushoekman', 116), ('smmbll', 96), ('paulmwatson', 89), ('desafinadude', 86), ('Lunga001', 39), ('knightebsuku', 30), ('PiDelport', 13), ('waracci', 10), ('dependabotbot', 8), ('michaelglenister', 8), ('havanhuy1997', 7), ('Morabaraba', 1)","[16, 'Peace, Justice and Strong Institutions']"
h5p/h5p-interactive-video,,"# H5P Interactive Video

Put texts, tasks and other media on top of your video.

[See it in action on H5P.org](https://h5p.org/interactive-video)

## Contributing

Translators, make sure to read through the [tips for language contributors](https://h5p.org/contributing). A good approach is to check that the updated language file matches the structure of the [norwegian translation](language/nn.json).  

Developers, take a look at the [developer guide](https://h5p.org/developers) which has information on [coding guidelines](https://h5p.org/code-style), [api-references](https://h5p.org/documentation/api/H5P.html) and much more. Before submitting pull-requests, please consider [testing your code thoroughly](https://github.com/h5p/h5p-interactive-video/wiki/Interactive-Video-Testplan-(November-2106-Release)) to speed up the review process.


## Building the distribution files
Downloading these files will not provide you with h5p libraries that you can upload to your system. They will have to be built and packed first.

Pull or download this archive files and go into the main folder. There run

```bash
npm install
```

to get the required modules. Then build the project using

```bash
npm run build
```

or

```bash
npm run watch
```

You can then use [H5P cli](https://github.com/h5p/h5p-cli) to pack the library e.g. using

```
h5p pack -r  
```

Alternatively, you can arrange and zip files manually, but make sure to adhere to the [H5P specification](https://h5p.org/documentation/developers/h5p-specification).

## License

(The MIT License)

Copyright (c) 2012-2014 Joubel AS

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",,2024-04-12T07:31:50Z,76,223,37,"('fnoks', 543), ('icc', 484), ('otacke', 145), ('tajakobsen', 121), ('timothyylim', 98), ('falcon-git', 96), ('weblate', 61), ('makmentins', 28), ('thomasmars', 26), ('andreascerpus', 24), ('larsmagu', 12), ('j0kerZ', 12), ('dependabotbot', 10), ('devland', 9), ('VildeStabell', 9), ('Gwenillia', 8), ('ceibg', 7), ('translate-h5p', 7), ('e-me', 7), ('ravimajithia', 7), ('fanncw', 7), ('zouhairloucif', 6), ('germanvaleroelizondo', 4), ('dnowba', 4), ('smartwayme', 4), ('Languafe', 3), ('martin-kosmos', 3), ('totoromaum', 3), ('izendegi', 3), ('Jarvil', 3), ('lesha724', 2), ('maxtetdev', 2), ('fredericraes', 2), ('flizotte', 2), ('amirsalkhori', 2), ('xtractorab', 2), ('WTurunen', 2), ('simondate', 2), ('miropuhek', 2), ('msr930', 2), ('jnavroski', 2), ('dinism', 2), ('Stephlabaraque', 1), ('suhreed', 1), ('TheGrowingPlant443', 1), ('Urpokarhu1', 1), ('qinwenshi', 1), ('christophelachance', 1), ('cruzrincon', 1), ('gR-xbY', 1), ('ingolist', 1), ('isa-realia', 1), ('jklindzic', 1), ('kagoya', 1), ('ravagsgit', 1), ('sdif-diba', 1), ('sokunthearithmakara', 1), ('viragom', 1), ('maria1186', 1), ('ademozgur87', 1), ('apienk', 1), ('antonioaneiros', 1), ('BV52', 1), ('damianciancio', 1), ('danieltebs', 1), ('duanchenggui', 1), ('erenmustafaozdal', 1), ('fredericpierron', 1), ('gerardofallani', 1), ('Hannaes', 1), ('Idamare', 1), ('IsabelZydun', 1), ('jannesarkela', 1), ('johanLTPT', 1), ('lucaboesch', 1), ('sabahuddin', 1)","[4, 'Quality Education']"
ClimateWatch-Vizzuality/climate-watch,Climate Watch: Data for Climate Action,"# Climate Watch

- [DOCS](https://climatewatch-vizzuality.github.io/climate-watch/)

- [CHANGELOG](https://climatewatch-vizzuality.github.io/climate-watch/_docs/changelog)

- [Local setup](https://climatewatch-vizzuality.github.io/climate-watch/_docs/local-setup)

- [Deploy](https://climatewatch-vizzuality.github.io/climate-watch/_docs/deploy)
","'climate', 'data', 'postgresql', 'rails', 'react'",2024-04-19T14:00:13Z,27,27,9,"('Bluesmile82', 2944), ('j8seangel', 1003), ('simaob', 998), ('edbrett', 632), ('tsubik', 415), ('kfilip94', 330), ('ticklemynausea', 258), ('agnessa', 230), ('aabdaab', 178), ('kevinlustig', 97), ('weberjavi', 77), ('tiagojsag', 67), ('andresgnlez', 62), ('santostiago', 51), ('vectorsize', 41), ('pjosh', 40), ('SARodrigues', 39), ('qoobaa', 34), ('mengping', 33), ('dependabotbot', 33), ('sw1115', 20), ('martintomas', 17), ('Leandro-vigna', 15), ('rjiang-bb', 15), ('rrequero', 9), ('juancarlosalonso', 5), ('benlaken', 4)","[13, 'Climate Action']"
benetech/MathShare,,"# MathShare 
This is a step by step editor built on top of mathlive. 

Open index.html to see a list of sample problems. Clicking on one of them will take you to the editor. Debug/index.html is for being able to debug mathlive code as well as this project's code and requires a few other files.

# Acknowledgement
Benetech and its funding partners are developing this tool to benefit all kids, regardless of their abilities, to learn and do math.

# Development
To make the MathShare application work properly there is a need to run the **MathShareBackend** server first. To do so, please follow the instruction: 
* https://github.com/benetech/MathShareBackend

MathShare uses **npm** for a development process. 
* for Linux, install npm 
```
sudo apt-get install npm
```
* for Windows it may be simply to use nodeJS installer which allows to install npm as well
http://blog.teamtreehouse.com/install-node-js-npm-windows

## Backend server connection
By default the backend server url is defined as 
```
http://localhost:8080
```
For **Windows** with Docker the real url may be different - Docker (based on Virtual Machine) uses a different IP (eg. 192.168.99.100). You can get the IP by opening Docker Quickstart Terminal - the URL should be displayed as a welcome message. 

If the backend server url is different from the default one, there is a need to change it in ```src/config.js``` file. Please set the server url value as follows:
```
""serverUrl"": ""http://:8080""
```
## Building and running 
To install all required dependencies use: 
```
npm install
``` 
To run MathShare locally on port 8080 use: 
```
npm start
``` 
or 
```
npm run start
```

## Debugging
If you want to debug MathLive from its original sources, you need to put them into the root folder and run: 
```
npm run debug
```
In this case, the application will use sources you've provided, assuming that the files structure is preserved (mathlive/src/mathlive.js as the main script). 

Additionally, React requires quite different file paths, that those used the 'define' statement in the MathLive library. To use it properly, 
* replace ```mathlive/``` in these statements to ```./``` in mathlive.js 
* replace ```mathlive/``` to ```../``` in all js files from subdirectories

Note that from time to time, the debugger won't stop on the breakpoint in imported sources - in this case simply put ```debugger;``` at the line you want to stop, and the debugger should stop there. After that debugger should use this file normally and you can remove this line and use casual breakpoints.
",,2022-12-13T14:38:02Z,11,9,9,"('rupeshparab', 1007), ('NSoiffer', 280), ('BenetechSchwab', 133), ('pawelcieszko', 68), ('oskarhinc', 61), ('johnhbenetech', 58), ('dserkowski', 16), ('jscholes', 10), ('dependabotbot', 6), ('pgesek', 5), ('sueannma', 1)","[4, 'Quality Education']"
sfbrigade/adopt-a-drain,"A web application that allows citizens to ""adopt"" a storm drain in San Francisco. In use, and in development at other brigades. Looking for a maintainer or someone interested in developing further in collaboration with others across the country.","# Adopt-a-Drain

[![Join the chat at https://gitter.im/sfbrigade/adopt-a-drain](https://badges.gitter.im/sfbrigade/adopt-a-drain.svg)](https://gitter.im/sfbrigade/adopt-a-drain?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Test Status](https://github.com/sfbrigade/adopt-a-drain/workflows/Test/badge.svg)](https://github.com/sfbrigade/adopt-a-drain/actions)

Claim responsibility for cleaning out a storm drain after it rains.


⚠️⚠️⚠️⚠️⚠️⚠️⚠️

This repository no longer reflects the [official San Francisco Adopt-a-Drain website](https://adoptadrain.sfwater.org/), which is now maintained by [Civic Hub](https://www.civichub.us/adopt-a-platform).

⚠️⚠️⚠️⚠️⚠️⚠️⚠️

## Screenshot
![Adopt-a-Drain](/adopt.png ""Adopt-a-Drain"")

## Installation
This application requires [Postgres](http://www.postgresql.org/) to be installed

    git clone git://github.com/sfbrigade/adopt-a-drain.git
    cd adopt-a-drain
    bundle install

    bundle exec rake db:create
    bundle exec rake db:schema:load

See the [wiki](https://github.com/sfbrigade/adopt-a-drain/wiki/Windows-Development-Environment) for a guide on how to install this application on Windows.

## Docker

To setup a local development environment with
[Docker](https://docs.docker.com/engine/installation/).   

```
# Override database settings as the docker host:
echo DB_HOST=db > .env
echo DB_USER=postgres >> .env

# Setup your docker based postgres database:
docker-compose run --rm web bundle exec rake db:setup

# Load data:
docker-compose run --rm web bundle exec rake data:load_things
# OR: don't load all that data, and load the seed data:
# docker-compose run --rm web bundle exec rake db:seed

# Start the web server:
docker-compose up

# Visit your website http://localhost:3000 (or the IP of your docker-machine)
```

## Usage
    rails server

## Seed Data
    bundle exec rake data:load_drains

## Deploying to Heroku
A successful deployment to Heroku requires a few setup steps:

1. Generate a new secret token:

    ```
    rake secret
    ```

2. Set the token on Heroku:

    ```
    heroku config:set SECRET_TOKEN=the_token_you_generated
    ```

3. [Precompile your assets](https://devcenter.heroku.com/articles/rails3x-asset-pipeline-cedar)

    ```
    RAILS_ENV=production bundle exec rake assets:precompile

    git add public/assets

    git commit -m ""vendor compiled assets""
    ```

4. Add a production database to config/database.yml

5. Seed the production db:

    `heroku run bundle exec rake db:seed`

Keep in mind that the Heroku free Postgres plan only allows up to 10,000 rows,
so if your city has more than 10,000 fire drains (or other thing to be
adopted), you will need to upgrade to the $9/month plan.

### Google Analytics
If you have a Google Analytics account you want to use to track visits to your
deployment of this app, just set your ID and your domain name as environment
variables:

    heroku config:set GOOGLE_ANALYTICS_ID=your_id
    heroku config:set GOOGLE_ANALYTICS_DOMAIN=your_domain_name

An example ID is `UA-12345678-9`, and an example domain is `adoptadrain.org`.

## Contributing
In the spirit of [free software][free-sw], **everyone** is encouraged to help
improve this project.

[free-sw]: http://www.fsf.org/licensing/essays/free-sw.html

Here are some ways *you* can contribute:

* by using alpha, beta, and prerelease versions
* by reporting bugs
* by suggesting new features
* by [translating to a new language][locales]
* by writing or editing documentation
* by writing specifications
* by writing code (**no patch is too small**: fix typos, add comments, clean up
  inconsistent whitespace)
* by refactoring code
* by closing [issues][]
* by reviewing patches
* [financially][]

[locales]: https://github.com/sfbrigade/adopt-a-drain/tree/master/config/locales
[issues]: https://github.com/sfbrigade/adopt-a-drain/issues
[financially]: https://secure.sfbrigade.org/page/contribute

## Submitting an Issue
We use the [GitHub issue tracker][issues] to track bugs and features. Before
submitting a bug report or feature request, check to make sure it hasn't
already been submitted. When submitting a bug report, please include a [Gist][]
that includes a stack trace and any details that may be necessary to reproduce
the bug, including your gem version, Ruby version, and operating system.
Ideally, a bug report should include a pull request with failing specs.

[gist]: https://gist.github.com/

## Submitting a Pull Request
1. [Fork the repository.][fork]
2. [Create a topic branch.][branch]
3. Add specs for your unimplemented feature or bug fix.
4. Run `bundle exec rake test`. If your specs pass, return to step 3.
5. Implement your feature or bug fix.
6. Run `bundle exec rake test`. If your specs fail, return to step 5.
7. Run `open coverage/index.html`. If your changes are not completely covered
   by your tests, return to step 3.
8. Add, commit, and push your changes.
9. [Submit a pull request.][pr]

[fork]: http://help.github.com/fork-a-repo/
[branch]: https://guides.github.com/introduction/flow/
[pr]: http://help.github.com/send-pull-requests/

## Supported Ruby Version
This library aims to support and is [tested against][travis] Ruby version 2.2.2.

If something doesn't work on this version, it should be considered a bug.

This library may inadvertently work (or seem to work) on other Ruby
implementations, however support will only be provided for the version above.

If you would like this library to support another Ruby version, you may
volunteer to be a maintainer. Being a maintainer entails making sure all tests
run and pass on that implementation. When something breaks on your
implementation, you will be personally responsible for providing patches in a
timely fashion. If critical issues for a particular implementation exist at the
time of a major release, support for that Ruby version may be dropped.

## Copyright
Copyright (c) 2015 Code for San Francisco. See [LICENSE.md](https://github.com/sfbrigade/adopt-a-drain/blob/master/LICENSE.md) for details.

[license]: https://github.com/sfbrigade/adopt-a-drain/blob/master/LICENSE.md
","'civic-infrastructure', 'code-for-america', 'code-for-sf', 'docker', 'environment', 'full-stack-developer', 'ruby', 'stable'",2023-09-28T20:09:28Z,31,44,17,"('sferik', 1486), ('jszwedko', 352), ('dependabot-support', 94), ('jasonlally', 80), ('dependabot-previewbot', 43), ('afomi', 19), ('dependabotbot', 18), ('tonyta', 12), ('therebelrobot', 8), ('squidarth', 6), ('ralam', 4), ('mlevans', 3), ('monfresh', 3), ('bensheldon', 2), ('howdoicomputer', 2), ('mick', 2), ('kylefantastic', 2), ('jasnow', 2), ('danmelton', 1), ('dsummersl', 1), ('EvanHahn', 1), ('ralreegorganon', 1), ('kmacedo', 1), ('kmcurry', 1), ('equiamos', 1), ('24glinka', 1), ('raudabaugh', 1), ('gitter-badger', 1), ('softwaresteph', 1), ('drkk', 1), ('max-mapper', 1)","[11, 'Sustainable Cities and Communities']"
opt-out-tools/opt-out,"our browser extension, that works like adBlocker but removes online misogyny instead of adverts","# Opt Out

[![CircleCI](https://circleci.com/gh/opt-out-tools/opt-out.svg?style=svg)](https://circleci.com/gh/opt-out-tools/opt-out) [![Contributor Code Of Conduct](https://img.shields.io/badge/Code%20Of%20Conduct-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)

Opt Out is a browser extension for Firefox that hides online misogyny from an individual’s twitter feed.

The General Data Protection Regulation (GDPR) has changed our lives online on social media platforms. We have the right to be forgotten, to see what is being collected about us and to opt-out if we wish. The current abuse that those who identify as women suffer is not avoidable. We see Opt Out as an extension of the GDPR that also protects the human rights of women and those with intersecting identities online. While steps have been made to protect these people online, not enough has been done. This is a global tragedy affecting the well-being, economical potential and political representation of these people. Let's **Opt Out.**

Please see the [theory](https://github.com/opt-out-tools/theory-of-online-misogyny) repository, to see our research about this problem.

Opt Out is an open source project under active development. Currently, machine learning models are being evaluated for their ability to classify sexual harassment text. If you would like to test the current model (trained on troll data), please see the 'Installation Instructions' below. If you would like to contribute to the project, please see [Contributing](https://github.com/opt-out-tools/start-here/blob/master/CONTRIBUTING.md) first, and then check out the find-out and try-out repos. Please note that this project is released with a [Contributor Code of Conduct](https://github.com/malteserteresa/opt-out/blob/master/CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## Development

### Run local development environment

```
npm run start:firefox
```

### Debugging tips

Check out these debugging tips at the [Mozilla Extension Workshop](https://extensionworkshop.com/documentation/develop/debugging/).

#### Force reload webpack build

The module hot-refreshes the browser when there is a change in the source code. To force an update in the browser, press `r` in the terminal where web-ext is running.

#### Keep the popup window open

1. In firefox, open `about:debugging`
2. Go to 'This firefox'
3. Click 'Inspect' next to Opt-Out
4. The dev tools for the popup will open in a new tab
5. **Click the three horizontal dots on the right side**
6. Disable Popup Autohide.

This will keep the popup open unless you press `Esc`, which makes it possible to debug the HTML of the Popup as if it were a static website.

#### Persisting settings on dev browser

To persist your twitter login data after stopping the process, follow these instructions:

- Create a new profile on firefox (at `about:profiles`)
- Open an instance of firefox as this profile
  - `firefox --new-instance -p your_profile_name`
- Sign in to twitter in the browser that opens, you can then close the browser
- Run the development environment start command with a flag to point to this profile.
  - `npm run start:firefox -- -p=your_profile_name`

**Important** Make sure that you do not choose a 'default' profile, such as the profile you use for your personal browsing. Here's why

> This option makes the profile specified by --firefox-profile completely insecure for daily use. It turns off auto-updates and allows silent remote connections, among other things. Specifically, it will make destructive changes to the profile that are required for web-ext to operate.

### Development Guidelines

Please:

- Join the slack if you're planning to contribute
- Keep pull requests isolated to one feature.
- Explain the changes made and how to test it in the PR description
- Ask for help on the slack or on github if needed

## More instructions

### Adding a local pre-commit hook

In case you want to be 100% sure that the linter is always running before you commit you can add this as a [git hook](https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks).
This means it will run the linter before every commit you try to make:

#### The easy way:

If you do not have a pre-commit hook defined yet, there is a script that will copy the file for you.
Run this command and it will create the pre-commit from a template for you:

```
npm run git:initHook
```

If you already have a pre-commit hook defined but don't care about overwriting it, you can use the same command with the `-f` flag.
This will copy the template even if the file exists already.

```
npm run git:initHook -f
```

#### The slightly harder way

1. Open the file `.git/hooks/pre-commit`
2. If the file does not exist, create it.
3. Add the following to the file and save it:

```
npm run lint
RESULT=$?
[ $RESULT -ne 0 ] && exit 1
exit 0
```


### Building for production

To build for production, simply run `npm run build:prod`.

The project will be bundled by webpack in production mode, and `web-ext` will build that project in to a zip file, which can be uploaded to the Mozilla Add Ons site.



#### Create a Github Release

- Decide on a tag i.e v0.3

- Update `manifest.json` with the tag version

- Create a tag
  `git tag -a tag-you-picked -m 'a message'`

- Push the tag to github and checkout the tag
  
  `git push origin tag-you-picked` [1]
  
  `git checkout tag-you-pick`
  
  
[1]:  See more on pushing tags [here](https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository#pushing-tags)
  
  
  _Note_ that we need write permissions to the repo to push a tag

- Create a prod build
  See [Building for production](#building-for-production)
 
- Create a release
  Follow Github's [creating a release](https://help.github.com/en/github/administering-a-repository/managing-releases-in-a-repository#creating-a-release) docs to create a release:
  
  - Fill in the tag-you-picked
  - Add a message that indicates changes made to the repo since the last release
  - Upload the `prod` zip already created to the release
  - _Optionally_ mark the release as a `pre-release`
  - Publish the release
  
#### Update the Release at Mozilla Addons

We follow Mozilla's [Submitting an Addon](https://extensionworkshop.com/documentation/publish/submitting-an-add-on/) to submit the extension.

- Download the [Github Release](https://github.com/opt-out-tools/opt-out/releases) (the zip file) you wanna submit and upload it.
- Create a production build see [Building for production](#building-for-production)
- Login using to the [Developer Hub](https://addons.mozilla.org/developers/) with OOT credentials
- Navigate to the ""Manage My Submissions"" page
- Find the `Manage the Status and Versions` link and click on it (click `More` and it should be in the drop down list) 
- Click the `Upload New Version` button and select the build you want to upload, it will then be validated
- Once this has finished, upload the *source* code too 
- Submit!
  
#### Managing release

- `master` is the main dev branch
- When it's release time we create a new ""release"" branch i.e `release-0.3`
- All testing happens on the release branch
- If bug fixes are needed on the release branch, they must be cherry-picked to `master` as well
- When we're ready to release:
  - we tag the release branch i.e `v0.3`
  - create a github release
  - submit the addon to mozilla
- The release branches can be left untouched for a future bug fix
  
  
  
",,2022-11-19T07:36:10Z,13,104,7,"('malteserteresa', 213), ('MikeJohnPage', 62), ('ribicn-softerrific', 30), ('Haimchen', 16), ('djbusstop', 11), ('Bacri', 3), ('cmcaine', 3), ('Morijarti', 3), ('Jolg42', 2), ('Cheukting', 1), ('jancborchardt', 1), ('nicoburns', 1), ('sarah-koehler', 1)","[5, 'Gender Equality']"
wri/gfw,"Global Forest Watch: An online, global, near-real time forest monitoring tool","# What is Global Forest Watch?

[Global Forest Watch](http://www.globalforestwatch.org/) (GFW) is a
dynamic online forest monitoring and alert system that empowers people
everywhere to better manage forests. This repository contains the GFW web app.

![Global forest watch map](/public/preview.jpg?raw=true ""Global Forest Watch"")

# Getting started

The GFW web app is built with [Nextjs](https://nextjs.org/), [React](https://reactjs.org/) and [Redux](https://redux.js.org/).

## Installing the app

Clone the repo:

```bash
$ git clone https://github.com/Vizzuality/gfw.git
```

Installing dependencies:

```bash
$ yarn
```

Copy the `.env.sample` to `.env.local`, and start the server:

```bash
$ yarn dev
```

The app should now be accessible on [http://0.0.0.0:3000](http://0.0.0.0:3000).

## Developing

We follow a [Gitflow Worklow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow) for development and deployment.  
We merge pull requests into `develop`, which is deployed automatically to both the staging and pre-production servers. In order to release features into production, we merge `develop` into `master`, triggering an automatic deployment to production.

![gitflow workflow](https://www.atlassian.com/dam/jcr:b5259cce-6245-49f2-b89b-9871f9ee3fa4/03%20(2).svg)

### Staging, pre-production and review apps

We use [Heroku](https://www.heroku.com/) to deploy our apps. Production is deployed automatically from `master` to [globalforestwatch.org](https://www.globalforestwatch.org).  

We have two staging environments: staging and pre-production. Both are deployed automatically from `develop`.  
The main difference is that staging points to the staging environments of the APIs we access, pre-production points to the production ones. This is set by the `NEXT_PUBLIC_FEATURE_ENV` env variable. 

We also make use of Heroku's [Review Apps](https://devcenter.heroku.com/articles/github-integration-review-apps) feature.  
When a pull request is created, a review app is deployed automatically by Heroku with a `NEXT_PUBLIC_FEATURE_ENV` of `preproduction`, and a link to the environment is added automatically to the respective pull request. 


## Releases

We are using github releases to record changes to the app. To help us manage this we are using [Zeit Releases](https://github.com/zeit/release), an npm package for handling github releases, tagging commits (major, minor, patch), and automating semantic release logs. For a more detailed explanation of semantic changelogs see [this post](https://semver.org/).


#### Managing commits for a release

When developing, you can tag your commits as follows: `fix some excellent bug (patch)` where `patch` can be `(major/minor/patch/ignore)`. This commit title will automatically be grouped into the correct section for the release. Otherwise you will be prompted during the release to assign (or ignore) each of your commits. You will have to do this for every commit so don't forget to squash!

So how do you make a release on GFW?

1. Checkout master and merge in develop (not compulsory but advised for consistency).
2. Run `npx release [type]` where type can be `major`, `minor`, `patch`, or `pre` (see [zeit docs](https://github.com/zeit/release) for more details).
3. Follow the prompts to manage commits.
4. You will be taken to github draft release editor with all your commits grouped and ready to go.
5. Enter your title and include any extra info you want.
6. Publish!

# RW API Documentation for GFW

Map layers and relevant datasets are stored in the [RW-API](http://api.resourcewatch.org/) and the `globalforestwatch.org/map` utilises the [layer-manager](https://github.com/Vizzuality/layer-manager) to render them.

The schema used to style these layers, their legends, and define their interactions are specific to the *Global Forest Watch* platform.

When creating or modifying layers/datasets for GFW, follow the schema and syntax outlined in the [API Documentation](./docs/API_Documentation.md) markdown file.

To view GFW-specific layers and datasets use the following endpoint:

https://api.resourcewatch.org/v1/dataset?app=gfw&includes=layer,vocabulary,metadata&page[size]=200

### BrowserStack

We use [BrowserStack](https://www.browserstack.com) to find and fix cross-browser issues.


","'deforestation', 'forest-monitoring', 'mapbox', 'nextjs', 'react', 'redux', 'satellite-imagery'",2024-05-02T14:45:38Z,63,260,40,"('edbrett', 6115), ('pjosh', 1599), ('01painadam', 1141), ('matallo', 1134), ('javierarce', 932), ('apercas', 892), ('dfrico', 721), ('adammulligan', 713), ('tomasmoose', 669), ('SARodrigues', 456), ('willian-viana', 402), ('aagm', 377), ('geriux', 374), ('benlaken', 276), ('simaob', 235), ('beej23', 224), ('j8seangel', 185), ('wri7tno', 182), ('KarlaRenschler', 158), ('d4weed', 135), ('Ferdev', 122), ('eightysteele', 122), ('pdgago', 113), ('pedrogpimenta', 84), ('mbarrenechea', 64), ('davidsingal', 60), ('andrewxhill', 57), ('elpamart', 56), ('aabdaab', 49), ('andresgnlez', 49), ('teresa-n-schofield', 39), ('dependabot-support', 38), ('dependabot-previewbot', 37), ('agnessa', 34), ('manuela-stg', 34), ('AngelArcones', 33), ('rachnp89', 26), ('dependabotbot', 23), ('cciciarelli8', 22), ('mluena', 19), ('ronnyccs24', 18), ('robinkraft', 14), ('javisantana', 11), ('MarcoVieiraAntunes', 9), ('jameshanderson', 8), ('bitTal', 8), ('osesenar', 7), ('rschumann', 7), ('mweisse', 6), ('xavijam', 5), ('brookisme', 4), ('Bluesmile82', 2), ('manuelasabina', 2), ('tiagojsag', 2), ('tatianova', 2), ('kant', 1), ('EnriqueCornejo', 1), ('ericboucher', 1), ('sorodrigo', 1), ('alicekg', 1), ('cmnaval', 1), ('jmessinger', 1), ('tamaramegan', 1)","[15, 'Life On Land']"
bothub-it/bothub,"Bothub is an open platform for predicting, training and sharing NLP datasets in multiple languages","# Bothub
[![Build Status](https://travis-ci.org/Ilhasoft/bothub-engine.svg?branch=master)](https://travis-ci.org/Ilhasoft/bothub-engine) [![Coverage Status](https://coveralls.io/repos/github/Ilhasoft/bothub-engine/badge.svg?branch=master)](https://coveralls.io/github/Ilhasoft/bothub-engine?branch=master) [![Python Version](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/) [![License GPL-3.0](https://img.shields.io/badge/license-%20GPL--3.0-yellow.svg)](https://github.com/Ilhasoft/bothub-engine/blob/master/LICENSE)

Bothub is an open platform for predicting, training and sharing NLP datasets in multiple languages.


## About

BotHub is an NLP as a service tool that enables users to build, improve or translate datasets that extracts metadata from text.

You can read more about the project's purpose on this
[blog post](https://push.al/en/this-is-how-bothub-started/).


## What's here

This repo is the ""master"" repo for all Bothub-related projects. It hosts 
the documentation and other misc. resources for Bothub. Code for other
projects, like the [WebApp](https://github.com/bothub-it/bothub-webapp), [Engine](https://github.com/bothub-it/bothub-engine), [NLP Worker](https://github.com/bothub-it/bothub-nlp), [NLP API](https://github.com/bothub-it/bothub-nlp-api) and [NLP On Demand](https://github.com/bothub-it/bothub-nlp-on-demand), are hosted in other 
repositories.

## Documentation

All documentation available on [docs.bothub.it](https://docs.bothub.it/).

# Deployment

## Instant Server Installation with Docker



Instead of using standard Docker commands, you may want a little more automated management of your deployment. This is where using Docker-compose can be useful.

* Make sure Docker and Docker-compone are installed and operational.
* Check if your docker-swarm is enabled, if not, go to the [configuration](https://docs.docker.com/engine/swarm/swarm-mode/) session.
* Edit image: bothubit/bothub-(project): develop to specify which image you want to use (see the section Images available in Docker)


Add two networks for internal project communication:
```
docker network create bothub-nlp -d overlay
```

```
docker network create postgres -d overlay
```

Then add docker-compose.yml with docker stack

```
docker stack deploy --compose-file=docker-compose.yml bothub
```

after carrying out all the deploy, check if all containers were started with the command:
```
docker service ls
```

If it is the first time that you have run the project, you will need to run the migrations to create the tables in the database, for this run the command:
```
make engine_migration
```

to populate the database with fakes data, you can use the command:
```
make engine_fakedata
```

This docker stack process allows you to upload our services quickly, it automatically downloads our images generated from the Docker Hub itself.
With that you have practically moved up our entire stack, you will only be missing the frontend.

To build the [bothub-webapp](https://github.com/bothub-it/bothub-webapp) project you need to have the dependencies installed correctly:

| # | Version |
|--|--|
| git | >= 2.x.x
| nodejs | >= 12.x.x
| yarn | >= 1.x.x

To install the project you must clone the project:

```
make clone_webapp
```

Then, you can notice that a new folder was created with the project code [bothub-webapp](https://github.com/bothub-it/bothub-webapp), just access the directory with the command:
```
cd bothub-webapp
```

and install the project dependencies with the yarn command:
```
yarn install
```

after installing the dependencies, just start bothub-webapp's development server with the command:
```
yarn start
```

this way you will already be able to use our entire stack, remembering that each project has its environment variables configurable, to change consult the documentation for each specific project.


## Contributing

**We are looking for collaboration from the Open Source community!** There's so much we want to do, 
including but not limited to: enhancing existing applications with new features, 
optimizing the NLP tools and algorithms involved that boost accuracy, and bringing our work closer to
the public to leverage their inputs via blog posts and tutorials.

* Please read our [contribution guidelines](https://github.com/ilhasoft/bothub/blob/master/.github/CONTRIBUTING.md) 
for details on what and how you can contribute.

* Report a bug by using [this guideline](https://github.com/ilhasoft/bothub/blob/master/.github/CONTRIBUTING.md#report-a-bug) 
for details on what and how you can contribute.

## Using the issue tracker

The issues created here will be analysed and validated. They can be submitted to the [bothub](https://github.com/ilhasoft/bothub), [bothub-webapp](https://github.com/ilhasoft/bothub-webapp), and/or [bothub-nlp](https://github.com/ilhasoft/bothub-nlp) repository as well.

The issue tracker is the preferred channel for [bug reports](https://github.com/ilhasoft/bothub/blob/master/.github/CONTRIBUTING.md#report-a-bug) and [features requests](#features), but please respect the following restrictions:

- Please **do not** use the issue tracker for personal support requests (send an email to bothub@ilhasoft.com.br).

- Please **do not** derail or troll issues. Keep the discussion on topic and respect the opinions of others.


## Feature requests

Feature requests are welcome. But take a moment to find out whether your idea fits with the scope and aims of the project. It's up to *you* to make a strong case to convince the project's developers of the merits of this feature. Please provide as much detail and context as possible.

To request a new feature, create a new issue using the label `feature request`.
","'bothub', 'bots', 'chatbot', 'data', 'database', 'docker', 'ilhasoft', 'issue-tracker', 'multiple-languages', 'nlp', 'nlp-datasets', 'push', 'python', 'sharing-nlp-datasets', 'webapp'",2023-01-04T17:35:52Z,4,34,11,"('dougppaz', 49), ('dyohan9', 19), ('johncordeiro', 4), ('WilliamTales', 3)","[4, 'Quality Education']"
VOLTTRON/volttron,VOLTTRON Distributed Control System Platform,"![image](docs/source/files/VOLLTRON_Logo_Black_Horizontal_with_Tagline.png)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/fcf58045b4804edf8f4d3ecde3016f76)](https://app.codacy.com/gh/VOLTTRON/volttron?utm_source=github.com&utm_medium=referral&utm_content=VOLTTRON/volttron&utm_campaign=Badge_Grade_Settings)

# VOLTTRON

This repository is for the current production VOLTTRON. We are working on VOLTTRON 10 (modular) which is available under 
github at https://github.com/eclipse-volttron/.  The modular version of VOLTTRON will help ease deployment and support 
flexible deployment where in only required agents/applications can be installed, thereby simplifying setup and upgrade 
steps for the end user. The VOLTTRON team are currently working on porting agents from monolithic VOLTTRON to the 
modular version of VOLTTRON. To know more about modular VOLTTRON, please visit our new documentation site available 
at https://eclipse-volttron.readthedocs.io/en/latest/. We would love for you to try it out and give us early 
feedback. Also, until our work on modular VOLTTRON is completed, please continue cloning and using this 
repository for your production systems.

VOLTTRON™ is an open source platform for distributed sensing and control. The
platform provides services for collecting and storing data from buildings and
devices and provides an environment for developing applications which interact
with that data.

## Upgrading Pre-8 to VOLTTRON 9.x

VOLTTRON 8 introduces four changes that require an explict upgrade step when upgrading from an earlier VOLTTRON version

    1. Dynamic RPC authorization feature - This requires a modification to the auth file. If you have a pre-existing
       instance of VOLTTRON running on an older version, the auth file will need to be updated.
    2. Historian agents now store the cache database (backup.sqlite file) in
       /agents///.agent-data directory instead of
       /agents// directory. In future all core agents will write data only
       to the .agent-data subdirectory. This is because vctl install --force backs up and restores
       only the contents of this directory.
    3. SQLHistorians (historian version 4.0.0 and above) now use a new database schema where metadata is stored in
       topics table instead of separate metadata table. SQLHistorians with version >= 4.0.0 can work with existing
       database with older schema however the historian agent code should be upgraded to newer version (>=4.0.0) to run
       with VOLTTRON 8 core.
    4. VOLTTRON feature to run individual agents as unique Unix users is now named ""agent-isolation-mode"" and is 
       consistently referred to using this name in code, configuration, and documentation. Before VOLTTRON 8.2 this 
       configuration parameter was called ""secure-agent-users"" and related documentation referred to this mode as 
       ""secure mode"".  

To upgrade:

    1. If upgrading historian, make sure historians are not in auto start mode. To remove any historian from auto start
       mode use the command 'vctl disable . This is necessary so that the old
       sqlhistorian does not automatically start after step 5. 
    2. Update volttron source code version to VOLTTRON 8
    3. activate the volttron environment, and run ```python bootstrap.py --force```. If you have 
       any additional bootstrap options that you need (rabbitmq, web, drivers, etc.) include these in the above command.
    4. Run ```volttron-upgrade``` to update the auth file, move historian cache files into agent-data directory, and 
       rename the config parameter ""secure-agent-users"" in VOLTTRON_HOME/config to ""agent-isolation-mode""
       **Note** that the upgrade script will only move the backup.sqlite file and will not move sqlite historian's db  
       file if they are within the install directory. If using a SQLite historian, please backup the database file of 
       sqlite historian before upgrading to the latest historian version.
    5. Start VOLTTRON
    6. Run ```vctl install --force --vip-identity  --agent-config ``` to upgrade 
       to the  latest historian version. vctl install --force will backup the cache in .agent-data 
       folder, installs the latest version of the historian and restore the contents of 
       .agent-data folder.

### Upgrading aggregate historians

VOLTTRON 8 also comes with updated SQL aggregate historian schema. However, there is no automated upgrade path for
aggregate historian. To upgrade an existing aggregate historian please refer to the CHANGELOG.md within 
SQLAggregateHistorian source directory

## Features

-   [Message Bus](https://volttron.readthedocs.io/en/latest/platform-features/message-bus/index.html) allows agents to subscribe to data sources and publish results and messages.
-   [Driver framework](https://volttron.readthedocs.io/en/latest/driver-framework/drivers-overview.html) for collecting data from and sending control actions to buildings and devices.
-   [Historian framework](https://volttron.readthedocs.io/en/latest/agent-framework/historian-agents/historian-framework.html) for storing data.
-   [Agent lifecycle managment](https://volttron.readthedocs.io/en/latest/platform-features/control/agent-management-control.html) in the platform
-   [Web UI](https://volttron.readthedocs.io/en/latest/agent-framework/core-service-agents/volttron-central/volttron-central-overview.html) for managing deployed instances from a single central instance.

## Installation

VOLTTRON is written in Python 3.6+ and runs on Linux Operating Systems. For
users unfamiliar with those technologies, the following resources are recommended:

-   
-   

### 1. Install prerequisites

[Requirements Reference](https://volttron.readthedocs.io/en/latest/introduction/platform-install.html#step-1-install-prerequisites)

From version 7.0, VOLTTRON requires python 3 with a minimum version of 3.6; it is tested only systems supporting that as a native package.
On Debian-based systems (Ubuntu bionic, debian buster, raspbian buster), these can all be installed with the following commands:

```sh
sudo apt-get update
sudo apt-get install build-essential libffi-dev python3-dev python3-venv openssl libssl-dev libevent-dev git
 ```
(Note: `libffi-dev` seems to only be required on arm-based systems.)

 On Redhat or CENTOS systems, these can all be installed with the following command:
```sh
sudo yum update
sudo yum install make automake gcc gcc-c++ kernel-devel python3.6-devel pythone3.6-venv openssl openssl-devel libevent-devel git
 ```

### 2. Clone VOLTTRON code

From version 6.0, VOLTTRON supports two message buses - ZMQ and RabbitMQ. 

```sh
git clone https://github.com/VOLTTRON/volttron --branch 
```

### 3. Setup virtual environment

#### Steps for ZMQ

Run the following command to install all required packages

```sh
cd 
python3 bootstrap.py
source env/bin/activate
```

Proceed to step 4.

You can deactivate the environment at any time by running `deactivate`.

#### Steps for RabbitMQ

##### 1. Install Erlang version 25 packages

###### Install Erlang pre-requisites
```shell
sudo apt-get update
sudo apt-get install -y gnupg apt-transport-https libsctp1 libncurses5
```
Please note there could be other pre-requisites that erlang requires based on the version of Erlang and OS. If there are other pre-requisites required, 
install of erlang should fail with appropriate error message. 

###### Purge previous versions of Erlang
```shell
sudo apt-get purge -yf erlang-base
```

###### Install Erlang

Download and install ErlangOTP from [Erlang Solutions](https://www.erlang-solutions.com/downloads/).
RMQ uses components - ssl, public_key, asn1, and crypto. These are by default included in the OTP
RabbitMQ 3.9.29 is compatible with Erlang versions 24.3.4.2 to 25.2. VOLTTRON was tested with Erlang version 25.2-1

Example:

On Ubuntu 22.04:

```shell
wget https://binaries2.erlang-solutions.com/ubuntu/pool/contrib/e/esl-erlang/esl-erlang_25.2-1~ubuntu~jammy_amd64.deb
sudo dpkg -i esl-erlang_25.2-1~ubuntu~jammy_amd64.deb
```

On Ubuntu 20.04:
```shell
wget https://binaries2.erlang-solutions.com/ubuntu/pool/contrib/e/esl-erlang/esl-erlang_25.2-1~ubuntu~focal_amd64.deb
sudo dpkg -i esl-erlang_25.2-1~ubuntu~focal_amd64.deb
```


##### 2. Configure hostname

Make sure that your hostname is correctly configured in /etc/hosts (See [this StackOverflow Post](https://stackoverflow.com/questions/24797947/os-x-and-rabbitmq-error-epmd-error-for-host-xxx-address-cannot-connect-to-ho)).
If you are testing with VMs make please make sure to provide unique host names for each of the VM you are using. 

The hostname should be resolvable to a valid IP when running on bridged mode. RabbitMQ checks for this during initial 
boot. Without this (for example, when running on a VM in NAT mode) RabbitMQ  start would fail with the error ""unable to 
connect to empd (port 4369) on ."" Note: RabbitMQ startup error would show up in syslog (/var/log/messages) file
and not in RabbitMQ logs (/var/log/rabbitmq/rabbitmq@hostname.log)

##### 3. Bootstrap
Remove older version of rabbitmq_server directory if you are upgrading from a older version. 
Defaults to /rabbitmq_server/rabbitmq_server-3.9.7

Run the rabbitmq boostrap command within an activated VOLTTRON environment

```sh
cd volttron
source env/bin/activate
python3 bootstrap.py --rabbitmq [optional install directory. defaults to
/rabbitmq_server]
```

This will build the platform and create a virtual Python environment and
dependencies for RabbitMQ. It also installs RabbitMQ server as the current user.
If an install path is provided, that path should exist and the user should have 
write permissions. RabbitMQ will be installed under `/rabbitmq_server-`.
The rest of the documentation refers to the directory `/rabbitmq_server-` as
`$RABBITMQ_HOME`

You can check if the RabbitMQ server is installed by checking its status. Please
note, the `RABBITMQ_HOME` environment variable can be set in ~/.bashrc. If doing so,
it needs to be set to the RabbitMQ installation directory (default path is
`/rabbitmq_server/rabbitmq_server-`)

```sh
echo 'export RABBITMQ_HOME=$HOME/rabbitmq_server/rabbitmq_server-3.9.29'|sudo tee --append ~/.bashrc
source ~/.bashrc

$RABBITMQ_HOME/sbin/rabbitmqctl status
```

##### 4. Activate the environment

```sh
source env/bin/activate
```

You can deactivate the environment at any time by running `deactivate`.

##### 5. Create RabbitMQ setup for VOLTTRON:

```sh
vcfg rabbitmq single [--config optional path to rabbitmq_config.yml]
```

Refer to [examples/configurations/rabbitmq/rabbitmq_config.yml](examples/configurations/rabbitmq/rabbitmq_config.yml)
for a sample configuration file.
At a minimum you will need to provide the hostname and a unique common-name
(under certificate-data) in the configuration file. Note: common-name must be
unique.  The general convention is to use `-root-ca`.

Running the above command without the optional configuration file parameter will
cause the user to be prompted for all the required data in the command prompt. 
`vcfg` will use that data to generate a rabbitmq_config.yml file in the `VOLTTRON_HOME` 
directory.

If the above configuration file is being used as a basis, be sure to update it with 
the hostname of the deployment (this should be the fully qualified domain name
of the system).

This script creates a new virtual host and creates SSL certificates needed
for this VOLTTRON instance. These certificates get created under the subdirectory 
""certificates"" in your VOLTTRON home (typically in ~/.volttron). It
then creates the main VIP exchange named ""volttron"" to route message between
the platform and agents and alternate exchange to capture unrouteable messages.

NOTE: We configure the RabbitMQ instance for a single volttron_home and
volttron_instance. This script will confirm with the user the volttron_home to
be configured. The VOLTTRON instance name will be read from volttron_home/config
if available, if not the user will be prompted for VOLTTRON instance name. To
run the scripts without any prompts, save the VOLTTRON instance name in
volttron_home/config file and pass the VOLTTRON home directory as a command line
argument. For example: `vcfg --vhome /home/vdev/.new_vhome rabbitmq single`

The Following are the example inputs for `vcfg rabbitmq single` command. Since no
config file is passed the script prompts for necessary details.

```sh
Your VOLTTRON_HOME currently set to: /home/vdev/new_vhome2

Is this the volttron you are attempting to setup?  [Y]:
Creating rmq config yml
RabbitMQ server home: [/home/vdev/rabbitmq_server/rabbitmq_server-3.9.29]:
Fully qualified domain name of the system: [cs_cbox.pnl.gov]:

Enable SSL Authentication: [Y]:

Please enter the following details for root CA certificates
Country: [US]:
State: Washington
Location: Richland
Organization: PNNL
Organization Unit: Volttron-Team
Common Name: [volttron1-root-ca]:
Do you want to use default values for RabbitMQ home, ports, and virtual host: [Y]: N
Name of the virtual host under which RabbitMQ VOLTTRON will be running: [volttron]:
AMQP port for RabbitMQ: [5672]:
http port for the RabbitMQ management plugin: [15672]:
AMQPS (SSL) port RabbitMQ address: [5671]:
https port for the RabbitMQ management plugin: [15671]:
INFO:rmq_setup.pyc:Starting rabbitmq server
Warning: PID file not written; -detached was passed.
INFO:rmq_setup.pyc:**Started rmq server at /home/vdev/rabbitmq_server/rabbitmq_server-3.9.29
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): localhost
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): localhost
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): localhost
INFO:rmq_setup.pyc:
Checking for CA certificate

INFO:rmq_setup.pyc:
Root CA (/home/vdev/new_vhome2/certificates/certs/volttron1-root-ca.crt) NOT Found. Creating root ca for volttron instance
Created CA cert
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): localhost
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): localhost
INFO:rmq_setup.pyc:**Stopped rmq server
Warning: PID file not written; -detached was passed.
INFO:rmq_setup.pyc:**Started rmq server at /home/vdev/rabbitmq_server/rabbitmq_server-3.9.29
INFO:rmq_setup.pyc:

#######################

Setup complete for volttron home /home/vdev/new_vhome2 with instance name=volttron1
Notes:

-   Please set environment variable `VOLTTRON_HOME` to `/home/vdev/new_vhome2` before starting volttron

-   On production environments, restrict write access to
    /home/vdev/new_vhome2/certificates/certs/volttron1-root-ca.crt to only admin user. For example: sudo chown root /home/vdev/new_vhome2/certificates/certs/volttron1-root-ca.crt

-   A new admin user was created with user name: volttron1-admin and password=default_passwd.
    You could change this user's password by logging into  Please update /home/vdev/new_vhome2/rabbitmq_config.yml if you change password

#######################
```

### 4. Test

We are now ready to start the VOLTTRON instance. If configured with a RabbitMQ message bus a config file would have been
 generated in `$VOLTTRON\_HOME/config` with the entry `message-bus=rmq`. If you need to revert to ZeroMQ based 
 VOLTTRON, you will have to either remove ""message-bus"" parameter or set it to default ""zmq"" in `$VOLTTRON\_HOME/config`
  and restart the volttron process. The following command starts the VOLTTRON process in the background:

```sh
volttron -vv -l volttron.log &
```

This command causes the shell to enter the virtual Python environment and then starts the platform in debug (vv) mode 
with a log file named volttron.log.

Next, start an example listener to see it publish and subscribe to the message bus:

```sh
vctl install examples/ListenerAgent
```

This script handles several commands for installing and starting an agent after removing an old copy. This 
simple agent publishes a heartbeat message and listens to everything on the message bus. Look at the VOLTTRON log to see 
the activity:

```sh
tail volttron.log
```

Listener agent heartbeat publishes appear in the logs as:

```sh
2020-04-20 18:49:31,395 (listeneragent-3.3 13458) __main__ INFO: Peer: pubsub, Sender: listeneragent-3.2_1:, Bus: , Topic: heartbeat/listeneragent-3.2_1, Headers: {'TimeStamp': '2020-04-20T18:49:31.393651+00:00', 'min_compatible_version': '3.0', 'max_compatible_version': ''}, Message:
'GOOD'
2020-04-20 18:49:36,394 (listeneragent-3.3 13458) __main__ INFO: Peer: pubsub, Sender: listeneragent-3.2_1:, Bus: , Topic: heartbeat/listeneragent-3.2_1, Headers: {'TimeStamp': '2020-04-20T18:49:36.392294+00:00', 'min_compatible_version': '3.0', 'max_compatible_version': ''}, Message:
'GOOD'
```

To top the platform run the following command:

```sh
./stop-volttron
```

## Next Steps

There are several walkthroughs to explore additional aspects of the platform:

-   [Agent Development Walkthrough](https://volttron.readthedocs.io/en/latest/developing-volttron/developing-agents/agent-development.html)
-   Demonstration of the [management UI](https://volttron.readthedocs.io/en/latest/deploying-volttron/multi-platform/volttron-central-deployment.html)
-   [RabbitMQ setup with Federation and Shovel plugins](https://volttron.readthedocs.io/en/latest/deploying-volttron/multi-platform/multi-platform-rabbitmq-deployment.html)
-   [Backward compatibility with the RabbitMQ message bus](https://volttron.readthedocs.io/en/latest/deploying-volttron/multi-platform/multi-platform-multi-bus.html)


## Acquiring Third Party Agent Code

Third party agents are available under the volttron-applications repository. In
order to use those agents, clone the volttron-applications repository into the same
directory as the VOLTTRON source code:

```sh
cd 
git clone https://github.com/VOLTTRON/volttron-applications.git develop
```

## Contribute

How to [contribute](https://volttron.readthedocs.io/en/latest/developing-volttron/contributing-code.html) back:

-   [Issue Tracker](https://github.com/VOLTTRON/volttron/issues)
-   [Source Code](https://github.com/VOLTTRON/volttron) 

## Support

There are several options for VOLTTRONTM [support](https://volttron.readthedocs.io/en/latest/developing-volttron/community.html).

-   A VOLTTRONTM office hours telecon takes place every other Friday at 11am Pacific over Zoom.
-   A mailing list for announcements and reminders
-   The VOLTTRONTM contact email for being added to office hours, the mailing list, and for inquiries is: volttron@pnnl.gov
-   The preferred method for questions is through [StackOverflow](https://stackoverflow.com/questions/tagged/volttron) since this is easily discoverable by others who may have the same issue.
-   [GitHub issue tracker](https://github.com/VOLTTRON/volttron/issues) for feature requests, bug reports, and following development activities
-   VOLTTRON now has a [Slack channel](https://volttron-community.slack.com/signup)

## License

The project is [licensed](LICENSE.md) under Apache 2.
","'bacnet', 'buildings', 'message-bus', 'modbus', 'office-hours', 'python', 'volttron', 'volttron-applications', 'volttron-instance'",2024-05-01T14:51:15Z,70,449,51,"('craig8', 3878), ('schandrika', 1207), ('shwethanidd', 868), ('jklarson', 457), ('jhaack', 376), ('zbeech', 313), ('jchap-pnnl', 267), ('bonicim', 182), ('edward-ellis', 170), ('rlutes', 123), ('timothykang', 117), ('davidraker', 101), ('acedrew', 82), ('poorva1209', 77), ('JoeyChaps', 75), ('riley206', 66), ('gwenkidd', 63), ('rcalvert-cpi', 50), ('kl5pierce', 48), ('kefeimo', 42), ('Siecje', 31), ('anhnguyen-cpi', 30), ('ntenney', 22), ('dvaidhyn', 17), ('hashstat', 11), ('laroque', 11), ('nikithark', 10), ('RichardLitt', 10), ('WangTigerUNCC', 10), ('sheridaj', 9), ('gnmerritt', 9), ('SKalt', 9), ('mvdisney', 8), ('gbhanda', 7), ('dependabotbot', 7), ('RoshanLKini', 7), ('hlngo', 6), ('so3500', 5), ('liamdiprose', 5), ('fstshrk', 5), ('aidnguyen', 5), ('rameezk', 4), ('mrssquid', 4), ('sgilbride', 4), ('bbarcklay', 4), ('carlatpnl', 4), ('kimw821', 3), ('maskedpatel', 3), ('kmonson', 3), ('rjdkolb', 3), ('jwickers', 3), ('anselmbradford', 2), ('ChrisFreeman', 2), ('plnordquist', 2), ('pccourt', 2), ('rmay-intwine', 1), ('sysid', 1), ('nhill-cpi', 1), ('kmorri09', 1), ('jessica-s-yang', 1), ('gitaroktato', 1), ('UrbanSwati', 1), ('subasah', 1), ('Bowriverstudio', 1), ('sanyalj', 1), ('jens18', 1), ('GHYOON', 1), ('elisesaxon', 1), ('deva3654', 1), ('deepaksood619', 1)","[9, 'Industry, Innovation and Infrastructure']"
codeforboston/voiceapp311,Voice assistant connection to Boston services,"[![Build Status](https://travis-ci.org/codeforboston/voiceapp311.svg?branch=master)](https://travis-ci.org/codeforboston/voiceapp311)

# Boston Info Alexa Skill

## How to Use This Skill
You can use our skill with the Amazon Alexa stand-alone home device or with the Alexa app on your phone.  You can start interacting with our skill by asking Alexa to ""Open Boston Info"" or starting any of your requests with ""Alexa, ask Boston Info ...""

Many of the skill's responses require an address to provide accurate information. To make the skill more convenient, we will ask for permissions to use your device's location. If you prefer not to provide these permissions, you can always provide an address with your question (ex: ""Alexa, ask Boston Info when is garbage day at 206 Washington Street"").

## Current Skill Abilities:
#### Find out your Voting/Polling Location
The Boston Info app can be used to find your election voting location.

Example Request:
> ""Alexa, ask Boston Info where do I vote for {Address}""

#### Get the latest information on the Coronavirus/COVID-19
The City of Boston website is [providing updates](https://www.boston.gov/news/coronavirus-disease-covid-19-boston) for the Coronavirus and its impact on the city. The Boston Info app will get this latest information for you.

Example Request:
> ""Alexa, ask Boston Info for the latest information about the Coronavirus""

#### Find out your Trash Day
This is used to find trash and recycling pickup and collection days. You can ask for recycling or trash/garbage days specifically. It will tell you what day of the week trash is picked up from the given address.

Example Request:
> ""Alexa, ask Boston Info what my garbage day is.""
>
> ""Alexa, have Boston Info find the recyclables collection days at {Address}""

#### Find out if there is a snow emergency in effect
You can ask Boston Info if there is currently a snow emergency in the City of Boston.

Example Request:
> ""Alexa, ask Boston Info if there is a snow emergency""


#### Find the nearest Emergency Snow Parking
This is used to find snow emergency parking. Given an address, it will find the nearest emergency snow parking lot by distance. It will tell you the name and address of the parking lot, as well as estimating how long it will take you to drive there. If the information is available this intent can also tell you how many spaces are in the lot, if the lot charges a fee, and what phone number to call for more information about the parking lot.

Example Request:
> ""Alexa, Ask Boston Info where I can park during a snow emergency.


#### Find any current Alerts
This is used to get any alerts for the day put out by the Boston government. The alerts can be about any of the following services:
  * street cleaning
  * trash and recycling
  * city building hours
  * parking meters
  * tow alerts

Example Request:
> ""Alexa, ask Boston Info if there are any alerts.""


#### Find out the latest Three One One
This intent will give you the most recent BOS:311 reports. BOS:311 is an app where residents of Boston can report non-emergency issues like potholes or traffic light outages.

Example Request:
> ""Alexa, ask Boston Info what's the most recent three one one?""
>
> ""Alexa, ask Boston Info to tell me the 4 most recent three one ones""


### Find any nearby Food Trucks
This intent gets near-by food trucks for the given or stored Address. It returns the first five food trucks it finds within one mile of the given address as well as the period that the food truck is expected to be at the location.

Example Requests:
> ""Alexa, ask Boston Info where are the nearest food trucks?""
>
> ""Alexa, ask Boston Info what's for lunch?""

### Find any open farmers markets
The Boston Info app can let you know if there are any farmers markets open today.

Example Requests:
> ""Alexa, ask Boston Info what farmers markets are open?""


### Find any nearby Crime incidents
The crime incidents intent will tell you any recently reported crimes in your area.

Example Requests:
> ""Alexa, ask Boston Info what's the crime activity in my area?""
>
> ""Alexa, ask Boston Info what is the crime near me?""

#### Leave Feedback for us!
This is used to provide feedback about the skill, including bug reports and suggestions for new intents. You can leave feedback on our skill by telling Alexa directly!

Example Request:
> ""Alexa, I have a suggestion for Boston Info.""
>
> ""Alexa, I'd like to report a bug to Boston Info.""


## Intents in Development:
You can see intents that are currently in development or that we hope to develop in the future by checking out our [new intent ideas page.](https://github.com/codeforboston/voiceapp311/issues?q=is%3Aopen+is%3Aissue+label%3A%22new+intent%22)

We are also looking for user feedback on what intents would be most helpful or would be most wanted. So if you don't see an intent you are looking for please let us know.

## Want to Help Out?
Boston Info is an Alexa skill designed to provide easy access to information
about municipal services in Boston.

If you'd like to contribute or learn more about what we're working on, please
visit our [wiki page](https://github.com/codeforboston/voiceapp311/wiki). There,
you will find basic information about Alexa's development and instructions on
setting up a local version of Boston Info that you can experiment with.
","'alexa', 'code-for-america', 'code-for-boston', 'trash'",2022-12-08T00:36:09Z,23,40,25,"('wesdrew', 110), ('jmartini', 89), ('Joshhw', 80), ('curranjm', 33), ('Kly4', 16), ('thiteixeira', 13), ('alidaniell', 7), ('chrislreich', 7), ('GreenGiraffe1', 6), ('giff-h', 5), ('saratlingamarla', 5), ('fords', 3), ('A-Camp', 2), ('rongood', 2), ('endy-imam', 1), ('WheresHJ', 1), ('JPHaus', 1), ('petermarathas', 1), ('SKalmane', 1), ('TirthShah', 1), ('eric-s-s', 1), ('ethanstrominger', 1), ('venomouse', 1)","[17, 'Partnerships for the Goals']"
Recidiviz/pulse-data,A data platform for criminal justice reform [public mirror],"# Recidiviz Data Platform

[![Coverage Status](https://coveralls.io/repos/github/Recidiviz/pulse-data/badge.svg?branch=main)](https://coveralls.io/github/Recidiviz/pulse-data?branch=main)

At the center of Recidiviz is our platform for tracking granular criminal justice metrics in real time. It includes a system
for the ingest of corrections records from different source data systems, and for calculation of various metrics from the
ingested records.

Read more on data ingest in [`/recidiviz/ingest`](./recidiviz/ingest) and calculation in [`/recidiviz/calculator`](./recidiviz/calculator).

## License

This project is licensed under the terms of the GNU General Public License as published by the Free Software Foundation,
either version 3 of the License, or (at your option) any later version.

## Data Access

The data that we have gathered from criminal justice systems has been sanitized, de-duplicated, and standardized in a
single schema. This processed data is central to our purposes but may be useful to others, as well. If you would like
access to the processed data, in whole or in part, please reach out to us at `hello@recidiviz.org`. We evaluate such
requests on a case-by-case basis, in conjunction with our partners.

Calculated metrics can also be made available through the same process, though we anticipate publishing our analysis
in various forms and channels over time.

## Forking

The Recidiviz data system is provided as open source software - for transparency and collaborative development, to
help jump-start similar projects in other spaces, and to ensure continuity if Recidiviz itself ever becomes inactive.

If you plan to fork the project for work in the criminal justice space (to ingest from the same systems we are, or similar),
we ask that you first [contact us](mailto:hello@recidiviz.org) for a quick consultation. We work carefully to ensure
that our ingest activities don't disrupt other users' experiences with the public data services we read, but if
multiple ingest processes are running against the same systems, without knowing about one another, it may place excessive
strain on them and impact the services those systems provide.

If you have ideas or new work for the same data we're collecting, let us know and we'll work with you to find the
best way to get it done.

## Development

If you are contributing to this repository regularly for an extended period of time, [request GitHub collaborator access](mailto:hello@recidiviz.org?subject=GitHub%20collaborator%20request&body=) to commit directly to the main repository.

### Local Development

#### Environment setup

##### Option 1: Local Python installation

If you can install `python3.9` locally, do so. For local Python development, you will also need to install the `libpq` PostgreSQL client library and `openssl`.

On a Mac with [Homebrew](https://brew.sh/), you can install `python3.9` by first installing `pyenv` with:

```bash
brew install pyenv
brew install xz
mkdir ~/.pyenv
```

Then, add the following to your `~/.zshrc` (or equivalent):

```
export PATH=""$HOME/.local/bin:$PATH""
if command -v pyenv 1>/dev/null 2>&1; then
eval ""$(pyenv init -)""
fi
```

Then run:

```
pyenv install 3.9.12
pyenv global 3.9.12
```

Verify that you have the correct version of python across contexts by opening a new terminal window and running:

```
python -V
```

Once python is installed, you can install `libpq` and `openssl` with:

```bash
$ brew install postgresql@13 openssl
```

and add the following to your `~/.zshrc` (or equivalent):

```
export PATH=""/opt/homebrew/opt/postgresql@13/bin:$PATH""
```

On Ubuntu 18.04,`openssl` is installed by default, you can install `python3.9` and `libpq` with:

```bash
$ apt update -y && apt install -y python3.9-dev python3-pip libpq-dev
```

You do not need to change your default python version, as `pipenv` will look for 3.9.

Upgrade your `pip` to the latest version:

```bash
$ pip install -U pip
```

**NOTE**: if you get `ImportError: cannot import name 'main'` after upgrading
pip, follow the suggestions in
[this issue](https://github.com/pypa/pip/issues/5599).

If you do not already have `pip` installed, you can install it on a Mac with these commands:

```bash
$ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
$ python get-pip.py --user
```

On Ubuntu 18.04, you can install `pip` with:

```
$ sudo apt-get install python-pip
```

Install [`pipenv`](https://pipenv.readthedocs.io/en/latest/):

```bash
$ pip install pipenv --user
```

[Fork this repository](https://github.com/Recidiviz/pulse-data/fork), clone it locally, and enter its directory:

```
$ git clone git@github.com:your_github_username/pulse-data.git
$ cd pulse-data
```

Create a new pipenv environment and install all project and development dependencies

On a Mac, run the `initial_pipenv_setup_mac` script.

**NOTE**: Installation of one of our dependencies (`psycopg2`) requires OpenSSL, and as OpenSSL is not linked on Macs by default, this script temporarily sets the necessary compiler flags and then runs `pipenv sync --dev`. After this initial installation all `pipenv sync/install`s should work without this script.

```bash
$ ./initial_pipenv_setup_mac.sh
```

On a Linux machine, run the following:

```bash
$ pipenv sync --dev
```

**NOTE**: if you get `pipenv: command not found`, add the binary directory to
your PATH as described
[here](https://pipenv.readthedocs.io/en/latest/install#pragmatic-installation-of-pipenv).

To activate your pipenv environment, run:

```bash
$ pipenv shell
```

On a Mac with [Homebrew](https://brew.sh/), you can install the JRE with:

```bash
$ brew install java
```

On Ubuntu 18.04, you can install the JRE with:

```bash
$ apt update -y && apt install -y default-jre
```

On a Mac with [Homebrew](https://brew.sh/), you can install jq (needed to deploy calculation pipelines) with:

```bash
$ brew install jq
```

On Ubuntu 18.04, you can install jq with:

```bash
$ apt update -y && apt install -y jq
```

Finally, run `pytest`. As of Feb 2022, one might expect ~200 tests to fail locally, with errors
mainly falling into one of two categories: `Receiver() takes no arguments` and
`Already initialized database/ValueError: Accessing SQLite in-memory database on multiple threads`.
The former error is due to an incompatibility with Cython that may be due to newer Mac models or
python versions, and the latter is due to tests not properly cleaning up after themselves. All of
these tests pass in CI.
You can ignore any failing tests with (for example):

```bash
$ pytest --ignore=recidiviz/tests/path/to/tests
```

##### Option 2: Docker container

If you can't install `python3.9` locally, you can use Docker instead.

See [below](#docker) for installation instructions. Once Docker is installed, [fork this repository](https://github.com/Recidiviz/pulse-data/fork), clone it locally, and enter its directory:

```bash
$ git clone git@github.com:your_github_username/pulse-data.git
$ cd pulse-data
```

Build the image:

```bash
$ docker build -t recidiviz-image . --build-arg DEV_MODE=True
```

Stop and delete previous instances of the image if they exist:

```bash
$ docker stop recidiviz && docker rm recidiviz
```

Run a new instance, mounting the local working directory within the image:

```bash
$ docker run --name recidiviz -d -t -v $(pwd):/app recidiviz-image
```

Open a `bash` shell within the instance:

```bash
$ docker exec -it recidiviz bash
```

Once in the instance's `bash` shell, update your pipenv environment:

```bash
$ pipenv sync --dev
```

To activate your pipenv environment, run:

```bash
$ pipenv shell
```

Finally, run `pytest`. If no tests fail, you are ready to develop!

Using this Docker container, you can edit your local repository files and use `git` as usual within your local shell environment, but execute code and run tests within the Docker container's shell environment. Depending on your IDE, you may need to install additional plugins to allow running tests in the container from the IDE.

#### Google Cloud

Recidiviz interacts with Google Cloud services using [`google-cloud-*` Python client libraries](https://cloud.google.com/python/docs/reference).
During development, you may find it useful to verify the integration with these services.
First, [install the Google Cloud SDK](https://cloud.google.com/sdk/docs/install), then login to the SDK:

```bash
gcloud auth login # Gets credentials to interact with services via the CLI
gcloud auth application-default login # Gets credentials which will be automatically read by our client libraries
```

Lastly, in a test script, use the [`local_project_id_override` helper](https://github.com/Recidiviz/pulse-data/blob/c6972e132a6e68453e2d0baeb617b1f446a3e94f/recidiviz/utils/metadata.py#L69) to override configuration used by our client library wrappers:

```python
from recidiviz.utils.metadata import local_project_id_override
from recidiviz.utils.environment import GCP_PROJECT_STAGING

# Override configuration used by our client libraries
with local_project_id_override(GCP_PROJECT_STAGING):
    # Google Cloud Client libraries will use `recidiviz-staging` in this context
```

Now the code run in the above context will interact directly with our staging services. Use conservatively & exercise caution!

#### Terraform

Run the following to install Terraform:

```
brew tap hashicorp/tap
brew install hashicorp/tap/terraform
```

To test your installation, run:

```
terraform -chdir=recidiviz/tools/deploy/terraform init -backend-config ""bucket=recidiviz-staging-tf-state""
recidiviz/tools/deploy/terraform_plan.sh recidiviz-staging
```

If the above commands succeed, the installation was successful. For employees, see more information
on running Terraform at go/terraform.

#### Docker (🐳 [go/docker](http://go/docker))

Docker is needed for deploying new versions of our applications.

Follow these instructions to install Docker on Linux:

- [Debian](https://docs.docker.com/install/linux/docker-ce/debian/#install-using-the-repository)
- [Ubuntu](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository)

Go to [this page](https://www.docker.com/products/docker-desktop) to download Docker Desktop for Mac and Windows.

Once installed, increase the memory available to Docker to ensure it has enough resources to build the container. On Docker Desktop, you can do this by going to Settings > Resources and increasing Memory to 4GB.

#### Adding secrets

Recidiviz depends on sensitive information to run. This data is stored in Cloud Datastore, which should be added
manually to your production environment (see `utils/secrets` for more information on the Datastore kind used).

#### Running tests

Individual tests can be run via `pytest filename.py`. To run all tests, go to the root directory and run `pytest recidiviz`.

The configuration in `setup.cfg` and `.coveragerc` will ensure the right code is tested and the proper code coverage
metrics are displayed.

[A bug in the google client](https://github.com/googleapis/google-cloud-python/issues/5738) requires that you have
default application credentials. This should not be necessary in the future. For now, make sure that you have done
both `gcloud config set project recidiviz` and `gcloud auth application-default login`.

#### Checking code style

Run Pylint across the main body of code, in particular: `pylint recidiviz`.

The output will include individual lines for all style violations, followed by a handful of reports, and finally a
general code score out of 10. Fix any new violations in your commit. If you believe there is cause for a rule change,
e.g. if you believe a particular rule is inappropriate in the codebase, then submit that change as part of your
inbound pull request.

#### Autoformatting

We use `black` to ensure consistent formatting across the code base and `isort` to sort imports. There is a pre-commit hook that will format all of your files automatically. It is defined in `githooks/pre-commit` and is installed by `./initial_pipenv_setup_mac.sh`.

You can also set up your editor to run `black` and `isort` on save. See [the black docs](https://black.readthedocs.io/en/stable/integrations/editors.html) for how to configure external tools (both `black` and `isort`) to run in PyCharm (more info in PyCQA/isort#258).

In VSCode just add the following to your `.vscode/settings.json`:

```json
    ""editor.formatOnSave"": true,
    ""python.formatting.provider"": ""black"",
    ""[python.editor.codeActionsOnSave]"": {
        ""source.organizeImports"": true
    },
```

#### Static type checking

Run Mypy across all code to check for static type errors: `mypy recidiviz`.

#### Static security checking

We use `bandit` to check for static security errors within the `recidiviz` folder. This is run in the CI.
Adding `# nosec` to the effected line will ignore false positive issues.

### Deployment

Install the GCloud SDK using the [interactive installer](https://cloud.google.com/sdk/docs/downloads-interactive).

Note: make sure the installer did not add `google-cloud-sdk/platform/google_appengine` or subdirectories thereof to your `$PYTHONPATH`, e.g. in your bash profile. This could break attempts to run tests within the `pipenv shell` by hijacking certain dependencies.

Make sure you have docker installed (see instructions above), then configure docker authentication:

```bash
$ gcloud auth login
$ gcloud auth configure-docker
```

### Troubleshooting

If you see a pipenv error (either during install or sync) with the following:

```
An error occurred while installing psycopg2==...
```

On a Mac:

1. Ensure `postgresql` and `openssl` are installed with: `brew install postgresql openssl`
2. Run the initial pipenv setup script: `./initial_pipenv_setup_mac.sh`

On Linux: Ensure `libpq` is installed with: `apt update -y && apt install -y libpq-dev`
",,2024-05-03T09:04:10Z,119,37,25,"('ageiduschek', 1497), ('elchao96', 693), ('colincadams', 653), ('jjdressel11', 619), ('nbhargava', 554), ('ohaibbq', 394), ('danawillow', 383), ('nichelle-hall', 350), ('caroletouma', 339), ('brendali121', 310), ('lilidworkin', 289), ('gheidkamp11', 255), ('ChineduOzodi', 255), ('elisegonzal', 253), ('jamwalla', 246), ('morden35', 243), ('alexabatino', 238), ('terinpw', 226), ('daschi', 217), ('jessex', 214), ('lydiamasri2', 203), ('DSharm', 185), ('jovergaag', 176), ('glorialiu', 175), ('Catacola', 174), ('macfarlandian', 167), ('samanthahuff', 166), ('hugosr-r', 161), ('mayukas', 149), ('Gina-Valderrama', 141), ('zdg2102', 140), ('mxosman', 136), ('justkunz', 132), ('ohinds', 120), ('n-damiani', 119), ('zacharylawrence', 114), ('jkgerig', 111), ('arian487', 109), ('helperbot-recidiviz', 100), ('zagoodman', 100), ('dependabotbot', 99), ('phenggeler', 78), ('agaidus', 76), ('not-a-doctor-stromberg', 75), ('terryttsai', 58), ('brandon-hills', 51), ('santymendoza', 49), ('mantzw', 47), ('shalindb', 46), ('recidinick', 41), ('danielsmc', 40), ('pacopoler', 38), ('jmielke-recidiviz', 35), ('ethan-oro', 34), ('github-actionsbot', 34), ('dallen5', 33), ('rzgn', 31), ('madeleine-charity', 28), ('rasmi', 27), ('adeenal', 23), ('mhilderbran', 18), ('blake-recidiviz', 17), ('ajsecord', 16), ('eitalse', 16), ('olalikeola', 16), ('cawarren', 16), ('abromanos', 14), ('carole-touma', 14), ('jasonpingjp', 13), ('rhea-kak', 10), ('HarryZ10', 9), ('abbylsmith', 8), ('ayeshakhawaja', 8), ('jasonqng', 8), ('HalfdanJ', 7), ('nojibe', 7), ('claire25camacho', 6), ('MonicaHicks', 6), ('pragyakallanagoudar', 6), ('kirtanac', 6), ('alannaflores', 5), ('RicardoZamora01', 5), ('Paruer', 5), ('emilyemilyemilyemilyemilyemily', 5), ('alfonsogoberjr', 4), ('bpacker', 4), ('etiry', 4), ('jazzPouls', 4), ('zacharykatz1', 3), ('dan3ny', 3), ('olleicua', 3), ('rcarroll901', 3), ('mikelmcdaniel', 3), ('jbrambleDC', 3), ('dmcc', 3), ('albertyusun', 3), ('jtagvera', 2), ('KareemG', 2), ('joshuagrossman', 2), ('tstein', 1), ('cblinder', 1), ('inessheppard', 1), ('jasaczek', 1), ('jennai', 1), ('jkadams', 1), ('luketorjussen', 1), ('nthain', 1), ('pgondi13', 1), ('ssn2013', 1), ('yentangen', 1), ('roshcagra', 1), ('piyush13290', 1), ('mscott2757', 1), ('ljfranklin', 1), ('lpappone', 1), ('hobuobi', 1), ('cspickert', 1), ('toannazhao', 1), ('omgimanerd', 1)","[16, 'Peace, Justice and Strong Institutions']"
Growstuff/growstuff,Open data project for small-scale food growers,"# 🌱 Growstuff

![Build status](https://github.com/Growstuff/growstuff/workflows/CI/badge.svg)
[![Code Climate](https://codeclimate.com/github/Growstuff/growstuff/badges/gpa.svg)](https://codeclimate.com/github/Growstuff/growstuff)

Welcome to the Growstuff project.

You can find our app at https://www.growstuff.org

Growstuff is an open source/open data project for food gardeners.  We
crowdsource information on what our members are growing and harvesting,
aggregate it, and make it available as open data via our API.

Growstuff was founded in 2012 and has been built by dozens of
[contributors](CONTRIBUTORS.md).  We are an inclusive, welcoming project, and
encourage participation from people of all backgrounds and skill levels.

## Want to contribute?

Don't ask to ask, the best way to get started is to fork the project, start a codespace and get hacking.
Dive on in and submit your PRs.

## Important links

* [Issues](https://github.com/orgs/Growstuff/projects/1) (features we're
  working on, known bugs, etc)
* [![Gitter](https://badges.gitter.im/Growstuff/growstuff.svg)](https://gitter.im/Growstuff/growstuff?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
* [Wiki](https://github.com/Growstuff/growstuff/wiki) (general documentation, etc.)

## For coders

Growstuff is built in Ruby on Rails and also uses JavaScript for
frontend features. We welcome contributions -- see
[CONTRIBUTING](CONTRIBUTING.md) for details.

* To set up your development environment, see [Getting started](https://github.com/Growstuff/growstuff/wiki/New-contributor-guide).
* You may also be interested in our [API](https://github.com/Growstuff/growstuff/wiki/API).

## For designers, writers, researchers, data wranglers, and other contributors

There are heaps of ways to get involved and contribute no matter what
your skills and interests.

You might like to check out:

* The [New Contributor Guide](https://github.com/Growstuff/growstuff/wiki/New-contributor-guide)
  page on our wiki, which has lots of detail for different areas

Here on Github, you might find these useful:

* [Github Project Board](https://github.com/orgs/Growstuff/projects/1) has stories in ""ready"" that can be worked on.
* [needs: design](https://github.com/Growstuff/growstuff/labels/needs:%20design) - tasks requiring high-level design
* [needs: visual design](https://github.com/Growstuff/growstuff/labels/needs:%20visual+design) - tasks requiring visual/graphical design
* [needs: documentation](https://github.com/Growstuff/growstuff/labels/needs:%20documentation)
* [needs: data](https://github.com/Growstuff/growstuff/labels/needs:%20data) - tasks requiring data entry, data design, data import, or similar
* [curated:beginner](https://github.com/Growstuff/growstuff/labels/curated:%20beginner) - tasks that are ideal for beginner programmers or people new to the project

Feel free to comment on any of the issues on [Github](https://github.com/Growstuff/growstuff/issues).

## Contact

For more information about this project, contact [info@growstuff.org](mailto:info@growstuff.org).

Security Issues: If you find an authorization bypass or data breach, please contact our maintainers directly at [maintainers@growstuff.org](mailto:maintainers@growstuff.org).

You can also contact us on [Twitter](http://twitter.com/growstufforg/) or
[Facebook](https://www.facebook.com/pages/Growstuff/1531133417099494) or [Github](https://github.com/Growstuff/growstuff/issues)..
","'food', 'gardening'",2024-05-03T09:35:04Z,78,422,26,"('Br3nda', 4168), ('CloCkWeRX', 1020), ('dependabotbot', 832), ('dependabot-previewbot', 654), ('pozorvlak', 611), ('cesy', 291), ('tygriffin', 210), ('maco', 194), ('dependabot-support', 185), ('oshiho3', 93), ('deppbot', 82), ('Skud', 62), ('code-factor', 60), ('sabreuse', 51), ('yoongkang', 35), ('jenkr55', 34), ('Marlena', 31), ('jcaudle', 24), ('simicic', 22), ('sksavant', 19), ('brandonbaker40', 14), ('phazel', 11), ('rocky-jaiswal', 10), ('krio', 8), ('twconquest', 8), ('robertlandreaux', 7), ('bennett-zink', 7), ('le-jun', 7), ('martyhines', 7), ('wombleton', 7), ('lucasnogueira', 7), ('korabh', 6), ('attlebish', 6), ('bestest-mensch', 6), ('ameliagreenhall', 5), ('ImgBotApp', 5), ('juzham', 5), ('robotscissors', 5), ('snyk-bot', 5), ('kevieyang', 5), ('cephLpod', 4), ('wsmoak', 4), ('julietk', 4), ('leto', 4), ('italopires', 4), ('Catharz', 4), ('ctlewitt', 4), ('lambda2', 4), ('andrba', 4), ('GabrielSandoval', 3), ('emmawinston', 3), ('catfriend', 3), ('nbancajas', 3), ('maia-tw', 3), ('gabrielle27', 3), ('Thrillberg', 3), ('polveenomials', 2), ('maccath', 2), ('apdarr', 2), ('Prashanth261993', 2), ('jacarandang', 2), ('hbrodsk1', 2), ('federicomenaquintero', 2), ('cesullivan', 2), ('tehjaymo', 1), ('soborok', 1), ('naomiwattsup', 1), ('gustavor-souza', 1), ('wingyu', 1), ('Thomascountz', 1), ('sj26', 1), ('mftaff', 1), ('meganft', 1), ('manmeetsingh', 1), ('logangingerich', 1), ('blimey85', 1), ('flov', 1), ('arku', 1)","[12, 'Responsible Consumption and Production']"
kimetrica/MERON_model,Model for the detection of malnutrition using facial imagery,"# MERON
Model for the detection of malnutrition using facial imagery
",,2018-10-01T12:44:05Z,1,1,4,"('e-baumer', 20)","[3, 'Good Health and Well-Being']"
openaq/openaq-api,OpenAQ Platform API - NO LONGER IN USE see https://github.com/openaq/openaq-api-v2,"# OpenAQ Platform API
[![Build Status](https://travis-ci.org/openaq/openaq-api.svg?branch=master)](https://travis-ci.org/openaq/openaq-api)
[![Slack Chat](https://img.shields.io/badge/Chat-Slack-ff69b4.svg ""Join us. Anyone is welcome!"")](https://openaq-slackin.herokuapp.com/)

## NO LONGER IN USE

This codebase is no longer in use, please see https://github.com/openaq/openaq-api-v2.  Version 1 of the OpenAQ API is still available via api.openaq.org/v1 but has been reimplemented in the same repository as version 2.

## Overview
This is the main API for the [OpenAQ](https://openaq.org) project.

Starting with `index.js`, there is a web-accessible API that provides endpoints to query the air quality measurements. Documentation can be found at [https://docs.openaq.org/](https://docs.openaq.org/).

[openaq-fetch](https://github.com/openaq/openaq-fetch) takes care of fetching new data and inserting into the database. Data format is explained in [openaq-data-format](https://github.com/openaq/openaq-data-format).

## Getting started

Install prerequisites:

- [git](https://git-scm.com)
- [nvm](https://github.com/creationix/nvm)
- [Docker](https://www.docker.com/)

Clone this repository locally (see these [instructions](https://help.github.com/en/articles/cloning-a-repository)) and activate the required Node.js version with:

`nvm install`

The last step can be skipped if the local Node.js version matches the one defined at [.nvmrc](.nvmrc). 

Install module dependencies:

`npm install`

### Development

Initialize development database:

`npm run init-dev-db`

This task will start a PostgreSQL container as daemon, run migrations and seed data. Each of these tasks is available to be run independently, please refer to [package.json](package.json) to learn the options.

After initialization is finished, start the development server:

`npm run dev`

Access http://localhost:3004.

Stop database container after finishing:

`npm run stop-dev-db`

### Testing 

Initialize test database:

`npm run init-test-db`

This task will start a PostgreSQL container as daemon, run migrations and seed data. After initialization is finished, run tests:

`npm run test`

Stop database container after finishing:

`npm run stop-test-db`

## Deploying to production

The server needs to fetch data about locations and cities in the measurement history using AWS Athena. This service must be configured via the following environment variables:

- `ATHENA_ACCESS_KEY_ID`: an AWS Access Key that has permissions to create Athena Queries and store them in S3;
- `ATHENA_SECRET_ACCESS_KEY`: the corresponding secret;
- `ATHENA_OUTPUT_BUCKET`: S3 location (in the form of `s3://bucket/folder`) where the results of the Athena queries should be stored before caching them;
- `ATHENA_FETCHES_TABLE`: the name of the table registered in AWS Athena.

Automatic Athena synchronization is disabled by default. It can be enabled by setting the environment variable `ATHENA_SYNC_ENABLED` to `true`. The sync interval can be set using `ATHENA_SYNC_INTERVAL` variable, in miliseconds. The default interval is set in file [config/default.json](config/default.json).

If needed, the synchronization can be fired manually. First, the `WEBHOOK_KEY` variable must be set to allow access webhooks endpoint. Sending a POST request to /v1/webhooks, including the parameters `key=` and `action=ATHENA_SYNC`, will start a sync run. An example with curl:

```
curl --data ""key=123&action=ATHENA_SYNC"" https://localhost:3004/v1/webhooks
```

Other environment variables available:

| Name | Description | Default |
|---|---|---|
| API_URL | Base API URL after deployment | http://:3004 |
| NEW_RELIC_LICENSE_KEY | New Relic API key for system monitoring | not set |
| WEBHOOK_KEY | Secret key to interact with openaq-api | '123' |
| USE_REDIS | Use Redis for caching? | not set (so not used) |
| USE_ATHENA | Use AWS Athena for aggregations? | not set (so not used) |
| REDIS_URL | Redis instance URL | redis://localhost:6379 |
| DO_NOT_UPDATE_CACHE | Ignore updating cache, but still use older cached results. | not set |
| AGGREGATION_REFRESH_PERIOD | How long to wait before refreshing cached aggregations? (in ms) | 45 minutes |
| REQUEST_LIMIT | Max number of items that can be requested at one time. | 10000 |
| UPLOADS_ENCRYPTION_KEY | Key used to encrypt upload token for /upload in database. | 'not_secure' |
| S3_UPLOAD_BUCKET | The bucket to upload external files to for /upload. | not set |

### AWS Athena for aggregations


The Athena table is `fetches_realtime` that represents the fetches from `openaq-data` and has the following schema:

```sql
CREATE EXTERNAL TABLE fetches.fetches_realtime (
  date struct,
  parameter string,
  location string,
  value float,
  unit string,
  city string,
  attribution array>,
  averagingPeriod struct,
  coordinates struct,
  country string,
  sourceName string,
  sourceType string,
  mobile string
 )
 ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
 LOCATION 's3://EXAMPLE_BUCKET'
```

## Uploads & Generating S3 presigned URLs

Via an undocumented `/upload` endpoint, there is the ability to generate presigned S3 PUT URLs so that external clients can authenticate using tokens stored in the database and upload data to be ingested by `openaq-fetch`. There is a small utility file called `encrypt.js` that you can use like `UPLOADS_ENCRYPTION_KEY=foo node index.js your_token_here` to generate encrypted tokens to be manually stored in database.

## Dockerfile

There is a Dockerfile included that will turn the project into a Docker container. The container can be found [here](https://hub.docker.com/r/flasher/openaq-api/) and is currently mostly used for deployment purposes for [AWS ECS](https://aws.amazon.com/ecs/). If someone wanted to make it better for local development, that'd be a great PR!

## Contributing
There are a lot of ways to contribute to this project, more details can be found in the [contributing guide](CONTRIBUTING.md).

## Projects using the API

- openaq-browser [site](http://dolugen.github.io/openaq-browser) | [code](https://github.com/dolugen/openaq-browser) - A simple browser to provide a graphical interface to the data.
- openaq [code](https://github.com/nickolasclarke/openaq) - An isomorphic Javascript wrapper for the API
- py-openaq [code](https://github.com/dhhagan/py-openaq) - A Python wrapper for the API
- ropenaq [code](https://github.com/ropenscilabs/ropenaq) - An R package for the API

For more projects that are using OpenAQ API, checkout the [OpenAQ.org Community](https://openaq.org/#/community) page.
",,2022-06-22T22:06:53Z,13,149,16,"('jflasher', 364), ('dolugen', 39), ('danielfdsilva', 31), ('olafveerman', 29), ('kamicut', 12), ('sruti', 11), ('vgeorge', 5), ('sethvincent', 5), ('andrewharvey', 1), ('RocketD0g', 1), ('webbkyr', 1), ('nickolasclarke', 1), ('russbiggs', 1)","[13, 'Climate Action']"
edx/edx-platform,,"Please update your edx-platform URLs
####################################

This URL for this repository is no longer in use -- use https://github.com/openedx/edx-platform instead (in the openedx org).

For more information about this move and why you're seeing this instead of being redirected, see:
https://discuss.openedx.org/t/please-update-your-git-urls-for-edx-platform-and-several-other-repos/12387
",,2024-02-27T15:19:25Z,1,0,62,"('timmc-edx', 2)","[4, 'Quality Education']"
bayesimpact/encompass,Bayes Impact's Encompass Tool for Visualizing and Analyzing Geographic Access,"# Encompass

[![Build Status][build]](https://circleci.com/gh/bayesimpact/encompass) [![apache2]](https://www.apache.org/licenses/LICENSE-2.0)

[build]: https://img.shields.io/circleci/project/bayesimpact/encompass/master.svg
[apache2]: https://img.shields.io/badge/License-Apache%202.0-blue.svg

## Introduction
Encompass is an analytics and mapping tool by [Bayes Impact](http://bayesimpact.org) that enables policymakers, researchers, and consumer advocates to analyze how accessibility to social services varies across demographic groups. Inadequate and untimely access to health care services is a major barrier to health equity for disadvantaged communities. Existing tools used to map systems at this scale are prohibitively expensive, require significant amounts of manual data processing, and are too coarse in their analysis methods to accurately depict accessibility issues. We set out to build a solution that eliminates those barriers.

This is an open-source project. We invite researchers, developers, and the public to contribute to our project. See below for details.

## [Launch Encompass](https://encompass.bayesimpact.org)

[![alt text][screenshot]](https://encompass.bayesimpact.org)

[screenshot]: data/images/encompass_texas.png

## How to contribute
__Researchers__: We’d love to collaborate with any researchers who might find our tool useful! Please let us know what other applications or datasets you would like to analyze with Encompass. Send your inquiries to [encompass@bayesimpact.org](mailto:encompass@bayesimpact.org).

__Developers__: We want to invite the developer community to contribute to our mission of promoting a culture of evidence-based and transparent policymaking. Please read [CONTRIBUTING.md](https://github.com/bayesimpact/encompass/blob/master/CONTRIBUTING.md) to learn more about how you can get involved.
",,2024-04-27T03:24:28Z,9,26,10,"('ericboucher', 186), ('bcherny', 180), ('tetraptych', 124), ('philipdickinson', 91), ('akegan', 46), ('pcorpet', 6), ('mjamei', 5), ('viviandien', 4), ('dependabotbot', 1)","[8, 'Decent Work and Economic Growth']"
datamade/census,A Python wrapper for the US Census API. ,"======
census
======
.. image:: https://github.com/datamade/census/workflows/Python%20package/badge.svg
 

A simple wrapper for the United States Census Bureau's API.

Provides access to ACS and SF1 data sets.

Install
=======

::

    pip install census

You may also want to install a complementary library, `us `_, which help you figure out the
`FIPS `_ codes for many geographies. We use it in the examples below.

::

   pip install us

Usage
=====

First, get yourself a `Census API key `_.

::

    from census import Census
    from us import states

    c = Census(""MY_API_KEY"")
    c.acs5.get(('NAME', 'B25034_010E'),
              {'for': 'state:{}'.format(states.MD.fips)})

The call above will return the name of the geographic area and the number of
homes that were built before 1939 for the state of Maryland. Helper methods have
been created to simplify common geometry calls::

    c.acs5.state(('NAME', 'B25034_010E'), states.MD.fips)

Full details on geometries and the states module can be found below.

The get method is the core data access method on both the ACS and SF1 data sets.
The first parameter is either a single string column or a tuple of columns. The
second parameter is a geoemtry dict with a `for` key and on option `in` key. The
`for` argument accepts a `""*""` wildcard character or `Census.ALL`. The wildcard
is not valid for the `in` parameter.

By default, the year for a dataset is the most recent year available. To access earlier data,
pass a year parameter to the API call::

    c.acs5.state(('NAME', 'B25034_010E'), states.MD.fips, year=2010)

The default year may also be set client-wide::

    c = Census(""MY_API_KEY"", year=2010)


Detailed information about the API can be found at the `Census Data API User Guide `_.

Datasets
========

For each dataset, the first year listed is the default.

* acs5: `ACS 5 Year Estimates `_ (2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009)
* acs3: `ACS 3 Year Estimates `_ (2013, 2012)
* acs1: `ACS 1 Year Estimates `_ (2021, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011)
* acs5dp: `ACS 5 Year Estimates, Data Profiles  `_ (2021, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009)
* acs3dp: `ACS 3 Year Estimates, Data Profiles `_ (2013, 2012)
* acs1dp: `ACS 1 Year Estimates, Data Profiles `_ (2021, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011)
* acs5st: `ACS 5 Year Estimates, Subject Tables `_ (2021, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009)
* sf1: `Census Summary File 1 `_ (2010)
* pl: `Redistricting Data Summary File `_ (2020, 2010, 2000) 


Geographies
===========

The API supports a wide range of geographic regions. The specification of these
can be quite complicated so a number of convenience methods are provided. Refer to the `Census API documentation `_
for more geographies beyond the convenience methods.

*Not all geographies are supported in all years. Calling a convenience method
with a year that is not supported will raise census.UnsupportedYearException.*

`Geographic relationship files `_ are provided on the Census developer site as a tool to help users compare the geographies from the 1990, 2000 and 2010 Censuses. From these files, data users may determine how geographies from one Census relate to those from the prior Census.

ACS5 Geographies
----------------

* state(fields, state_fips)
* state_county(fields, state_fips, county_fips)
* state_county_blockgroup(fields, state_fips, county_fips, blockgroup)
* state_county_subdivision(fields, state_fips, county_fips, subdiv_fips)
* state_county_tract(fields, state_fips, county_fips, tract)
* state_place(fields, state_fips, place)
* state_congressional_district(fields, state_fips, congressional_district)
* state_legislative_district_upper(fields, state_fips, legislative_district)
* state_legislative_district_lower(fields, state_fips, legislative_district)
* us(fields)
* state_zipcode(fields, state_fips, zip5)

ACS1 Geographies
----------------

* state(fields, state_fips)
* state_congressional_district(fields, state_fips, district)
* us(fields)

SF1 Geographies
---------------

* state(fields, state_fips)
* state_county(fields, state_fips, county_fips)
* state_county_subdivision(fields, state_fips, county_fips, subdiv_fips)
* state_county_tract(fields, state_fips, county_fips, tract)
* state_place(fields, state_fips, place)
* state_congressional_district(fields, state_fips, district)
* state_msa(fields, state_fips, msa)
* state_csa(fields, state_fips, csa)
* state_district_place(fields, state_fips, district, place)
* state_zipcode(fields, state_fips, zip5)

PL Geographies
--------------

* state(fields, state_fips)
* state_county(fields, state_fips, county_fips)
* state_county_subdivision(fields, state_fips, county_fips, subdiv_fips)
* state_county_tract(fields, state_fips, county_fips, tract)
* state_county_blockgroup(fields, state_fips, county_fips, blockgroup)
* state_place(fields, state_fips, place)
* state_congressional_district(fields, state_fips, district)
* state_legislative_district_upper(fields, state_fips, legislative_district)
* state_legislative_district_lower(fields, state_fips, legislative_district)

States
======

This package previously had a `census.states` module, but now uses the
*us* package. ::

    >>> from us import states
    >>> print states.MD.fips
    u'24'

Convert FIPS to state abbreviation using `lookup()`: ::

    >>> print states.lookup('24').abbr
    u'MD'


BYOS - Bring Your Own Session
=============================

If you'd prefer to use a custom configured requests.Session, you can pass it
to the Census constructor::

    s = requests.session()
    s.headers.update({'User-Agent': 'census-demo/0.0'})

    c = Census(""MY_API_KEY"", session=s)

You can also replace the session used by a specific data set::

    c.sf1.session = s


Examples
========

The geographic name for all census tracts for county 170 in Alaska::

    c.sf1.get('NAME', geo={'for': 'tract:*',
                           'in': 'state:{} county:170'.format(states.AK.fips)})

The same call using the `state_county_tract` convenience method::

    c.sf1.state_county_tract('NAME', states.AK.fips, '170', Census.ALL)

Total number of males age 5 - 9 for all states::

    c.acs5.get('B01001_004E', {'for': 'state:*'})

The same call using the state convenience method::

    c.acs5.state('B01001_004E', Census.ALL)

Don't know the list of tables in a survey, try this:

    c.acs5.tables()
","'census', 'census-api', 'python'",2024-04-14T21:27:14Z,28,608,28,"('fgregg', 100), ('jcarbaugh', 48), ('Detinator10', 7), ('hancush', 6), ('arturo-ramos', 4), ('joehand', 4), ('ronnie-llamado', 4), ('sweatercomeback', 3), ('bahoo', 3), ('jcgiuffrida', 3), ('prha', 3), ('palewire', 2), ('MaxGhenis', 2), ('koshy1123', 2), ('jeancochrane', 2), ('dwbond', 2), ('rifferreinert', 1), ('sirwart', 1), ('jiffyclub', 1), ('selik', 1), ('ryanvmenezes', 1), ('rrizner7', 1), ('andytay', 1), ('bsweger', 1), ('mattspence', 1), ('mr-fuller', 1), ('noahlee826', 1), ('thegreatmagnet', 1)","[16, 'Peace, Justice and Strong Institutions']"
mapswipe/python-mapswipe-workers,MapSwipe Back-End,"# MapSwipe Back-End

[MapSwipe](http://mapswipe.org/) is a mobile app that lets you search satellite imagery to help put the world's most vulnerable people on the map. If you are new to MapSwipe it might be good to have a look at the [FAQs](http://mapswipe.org/faq.html) first.

The MapSwipe Back-End consists of a number of components:

1. Firebase Project
2. MapSwipe Workers
4. Postgres Database
3. Manager Dashboard
5. API

Please refer to the documentation for more information: https://mapswipe-workers.readthedocs.io/


## Resources

- MapSwipe Back-End: https://github.com/mapswipe/python-mapswipe-workers
- MapSwipe App https://github.com/mapswipe/mapswipe
- MapSwipe Website: https://mapswipe.org
- MapSwipe OSM-Wiki: https://wiki.openstreetmap.org/wiki/MapSwipe


## Development Setup 

Please see here: https://mapswipe-workers.readthedocs.io/en/master/dev_setup.html

## Contributing Guidelines

### Feature Branch

To contribute to the MapSwipe back-end please create dedicated feature branches based on the `dev` branch. After the changes create a Pull Request of the `feature` branch into the `dev` branch on GitHub:

```bash
git checkout dev
git checkout -b featureA
# Hack away ...
git commit -am 'Describe changes.'
git push -u origin featureA
# Create a Pull Request from feature branch into the dev branch on GitHub.
```

> Note: If a bug in production (master branch) needs fixing before a new versions of MapSwipe Workers gets released (merging dev into master branch), a hotfix branch should be created. In the hotfix branch the bug should be fixed and then merged with the master branch (and also dev).


### Style Guide

This project uses [black](https://github.com/psf/black) and [flake8](https://gitlab.com/pycqa/flake8) to achieve a unified style.

Use [pre-commit](https://pre-commit.com/) to run `black` and `flake8` prior to any git commit. `pre-commit`, `black` and `flake8` should already be installed in your virtual environment since they are listed in `requirements.txt`. To setup pre-commit simply run:

```
pre-commit install
```

From now on `black` and `flake8` should run automatically whenever `git commit` is executed.


## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details


## Authors

* **Benjamin Herfort** - HeiGIT - [Hagellach37](https://github.com/Hagellach37)
* **Marcel Reinmuth** - HeiGIT - [maze2point0](https://github.com/maze2point0)
* **Matthias Schaub** - HeiGIT - [Matthias-Schaub](https://github.com/Matthias-Schaub)

See also the list of [contributors](contributors.md) who participated in this project.


## Acknowledgements 

* Humanitarian organizations can't help people if they can't find them.
","'firebase', 'gis', 'gis-data', 'hacktoberfest', 'humanitarian', 'nonprofit', 'ong'",2024-04-29T06:34:21Z,27,26,15,"('Hagellach37', 1058), ('Matthias-Schaub', 434), ('ElJocho', 190), ('maze2point0', 114), ('thenav56', 97), ('laurentS', 83), ('frozenhelium', 83), ('tnagorra', 64), ('shreeyash07', 44), ('samshara', 43), ('ericboucher', 34), ('TahiraU', 27), ('k9845', 14), ('matthiasschaub', 13), ('ddevault', 11), ('ofr1tz', 10), ('mathcass', 6), ('faevourite', 6), ('danbjoseph', 6), ('itisacloud', 5), ('masain', 5), ('kopitek8', 3), ('AdityaKhatri', 1), ('gcaria', 1), ('jhollowe', 1), ('mcauer', 1), ('poulamb', 1)","[17, 'Partnerships for the Goals']"
LibreHealthIO/lh-toolkit,LibreHealth Toolkit is a software API and user interface that can be used to create electronic health record systems. Canonical Repository is at https://gitlab.com/librehealth/toolkit/lh-toolkit,"[![build status](https://gitlab.com/librehealth/lh-toolkit/badges/master/build.svg)](https://gitlab.com/librehealth/lh-toolkit/commits/master)

Looking to contribute? See [CONTRIBUTING.md](CONTRIBUTING.md) for how to get started.

LibreHealth Toolkit is a software API and user interface components that can be used to create electronic health record systems.
Read more about the project & find documentation at: http://toolkit.librehealth.io

The project tree is set up as follows:


    
        api/
        java and resource files for building the java api jar file.
    
    
        puppet
        Puppet scripts for local development and deployment.
    
	
        release-test
        Cucumber/selenium integration tests. Run daily against a running web application.
    
    
        test
        Maven project to share tools that are used for testing.
    
    
        tools
        Meta code used during compiling and testing. Does not go into any released binary (like doclets).
    
    
        web/
        java and resource files that are used in the web application.
    
    
        webapp/
        jsp files used in building the war file.
    
    
        .gitlab-ci.yml
        Used to configure Gitlab CI. Each branch can have its own configuration.
    
    
        license-header.txt
        Used by license-maven-plugin to check license on all source code headers.
    
    
        pom.xml
        The main maven file used to build and package OpenMRS.
    

",,2023-02-12T20:30:19Z,200,12,10,"('dkayiwa', 1647), ('wluyima', 1485), ('rkorytkowski', 821), ('djazayeri', 770), ('mseaton', 195), ('mogoodrich', 183), ('sunbiz', 182), ('jlkeiper', 125), ('jmiranda', 121), ('k-joseph', 96), ('lluismf', 89), ('pihdave', 82), ('kishoreyekkanti', 77), ('syhaas', 59), ('umupfumu', 52), ('bmamlin', 47), ('robbyoconnor', 40), ('jkondrat', 40), ('harsha89', 37), ('rpuzdrowski', 35), ('bwolfe', 33), ('mblanchette', 33), ('dkithmal', 28), ('tomaszmueller', 24), ('mvorobey', 22), ('pmuchowski', 22), ('akollegger', 16), ('vinkesh', 16), ('gauravpaliwal', 15), ('geoff-wasilwa', 15), ('surangak', 15), ('teleivo', 14), ('saramirza14', 14), ('akolodziejski', 13), ('kristopherschmidt', 13), ('vaibhav-hp', 11), ('bhashitha', 10), ('madawas', 9), ('dadepo', 9), ('akmad', 9), ('ivange94', 9), ('dszafranek', 8), ('gsluthra', 8), ('Fahv', 8), ('ArcTanSusan', 8), ('vmanand', 8), ('LeeBreisacher', 7), ('thilobeckmann', 7), ('DraggonZ', 7), ('ShekharReddy4', 7), ('akshika47', 6), ('andreapat', 6), ('preethi29', 6), ('GarimaAhuja', 6), ('NameFILIP', 6), ('anishbasu', 6), ('aka001', 6), ('shruthidipali', 5), ('anotherdave', 5), ('chaityabshah', 5), ('rowanseymour', 5), ('vencik', 5), ('sgithens', 5), ('ujjwalarora', 5), ('roman-zaiats', 4), ('jdegraft', 4), ('itatriev', 4), ('maany', 4), ('gordonbr', 4), ('WolfSchlegel', 4), ('ShubhamRai', 4), ('namratanehete', 4), ('gnarula', 4), ('endeepak', 4), ('cliftonmcintosh', 4), ('ojQj', 4), ('channab', 3), ('ern2', 3), ('esudharaka', 3), ('kushal8', 3), ('samshuster', 3), ('tobivogel', 3), ('usn1485', 3), ('vishnuraom', 3), ('vchilkuri', 3), ('astorga2', 3), ('sjmckee', 3), ('suniala', 3), ('macorrales', 3), ('mszukalski', 3), ('mnsowmya', 3), ('LukasBreitwieser', 3), ('jasper-vandemalle', 3), ('ckomlo', 3), ('alexei-grigoriev', 3), ('mhawila', 2), ('ishara-p', 2), ('apien', 2), ('yousefhamza', 2), ('yony258', 2), ('winglam', 2), ('vikashgupta2000', 2), ('Undo1', 2), ('tommunist', 2), ('nithyagubbala', 2), ('rohanpoddar', 2), ('sashrika', 2), ('sintjuri', 2), ('smv-recommind', 2), ('tanayabh', 2), ('tonybeing', 2), ('sindur', 2), ('spereverziev', 2), ('limitless-horizon', 2), ('acbarbosa', 2), ('wongalvis', 2), ('fouadz', 2), ('gitahinganga', 2), ('hablutzel1', 2), ('jeandamore', 2), ('LishaR', 2), ('marc-harrison', 2), ('marvinyan', 2), ('mikechong', 2), ('ThisuraThejith', 2), ('tjtunnell', 2), ('rcrichton', 2), ('nutsiepully', 2), ('ozge-babylon', 2), ('owolabileg', 2), ('MitchellBot', 2), ('MichalSlawin', 2), ('midnighteuler', 2), ('michaelhofer-slg', 2), ('shiangree', 1), ('gto11520', 1), ('foolchan2556', 1), ('hnnesv', 1), ('cintiadr', 1), ('cardy31', 1), ('blobbered', 1), ('bholagabbar', 1), ('atmohsin', 1), ('Asha-surapaneni', 1), ('aniketha', 1), ('angshu', 1), ('andreweskeclarke', 1), ('alexisduque', 1), ('vincentes', 1), ('felixhammerl', 1), ('wsvalentine94', 1), ('stefanneuhaus', 1), ('ulakkaraju', 1), ('sushmitharaos', 1), ('ssmusoke', 1), ('LokeshBabu11', 1), ('SharonVarghese', 1), ('satwikreddy', 1), ('sachethgupta', 1), ('rohitmukherjee', 1), ('rahulruikar', 1), ('mylesbarros', 1), ('mujir', 1), ('mihirk', 1), ('karrtikiyer-tw', 1), ('jeffblack360', 1), ('haychris', 1), ('durac', 1), ('finchie', 1), ('dengfengli', 1), ('TheLastAngryMan', 1), ('ddworken', 1), ('dlahn', 1), ('DrRacket', 1), ('cioan', 1), ('ctreatma', 1), ('carloca68', 1), ('sidtharthanan', 1), ('drbobrinkman', 1), ('BhavanaRamasayam', 1), ('BakirK', 1), ('ashishk1994', 1), ('albertsaave', 1), ('thomasvandoren', 1), ('STheis', 1), ('SSS475', 1), ('pturley', 1), ('ningosi', 1), ('NicholasFolk', 1), ('michaelpigg', 1), ('downeymj', 1), ('MatthiasZerau', 1), ('mmanika', 1), ('jart', 1), ('jsala1990', 1), ('jkantorjunk', 1), ('aquSiLwH', 1), ('gauravsaini03', 1), ('firzhan', 1), ('fernando-toledo', 1)","[3, 'Good Health and Well-Being']"
OADA/server,Reference implementation of an OADA-conformant API server,"# OADA Reference API Server

[![CodeFactor](https://www.codefactor.io/repository/github/OADA/server/badge)](https://www.codefactor.io/repository/github/OADA/server)
[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)
[![License](https://img.shields.io/github/license/OADA/server)](LICENSE)

This project is a reference implementation of an OADA-conformant API server.
It can be used to host your own OADA instance,
or for comparison when creating an OADA-conformant API implementation.

The repository and releases come with configurations for easily running
with [docker-compose].

You can also theoretically run all the parts without docker or [docker-compose],
but you will need [arangodb] and either [redpanda] or [kafka]
for the micro-services to use.

## OADA micro-services

For information on
the various micro-services comprising this reference implementation,
see [the `oada` folder](oada/).

## Installing

### Kubernetes

There is a Helm chart of the OADA server.
It is the preferred method of using the server now.

See [charts/oada](charts/oada) for more information.

### docker-compose

#### Running a release in docker-compose

Download one of our [releases] and start it using [docker-compose].

```shell
cd folder/containing/release/docker-compose
# Will pull the corresponding release images from DockerHub
DOMAIN=yourdomain.com docker-compose up -d
```

#### Running as localhost with docker-compose

localhost doesn't work out of the box because you need to
create a self-signed SSL certificate and map it into your OADA installation.

To generate a certificate, get [mkcert](https://github.com/FiloSottile/mkcert)
(works on pretty much every platform).
Then, install `mkcert` to your machine's local certificate authority
(so when you make requests to your OADA, it will trust the self-signed certificate)

```shell
# Only do this if you did not previously have mkcert installed.
mkcert -install
```

Now generate a certificate for localhost (in a folder named localhost)

```shell
mkdir localhost
mkcert -cert-file ./localhost/fullchain.pem -key-file ./localhost/privkey.pem localhost
# The certificate is at ""./localhost/fullchain.pem"" and the key at ""./localhost/privkey.pem""
```

Finally, tell OADA to use it by mapping it
into the docker-compose service definition for the proxy.
NOTE: if you already have a `docker-compose.override.yml`,
do not run this command as it will overwrite it.

```shell
# Configure docker to map in the new cert as an override:
printf ""services:\n  proxy:\n    volumes:\n      - ./localhost:/config/keys/letsencrypt:ro"" > docker-compose.override.yml
# Re-up the proxy container with the new cert:
DOMAIN=localhost docker-compose up -d proxy
```

Your server should now be working on localhost. Time to add a user and a token with which to make requests.

### Add a user

You will need to add a user and a token to start making requests against your new OADA server. To add a user:

```shell
docker-compose run users add -u username -p password -a
```

replacing `username` and `password` with the username and password that you want. The `-a` means that the created user will be an admin user with the ability to add other users via the API.0

### Add a token for a user

You need a token to make a request against an OADA server. While you can get a token via OAuth2, to add a token
for a user from the command line:

```shell
docker-compose run auth token create -u username -s all:all
```

The ""-s"" is the ""scope"" for the token. `all:all` means read/write for all content types. The token will have permission to any resources that the user has permission to.

That command will print the last line `Successfully wrote token `. Copy the `` for the authorization header on your requests.

### Running from the git

If you want to contribute, or do other development type things,
you can run the server straight from this codebase.

```shell
git clone https://github.com/OADA/server.git
cd server
# Running up the first time will automatically build the docker images
DOMAIN=yourdomain.com docker-compose up -d
```

Note that running from the git is **not** recommended for production use.

## Configuration

To modify the docker-compose configuration of your OADA instance,
you can simply create a `docker-compose.override.yml`
in the same directory as the `docker-compose.yml` file.
Any settings in this [override file] will be merged with ours
when running docker-compose.

### Environment variables

Additionally, there are various configuration environment variables available.
Some important ones are:

- DOMAIN: set to the domain name of your API server
  (e.g., `oada.mydomain.net`)
- EXTRA_DOMAINS: Additional domains to serve
  (e.g., `oada.mydomain.org,oada.myotherdomain.net`)
- DEBUG: set the namespace(s) enabled in [debug]
  (e.g., `*:info,*:error,*:warn`)

Rather than trying to always remember to set your environment variables,
you probably want to use a [.env file] for things like `DOMAIN`.

[releases]: https://github.com/OADA/server/releases
[docker-compose]: https://docs.docker.com/compose/
[.env file]: https://docs.docker.com/compose/environment-variables/#substitute-environment-variables-in-compose-files
[arangodb]: https://www.arangodb.com
[redpanda]: https://vectorized.io/redpanda
[kafka]: https://kafka.apache.org
[override file]: https://docs.docker.com/compose/extends/#understanding-multiple-compose-files
[debug]: https://www.npmjs.com/package/debug#usage
[`oadadeploy`]: https://github.com/OADA/oadadeploy
","'docker', 'docker-compose', 'node-js', 'oada'",2024-05-01T22:04:26Z,11,12,14,"('awlayton', 849), ('sanoel', 116), ('dependabotbot', 88), ('aultac', 54), ('abalmos', 32), ('YaguangZhang', 26), ('tomohiroarakawa', 22), ('cyrusbowman', 14), ('wang701', 13), ('N-give', 8), ('snyk-bot', 1)","[12, 'Responsible Consumption and Production']"
scratchfoundation/scratch-blocks,Scratch Blocks is a library for building creative computing interfaces.,"# scratch-blocks

Scratch Blocks is a library for building creative computing interfaces.

[![CircleCI](https://dl.circleci.com/status-badge/img/gh/LLK/scratch-blocks/tree/develop.svg?style=shield)](https://dl.circleci.com/status-badge/redirect/gh/LLK/scratch-blocks/tree/develop)

![An image of Scratch Blocks running on a tablet](https://cloud.githubusercontent.com/assets/747641/15227351/c37c09da-1854-11e6-8dc7-9a298f2b1f01.jpg)

## Introduction

Scratch Blocks is a fork of Google's [Blockly](https://github.com/google/blockly) project that provides a design
specification and codebase for building creative computing interfaces. Together with the [Scratch Virtual Machine
(VM)](https://github.com/scratchfoundation/scratch-vm) this codebase allows for the rapid design and development of visual
programming interfaces. Unlike [Blockly](https://github.com/google/blockly), Scratch Blocks does not use [code
generators](https://developers.google.com/blockly/guides/configure/web/code-generators), but rather leverages the
[Scratch Virtual Machine](https://github.com/scratchfoundation/scratch-vm) to create highly dynamic, interactive programming
environments.

*This project is in active development and should be considered a ""developer preview"" at this time.*

## Two Types of Blocks

![A divided image showing horizontal blocks on the left and vertical blocks on the right](https://cloud.githubusercontent.com/assets/747641/15255731/dad4d028-190b-11e6-9c16-8df7445adc96.png)

Scratch Blocks brings together two different programming ""grammars"" that the Scratch Team has designed and continued
to refine over the past decade. The standard [Scratch](https://scratch.mit.edu) grammar uses blocks that snap together
vertically, much like LEGO bricks. For our [ScratchJr](https://scratchjr.org) software, intended for younger children,
we developed blocks that are labelled with icons rather than words, and snap together horizontally rather than
vertically. We have found that the horizontal grammar is not only friendlier for beginning programmers but also better
suited for devices with small screens.

## Documentation

The ""getting started"" guide including [FAQ](https://scratch.mit.edu/developers#faq) and [design
documentation](https://github.com/scratchfoundation/scratch-blocks/wiki/Design) can be found in the
[wiki](https://github.com/scratchfoundation/scratch-blocks/wiki).

## Donate

We provide [Scratch](https://scratch.mit.edu) free of charge, and want to keep it that way! Please consider making a
[donation](https://secure.donationpay.org/scratchfoundation/) to support our continued engineering, design, community,
and resource development efforts. Donations of any size are appreciated. Thank you!

## Committing

This project uses [semantic release](https://github.com/semantic-release/semantic-release) to ensure version bumps
follow semver so that projects depending on it don't break unexpectedly.

In order to automatically determine version updates, semantic release expects commit messages to follow the
[conventional-changelog](https://github.com/bcoe/conventional-changelog-standard/blob/master/convention.md)
specification.

You can use the [commitizen CLI](https://github.com/commitizen/cz-cli) to make commits formatted in this way:

```bash
npm install -g commitizen@latest cz-conventional-changelog@latest
```

Now you're ready to make commits using `git cz`.
",,2024-05-03T06:11:14Z,116,2531,170,"('rachel-fenichel', 884), ('NeilFraser', 771), ('renovatebot', 744), ('dependabot-previewbot', 452), ('tmickel', 331), ('paulkaplan', 304), ('ericrosenbaum', 236), ('kchadha', 205), ('chrisgarrity', 161), ('seanlip', 159), ('renovate-bot', 157), ('semantic-release-bot', 116), ('picklesrus', 115), ('marisaleung', 104), ('thisandagain', 100), ('AnmAtAnm', 78), ('Nikerabbit', 73), ('drigz', 63), ('cwillisf', 61), ('espertus', 53), ('rschamp', 45), ('towerofnix', 39), ('siebrand', 34), ('fsih', 26), ('quachtina96', 24), ('edauterman', 23), ('daarond', 20), ('CoryDCode', 18), ('carlosperate', 18), ('TheBrokenRail', 17), ('epicfaace', 16), ('RoboErikG', 14), ('Immortalin', 12), ('timdawborn', 11), ('Kenny2github', 10), ('mrjacobbloom', 10), ('benjiwheeler', 8), ('trodi', 7), ('tansly', 7), ('adroitwhiz', 7), ('K-ran', 7), ('khanning', 6), ('mzgoddard', 6), ('gnarf', 6), ('Kaworru', 6), ('Tymewalk', 5), ('joshyrobot', 5), ('kyleplo', 5), ('miguel76', 5), ('dlaliberte', 5), ('carloslfu', 4), ('shirletan', 4), ('pkendall64', 4), ('Clark-E', 4), ('svbatalov', 4), ('aoneill01', 4), ('allisonshaw', 4), ('julescubtree', 3), ('ivanixgames', 3), ('vincentbriglia', 3), ('MicrosoftSam', 3), ('mfrtrifork', 3), ('MarkusBordihn', 3), ('ErikMejerHansen', 3), ('ZenithRogue', 3), ('zgtm', 2), ('vicng', 2), ('hyperobject', 2), ('apple502j', 2), ('bigeyex', 2), ('harbaum', 2), ('seotts', 2), ('samelhusseini', 2), ('RaniSputnik', 2), ('pjkui', 2), ('niccokunzmann', 2), ('moniika', 2), ('madCode', 2), ('tarling', 2), ('twodee', 2), ('TechplexEngineer', 2), ('NAllred91', 1), ('Grahack', 1), ('rexpie', 1), ('sclements', 1), ('svipul', 1), ('gengshenghong', 1), ('ensonic', 1), ('petrusek', 1), ('TonyLianLong', 1), ('vetrovosk', 1), ('dependabotbot', 1), ('g-rocket', 1), ('a49594a', 1), ('joker314', 1), ('sjhuang26', 1), ('lindt', 1), ('BBI-YggyKing', 1), ('cesium12', 1), ('IAmAnubhavSaini', 1), ('benjie', 1), ('cdjackson', 1), ('ciaranj', 1), ('DerekTBrown', 1), ('ewpatton', 1), ('hasso', 1), ('eqot', 1), ('jano42', 1), ('jimmo', 1), ('jordiorlando', 1), ('justingeeslin', 1), ('waitingcheung', 1), ('takaokouji', 1), ('lizlooney', 1), ('mraerino', 1), ('MichaelZawadzki', 1)","[4, 'Quality Education']"
sozialhelden/accessibility-cloud,"👩🏽‍🦯🦮👩🏻‍🦽👩🏿‍🦼 the platform to exchange physical accessibility data in a standardized, future-proof, easy-to-use way.","# accessibility.cloud

accessibility.cloud is an initiative by [SOZIALHELDEN e.V.](https://www.sozialhelden.de), a non-profit organization based in Germany. The organization runs Wheelmap.org – world's largest open online map for wheelchair accessible places.

We believe in an inclusive world and want to make it easy to find accessibility information—wherever people need it. That’s why we want to encourage everybody to share this kind of data with each other.

As a tool, we suggest accessibility.cloud to share data in a standardized, future-proof, easy-to-use way.

## Contributing data

- [A11yJSON](https://sozialhelden.github.io/ac-format/attributes.html) - everything about AllyJSON, our data exchange standard
- [Importing Data](./app/docs/importing-data.md) - a guide on how to import and share new data sources on accessibility.cloud

## Using data

- [JSON-API](./app/docs/json-api.md) - Comprehensive documentation of the official accessibility.cloud API
- [JavaScript widget code](https://github.com/sozialhelden/accessibility-cloud-js) - a JavaScript sample widget that shows how to integrate the API
- [JavaScript widget demo](https://sozialhelden.github.io/accessibility-cloud-js/) - shows how the integrated JS widget looks

## Contributing Code

- [Development guide](./app/docs/development.md) - a short introduction on how to get the accessibility.cloud backend running locally as a developer

## Legal Information

- [Terms & Conditions](./app/docs/terms-for-signup.md)

","'a11y', 'accessibility', 'api', 'api-server', 'data-schema', 'elevator', 'escalator', 'geo-search', 'gis', 'json-api', 'location-services', 'meteor', 'meteor-application', 'ngo', 'openstreetmap', 'osm', 'poi', 'wheelchair'",2023-11-27T14:40:06Z,13,46,7,"('opyh', 884), ('pixtur', 172), ('codazzo', 79), ('mutaphysis', 68), ('nero-still', 36), ('holgerd', 10), ('dependabotbot', 5), ('chhu', 3), ('HansVonEisen', 3), ('schra', 2), ('snyk-bot', 2), ('hadiasghari', 1), ('karolinamakaruk', 1)","[10, 'Reduced Inequalities']"
rightoneducation/righton-app,React Native mobile app & React web app,"# RightOn Education

### Our Mission & Vision
Mission: Maybe you too know kids (or even adults) who'll say things like, ""I'm not a math person."" or ""I just can't do science."" Our mission is to unlock every student’s potential in STEM: building self-confidence, developing conceptual understanding, and helping them overcome hurdles in both school & life: whether they'd like to build apps, go to Mars, or become the next Beyonce!

While many apps focus on getting answers as quickly as possible, we're turning things around: RightOn not only makes it OK to make mistakes: players get rewarded for the most popular wrong answers. Through learning from mistakes and misconceptions, everyone takes their learning to the next level.

Vision: In the future, we envision RightOn! to become a multimedia learning platform and game show (spanning mobile apps and livestream video), one that eventually helps connect students of all ages and backgrounds to learn new subjects -- from math today to others in the future. 

### Our Values
We’re a team that sometimes falls down, always gets back up, and never stops having fun.
Trust and open communication: first seeking to understand, then to be understood
Integrity: doing the right thing
Compassion: lending a hand to others, giving back, expressing gratitude
Humility: working together on something bigger than ourselves
Beginner’s mindset: going into things with an open mind, always learning and making new mistakes
Fun: having fun along the way!

### Project Description
We're currently developing two apps:
1. A React Native mobile app for middle/high school teachers to use in classroom environments. Think Balderdash/Fibbage + STEM + learning from and having fun with mistakes and misconceptions. 
2. A React web app that enables teachers to create games/questions that appear in the mobile app and view classroom results.

We're running on AWS, including AppSync/GraphQL, DynamoDB, and Lambda. A future phase might include a livestream game show with elements similar to the following online rock-papers-scissors tournament. 

### Getting Started

Click into the **web** or **mobile** directory to learn how to get started. 

### Files Needed

The `react-native` npm package.

A `local.properties` file if testing on Android.

The Xcode editor if testing on iOS.


#### Notes:
Some NPM packages may be out of date and need to be updated for the Android build.

If you encounter the following error:

> A problem occurred evaluating project ':amazon-cognito-identity-js'.
> Could not find method implementation() for arguments [com.facebook.react:react-native:+]

Locate the `build.gradle` file in `node_modules/amazon-cognito-identity.js/android`, find the line mentioned in the `dependencies` object, and change `implementation` to `compile`.


If you receive a `hasteImpl returning the same name for different files` error in your packager: This is due to identical path names created by AWS Amplify. To fix this issue, simply drag the `#current-cloud-backend` folder from `${root}/amplify` out, restart the packager and run again.

### Contributing

We welcome contributions from all, and each contribution makes a difference! Please report issues directly to us or make pull requests for any features or bug fixes. Please refer to our [Contribution Guide](https://github.com/righton-dev/righton-app/tree/master/CONTRIBUTING.md) for further details.

### Additional Documentation

Please refer to our [docs](https://github.com/righton-dev/righton-app/tree/master/docs)

### Communication Channels

We are always happy to hear from people who are also interested in building learning apps and making math more fun and achievable for all. We're building this plane as we fly it, and suggestions are always welcome and appreciated. Give us a shout anytime at info@rightoneducation.com. You're also welcome to join our Slack community -- just let us know!

### Testing

You are welcome to download our latest beta version to test and give feedback. Please use Test Flight to install our app from this link: https://testflight.apple.com/join/2l8414MU

### Deployment

Coming soon!

### Acknowledgments

#### Meet our team
Andy Li - Mobile App Lead
Allison Liu - Education Research
Anna Roberds - Math Educator / Community Engagement
Daz Yang - Full-Stack Web Developer
Drew Hart - Dev Lead
Edward Tan - Architect Lead
Katerina Schenke - Education Research
Mani Ramezan - Mobile App Lead
Marizza Bailey - Math Educator / Content & Pedagogy
Mozzie Dosalmas - Math Educator / Equity & Community Engagement
Ryan Booth - Web App Lead
Sinclair Wu  - Product Lead
Yong Lin  - UX Lead
#### Advisors
Ay-Nur Najm - Independent math consultant and computer science teacher
Ben Woodford - Doctoral scholar at Stanford specializing in math education
Bunmi Esho - STEM Advocacy Executive Director
Esmeralda Ortiz - Senior Director, Boys & Girls Clubs of the Peninsula
Eric Boucher - Cofounder & CEO of Ovio
Iman Howard - STEM Educator
Lybroan James - Chief Education Officer, STEMulate
Paul Chin - Assistant Professor of Practice, Relay Graduate School of Education
Payton Richardson - Data Officer, Eastside Pathways
Ronald Towns - STEM Administrator


Learn more about RightOn! & our team at https://rightoneducation.com.
",,2024-05-03T05:06:05Z,20,21,5,"('drewjhart', 1504), ('azanli', 1035), ('AlexandraKushnirsky', 187), ('lucahendicott', 183), ('maniramezan', 177), ('josephinaim', 128), ('ankittrehan2000', 97), ('dependabotbot', 67), ('ryanabooth', 50), ('zach2025', 47), ('Alexandra-Kushnirsky', 45), ('RWu603', 26), ('guofoo', 22), ('JJackson914', 21), ('EricNguyen-B', 17), ('CCosmoe', 17), ('sincwu', 14), ('NeuveltP', 8), ('amanbhandal', 2), ('amarax', 2)","[4, 'Quality Education']"
openbudgets/participatory-budgeting,OpenBudgets Participatory Budgeting,"# OpenBudgets Participatory Budgeting

In a democracy, budgets are a key instrument of policy making. They are the result of an extensive and complex process,
traditionally executed by the political leadership. The decisions involving budgets have the potential to affect the
lives of citizens on the entire social spectrum by shifting focus to one priority over another, by introducing change
to the environment, or by impacting long-term nexuses. The idea behind the concept of participatory budgeting is to
provide the electorate with an opportunity to impact the decision-making process behind budgets.

An often neglected but extremely important aspect is accountability. What were the results of the last budgeting
process? How did these results get implemented? Leaving such questions unaswered may have a negative impact on the
number of citizens getting involved in the future. That is why Offering different levels of engagement such as voting
and suggesting an idea further helps citizens to gather an understanding of the nature of the process allows them to
decide how strongly they want to commit to it in the future.

OpenBudgets Participatory Budgeting aims to be a tool where citizens can express their budget allocation priorities
during the budget approval process and along the lines and within the process defined by each administration concerned,
but also a tool where citizens can monitor budget transactions, auditing budget compromised and giving feedback to
the administrations.

## Sample deployments

There are currently two different sample deployments: [openbudgets-voting-sample](http://openbudgets-voting-sample.herokuapp.com/)
and [openbudgets-monitoring-sample](http://openbudgets-monitoring-sample.herokuapp.com/).

Both have roughly the same sample data, but in the first case the participatory process is in the voting phase, where
citizens can browse the existing proposals and vote for them, and in the second case the participatory process is already
closed for voting and has transitioned into the monitoring phase.

![Voting screen shot](https://cloud.githubusercontent.com/assets/577074/19088006/8c982e66-8a74-11e6-9814-bc1ee861f011.png)

## Local installation

OpenBudgets Participatory Budgeting is a Rails 5.0 application.

### Development environment configuration

Prerequisites: Git, Ruby 2.5.1, Bundler gem, Node 8+ and PostgreSQL 9.4+

To prepare the environment:
```
$ bundle install
$ bin/rails db:create
$ bin/rails db:migrate
$ bin/rails db:seed
$ bin/rails db:test:prepare
```

To create d3.js custom bundle ([using npm & Rollup](https://bl.ocks.org/mbostock/bb09af4c39c79cffcde4)):
```
$ npm install
```

Run the tests with:
```
$ bin/rails t
```

There's also an included Guardfile to automate the test execution whenever file or directories
are modified.

Run Guard with:
```
$ bin/guard
```

Run the app with:
```
$ bin/rails s
```

To inspect the contents of the emails sent by the app in development environment, use the following URL:
```
http://localhost:3000/rails/mailers/
```
",,2018-09-19T20:01:40Z,2,12,17,"('esebastian', 41), ('dcabo', 1)","[16, 'Peace, Justice and Strong Institutions']"
osm2pgsql-dev/osm2pgsql,OpenStreetMap data to PostgreSQL converter,"# osm2pgsql

https://osm2pgsql.org

osm2pgsql is a tool for loading OpenStreetMap data into a PostgreSQL / PostGIS
database suitable for applications like rendering into a map, geocoding with
Nominatim, or general analysis.

See the [documentation](https://osm2pgsql.org/doc/) for instructions on how
to install and run osm2pgsql.

[![Github Actions Build Status](https://github.com/openstreetmap/osm2pgsql/workflows/CI/badge.svg?branch=master)](https://github.com/openstreetmap/osm2pgsql/actions)
[![Packaging Status](https://repology.org/badge/tiny-repos/osm2pgsql.svg)](https://repology.org/project/osm2pgsql/versions)

## Features

* Converts OSM files to a PostgreSQL DB
* Conversion of tags to columns is configurable in the style file
* Able to read .gz, .bz2, .pbf and .o5m files directly
* Can apply diffs to keep the database up to date
* Support the choice of output projection
* Configurable table names
* Support for hstore field type to store the complete set of tags in one database
  field if desired

## Installing

Most Linux distributions include osm2pgsql. It is available on macOS with
[Homebrew](https://brew.sh/) and Windows builds are also available. See
https://osm2pgsql.org/doc/install.html for details.

## Building

The latest source code is available in the osm2pgsql git repository on GitHub
and can be downloaded as follows:

```sh
git clone https://github.com/openstreetmap/osm2pgsql.git
```

Osm2pgsql uses the cross-platform [CMake build system](https://cmake.org/)
to configure and build itself.

Required libraries are

* [CLI11](https://github.com/CLIUtils/CLI11)
* [expat](https://libexpat.github.io/)
* [proj](https://proj.org/)
* [bzip2](http://www.bzip.org/)
* [zlib](https://www.zlib.net/)
* [Boost libraries](https://www.boost.org/) (for boost geometry)
* [nlohmann/json](https://json.nlohmann.me/)
* [OpenCV](https://opencv.org/) (Optional, for generalization only)
* [potrace](https://potrace.sourceforge.net/) (Optional, for generalization only)
* [PostgreSQL](https://www.postgresql.org/) client libraries
* [Lua](https://www.lua.org/) (Optional, used for Lua tag transforms
  and the flex output)
* [Python](https://python.org/) (only for running tests)
* [Psycopg](https://www.psycopg.org/) (only for running tests)

The following libraries are included in the `contrib` directory. You can build
with other versions of those libraries (set the `EXTERNAL_*libname*` option to
`ON`) but make sure you are using a compatible version:

* [fmt](https://fmt.dev/) (>= 7.1.3)
* [libosmium](https://osmcode.org/libosmium/) (>= 2.17.0)
* [protozero](https://github.com/mapbox/protozero) (>= 1.6.3)

It also requires access to a database server running
[PostgreSQL](https://www.postgresql.org/) 9.6+ and
[PostGIS](https://www.postgis.net/) 2.2+.

Make sure you have installed the development packages for the libraries
mentioned in the requirements section and a C++ compiler which supports C++17.
We officially support gcc >= 8.0 and clang >= 8.

To rebuild the included man page you'll need the [pandoc](https://pandoc.org/)
tool.

First install the dependencies.

On a Debian or Ubuntu system, this can be done with:

```sh
sudo apt-get install make cmake g++ libboost-dev \
  libexpat1-dev zlib1g-dev libpotrace-dev \
  libopencv-dev libbz2-dev libpq-dev libproj-dev lua5.3 liblua5.3-dev \
  pandoc nlohmann-json3-dev pyosmium
```

On a Fedora system, use

```sh
sudo dnf install cmake make gcc-c++ libtool boost-devel bzip2-devel \
  expat-devel fmt-devel json-devel libpq-devel lua-devel zlib-devel \
  potrace-devel opencv-devel python3-osmium \
  postgresql-devel proj-devel proj-epsg pandoc
```

On RedHat / CentOS first run `sudo yum install epel-release` then install
dependencies with:

```sh
sudo yum install cmake make gcc-c++ boost-devel expat-devel zlib-devel \
  potrace-devel opencv-devel json-devel python3-osmium \
  bzip2-devel postgresql-devel proj-devel proj-epsg lua-devel pandoc
```

On a FreeBSD system, use

```sh
pkg install devel/cmake devel/boost-libs textproc/expat2 \
  databases/postgresql94-client graphics/proj lang/lua52
```

On Alpine, use

```sh
apk --update-cache add cmake make g++ nlohmann-json \
  postgresql-dev boost-dev expat-dev bzip2-dev zlib-dev \
  libpq proj-dev lua5.3-dev luajit-dev
```

Once dependencies are installed, use CMake to build the Makefiles in a separate
folder:

```sh
mkdir build && cd build
cmake ..
```

If some installed dependencies are not found by CMake, more options may need
to be set. Typically, setting `CMAKE_PREFIX_PATH` to a list of appropriate
paths is sufficient.

When the Makefiles have been successfully built, compile with

```sh
make
```

The man page can be rebuilt with:

```sh
make man
```

The compiled files can be installed with

```sh
sudo make install
```

To install the experimental `osm2pgsql-gen` binary use

```sh
sudo make install-gen
```

By default, the Release build with debug info is created and no tests are
compiled. You can change that behavior by using additional options like
following:

```sh
cmake .. -G ""Unix Makefiles"" -DCMAKE_BUILD_TYPE=Debug -DBUILD_TESTS=ON
```

Note that `Debug` builds will be much slower than release build. For production
`Release` or `RelWithDebInfo` builds are recommended.

### Using the PROJ library

Osm2pgsql has builtin support for the Latlong (WGS84, EPSG:4326) and the
WebMercator (EPSG:3857) projection. Other projections are supported through
the [Proj library](https://proj.org/) which is used by default. Set the CMake
option `WITH_PROJ` to `OFF` to disable use of that library.

## Using LuaJIT

To speed up Lua tag transformations, [LuaJIT](https://luajit.org/) can be
optionally enabled on supported platforms. This can speed up processing
considerably.

On a Debian or Ubuntu system install the LuaJIT library:

```sh
sudo apt-get install libluajit-5.1-dev
```

Configuration parameter `WITH_LUAJIT=ON` needs to be added to enable LuaJIT.
Otherwise make and installation steps are identical to the description above.

```sh
cmake -D WITH_LUAJIT=ON ..
```

Use `osm2pgsql --version` to verify that the build includes LuaJIT support.
The output should show something like

```
Lua 5.1.4 (LuaJIT 2.1.0-beta3)
```

## Generalization

There is some experimental support for data generalization. See
https://osm2pgsql.org/generalization/ for details.

## Help/Support

If you have problems with osm2pgsql or want to report a bug, go to
https://osm2pgsql.org/support/ .

## License

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 2
of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

## Contributing

We welcome contributions to osm2pgsql. See [CONTRIBUTING.md](CONTRIBUTING.md)
and https://osm2pgsql.org/contribute/ for information on how to contribute.

","'mapnik', 'nominatim', 'openstreetmap', 'osm2pgsql', 'postgis'",2024-04-12T11:54:58Z,77,1436,75,"('lonvia', 1477), ('joto', 1227), ('pnorman', 487), ('apmon', 164), ('jburgess777', 143), ('kevinkreiser', 115), ('zerebubuth', 110), ('twain47', 76), ('woodpeck', 53), ('mmd-osm', 30), ('Nakaner', 17), ('hollinger', 17), ('amandasaurus', 16), ('artemp', 11), ('jocelynj', 11), ('DerDakon', 9), ('StyXman', 8), ('gravitystorm', 8), ('daniel-j-h', 6), ('JakobMiksch', 6), ('RavuAlHemio', 6), ('tomhughes', 5), ('giggls', 5), ('ImreSamu', 5), ('sebastic', 4), ('rodo', 4), ('schuyler', 4), ('SteveC', 4), ('alex85k', 4), ('iboates', 3), ('hoxell', 3), ('mojodna', 3), ('rmarianski', 3), ('nickw1', 3), ('keithsharp', 3), ('hiddewie', 3), ('AMDmi3', 3), ('chenrui333', 2), ('tpitale', 2), ('sandykurniawan19', 2), ('MaZderMind', 2), ('ppKrauss', 2), ('nicolas17', 2), ('matthijsmelissen', 2), ('rachekalmir', 2), ('Komzpa', 2), ('landryb', 1), ('matkoniecz', 1), ('matsbov', 1), ('MaxSem', 1), ('feixm1', 1), ('systemed', 1), ('rustprooflabs', 1), ('remyleone', 1), ('simonpoole', 1), ('stevendlander', 1), ('timwaters', 1), ('WillieMaddox', 1), ('cquest', 1), ('davideberra', 1), ('dperpeet', 1), ('avar', 1), ('kanwei', 1), ('jimmyrocks', 1), ('jeffjanes', 1), ('onepremise', 1), ('Zverik', 1), ('Firefishy', 1), ('bagage', 1), ('molind', 1), ('claytongulick', 1), ('roques', 1), ('bradh', 1), ('balrog-kun', 1), ('andrewshadura', 1), ('ahutchings', 1), ('rurseekatze', 1)","[17, 'Partnerships for the Goals']"
patadejaguar/safeosms,SAFE Open Source Microfinace Suite,,"'accounting', 'banking', 'microfinance', 'mysql', 'php', 'risk'",2020-06-18T14:59:06Z,1,18,7,"('patadejaguar', 65)","[17, 'Partnerships for the Goals']"
primeroIMS/primero,"Primero is an application designed to help child protection workers and social workers in humanitarian and development contexts manage data on vulnerable children and survivors of violence.  Please carefully read our LICENSE. If you would like access to the CPIMS+ and GBVIMS+ configurations, please contact: childprotectioninnovation@gmail.com ","

Primero
========
[![Build Status](https://github.com/primeroIMS/primero/actions/workflows/app.yml/badge.svg?branch=main)](https://github.com/primeroIMS/primero/actions)


> [!WARNING]
> **Primero v2.10 adds support for PostgreSQL 15!**
> Support for PostgreSQL 14 is retained and remains the default when running using Ansible/Docker Compose. Please use this opportunity to upgrade! PostgreSQL 15 will be the default starting with Primero v2.11, and support for PostgreSQL 10 and 14 will be eventually dropped. See [here](doc/postgres_upgrade.md) for a recommended upgrade process.

## Development

A guide to getting started with Primero development is available [here](doc/getting_started_development.md).

## Notes

- It is known that a few npm packages will throw a `requires a peer of` warning. Examples: Mui-datatables is behind on updating dependecies. Jsdom requires canvas, but we are mocking canvas. Canvas also requires extra packages on alpine, which is the reason for mocking canvas.

## Contributing
- If contributing to the UI, make sure to read over the [UI/UX Development](doc/ui_ux.md) documents.
- If you are contributing via the DAO, make sure to read the relevant documents [here](doc/dao/README.md).

## Production

Primero is deployed in production using Docker. Detailed Docker instructions exist in the file [docker/README.md](docker/README.md)
",,2024-05-03T15:28:36Z,139,46,13,"('jtoliver-quoin', 4324), ('relledge', 3823), ('pnabutovsky', 3378), ('dhernandez-quoin', 2846), ('gpadgettquoin', 1912), ('aespinoza-quoin', 1855), ('matt-quoin', 686), ('cchavez', 557), ('farismosman', 356), ('jasper-lyons', 343), ('josh-quoin', 247), ('kishoreyekkanti', 216), ('kavithaRajagopalan', 147), ('stanx12', 144), ('chrisgeorge', 124), ('duelinmarkers', 124), ('rdsubhas', 103), ('jgutierr-quoin', 95), ('raymondyan', 90), ('JackMiszencin', 78), ('sanchariGr', 77), ('awensaunders', 75), ('jennifersmith', 74), ('gitdivyanshu', 65), ('mtaylortw', 65), ('vivihuang', 62), ('moredip', 54), ('jeyakar', 46), ('quoininc-huudatran', 45), ('bteeman-quoin', 44), ('tcochran', 42), ('frankmt', 38), ('sumitmah', 33), ('jangie', 33), ('jeandamore', 32), ('davcamer', 31), ('zkhan', 29), ('shyamvala', 27), ('Maxim-Filimonov', 24), ('uzair-khan-10p', 23), ('NidhiKJha', 22), ('johncowie', 20), ('Pranjalita', 20), ('prasann', 20), ('kjayamit', 18), ('afrahtrigyn', 18), ('slak44', 16), ('keli89', 16), ('Srinivas9933', 15), ('cv', 14), ('ghulamjilani', 13), ('andrew', 12), ('rikeshdhokia', 12), ('dlbock', 11), ('hugoxiang', 11), ('NazneenRupawalla', 11), ('singhgarima', 11), ('aravindm', 11), ('noppanit', 10), ('jorgej', 9), ('schubert', 8), ('yuanshenjian', 8), ('alabeduarte', 8), ('minhngocd', 8), ('kramuenke', 7), ('ajit3190', 7), ('marano', 7), ('runemadsen', 6), ('cvortmann', 6), ('Bharanikumar-Gandla', 6), ('bguthrie', 6), ('pjmurray', 6), ('computellect', 6), ('tailrecur', 6), ('JamesMura', 5), ('jvortmann', 5), ('ketan', 5), ('lfendy', 5), ('rcganesh', 5), ('pturley', 5), ('rsliter', 5), ('ashokkumar', 5), ('yutyang', 4), ('migore', 4), ('josernestodavila', 4), ('jtoliver', 4), ('nandokakimoto', 4), ('dependabotbot', 3), ('paramadeep', 3), ('argha-c', 3), ('itirkarp', 3), ('mneedham', 3), ('LouisSayers', 3), ('CamfedCode', 3), ('gsluthra', 3), ('grillp', 3), ('agallardo', 2), ('vishnun', 2), ('smontanari', 2), ('rgan', 2), ('acellam', 2), ('gsemwezi', 2), ('deeolutayo', 2), ('eraser88', 2), ('sriprasanna', 2), ('SowmyaSMR', 2), ('keitwb', 2), ('cteicher-m', 2), ('ctford', 2), ('harishak', 2), ('maiklam', 2), ('marcinkwiatkowski', 2), ('matstc', 2), ('PallaviVadlamani', 2), ('RobertMacTavish', 2), ('shaan7', 2), ('sheroy', 2), ('Pavithrak', 1), ('rsuniev', 1), ('soundar', 1), ('sragu', 1), ('iamsuganthi', 1), ('TosinAji', 1), ('deepthirera', 1), ('pridar2k7', 1), ('tinygrasshopper', 1), ('vikytech', 1), ('wgarcia1221', 1), ('saravananselvamani', 1), ('ionutdobre', 1), ('IanLawrence', 1), ('farooqali', 1), ('dchapman1988', 1), ('lemoncurd', 1), ('arvindsv', 1), ('anagri', 1), ('amaladevi', 1), ('AlexTheProg', 1), ('ahazan98', 1)","[16, 'Peace, Justice and Strong Institutions']"
ifmeorg/ifme,"Free, open source mental health communication web app to share experiences with loved ones","[![CircleCI](https://circleci.com/gh/ifmeorg/ifme/tree/main.svg?style=svg)](https://circleci.com/gh/ifmeorg/ifme/tree/main)
[![Code Climate](https://codeclimate.com/github/ifmeorg/ifme/badges/gpa.svg)](https://codeclimate.com/github/ifmeorg/ifme)
[![Test Coverage](https://api.codeclimate.com/v1/badges/f9444a4d4116720518fe/test_coverage)](https://codeclimate.com/github/ifmeorg/ifme/test_coverage)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](code_of_conduct.md)
[![Backers on Open Collective](https://opencollective.com/ifme/backers/badge.svg)](#backers)
[![Sponsors on Open Collective](https://opencollective.com/ifme/sponsors/badge.svg)](#sponsors)
[![Awesome Humane Tech](https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true)](https://github.com/humanetech-community/awesome-humane-tech)

README in: [Italian](https://github.com/ifmeorg/ifme/blob/main/README-IT.md), [Spanish](https://github.com/ifmeorg/ifme/blob/main/README-ES.md), [Portuguese](https://github.com/ifmeorg/ifme/blob/main/README-PT.md), [French](https://github.com/ifmeorg/ifme/blob/main/README-FR.md), [Korean](https://github.com/ifmeorg/ifme/blob/main/README-KO.md), [Indonesian](https://github.com/ifmeorg/ifme/blob/main/README-ID.md), [Turkish](https://github.com/ifmeorg/ifme/blob/main/README-TR.md), [Sinhala](https://github.com/ifmeorg/ifme/blob/main/README-LK.md), [Chinese](https://github.com/ifmeorg/ifme/blob/main/README-CN.md),
[Hindi](https://github.com/ifmeorg/ifme/blob/main/README-HI.md), [Arabic](https://github.com/ifmeorg/ifme/blob/main/README-AR.md), [Vietnamese](https://github.com/ifmeorg/ifme/blob/main/README-VI.md)

# if-me.org

[if-me.org](https://www.if-me.org/) is a community for mental health experiences
that encourages people to share their personal stories with trusted allies.
Trusted allies are the people we interact with on a daily basis, including
friends, family members, co-workers, teachers, and mental health workers.

Dealing with mental health is what makes us human. But for a lot of us, it's a
struggle to be open about it. Not everyone is a counsellor or therapist. The
people who we interact with everyday shape our emotions and behavior. Getting
them involved in mental health treatment is the key to recovery.

The live site can be found at [if-me.org](https://www.if-me.org/). The live design systems can be found at [design.if-me.org](http://design.if-me.org/).

We use the wonderful [Contributor Covenant](http://contributor-covenant.org) for
our code of conduct. Please
[read it](https://github.com/ifmeorg/ifme/blob/main/code_of_conduct.md)
before joining our project.

**Read about our project goals and how to contribute (not just as a developer) [here](https://github.com/ifmeorg/ifme/blob/main/CONTRIBUTING.md).**

## Documentation

Please check out our [Wiki](https://github.com/ifmeorg/ifme/wiki) for full documentation.

### Installation

[These are the instructions](https://github.com/ifmeorg/ifme/wiki/Installation) for setting up and installing the app. Test, development, and production instances are covered.

### Contributor Blurb

Everyone is highly encouraged to [add themselves](https://github.com/ifmeorg/ifme/wiki/Contributor-Blurb) to our Contribute page.

## Donate

We also welcome financial contributions in full transparency on our
[Open Collective](https://opencollective.com/ifme).
Anyone can file an expense. If the expense makes sense for the development of
the community, it will be ""merged"" in the ledger of our Open Collective by the
core contributors and the person who filed the expense will be reimbursed.

### Backers

Thank you to our Patreon backers [Rob Drimmie](https://www.patreon.com/user?u=3251857),
[Joseph D. Marhee](https://www.patreon.com/user?u=2899171), and
[Carol Willing](https://www.patreon.com/user?u=202458)!

Thank you to all our Open Collective backers!
[Become a backer!](https://opencollective.com/ifme#backer)



### Sponsors

Thank you to all our sponsors! (please ask your company to also support this
open source project by [becoming a sponsor](https://opencollective.com/ifme#sponsor))


  
  
  
  
  
  
  
  
  
  


## License

The source code is licensed under GNU AGPLv3. For more information see
http://www.gnu.org/licenses/agpl-3.0.txt or
[LICENSE.txt](https://github.com/ifmeorg/ifme/blob/main/LICENSE.txt).
","'capybara', 'community-driven', 'flow', 'free-software', 'hacktoberfest', 'html', 'i18n', 'javascript', 'jest', 'mental-health', 'rails', 'react', 'react-testing-library', 'rspec', 'ruby', 'ruby-on-rails', 'scss', 'social-impact', 'storybook', 'webpack'",2024-04-19T22:29:47Z,310,1433,88,"('julianguyen', 1456), ('dependabot-support', 126), ('dependabotbot', 113), ('HashNotAdam', 103), ('GayathriVenkatesh', 71), ('andy5995', 58), ('kkelleey', 57), ('sienatime', 55), ('brunoocasali', 53), ('abrophy', 47), ('nchambe2', 43), ('baohouse', 42), ('nma', 35), ('nshki', 34), ('BonMatts', 30), ('alexiamcdonald', 26), ('haleymnatz', 25), ('SonuToor', 25), ('bennpham', 23), ('emitche', 22), ('wdhorton', 22), ('SophieRoseMcDonald', 22), ('bmwachajr', 21), ('stellaconyer', 19), ('Manasa2850', 19), ('nagmak', 19), ('sbpipb', 18), ('dependabot-previewbot', 17), ('zachariast', 17), ('alekpentchev', 17), ('gurpreetg', 16), ('leeacto', 16), ('discombobulateme', 14), ('bellawoo', 13), ('janessatran', 13), ('rahmatullah5', 13), ('dhuang612', 13), ('Tera15', 13), ('0xGhada', 13), ('ScotianMan', 12), ('scw248', 12), ('jzshen', 12), ('fpcyan', 12), ('LMulvey', 12), ('Wowu', 11), ('rhiroyuki', 11), ('yasirazgar', 10), ('andrew-schutt', 10), ('neon98', 10), ('maestromusica', 9), ('gerssonmg', 9), ('kindlingscript', 9), ('juliepagano', 9), ('asquare14', 9), ('prateksha', 9), ('camillevilla', 8), ('jsrn', 8), ('sylviapereira', 7), ('tyler-wel', 7), ('eiguike', 7), ('briceyokoyama', 6), ('eric199wen', 6), ('jnamster', 6), ('tsara27', 6), ('tcdowney', 6), ('sylvain-ch21', 6), ('chrisrp', 6), ('Dawa12', 6), ('rohandaxini', 6), ('pdpl89', 6), ('liirene1', 6), ('greysteil', 6), ('js-sapphire', 5), ('FranceJ', 5), ('romaincv12', 5), ('Seunadex', 5), ('lksevans12', 5), ('gdemu13', 5), ('franpb14', 5), ('tmdgnsle', 5), ('udayanshevade', 5), ('andrewhchang', 5), ('camilacampos', 5), ('mikachan', 5), ('nicovak', 5), ('IuliiaKot', 5), ('coryjthompson', 5), ('toryalsip', 4), ('Rxthew', 4), ('vananhle159', 4), ('newpro', 4), ('uzorjchibuzor', 4), ('DamonClark', 4), ('pederjohnsen', 4), ('thesid01', 4), ('Ashwithabg', 4), ('Mera-Gangapersaud', 4), ('shelleyvadams', 4), ('hmasila', 4), ('guilhermekbral', 4), ('siyegen', 4), ('Caro-Lin', 4), ('Bugs-b-gone', 4), ('meanmachin3', 4), ('gpks', 4), ('minhtran5109', 4), ('AudreyKj', 4), ('andrewjprice', 4), ('rashmiagar', 4), ('thomasoboyle', 4), ('vildevev', 4), ('geppy', 4), ('rizkaluthfiani', 4), ('S-Warmenhoven', 3), ('yigitozkavci', 3), ('deejayres', 3), ('alvarocasadoc', 3), ('xdamman', 3), ('tryantwit', 3), ('TimMoore', 3), ('buitranquyet', 3), ('clairewild', 3), ('bianca93', 3), ('andrezacm', 3), ('dipil-saud', 3), ('anne27', 3), ('dleve123', 3), ('dominicprescod', 3), ('DRBragg', 3), ('gr1d99', 3), ('jaimevelaz', 3), ('jenniferlynparsons', 3), ('ludamillion', 3), ('maribelduran', 3), ('su6a12', 3), ('hassan11196', 3), ('GuillermoCoding', 2), ('cpretzer', 2), ('ziyue5', 2), ('HolyWalley', 2), ('toshitapandey', 2), ('tienyuan', 2), ('Gibbo3771', 2), ('SOURABH358', 2), ('sieraford-zz', 2), ('seanmfox', 2), ('Revathyne', 2), ('hnarasaki', 2), ('jakaya123', 2), ('jeffbirkholz', 2), ('jollyjerr', 2), ('RomanMaru', 2), ('kolisnyklera', 2), ('kshashwat007', 2), ('paulonbc', 2), ('placy2', 2), ('arku', 2), ('jvargas98', 2), ('delbetu', 2), ('sabrinagannon', 2), ('TedVu', 2), ('zeeshansarwar38', 2), ('Arunthogadiya', 2), ('beemtz', 2), ('afeld', 2), ('adang48', 2), ('alexmanzo', 2), ('nad0m', 2), ('salzig', 2), ('bcarberry', 2), ('calebhearth', 2), ('CSumm', 2), ('CCoupris', 2), ('DanielSauve', 2), ('StaphSynth', 2), ('ehashman', 2), ('estermer', 2), ('MuhammadAakash', 2), ('nikhilbhatt', 2), ('Orelongz', 2), ('mjalkio', 2), ('martijndeb', 2), ('renjithabby', 2), ('MarijkeM', 2), ('marcelakopko', 2), ('logeshmallow', 2), ('lisavogtsf', 2), ('lpatmo', 2), ('jmsardina', 2), ('jaspervanbrian', 2), ('Iphytech', 2), ('GeoDoo', 2), ('sshastri', 1), ('staceyastewart', 1), ('stefannibrasil', 1), ('stvtortora', 1), ('faraazahmad', 1), ('tiffanysun', 1), ('timagixe', 1), ('timorthi', 1), ('tracypholmes', 1), ('wilburhimself', 1), ('Xinyiguo2', 1), ('YuridiaLarios', 1), ('zdoc01', 1), ('ericlee1996', 1), ('seannemann21', 1), ('seanmarcia', 1), ('SashaTlr', 1), ('saminarp', 1), ('yakryder', 1), ('noahfin', 1), ('damiong28', 1), ('PochecoPachico', 1), ('MollyJameson', 1), ('MicahRamirez', 1), ('mialahmoka', 1), ('mattbischoff', 1), ('Hissvard', 1), ('sea-witch', 1), ('thepante', 1), ('tatoberres', 1), ('sonalan', 1), ('slauppy', 1), ('sebassebas1313', 1), ('sananta', 1), ('developer22-university', 1), ('oubawaleh', 1), ('oliverTwist2', 1), ('nayangupta824', 1), ('Yoon-Dev', 1), ('marcelkalveram', 1), ('lilivelazquezz', 1), ('kb0rg', 1), ('KaraAJC', 1), ('haroruhomer', 1), ('evykassirer', 1), ('eshnine', 1), ('ellenmacpherson', 1), ('gitburd', 1), ('danielglatstein', 1), ('daaimah123', 1), ('cjhaddad', 1), ('chenghw', 1), ('cdeborja', 1), ('bshap93', 1), ('architapatelis', 1), ('zidanehammouda', 1), ('zawmoelwin', 1), ('SimplyComplexable', 1), ('gabrielpedepera', 1), ('felipemrvieira', 1), ('favy-codez', 1), ('thumbsupep', 1), ('emilyyz92', 1), ('parkedwards', 1), ('dhurimkelmendi', 1), ('dhavalpur0hit', 1), ('dbarner1', 1), ('dburgoyne', 1), ('thechinedu', 1), ('paschalidi', 1), ('Chi-square-test', 1), ('cpanato', 1), ('Buzunda', 1), ('brenocastelo', 1), ('aturkewi', 1), ('aelydens', 1), ('annezhou920', 1), ('Andykmcc', 1), ('andyfry01', 1), ('adorgan', 1), ('andrearosr', 1), ('AlineRibeiro', 1), ('alextleach', 1), ('waxidiotic', 1), ('albertopasqualetto', 1), ('adamjpena', 1), ('256hz', 1), ('akp2603', 1), ('manishElitmus', 1), ('maniSHarma7575', 1), ('lvcg', 1), ('lucyyu24', 1), ('lindseylonne', 1), ('leereilly', 1), ('Veith3n', 1), ('Le6ow5k1', 1), ('Kell-Stack', 1), ('keitheous', 1), ('KatieFujihara', 1), ('karolina-benitez', 1), ('gutjuri', 1), ('JEscalanteGT', 1), ('jfriestedt', 1), ('jolim24601', 1), ('jiradeto', 1), ('nguyenjenny', 1), ('jischr', 1), ('jellene4eva', 1), ('jeffreyroy', 1), ('leejeannes', 1), ('claridiva2000', 1), ('Jacek-202', 1), ('venit1123', 1), ('isabelleyiu', 1), ('isa3bel', 1), ('IngridGdesigns', 1), ('giddke', 1), ('gpetrioli', 1)","[3, 'Good Health and Well-Being']"
sozialhelden/wheelmap-frontend,Source code of the wheelmap.org React.js frontend application,"# [](https://wheelmap.org)

[Wheelmap.org](https://www.wheelmap.org) by [Sozialhelden e.V.](https://sozialhelden.de) is the world’s largest free online map for accessible places.

[](https://sozialhelden.de)

This project contains the Node.js/React.js-based frontend for the app.

## Development

The app itself is a [React.js](https://facebook.github.io/react/) application, and [wrapped into native apps](https://github.com/sozialhelden/wheelmap-native-wrapper) for Android and iOS. [Next.js](https://nextjs.org) provides a server and webpack compilation process. For CSS styling, we use [styled-components](https://www.styled-components.com).

The web server serves the app as server-side rendered static page, and runs on the same domain as the backend API in production.

### Setup

Prepare your development environment:

```bash
# Environment variables
cp .env.example .env

# npm dependencies
npm install

# install transifex i18n / localization tool
pip install transifex-client

# start a local test web server
npm run dev
```

You will get some error messages from the Elastic APM client (our error collector for both [server](https://www.elastic.co/products/apm) and [client](https://www.elastic.co/guide/en/apm/agent/rum-js/4.x/getting-started.html)). If you want to test/develop the Elastic APM integration, you can get a valid token from the project maintainers.

You can configure the app using process environment variables or a `.env` file. Process environment variables override values set in the `.env` file. Note that the build configuration [deviates from Webpack’s default behavior](https://github.com/sozialhelden/twelve-factor-dotenv) to improve build and bug reproducibility.

### Recompile SVG

You have to convert SVG graphics to React JS components to make them styleable with CSS. Run this command:

```bash
npm run compile-svgs
```

Check `package.json`, it defines more scripts that can speed up this task.

## Website deployment

To deploy the web application:

- ask the backend maintainers to include your public SSH key in the authorized deployment keys
- Add hostnames for the staging/production systems to your `/etc/hosts`
- Log in once on the staging/production server
- Deploy the application with `npm run deploy-staging` or `npm run deploy-production`.

## Embedding Wheelmap as a widget on other websites

You can embed the Wheelmap web app in any other website’s HTML like this:

``

Having the `embedded=true` query parameter attached to the URL ensures that the app renders less UI (no search, no header etc.). Add `allow=""geolocation""` to let the `` access the browser’s location feature.
Like the full app, the widget can initially position the map at a certain location defined by `lat` and `lon` URL parameters.
The app disallows embedding via `` if you provide no valid `embedToken` URL parameter. You can create an embed token on accessibility.cloud’s app admin interface (in the widget options).

## Translation process

Ensure the transifex client is setup 

Use `npm run push-translations` to push a new translation resource to our translation service [transifex](http://transifex.com).

We deploy every new feature in English and German first, and add support for all languages in the following sprint.

After pushing new strings, translators can begin translating them on [Transifex](https://www.transifex.com/sozialhelden/wheelmap-react-frontend/translate/#de/translationspot/335454735?q=translated%3Ano).

When there are new strings on transifex, you can run `npm run pull-translations` to pull them into the local project and to inject the translations into the application.

We have a retranslate tool that allows to use the `en_US` language on transifex to refine source strings directly in the source code. This parses the whole source code into an abstract syntax tree using Babel, then re-assembles it with new versions of the strings fetched from the `en_US` locale. Re-assembly can break formatting.

## Testing



For testing the apps, we use [BrowserStack](https://browserstack.com) - it can run test suites on various browsers and live devices. Currently, our testing happens mostly manually on [BrowserStack Live](https://www.browserstack.com/live), but pull requests will soon get automatic CI checks using [BrowserStack Automate](https://www.browserstack.com/automate) and [BrowserStack App Automate](https://www.browserstack.com/app-automate). We thank the BrowserStack team for their great products and their support! ❤️

### Testing locally

To test locally, get BrowserStack login data, put them into environment variables and run the tests like this on the shell:

```bash
export BROWSERSTACK_USERNAME=...
export BROWSERSTACK_ACCESS_KEY=...
export CI_TEST_DEPLOYMENT_BASE_URL=http://localhost
npm run test
```

### Testing via CI

New code is automatically tested on pushing it to the git repository. GitHub displays the test status next to commits.

## Contributing data and code

- You have a related project? You want your accessibility data visible on Wheelmap.org and in other apps, or your project would profit from Wheelmap.org’s data? Register an account on [accessibility.cloud](https://www.accessibility.cloud) (Wheelmap.org’s backend) and [contact us](mailto:support@accessibility.cloud)!
- For reporting bugs or other issues, please create issues on our [GitHub issue tracker](https://github.com/sozialhelden/wheelmap-react-frontend/issues).
- If you have a concrete bugfix, you can create a pull request - please create an issue first so we can organize collaboration together.

## Code of Conduct

We follow the [Berlin Code of Conduct](https://berlincodeofconduct.org).
",,2024-05-02T10:10:45Z,15,46,6,"('opyh', 1878), ('mutaphysis', 627), ('lennerd', 166), ('bjoernuhlig', 27), ('snyk-bot', 26), ('dependabotbot', 24), ('nero-still', 15), ('Akii', 7), ('pixtur', 6), ('janDo-dev', 2), ('anasalhakim', 1), ('HansVonEisen', 1), ('k00ni', 1), ('danieldegroot2', 1), ('Lulezimukaj', 1)","[10, 'Reduced Inequalities']"
5calls/5calls,Frontend for the 5calls.org site,"# 5calls

## Quicknav
### Automated testing
[![CircleCI](https://circleci.com/gh/5calls/5calls.svg?style=svg)](https://circleci.com/gh/5calls/5calls)
### Cross-browser testing
[![BrowserStack Status](https://www.browserstack.com/automate/badge.svg?badge_key=UjRNc0oxeXBESnloeEtyaUNZNEhYaHVVd25tS0x2VHZvUlVuK0gwaFNYTT0tLTlNcFhySGROOThHZW1YSDYxZ2Zid3c9PQ==--505e91904e448bf498ea1905b0e8d17b5fedc7bf)](https://www.browserstack.com/automate/public-build/UjRNc0oxeXBESnloeEtyaUNZNEhYaHVVd25tS0x2VHZvUlVuK0gwaFNYTT0tLTlNcFhySGROOThHZW1YSDYxZ2Zid3c9PQ==--505e91904e448bf498ea1905b0e8d17b5fedc7bf)


# Table of Contents
* [Development](#Development)
    * [Front End](#Front_End)
    * [Application Server](#Application_Server)
    * [Quality Assurance](#QA)
        * [JavaScript Unit Tests](#JavaScript_Unit_Tests)
        * [End-to-end Integration Tests](#End-to-end_Integration_Tests)
        * [JavaScript Linting](#ESLint)
* [Contributor Guidelines](#Contributor_Guidelines)
* [Contributors](#Contributors)
* [Other Client Projects](#Other_Client_Projects)

## Development Notes

The frontend is written in [React](https://facebook.github.io/react/) with [Redux](http://redux.js.org/) for state management and [Typescript](https://www.typescriptlang.org/) for type safety and documentation. The application server back end -- for data processing -- is written in [Go](https://golang.org/).

To build the application, you need to install [Yarn](https://yarnpkg.com/) and run the following commands:
```
# install dependencies and
#   compile .scss files to .css:
yarn

# Run unit tests in watch mode
yarn test

# Run unit tests with a code coverage report
yarn test:coverage

# start the app running in the
#   webpack development server:
yarn start

# start the app running in https mode
#   (needed for browser geolocation):
yarn start:https

# build the app into build folder
#  for server deployment:
yarn build

# any updates to .scss files need
#  to be compiled to css using:
yarn clean-build-css
```

Using `yarn add` to add new dependencies
will throw an error related to node-sass-chokidar, which can be ignored.

For the best development experience, you should install both the React and Redux Development Tools extensions into your browser. Both browser extensions are available for Chrome and Firefox.

### Unit testing
Unit testing in this repository is done using [Jest](https://facebook.github.io/jest/) with [Enzyme](https://github.com/airbnb/enzyme) in addition to the [redux-mock-store](https://github.com/arnaudbenard/redux-mock-store) library to support Redux-related tests.


### End-to-end Integration Tests
E2E testing for this project is done using [TestCafe](https://testcafe.devexpress.com/) with [testcafe-react-selectors](https://github.com/DevExpress/testcafe-react-selectors). This provides a web scraping test bed that provides an assertion library to ensure that the expected elements appear on each page under test. 

#### Running the tests
Before running the tests, the development server must be running using the command
`yarn start`
Once the test server is running, you may either run all the tests on your local browser using one of the following commands
```
testcafe  web-tests/*.ts
yarn web-tests:all
```
Or if you have access to browserstack, you may run on multiple browsers using the command

`yarn web-tests:browserstack`

#### Running on browserstack
To run on browserstack, ensure that you have environment variables set for BROWSERSTACK_ACCESS_KEY and BROWSERSTACK_USERNAME to the values of your browserstack account
You may view test results on the [dashboard](https://automate.browserstack.com/builds)

#### Debugging and running individual tests
You may run specific tests by running testcafe with the -t or -T option
-t 
-T 

Tests may be run in debug mode with the --inspect-brk option. 
```
testcafe --inspect-brk chrome web-tests/test.ts 
```
Breakpoints may be put in the test with the following syntax
```
// tslint:disable-next-line:no-debugger
debugger;
```
Then in chrome, navigate to `chrome://inspect`
On this page, under Remote Target there will be an inspect link, click to start the debugger.

## Architecture, Data Flow and Strong Typing
A selection of files in this repository include code comments describing the architecture, data flow and strong typing conventions used in developing the React-Redux-TypeScript version of the 5 Calls application. These include files that illustrate the following (see the individual files for more details):

### Use of TypeScript to Strongly Type Request Parameters Passed by React-Router
Also illustrates the use of Redux to loosely couple a component to data passed to its props.
[CallPageContainer.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/CallPageContainer.tsx)
[CallPage.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/CallPage.tsx)

### Data Flow through a Component Heirarcy
Also note the TypeScript conventions used in these files.
[CallPageContainer.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/CallPageContainer.tsx)
[CallPage.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/CallPage.tsx)
[Why5calls.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/Call.tsx)

### Redux Data Flow

See code comments containing the token 'REDUX DATA FLOW'. Also note the use of TypeScript in these files.
[CallPageContainer.tsx](https://github.com/5calls/react-dev/blob/master/src/components/call/CallPageContainer.tsx)
[redux/callState/action.ts](https://github.com/5calls/react-dev/blob/master/src/redux/callState/action.ts)
[redux/callState/actionCreator.ts](https://github.com/5calls/react-dev/blob/master/src/redux/callState/actionCreator.ts)
[redux/callState/reducer.ts](https://github.com/5calls/react-dev/blob/master/src/redux/callState/reducer.ts)

## Contributor Guidelines

Contributions to this repository are welcome. Please see the [Contributing.md](https://github.com/5calls/5calls/blob/master/CONTRIBUTING.md) file in the 5calls/5calls repository for information on contributing to this repository.


## Contributors
 - [Nick O'Neill](https://github.com/nickoneill)
 - [Matt Jacobs](https://github.com/capndesign)
 - [Liam Campbell](https://github.com/liamdanger)
 - [James Home](https://github.com/jameshome)
 - [Beau Smith](https://github.com/beausmith)
 - [Anthony Johnson](https://github.com/agjohnson)
 - [Craig Doremus](https://github.com/cdoremus)
 - [Owen Derby](https://github.com/oderby)
 - [All contributors](https://github.com/5calls/5calls/graphs/contributors)

 
## Other Client Projects
 - [Android](https://github.com/5calls/android)
 - [iOS](https://github.com/5calls/ios)

## Create React App Code Generation

This project was created with [create-react-app](https://github.com/facebookincubator/create-react-app) (CRA, react-scripts ver 1.0.0) using [react-scripts-ts](https://github.com/wmonk/create-react-app-typescript) (ver 2.2.0) to add TypeScript support. In addition, the `node-sass-chokidar` library was added for preprocessing of SASS (.scss files) to CSS.

Subsequently, the CRA-created configurations were exposed using the eject command (`yarn eject`). This created the `config` and `scripts` folders and added dependencies and other configurations to `package.json`.

[CRA_README.md](CRA_README.md) is the original README.md file created when the create-react-app command was run.
","'5calls', 'civic', 'civic-tech', 'javascript', 'politics', 'react'",2022-12-15T23:33:18Z,34,373,30,"('nickoneill', 537), ('cdoremus', 131), ('greggraf', 46), ('capndesign', 22), ('schraj', 19), ('Mr0grog', 18), ('oderby', 16), ('beausmith', 14), ('jameshome', 13), ('josharian', 13), ('aholachek', 12), ('dependabotbot', 7), ('benjaffe', 6), ('agjohnson', 5), ('jadametz', 5), ('nlsandler', 4), ('ZachGawlik', 4), ('bernardwang', 3), ('ColemanCollins', 2), ('murribu', 2), ('nabrahamson', 2), ('github-actionsbot', 2), ('marshmallowrobot', 1), ('itbeauregard', 1), ('buddhamagnet', 1), ('rboyce', 1), ('blip-lorist', 1), ('dektar', 1), ('JohnTurnerPGH', 1), ('jasonm', 1), ('ejlangev', 1), ('thebrengun', 1), ('bradfitz', 1), ('benjamincarp', 1)","[16, 'Peace, Justice and Strong Institutions']"
NREL/OpenStudio-server,The OpenStudio Server is a docker or Helm deployable instance which allows for large-scale parametric analyses of building energy models using the OpenStudio or URBANopt CLIs. ,"# OpenStudio(R) Server

[![Build Status][gh-img]][gh-url] 
[![Coverage Status][coveralls-img]][coveralls-url]

Please refer to the [wiki](https://github.com/NREL/OpenStudio-server/wiki) for additional documentation.




## About

OpenStudio Server is a web application and distributed computing tool, which is the backbone of the OpenStudio Analysis Framework (OSAF).
It is intended to make parametric analysis of building energy models accessible to architects, engineers, and designers via the [OpenStudio PAT](http://nrel.github.io/OpenStudio-user-documentation/reference/parametric_studies/) GUI or the [OpenStudio Analysis Gem](https://github.com/NREL/OpenStudio-analysis-gem). 
OpenStudio Server analyses are defined by PAT projects or OSA's.  Each analysis may include many OpenStudio simulations, as determined by project configuration.

Journal of Building Performance Simulation article: [An open source analysis framework for large-scale building energy modeling](https://www.tandfonline.com/doi/full/10.1080/19401493.2020.1778788)

## Application Development and Deployment

There are primarily two ways to utilize and deploy this codebase.
 
* [openstudio-server-helm](https://github.com/NREL/openstudio-server-helm) This helm chart installs a OpenStudio-server instance deployment on a AWS, Azure, or Google Kubernetes cluster using the Helm package manager. You can interface with the OpenStudio-server cluster using the Parametric Analysis Tool or the [openstudio_meta](./bin/openstudio_meta) CLI.
   
* [Docker Swarm](https://docs.docker.com/engine/swarm/): This is the recommended local deployment pathway. Swarm is an 
orchestration engine which allows for multi-node clusters and provides significant benefits in the forms of 
customization and hardening of network and storage 
fundamentals.

### openstudio_meta

The [openstudio_meta](./bin/openstudio_meta) file is a ruby script which provides access to packaging and execution 
commands which allow for this codebase to be embedded in applications deployed to computers without docker. Deployment 
requires that [MongoDB 6.0.7](https://www.mongodb.com/download-center/community/releases/archive) and [Ruby v2.7](https://www.ruby-lang.org/en/downloads/) 
are additionally packaged. 

The openstudio_meta deployment relies on the `install_gems` command, which uses local system libraries to build all 
required gem dependencies of the server. Additionally, the export flag allows for the resulting package to be 
automatically assembled and zipped for deployment. It is important to note that when used on OSX and Linux systems, 
it is critical to not specify the export path with home (`~`) substitution. Instead, pass a fully specified path to the 
desired output directory. 

Once compiled or unpacked, the openstudio_meta file can be used for starting and stopping the local server for the [Parametric Analysis Tool (PAT)](https://github.com/NREL/OpenStudio-PAT) and 
submitting analyses to it. Assembling the required files for the analysis is done with the [Analysis-gem](https://github.com/NREL/OpenStudio-analysis-gem) or the export OSA function in PAT. For more details, please 
refer to the [wiki](https://github.com/NREL/OpenStudio-server/wiki/CLI).  For examples, please refer to [OSAF notebooks](https://github.com/NREL/docker-openstudio-jupyter/tree/master).

### Local Docker Development

To develop locally the following dependency stack is recommended. 

* Install Docker (Version 20.10.5 or greater is required)
    * OSX Users: [install Docker CE for Mac](https://docs.docker.com/docker-for-mac/install/). Please refer to [this guide](https://docs.docker.com/docker-for-mac/install/)
    * Windows 10 Users: [Docker Desktop](https://www.docker.com/products/docker-desktop/).
    * Linux Users: Follow the instructions in the [appropriate guide](https://www.docker.com/community-edition)
    
    *Note: Although generally newer versions of docker will behave as expected, certain CLI interactions change between
    releases, leading to scripts breaking and default behaviours, particularly regarding persistence, changing. The 
    docker version installed and running can be found by typing `docker info` on the command line.*
    
#### Docker Compose 

```bash
docker-compose build
```
... [be patient](https://www.youtube.com/watch?v=f4hkPn0Un_Q) ... If the containers build successfully start them by 
running `docker volume create --name=osdata && docker volume create --name=dbdata && OS_SERVER_NUMBER_OF_WORKERS=4 docker-compose up` 
where 4 is equal to the number of worker nodes you wish to run. For single node servers this should not be greater 
than the total number of available cores minus 4.

Resetting the containers can be accomplished by running:

```bash
docker-compose rm -f
docker volume rm osdata dbdata
docker volume create --name=osdata
docker volume create --name=dbdata
OS_SERVER_NUMBER_OF_WORKERS=N docker-compose up
docker-compose service scale worker=N

# Or one line
docker-compose rm -f && docker-compose build && docker volume rm osdata dbdata && docker volume create --name=osdata && docker volume create --name=dbdata && OS_SERVER_NUMBER_OF_WORKERS=N docker-compose up && docker-compose service scale worker=N
```

Congratulations! Visit `http://localhost:8080` to see the OpenStudio Server Management Console.

#### Running the Docker CI testing locally

```bash
export OPENSTUDIO_TAG=develop
export RAILS_ENV=docker-test

docker-compose rm -f
docker volume rm osdata
sed -i -E ""s/.git//g"" .dockerignore
docker volume create --name=osdata
docker-compose -f docker-compose.test.yml pull
docker-compose -f docker-compose.test.yml build --build-arg OPENSTUDIO_VERSION=$OPENSTUDIO_TAG
docker-compose -f docker-compose.test.yml up -d
docker-compose exec -T web /usr/local/bin/run-server-tests
docker-compose stop
git checkout -- .dockerignore && git checkout -- Dockerfile
docker-compose rm -f
```

### Local Docker Swarm Deployment

To deploy the OpenStudio Server in a docker-based production environment one or more machines need to be running Docker 
Server version 20.10.05. If using docker on a Linux machine it is recommended that significant storage be available to 
the `/var` folder. This is where Docker reads and writes all data to by default unless changed in the docker-compose.yml file. 
There are scripts to help with docker swarm deployment [here](https://github.com/NREL/OpenStudio-server/tree/develop/local_setup_scripts).
Make sure to change the defaults to be applicable to your hardware requirements.

## Testing procedure

The OpenStudio Server project uses several CI systems to test both local and cloud deployments across multiple 
platforms. GitHub Actions is used to build and test local deployments of the server on OSX hardware for each commit, as well 
as to build and test docker containers for each commit. It is important to note that during the middle of the 
day, these tests can take several hours to begin. Finally, AppVeyor is used to build and test local deployments against
Windows. 

In the case of local deployments (non-docker deployments) the build step uses the meta-cli's install_gems command to 
create a new set of cached ruby dependencies to test against. The test phase is made up of two separate testing 
methodologies. The first uses rspec to run a number of unit tests against a locally instantiated server. The 
second instantiates the server in the same manner as PAT, runs analyses against said server, and ensures that it stops 
as expected, using the meta-cli.

For cloud deployments, the two critical artifacts are the docker containers and AMIs. Currently AMI testing is not 
automated, and unlikely to be automated for several reason. The docker containers, however, are extensively tested using 
the same rspec functionality as mentioned above. 

For a pull request to be merged under regular order, all CI tests need to return green: GitHub Actions and AppVeyor 
PR and push. All of these tests write verbose results and logs on failure, which should allow for local reproduction 
of the bug and subsequent fixes. In the case of a failure of the CI infrastructure, please open an issue in the 
repository regarding the failure. 

## Commands to update gems used in PAT manually

To test the impact of upgraded gems on PAT's functionality the currently recommended path is to manually remove and 
recreate the cached set of gems, including compiled binary components. This process is platform specific. Currently 
instructions are only available for OSX, due to complications compiling the binary component of gems with the ruby 
instillation provided in the OpenStudio installer package.

### OSX

```bash
# Change directory to the install location of the Server
cd /Applications/OpenStudio-X.Y.Z/ParametricAnalysisTool.app/Contents/Resources/OpenStudio-server 
rm -rf /gems # Remove the pre-packaged gems
vi server/Gemfile # Edit the Gemfile
rm server/Gemfile.lock # Remove the cached gem specifications
../ruby/bin/ruby ./bin/openstudio_meta install_gems # Reinstall the gems required (including new gems)
chmod -R 777 gems # Modify privileges on the installed gems
```

## Questions?

Please contact @tijcolem, @bball, or @nllong with any question regarding this project. Thanks for you interest!

[coveralls-img]: https://coveralls.io/repos/github/NREL/OpenStudio-server/badge.svg?branch=develop
[coveralls-url]: https://coveralls.io/github/NREL/OpenStudio-server
[gh-img]: https://github.com/nrel/openstudio-server/actions/workflows/openstudio-server-tests.yml/badge.svg?branch=develop
[gh-url]: https://github.com/nrel/openstudio-server/actions
[appveyor-img]: https://ci.appveyor.com/api/projects/status/j7hqgh2p7bae9xn8/branch/dockerize-appveyor?svg=true
[appveyor-url]: https://ci.appveyor.com/project/rHorsey/openstudio-server/branch/dockerize-appveyor

",,2024-05-01T22:30:47Z,16,44,29,"('nllong', 1967), ('brianlball', 1259), ('anyaelena', 594), ('rHorsey', 579), ('tijcolem', 330), ('kflemin', 131), ('macumber', 127), ('elainethale', 80), ('axelstudios', 62), ('evanweaver', 31), ('asparke2', 15), ('henryHorsey', 11), ('kbenne', 10), ('stephen-frank', 2), ('wenyikuang', 2), ('jmarrec', 1)","[7, 'Affordable and Clean Energy']"
avniproject/avni-client,Android app for the fieldworkers.,"# Build Status

[![CircleCI](https://circleci.com/gh/OpenCHS/openchs-client.svg?style=svg)](https://circleci.com/gh/OpenCHS/openchs-client)

# Join our discussions
Join the chat on [Skype](https://join.skype.com/xiTU162DSJTd)

# License
[![License](https://img.shields.io/badge/license-AGPL-green.svg?style=flat)](https://github.com/openchs/openchs-client/blob/master/LICENSE)

# [Developer Documentations](https://avni.readme.io/docs/developer-environment-setup-ubuntu)
","'android', 'javascript', 'react-native', 'realm-js'",2024-05-03T10:49:10Z,27,7,6,"('petmongrels', 975), ('mihirk', 851), ('vindeolal', 757), ('vinayvenu', 574), ('arjunk', 433), ('sidtharthanan', 424), ('himeshr', 389), ('hithacker', 313), ('1t5j0y', 226), ('mahalakshme', 149), ('charl3sj', 128), ('sachsk', 66), ('sidsamanvay', 56), ('pasviegas', 20), ('rsatishm', 13), ('swapnil106111', 10), ('nupoorkhandelwal', 10), ('garimadosar5', 5), ('abhi11verma', 2), ('amarkamthe', 2), ('dhananjayvalte', 2), ('Balamuruganjeevi', 1), ('mohan-13', 1), ('ricardomiron', 1), ('gitter-badger', 1), ('deeptirawat1510', 1), ('shruthidipali', 1)","[16, 'Peace, Justice and Strong Institutions']"
openhie/facility-recon,Facility Reconciliation Tool,"# Facility Reconciliation Tool

https://facility-recon.readthedocs.io/en/latest/

This tool enables matching of facility lists between different sources. It has a pluggable architecture to enable a myriad of data sources and matching algorithms.

## Features
* CSV, DHIS2, and FHIR servers as data sources.
* Both automatic and manual matching, including monitoring the status of existing matches.
* Supports nested lists, ie. facilities that are administrative hierarchies like state->county->hospital.
* An API and backend engine that use [FHIR](https://www.hl7.org/fhir/location.html)) Location resources based on the [mCSD](http://wiki.ihe.net/index.php/Mobile_Care_Services_Discovery_(mCSD)) profile.
* Modular system to extend algorithms for matching.

## Contributing and Community
- For announcements and discussions, join the [Facility Registry Google Group](https://groups.google.com/forum/#!forum/facility-registry).
- For open monthly discussions, join the [OpenHIE Facility Registry Community](https://wiki.ohie.org/display/SUB/Facility+Registry+Community).
- Search through or create an issue in the [GitHub Repository](https://github.com/openhie/facility-recon/issues).

## License
The Facility Reconciliation Tool is distributed under the Apache 2.0 license.
",,2023-01-04T04:01:46Z,3,9,7,"('ashaban', 312), ('citizenrich', 69), ('lukeaduncan', 47)","[3, 'Good Health and Well-Being']"
OpenTreeMap/otm-core,"OpenTreeMap is a collaborative platform for crowdsourced tree inventory, ecosystem services calculations, urban forestry analysis, and community engagement.","![OTM2 open source logo](https://opentreemap.github.io/images/logo@2x.png)

[![Code Health](https://landscape.io/github/OpenTreeMap/otm-core/master/landscape.png)](https://landscape.io/github/OpenTreeMap/otm-core/master)
[![Build Status](https://travis-ci.org/OpenTreeMap/otm-core.svg?branch=master)](https://travis-ci.org/OpenTreeMap/otm-core)
[![Coverage Status](https://coveralls.io/repos/OpenTreeMap/otm-core/badge.png)](https://coveralls.io/r/OpenTreeMap/otm-core)

# OpenTreeMap 2

## Questions?

Join the user mailing list and let us know: 
http://groups.google.com/group/opentreemap-user

Or, try our Gitter channel: [![Join the chat at https://gitter.im/OpenTreeMap/otm-core](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/OpenTreeMap/otm-core?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## Installation

For full installation instructions, see the [Github 
wiki](https://github.com/OpenTreeMap/otm-core/wiki/Installation-Guide).

Alternatively, you can also use the [otm-vagrant 
project](https://github.com/OpenTreeMap/otm-vagrant) to get started. 
While not recommended for production, otm-vagrant greatly simplifies 
getting a development environment for testing and contributing to OTM locally.

### Environment variables
This project requires several environment variables, to provide API keys for several services.

```
ROLLBAR_SERVER_SIDE_ACCESS_TOKEN=....
GOOGLE_MAPS_KEY=...
```
`ROLLBAR_SERVER_SIDE_ACCESS_TOKEN` is a token for [Rollbar](rollbar.com).
`GOOGLE_MAPS_KEY` is a browser key for the Google Maps Javascript API, [which can be obtained here](https://developers.google.com/maps/documentation/javascript/get-api-key).

## Other Repositories

This repository (ie, otm-core) is but one of a few separate repositories 
that together compose the OpenTreeMap project. Others include:

* [otm-tiler](https://github.com/OpenTreeMap/otm-tiler) - map tile 
server based on [Windshaft](https://github.com/CartoDB/Windshaft)
* [otm-ecoservice](https://github.com/OpenTreeMap/otm-ecoservice) - ecosystem 
benefits calculation service
* [otm-ios](https://github.com/OpenTreeMap/otm-ios) - An 
OpenTreeMap client for iOS devices.
* [otm-android](https://github.com/OpenTreeMap/otm-android) - An OpenTreeMap client for Android devices.

## Additional Documentation

The REST API that provides data for the native mobile apps is documented in [api.md](doc/api.md)

### Deprecated Repositories

OpenTreeMap has a long history. These repositories still exist, but are 
deprecated and no development is happening here moving forward.

* [OpenTreeMap](https://github.com/OpenTreeMap/OpenTreeMap) - Otherwise 
known as ""OTM1"", this is previous generation codebase of OpenTreeMap. It 
has been entirely superceded by this repository and the others 
listed above. However, there are some live tree map sites still running 
on the old OTM1 code, and so we have left it up for archival purposes.

## Developer documentation
 - [Javascript module conventions](doc/js.md)
 - [Python mixins](doc/mixins.md)


## Acknowledgements

This application includes code based on [django-url-tools](https://bitbucket.org/monwara/django-url-tools), Copyright (c) 2013 Monwara LLC.

USDA Grant
---------------
Portions of OpenTreeMap are based upon work supported by the National Institute of Food and Agriculture, U.S. Department of Agriculture, under Agreement No. 2010-33610-20937, 2011-33610-30511, 2011-33610-30862 and 2012-33610-19997 of the Small Business Innovation Research Grants Program. Any opinions, findings, and conclusions, or recommendations expressed on the OpenTreeMap website are those of Azavea and do not necessarily reflect the view of the U.S. Department of Agriculture.
",,2023-08-03T07:09:29Z,25,184,31,"('maurizi', 1289), ('RickMohr', 933), ('ahinz', 744), ('jwalgran', 640), ('RobinIsTheBird', 170), ('kdeloach', 46), ('balexandrowicz', 33), ('designmatty', 16), ('hectcastro', 11), ('lederer', 10), ('requires', 4), ('ultradiv', 4), ('andrewbt', 4), ('bryant100000', 3), ('nicoali', 3), ('emstellato', 2), ('riromu', 2), ('george-mcintyre', 2), ('lliss', 2), ('cgarrard', 1), ('LarryLuTW', 1), ('lepittenger', 1), ('mmcfarland', 1), ('mauriciopasquier', 1), ('tnation14', 1)","[15, 'Life On Land']"
jellow-aac/Jellow-Communicator,,"# Jellow 
[![Build Status](https://travis-ci.com/jellow-aac/Jellow-Communicator.svg?branch=master)](https://travis-ci.com/jellow-aac/Jellow-Communicator)
[![codecov](https://codecov.io/gh/jellow-aac/Jellow-Communicator/branch/test-cases/graph/badge.svg)](https://codecov.io/gh/jellow-aac/Jellow-Communicator)

[Jellow Communicator](http://jellow.org/) is a friendly Augmentative and Alternative Communication (AAC) app that uses icons/images to enable speech. This freely downloadable Android app can be used for communication by those learning to speak or those having difficulty with speech. Jellow can also be used by toddlers and early learners to learn words and categories frequently used in their daily lives. Jellow’s colourful and friendly icons can help children develop an association between pictures and their corresponding word labels.
                                                          
Jellow has a simple, visually appealing and easy-to-learn interface consisting central 'category' buttons and 'expressive' side buttons. The content of the app is organized into basic category buttons that make it easy for the user to access and find desired icons. The user can make the app speak out sentences by simply clicking on any of the category buttons followed by any of the expressive buttons. 
                                                          
Jellow has over 1000 icons and over 10000 lines of pre-programmed text. In addition, using the 'keyboard' feature, the user can also generate new sentences and use the app to speak them out aloud. The current version of the app allows the user to choose from 3 languages  (English, Hindi, Bengali and Marathi) and multiple accents (Indian, American, British). 
                                                          
Jellow is developed at the IDC School of Design at the Indian Institute of Technology Bombay (IIT-B) located in Mumbai. The application is still under development. If you have any suggestions for improvement, please submit your feedback/comments via email at jellowcommunicator@gmail.com. 
                                                          
For further information on Jellow and FAQs, please visit www.jellow.org/

[](https://www.youtube.com/watch?v=tgPU4dEctE8&t=3s)

[](https://play.google.com/store/apps/details?id=com.dsource.idc.jellowintl&hl=en)



Whats new
----
- Addition of Visual accessibility (TalkBack support).
- Addition of new language: English (Australia)
- New design for Serial Keyboard.
- Addition of new language in Serial Keyboard : Bengali (India)
- Addition of new layouts to support 19 : 9 notch screen ratio devices.
- Corrected scrolls in intro screen.
- Bug reported on Crashlytics.

Contributing to Jellow
---
There are many ways to contribute to Jellow, and only a few of them involve writing code, language translations, promotions.

- Want to help to translate Jellow app content in your language? Contact us at jellowcommunicator@gmail.com
- Want to help to promote Jellow Communicator in relevant forums and communities. Let us know what is your idea at jellowcommunicator@gmail.com
- Want to contribute to Jellow app development check [Contributing Guide](https://github.com/jellow-aac/Jellow-Communicator/blob/master/CONTRIBUTING.md)

License
---

Copyright (c) NINAAD DIGITAL TECHNOLOGY PRIVATE LIMITED. All rights reserved.

The Jellow Communicator application's code is licensed under the [BSD](LICENSE.txt) License.

The Jellow Communicator application's contents including its interface, visual identity, icons and their associated vocabulary are licensed under a [Creative Commons Attribution-Non-Commercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)  License.

See our [Privacy Policy](http://jellow.org/privacypolicy/index.html).



Importaint message
---


The current repository is now archieved. However, the work on Jellow project is ongoing. If you wated to contribute to the Jellow project, contact us at the e-mail address given in the [Contributing to Jellow](#contributing-to-jellow) section.
","'aac', 'android', 'autism', 'autism-spectrum-disorder', 'communication', 'disabled-people', 'icons-to-speech', 'jellow-communicator', 'speech-disability'",2020-08-24T08:03:09Z,7,6,2,"('rahuljidgedev', 160), ('ayaz-alam', 31), ('anr007', 20), ('harshit-budhraja', 10), ('jellowcommunicator', 2), ('shrutygupta0297', 2), ('Vaibhav14890', 2)","[4, 'Quality Education']"
RefugeRestrooms/refugerestrooms,"REFUGE restrooms indexes and maps safe restroom locations for trans, intersex, and gender nonconforming individuals. ","Production CI: [![Build Status](https://travis-ci.com/RefugeRestrooms/refugerestrooms.svg?branch=master)](https://travis-ci.com/RefugeRestrooms/refugerestrooms)

Develop CI: [![Build Status](https://travis-ci.com/RefugeRestrooms/refugerestrooms.svg?branch=develop)](https://travis-ci.com/RefugeRestrooms/refugerestrooms)

Code Climate: [![Maintainability](https://api.codeclimate.com/v1/badges/a641d46a4ad2c2f01932/maintainability)](https://codeclimate.com/github/RefugeRestrooms/refugerestrooms/maintainability) [![Test Coverage](https://api.codeclimate.com/v1/badges/a641d46a4ad2c2f01932/test_coverage)](https://codeclimate.com/github/RefugeRestrooms/refugerestrooms/test_coverage)


Waffle.io: [![Stories in Ready](https://badge.waffle.io/RefugeRestrooms/refugerestrooms.png?label=ready)](https://waffle.io/RefugeRestrooms/refugerestrooms)

Bugsnag Open Source Bug Tracking:
[Bugsnag](https://www.bugsnag.com)



# REFUGE restrooms

Providing safe restroom access to transgender, intersex, and gender nonconforming individuals.

REFUGE is an effort to fill the void left by the now-defunct Safe2Pee website. It provides a free resource to trans\* and queer individuals in need of gender neutral and other safe restrooms.

This project is open source. Feel free to contribute. We could use the help.

## Deployed Environments
Production: [Link](http://www.refugerestrooms.org)

Staging: [Link](http://staging.refugerestrooms.org)

## Contributing

For more information on how to contribute to Refuge Restrooms, or how the technology works, see the [Wiki](https://github.com/RefugeRestrooms/refugerestrooms/wiki).

If you just want to get your environment set up for making changes locally and testing, you can head directly to [CONTRIBUTING.md](https://github.com/RefugeRestrooms/refugerestrooms/blob/develop/CONTRIBUTING.md).

Please also read our [Code of Conduct](https://github.com/RefugeRestrooms/refugerestrooms/blob/develop/CODE_OF_CONDUCT.md), which gives guidance on our standards of community and interaction.

## Tech

* Ruby Version - ruby-3.2.2
* Ruby on Rails
* RSpec
* Javascript
* HTML / SASS
* Postgres
* Geocoder Gem
* Google Maps API
* Twitter Bootstrap Framework
* Deployed on Heroku

## Links to Refuge project on other platforms

- [SMS messaging Twilio Application](https://github.com/RefugeRestrooms/refugerest_sms)
- [Android Native Application](https://github.com/RefugeRestrooms/refugerestrooms-android)
- [iOS Native Application](https://github.com/RefugeRestrooms/refuge-ios)
- [Yo Application](https://github.com/raptortech-js/YoRestrooms)

## Slack

If you want to join the Refuge Restrooms Slack channel, you can do so by [clicking on this link.](https://join.slack.com/t/refugelgbt/shared_invite/zt-3zaagpad-DvyfAPcepuRzFBJix1uYkg)

## License

Copyright (C) 2014–2017 Teagan Widmer and contributors

This program is free software; you can redistribute and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation; either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see .
","'lgbt', 'refuge', 'refuge-restrooms', 'safe-restrooms', 'trans', 'transgender'",2024-05-01T04:55:26Z,61,871,70,"('tkwidmer', 309), ('mi-wood', 149), ('DeeDeeG', 130), ('awoitte', 66), ('backspace', 54), ('stardust66', 46), ('johana-star', 45), ('f3ndot', 35), ('kiesia', 34), ('emilysaccount', 30), ('RSid', 20), ('btyy77c', 19), ('dominic', 19), ('nataliegroman', 17), ('kevjames3', 16), ('pamo', 16), ('cllns', 16), ('emfetz', 9), ('vancloss', 9), ('eleather', 8), ('clado', 7), ('mathonsunday', 7), ('sankalpchauhan-me', 7), ('MauroCoppola', 6), ('MatheusFreitag', 6), ('brunoocasali', 6), ('rakuista', 5), ('gnarf', 5), ('crackofdusk', 5), ('nonnullish', 5), ('mknadler', 4), ('Hkly', 4), ('g-knee', 4), ('brynmrk', 4), ('eriese', 3), ('kurtcarpenter', 3), ('shadowmaru', 3), ('valeriecodes', 3), ('cubeghost', 3), ('vinzruzell', 3), ('r19m89s', 2), ('emstans', 2), ('tbroadley', 2), ('piercebb', 2), ('GPrimola', 2), ('lisafrench', 2), ('juharris', 2), ('Intimaria', 2), ('Agrajag-Petunia', 2), ('waffle-with-pears', 1), ('ice1080', 1), ('tegandbiscuits', 1), ('SeanBriar', 1), ('darpr', 1), ('lsblakk', 1), ('lucas-carl', 1), ('KurtPrice', 1), ('hectorsector', 1), ('etagwerker', 1), ('iforgotband', 1), ('amadayami', 1)","[5, 'Gender Equality']"
tidepool-org/data-analytics,Home for code we use to analyze data for the Tidepool Big Data Donation project and similar.,"# data-analytics
Welcome to the Tidepool Data Analytics Repository. This is the home
for the code we use to download, clean, and analyze data for the Tidepool
Big Data Donation project.

## About our use of Python & R
We use the [Anaconda](https://www.anaconda.com/) distribution of Python & R.
You are welcome to install the full Anaconda installer, but will only need
Miniconda to get started.

## Getting started
### Project Setup
1. Install [Miniconda](https://conda.io/miniconda.html) for your platform.
1. In a terminal, navigate to the data-analytics directory where the environment.yml 
is located.
1. Run `conda env create`. This will download all of the package dependencies
and install them in a virtual environment named tidepool-analytics. PLEASE NOTE: this
may take close to 30 minutes to complete.

## To list the Virtual Environments
Run `conda env list`

## To use the Virtual Environment
In Bash run `source activate tidepool-analytics`, or in the Anaconda Prompt 
run `conda activate tidepool-analytics` to start the environment.

Run `deactivate` to stop the environment.

## Testing
This project uses the testing framework named pyTest. https://docs.pytest.org/en/latest/

After following the project setup instructions, including creating and activating the
virtual environment, you can simply run your tests within Bash

``` bash
# Run tests via  
pytest 
```

## Running Tests with Test Coverage 
This project uses pytest-cov (https://pytest-cov.readthedocs.io/en/latest/) to run test and produce code 
test coverage. 

To execute a basic test coverage report, run the following from within the virtual environment created during project setup
. This will give the output directly in the Terminal.
``` bash
# Run tests via  
pytest --cov 
```

To execute a detailed test coverage report, run the following command from within the virtual environment created during 
the project setup. 
This will create an htmlcov directory containing an index.html page with coverage details.
``` bash
# Run tests via  
pytest --cov --cov-report html
```
",,2022-03-29T20:04:37Z,7,9,17,"('ed-nykaza', 245), ('jameno', 17), ('KyleBolduc', 7), ('pazaan', 4), ('HowardLook', 2), ('tjotala', 2), ('damonbayer', 1)","[3, 'Good Health and Well-Being']"
CodeForAfrica/openAFRICA,openAFRICA aims to be largest independent repository of open data on the African continent. This repo contains the primary deployment scripts and files. Accessible at https://openafrica.net/,"# openAFRICA
*The continent's largest volunteer-driven open data portal.*

![CKAN version](https://img.shields.io/badge/CKAN-v2.6.8-brightgreen.svg)

This repo seeks to streamline deployment of the openAFRICA platform by pulling together the different components used for [openAFRICA](https://openafrica.net/) and deploy using [dokku](http://dokku.viewdocs.io/dokku/).

## CKAN

CKAN is an open-source DMS (data management system) for powering data hubs and data portals. CKAN makes it easy to publish, share and use data. It powers datahub.io, catalog.data.gov and data.gov.uk among many other sites.

We use CKAN's own vanilla releases but because they haven't properly adopted Docker and dockerhub (yet) for deployment, we're keeping a stable version (`codeforafrica/ckan:latest`) that we can be sure plays nice with our extenstions.

The ckan extensions we are using include:

- ckanext-openafrica - https://github.com/CodeForAfrica/ckanext-openafrica
- ckanext-datarequests - https://github.com/conwetlab/ckanext-datarequests
- ckanext-harvester - https://github.com/ckan/ckanext-harvest
- ckanext-s3filestore - https://github.com/okfn/ckanext-s3filestore
- ckanext-showcase - https://github.com/ckan/ckanext-showcase
- ckanext-googleanalytics - https://github.com/ckan/ckanext-googleanalytics
- ckanext-issues - https://github.com/ckan/ckanext-issues
- ckanext-gdoc - https://github.com/OpenUpSA/ckanext-gdoc
- ckanext-envvars - https://github.com/okfn/ckanext-envvars/


---

## Development


To set up your development environment:

```sh
$ git clone https://github.com/CodeForAfricaLabs/openAFRICA.git

$ cd openAFRICA
```

Run this command (found on the docker-compose.yml):

```sh
docker-compose build && docker-compose up
```

### Updating CKAN Docker Image

To update the `openafrica/ckan:latest` Docker image, edit `Makefile` and then run:

```sh
make ckan
```

### Tests

?

---

## Deployment

We use [dokku](http://dokku.viewdocs.io/dokku/) for deployment so you'd need to install and set it up first;

```
 # for debian systems, installs dokku via apt-get
 $ wget https://raw.githubusercontent.com/dokku/dokku/v0.11.3/bootstrap.sh
 $ sudo DOKKU_TAG=v0.11.3 bash bootstrap.sh
 # go to your server's IP and follow the web installer
```


### Install + Create Dependencies

Once installed, we can do the following:

1. Create the Dokku app and add a domain to it

```
dokku apps:create ckan
dokku domains:add ckan openafrica.net
```

2. Add letsencrypt for free `https` certificate

Install the [dokku-letsencrypt](https://github.com/dokku/dokku-letsencrypt) plugin and set the config variables

```
sudo dokku plugin:install https://github.com/dokku/dokku-letsencrypt.git
dokku config:set --no-restart ckan DOKKU_LETSENCRYPT_EMAIL=support@codeforafrica.org
```

3. Create CKAN Solr Instance

CKAN uses a special schema for Solr so you should deploy `openafrica/solr`

```
dokku apps:create ckan-solr

sudo docker volume create --name ckan-solr
dokku docker-options:add ckan-solr run,deploy --volume ckan-solr:/opt/solr/server/solr/ckan

sudo docker pull codeforafrica/ckan-solr:2.7.6
sudo docker tag codeforafrica/ckan-solr:2.7.6 dokku/ckan-solr:latest

dokku git:from-image ckan-solr dokku/ckan-solr:latest

```

4. Create Redis Instance

Install the [redis](https://github.com/dokku/dokku-redis) plugin.

```
sudo dokku plugin:install https://github.com/dokku/dokku-redis.git redis
dokku redis:create ckan-redis

```

5. Create CKAN DataPusher Instance

[DataPusher](https://github.com/ckan/datapusher) is a standalone web service that automatically downloads any CSV or XLS (Excel) data files from a CKAN site's resources when they are added to the CKAN site, parses them to pull out the actual data, then uses the DataStore API to push the data into the CKAN site's DataStore.

```
dokku apps:create ckan-datapusher

sudo docker pull openafrica/ckan-datapusher:latest
sudo docker tag openafrica/ckan-datapusher:latest dokku/ckan-datapusher:latest

dokku git:from-image ckan-datapusher dokku/ckan-datapusher:latest

```

6. Install Postgres (Optional)

This is an optional step if you'd like to have Postgres installed locally;

```
sudo dokku plugin:install https://github.com/dokku/dokku-postgres.git postgres
dokku postgres:create ckan-postgres

```

7. Install RabbitMQ

Install the [RabbitMQ](https://github.com/dokku/dokku-rabbitmq) plugin (The harvest extension uses this as its backend)

```
sudo dokku plugin:install https://github.com/dokku/dokku-rabbitmq.git rabbitmq
dokku rabbitmq:create ckan-rabbitmq
```

8. Set up S3

Create a bucket and a programmatic access user, and grant the user full access to the bucket with the following policy

```
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:*""
            ],
            ""Resource"": [
                ""arn:aws:s3:::openafrica/*"",
                ""arn:aws:s3:::openafrica""
            ]
        }
    ]
}
```

9. Create CKAN filestore volume

Create a named docker volume and configure ckan to use the volume just so we can configure an upload path. It should be kept clear by the s3 plugin.

```
sudo docker volume create --name ckan-filestore
dokku docker-options:add ckan run,deploy --volume ckan-filestore:/var/lib/ckan/default
```

### Configuration

Now we configure to pull the dependencies together:

Get the Redis Dsn (connection details) for setting in CKAN environment in the next step with /0 appended.

```
dokku redis:info ckan-redis
```
Get the RabbitMQ Dsn (connection details) and extract the `username`, `password`, `hostname`, `virtualhost` and `port`. You need these details because the harvester extension in its current form does not support configuration using RabbitMQ URI scheme. The URI is in the form

```
amqp://username:password@hostname:port/virtualhost
```


Set CKAN environment variables, replacing these examples with actual producation ones

- REDIS_URL: use the Redis Dsn
- SOLR_URL: use the alias given for the docker link below
- BEAKER_SESSION_SECRET: this must be a secret long random string. Each time it changes it invalidates any active sessions.
- S3FILESTORE__SIGNATURE_VERSION: use as-is - no idea why the plugin requires this.


```
dokku config:set ckan CKAN_SQLALCHEMY_URL=postgres://ckan_default:password@host/ckan_default \
                      CKAN_DATASTORE_READ_URL=postgresql://ckan_default:pass@localhost/datastore_default \
                      CKAN_DATASTORE_WRITE_URL=postgresql://datastore_default:pass@localhost/datastore_default \
                      CKAN_REDIS_URL=.../0 \
                      CKAN_INI=/ckan.ini \
                      CKAN_SOLR_URL=http://solr:8983/solr/ckan \
                      CKAN_SITE_URL=https://openafrica.net/ \
                      CKAN___BEAKER__SESSION__SECRET= \
                      CKAN_SMTP_SERVER= \
                      CKAN_SMTP_USER= \
                      CKAN_SMTP_PASSWORD= \
                      CKAN_SMTP_MAIL_FROM=hello@openafrica.net \
                      CKAN___CKANEXT__S3FILESTORE__AWS_BUCKET_NAME=openafrica \
                      CKAN___CKANEXT__S3FILESTORE__AWS_ACCESS_KEY_ID= \
                      CKAN___CKANEXT__S3FILESTORE__AWS_SECRET_ACCESS_KEY= \
                      CKAN___CKANEXT__S3FILESTORE__HOST_NAME=http://s3-eu-west-1.amazonaws.com \
                      CKAN___CKANEXT__S3FILESTORE__REGION_NAME=eu-west-1 \
                      CKAN___CKANEXT__S3FILESTORE__SIGNATURE_VERSION=s3v4 \
                      CKAN__HARVEST__MQ__VIRTUAL_HOST=ckan-rabbitmq \
                      CKAN__HARVEST__MQ__PORT=5672 \
                      CKAN__HARVEST__MQ__HOSTNAME=dokku-rabbitmq-ckan-rabbitmq \
                      CKAN__HARVEST__MQ__PASSWORD=912abee9882be7ca8718d3cab7263cfd \
                      CKAN__HARVEST__MQ__USER_ID=ckan-rabbitmq \
```

Link CKAN with Redis, Solr, and CKAN DataPusher;
```
dokku redis:link ckan-redis ckan  #noqa
dokku docker-options:add ckan run,deploy --link ckan-solr.web.1:solr
dokku docker-options:add ckan run,deploy --link ckan-datapusher.web.1:ckan-datapusher
```

### Scheduled Jobs
For openAFRICA to work perfectly, some jobs have to run at certain times e.g. updating tracking statistics and rebuilding the search index for newly uploaded datasets. To create a scheduled job that is executed by a Dokku application, follow these steps:

```sh
sudo su dokku
crontab -e
```

Add the following entries

```sh
0 * * * * echo '{}' | dokku --rm run ckan paster --plugin=ckan post -c /ckan.ini /api/action/send_email_notifications > /dev/null

0 * * * * dokku --rm run ckan paster --plugin=ckan tracking update -c /ckan.ini

*/15 * * * * dokku --rm run ckan paster --plugin=ckanext-harvest harvester run --config=/ckan.ini
```

### Deploy CKAN


Once done with installing and configuring, you can push this repository to dokku:

```
git remote add dokku dokku@openafrica.net:ckan
git push dokku
```

### Initialize Database

Before you can run CKAN for the first time, you need to run `db init` to initialize your database

```
dokku enter ckan
cd src/ckan
paster db init -c /ckan.ini
```

Lastly, let's make sure we encrypt traffic:

```
dokku letsencrypt ckan
```


***NOTE:** Make sure to have the [appropriate permissions to push to dokku](http://dokku.viewdocs.io/dokku/deployment/user-management/).*

---

## Contributing

Thank you for considering to contribute to this project. You are awesome. :)

To get you started, here are few pointers:

- We have a number of Github issues to work through here:
  - openAFRICA deploy: https://github.com/CodeForAfricaLabs/openAFRICA/issues
  - ckanext-openafrica: https://github.com/CodeForAfrica/ckanext-openafrica/issues
  - ckanext-socialite: https://github.com/CodeForAfricaLabs/ckanext-socialite/issues
  - ckanext-social: https://github.com/CodeForAfricaLabs/ckanext-social/issues
- If you believe an issue is with CKAN core or related extenstions, post them here:
  - CKAN core: https://github.com/ckan/ckan/issues
  - ckanext-harvester: ?

Check out the [development docs](#development) to get started on this repo locally.


### Security Vulnerabilities

Please report on security vulnerabilities to security@codeforafrica.org. These will be promptly acted on.

---

## License

GNU General Public License

openAFRICA aims to be the largest independent repository of open data on the African continent.
Copyright (C) 2017  Code for Africa

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see .
","'africa', 'ckan', 'ckanext-openafrica', 'ckanext-socialite', 'data-portal', 'deployment', 'docker', 'docker-image', 'dokku', 'kenya', 'nigeria', 'open-africa', 'open-data', 'openafrica', 'rabbitmq', 'redis', 'solr', 'south-africa', 'tanzania'",2024-04-22T06:49:25Z,5,27,19,"('DavidLemayian', 57), ('thepsalmist', 23), ('phillipahereza', 15), ('esirK', 8), ('WNjihia', 1)","[17, 'Partnerships for the Goals']"
HTBox/allReady,"This repo contains the code for allReady, an open-source solution focused on increasing awareness, efficiency and impact of preparedness campaigns as they are delivered by humanitarian and disaster response organizations in local communities.","[![Build status](https://ci.appveyor.com/api/projects/status/69iwhe2g11t30sj8/branch/master?svg=true)](https://ci.appveyor.com/project/HTBox/allready/branch/master)

![allReady project banner](./docs/media/all-ready-project-banner.jpg)

# Welcome to allReady

This repo contains the code for **allReady**, an open-source solution focused on increasing awareness, efficiency and impact of preparedness campaigns as they are delivered by humanitarian and disaster response organizations in local communities.

+ [Upcoming v1.0 Release](#upcoming-v10-release)
+ [Project overview](#project-overview)
+ [How you can help](#how-you-can-help)

## Long-Running and I/O-Bound Operations
If you are interested in working with Azure Functions and moving things like communications, importing data, image processing and more please check out the related repository for our [allReady-processing](https://github.com/HTBox/allReady-processing) project.

## Upcoming v1.0 Release
We are on the path to our first production v1.0 release!  We are tracking requirements in our [v1.0 Release Milestone](https://github.com/HTBox/allReady/milestone/21) and currently directing efforts to closing out issues starting with P1-P3 priority items that are currently being triaged and tagged.

More info to come in our [biweekly standups](https://www.youtube.com/channel/UCMHQ4xrqudcTtaXFw4Bw54Q/)

## .NET Core 2.0.x
As part of the work to support cross platform development, we have modified the allReady projects to support .NET Core.  This will allow development on Mac devices*. We are currently tracking .NET Core 2.0.x, which we expect to fall into LTS support.

Existing and new developers will need to ensure they have the latest .NET Core SDK supporting the current release 2.0.x. We have added basic steps for developers to setup their device at https://github.com/HTBox/allReady/wiki/Developer-Setup

More detailed setup information for new developers exists at https://github.com/HTBox/allReady/blob/master/docs/prerequisite_install_guide/prerequisite_install_guide.md

## Project overview
allReady is focused on increasing awareness, efficiency and impact of preparedness campaigns delivered by humanitarian and disaster response organizations in local communities.  As community preparedness and resliency increases, the potential for impactful disasters (both large and small) is greatly decreased.  Though not as visible or emotionally salient as saving children from a burning building, preparedness activities like ensuring working smoke detectors in a community, follows the industry rule of thumb where an hour or dollar spent before a disaster is worth 15-30 afterwards.  The goal of allReady hinges on growing awareness of, and engaging communities and their volunteers in preparedness campaigns, and more aspirationally, to ""put disaster response out of business"" by preparing communities to be resilient to inevitable disasters. 

To learn more about the need for allReady, the technologies involved and how the app came together, view the [project information](http://www.htbox.org/projects/allready) and [blog post](http://www.htbox.org/blog/allready-project-launched-at-visual-studio-2015-release-event) on the Humanitarian Toolbox website and watch the *[In the Code](https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/In-the-Code-App-Overview-and-Planning)* video series:

<a href=""http://www.youtube.com/watch?feature=player_embedded&v=XVRfcSej1l0
"" target=""_blank""><img src=""http://img.youtube.com/vi/XVRfcSej1l0/0.jpg"" 
alt=""IMAGE ALT TEXT HERE"" width=""240"" height=""180"" border=""10"" />

The allReady project was jumpstarted by volunteers at Microsoft and has been turned over to [Humanitarian Toolbox](http://www.htbox.org/) to be maintained and improved by the technical community at large and ultimately deployed in support of organizations delivering preparedness campaigns everywhere.

The initial launch of development for allReady started on 7/20/2015 during the [Visual Studio 2015 release event](http://aka.ms/vs2015event).

## How you can help
To help make improvements to this project, you can just clone this repository and start coding, testing, and/or designing. 

> **Important** Before jumping in, please review the [solution architecture](https://github.com/HTBox/allReady/wiki/Solution-architecture) and instructions below to [get started](https://github.com/HTBox/allReady/wiki/Solution-architecture#get-started-with-the-allready-solution). It contains critical information on how to configure the project to run locally and optionally deploy AllReady to Azure.

Also we have a guide on setting up git for open source projects like allReady that can help you get started making contributions.  You can find the [guide in our docs folder](https://github.com/HTBox/allReady/blob/master/docs/git/gitprocess.md) and it also reference a number of blog posts written with additional information on contributing to projects like ours.

Thank you for considering supporting this open source project for humanitarian support.
","'allready', 'asp-net-core', 'c-sharp', 'disaster-response', 'preparedness-campaigns', 'volunteers'",2022-12-09T00:00:05Z,162,890,132,"('stevejgordon', 450), ('mgmccarthy', 404), ('tonysurma', 375), ('MisterJames', 308), ('BillWagner', 240), ('mheggeseth', 177), ('dpaquette', 153), ('auroraocciduusadmin', 91), ('HamidMosalla', 91), ('mmoore99', 65), ('shawnwildermuth', 53), ('dchristensen', 49), ('c0g1t8', 41), ('SteveStrong', 41), ('VishalMadhvani', 41), ('DanielSchiavini', 38), ('rockfordlhotka', 35), ('bcbeatty', 32), ('robrich', 32), ('joelhulen', 32), ('digitaldrummerj', 31), ('pottereric', 29), ('YannickMeeus', 26), ('mk0sojo', 25), ('simon-20', 24), ('WilliamBerryiii', 24), ('danielepo', 24), ('binaryjanitor', 20), ('nedruk', 19), ('bsstahl', 17), ('thomfresn', 16), ('chinwobble', 15), ('darrylsk', 14), ('Michael-Marshall', 13), ('Paul-Hadfield', 13), ('shahiddev', 13), ('BBosman', 12), ('arst', 11), ('stimms', 11), ('SeanFeldman', 11), ('iloabn', 11), ('Kritner', 10), ('jgauffin', 10), ('sanilpaul', 10), ('glittle', 9), ('iant-ee', 9), ('Chris-Wooder', 8), ('joro550', 8), ('Daven-', 8), ('dangle1', 7), ('nemesv', 7), ('morcs', 7), ('jamesatgreymetis', 7), ('dracan', 7), ('Sobieck', 6), ('dcbrewster', 6), ('BlueNinjaSoftware', 6), ('gftrader', 6), ('cj-taylor', 5), ('timstarbuck', 5), ('duncangoodall', 5), ('rockydevnet', 5), ('pmarshalla', 5), ('Hamels', 5), ('pranap', 4), ('peteraritchie', 4), ('kylebcooke', 4), ('klabranche', 4), ('chris-sargood', 4), ('valdisiljuconoks', 3), ('chdbdm', 3), ('aliiftikhar', 3), ('seanepping', 3), ('nathanchere', 3), ('stefan-van-de-griendt-centric', 3), ('marigerr', 3), ('devfables', 3), ('trayburn', 3), ('rynowak', 3), ('PMCurry', 3), ('naeemsarfraz', 3), ('MattGal', 3), ('mjmilan', 3), ('joosthaneveer', 3), ('JesseLiberty', 3), ('jonparker', 3), ('cjmartin6162', 3), ('djhmateer', 3), ('recumbent', 3), ('bmaluijb', 2), ('shanecharles', 2), ('SirwanAfifi', 2), ('redware', 2), ('sdekok', 2), ('TomWalkerCodes', 2), ('brianmitchard', 2), ('garrmark', 2), ('ryanEcomp', 2), ('zimplicity', 2), ('adam-gligor', 2), ('bzannah', 2), ('SarabjotSingh1986', 2), ('bergerb', 2), ('curts', 2), ('DavidArno', 2), ('duanenewman', 2), ('gdereese', 2), ('GProulx', 2), ('GitaGunnam', 2), ('SarahMarz', 2), ('sgwill', 2), ('miroslavpopovic', 2), ('colhountech', 2), ('kmadof', 2), ('johnmwright', 2), ('stickboysoupup', 2), ('csharpfritz', 2), ('i-hardy', 2), ('pboyer84', 1), ('rayhiker', 1), ('ResaWildermuth', 1), ('russdaygh', 1), ('sarahbaileymadgex', 1), ('tmanville', 1), ('MadMonkeyCoder', 1), ('TristanGaydon', 1), ('allentsa', 1), ('achasik', 1), ('anobleperson', 1), ('benjaminettori', 1), ('dylan-smith', 1), ('ericbrumfield', 1), ('gordanac', 1), ('kgipson', 1), ('Odia1', 1), ('ofirgeller', 1), ('pmarshallandrew', 1), ('smahesh11', 1), ('ivwivw', 1), ('wmay1991', 1), ('enderdickerson', 1), ('berthertogen', 1), ('CristianCson', 1), ('dneelyep', 1), ('Diana380', 1), ('FvdG', 1), ('GITS-ONLINE', 1), ('jnSWPC', 1), ('jonatwabash', 1), ('julianharty', 1), ('khalidabuhakmeh', 1), ('forestcheng', 1), ('mido3ds', 1), ('manojattal', 1), ('maguir', 1), ('mdigon', 1), ('mattferderer', 1), ('michaellperry', 1), ('mikesigs', 1), ('ngm', 1), ('NRKirby', 1), ('nicolastarzia', 1)","[16, 'Peace, Justice and Strong Institutions']"
jembi/openhim-core-js,The Open Health Information Mediator core component. OpenHIM Support: Post your query on OpenHIE Discourse using the #openhim tag https://discourse.ohie.org/,"# OpenHIM Core Component

[![Build Status](https://travis-ci.org/jembi/openhim-core-js.png?branch=master)](https://travis-ci.org/jembi/openhim-core-js) [![Dependency Status](https://david-dm.org/jembi/openhim-core-js.png)](https://david-dm.org/jembi/openhim-core-js) [![devDependency Status](https://david-dm.org/jembi/openhim-core-js/dev-status.png)](https://david-dm.org/jembi/openhim-core-js#info=devDependencies) [![codecov](https://codecov.io/gh/jembi/openhim-core-js/branch/master/graph/badge.svg)](https://codecov.io/gh/jembi/openhim-core-js)

The OpenHIM core component is responsible for providing a single entry-point into an HIE as well as providing the following key features:

- Point of service client authentication and authorization
- Persistence and audit logging of all messages that flow through the OpenHIM
- Routing of messages to the correct service provider (be it an HIM orchestrator for further orchestration or the actual intended service provider)

> **To get started and to learn more about using the OpenHIM** see [the full documentation](http://openhim.org).

Some of the important information is repeated here, however, the above documentation is much more comprehensive.

See the [development road-map](http://openhim.org/docs/introduction/roadmap) for more details on what is to come!

---

## Requirements

Currently supported versions of NodeJS LTS are

| NodeJS (LTS) | MongoDB                    |
| ------------ | -------------------------- |
|  14.2x.x     | >= 3.6 &#124;&#124; <= 4.2 |
|  15.x        | >= 3.6 &#124;&#124; <= 4.2 |


- [NodeJS Release Versions](https://github.com/nodejs/Release)
- [MongoDB NodeJS Driver Versions](https://mongodb.github.io/node-mongodb-native/)
- [MongoDB Driver Compatibility](https://docs.mongodb.com/ecosystem/drivers/driver-compatibility-reference/#node-js-driver-compatibility)

## Getting started with the OpenHIM-core

### Docker Compose

1. Ensure that you have [Docker](https://docs.docker.com/install/) and [Docker Compose](https://docs.docker.com/compose/install/) installed.
1. Navigate to the [docker-compose.yml](https://github.com/jembi/openhim-core-js/blob/master/infrastructure/docker-compose.yml) file found in the `/infrastructure` directory.
1. Execute the Docker Compose file to pull the docker images and start the services in a detached mode:

  ```sh
  docker-compose up -d
  ```

1. Once the services have all started, you will be able to view the [OpenHIM Console](http://localhost:9000) in your browser.

---

## Developer guide

Clone the `https://github.com/jembi/openhim-core-js.git` repository.

Ensure you have the following installed:

- [Node.js](http://nodejs.org/) **v10(LTS) && != 10.15.1 || v12(LTS)**
- [NPM](https://www.npmjs.com/)
- [MongoDB](http://www.mongodb.org/) (in Ubuntu run `sudo apt install mongodb`, in OSX using [Homebrew](http://brew.sh), run `brew update` followed by `brew install mongodb`)

Navigate to the directory where the openhim-core-js source is located and run the following:

`npm install`

> This will install all the required modules and then build the project files.

In order to run the OpenHIM core server, [MongoDB](http://www.mongodb.org/) must be installed and running. Please refer to the [requirements table](#requirements) for accurate versions to use.

To run the server, execute:

`npm start` (this runs `node lib/server.js` behind the scenes)

The server will by default start in development mode using the mongodb database 'openhim-development'. To start the server in production mode use the following:

`NODE_ENV=production npm start`

This starts the server with production defaults, including the use of the production mongodb database called 'openhim'.

This project uses [mocha](https://mochajs.org/) as a unit testing framework with [should.js](https://github.com/visionmedia/should.js/) for assertions and [sinon.js](http://sinonjs.org/) for spies and mocks. The tests can be run using `npm test`.

**Pro tips:**

- `npm run lint` - ensure the code is lint free, this is also run before an `npm test`
- `npm link` - will symlink you local working directory to the globally installed openhim-core module. Use this so you can use the global openhim-core binary to run your current work in progress. Also, if you build any local changes the server will automatically restart.
- `npm test -- --grep ` - will only run tests with names matching the regex.
- `npm test -- --inspect` - enabled the node debugger while running unit tests. Add `debugger` statements and use `node debug localhost:5858` to connect to the debugger instance.
- `npm test -- --bail` - exit on first test failure.

---

## Creating CentOS RPM package

The build process for the RPM package is based off [this](https://github.com/bbc/speculate/wiki/Packaging-a-Node.js-project-as-an-RPM-for-CentOS-7) blog. The reason for using vagrant instead of docker is so that we can test the RPM package by running it as a service using SystemCtl - similar to how it will likely be used in a production environment. SystemCtl is not available out the box in docker containers.

Refer to this [blog](https://developers.redhat.com/blog/2014/05/05/running-systemd-within-docker-container/) for a more detailed description of a possible work-around. This is not recommended since it is a hack. This is where vagrant comes in since it sets up an isolated VM.

1. Setup environment

   Navigate to the infrastructure folder: `infrastructure/centos`

   Provision VM and automatically build RPM package:

   ```bash
   vagrant up
   ```

   or without automatic provisioning (useful if you prefer manual control of the process):

   ```bash
   vagrant up --no-provision
   ```

1. [Optional] The Vagrant file provisions the VM with the latest source code from master and attempts to compile the RPM package for you. However in the event an error occurs, or if you prefer to have manual control over the process, then you'll need to do the following:

   - Remote into the VM: `vagrant ssh`
   - Download or sync all source code into VM.
   - Ensure all dependencies are installed.

   ```bash
   npm i && npm i speculate
   ```

   - Run speculate to generate the SPEC files needed to build the RPM package.

   ```bash
   npm run spec
   ```

   - Ensure the directory with the source code is linked to the rpmbuild directory - the folder RPMBUILD will use.

   ```bash
   ln -s ~/openhim-core ~/rpmbuild
   ```

   - Build RPM package.

   ```bash
   rpmbuild -bb ~/rpmbuild/SPECS/openhim-core.spec
   ```

1. Install & Test package

   ```bash
   sudo yum install -y ~/rpmbuild/RPMS/x86_64/openhim-core-{current_version}.x86_64.rpm
   sudo systemctl start openhim-core
   curl https://localhost:8080/heartbeat -k
   ```

   Note: In order for openhim-core to run successfully, you'll need to point it to a valid instance of Mongo or install it locally:

   ```bash
   sudo yum install mongodb-org
   sudo service mongod start
   ```

1. How to check the logs?

   ```bash
   sudo systemctl status openhim-core
   sudo tail -f -n 100 /var/log/messages
   ```

1. If everything checks out then extract the RPM package by leaving the VM.

   Install Vagrant scp [plugin](https://github.com/invernizzi/vagrant-scp):

   ```bash
   vagrant plugin install vagrant-scp
   ```

   Then copy the file from the VM:

   ```bash
   vagrant scp default:/home/vagrant/rpmbuild/RPMS/x86_64/{filename}.rpm .
   ```

---

## Contributing

You may view/add issues here: 

To contribute code, please fork the repository and submit a pull request. The maintainers will review the code and merge it in if all is well.

## Data Privacy Disclaimer

All users downloading and using OpenHIM should note the following:

* All message data sent to the OpenHIM is retained indefinitely within the OpenHIM’s MongoDB database. By default, this data is stored indefinitely in line with the function of a middleware software with audit & transaction replay capabilities.
* All message data is stored in OpenHIM's MongoDB and is only accessible or viewable by a) An authorized admin-level user or a user that has been explicitly allowed to do so or; b) An authorized system administrator staff member having access to the server itself.
* Access to the message data stored in OpenHIM’s MongoDB database is controlled by the organization hosting OpenHIM. This organisation must know its responsibilities as a ‘Data Controller’ and potentially other roles, as defined in standard data privacy regulations, such as the General Data Protection Regulation (GDPR) and the South African Protection of Personal Information Act (POPIA). The organisation using OpenHIM is responsible for having the required policies in place to ensure compliance with the applicable laws and regulations in the country where the software is being operated.
* All message data stored in OpenHIM's MongoDB may be purged at any time by direct commands to the MongoDB database or the use of the data retention feature of OpenHIM channels.
* Basic data about OpenHIM users (name and email) is stored indefinately so that they may access the OpenHIM console. These users may be removed at any time if they are no longer needed.
",,2024-04-29T14:12:20Z,36,65,38,"('rcrichton', 674), ('hnnesv', 502), ('debonair', 385), ('MattyJ007', 291), ('BMartinos', 232), ('bausmeier', 225), ('bradsawadye', 158), ('litlfred', 125), ('nour-borgi', 87), ('nthfloor', 81), ('arran-standish', 50), ('rmrlangford', 43), ('greenkeeperio-bot', 40), ('tumijacob', 40), ('napiergit', 28), ('de-laz', 26), ('ishmaelmakitla', 22), ('brett-onions', 19), ('Zooloo2014', 18), ('lindajembi', 16), ('tmvumbi2', 13), ('michaelloosen', 10), ('marrouchi', 7), ('bluephone', 5), ('JohanVanZyl', 5), ('trevorgowing', 5), ('dependabotbot', 5), ('psbrandt', 4), ('cseebregts', 3), ('kaweesi', 2), ('MatthewErispe', 2), ('Ken473', 1), ('medchedli', 1), ('mortonfox', 1), ('armageddon', 1), ('gussery3', 1)","[3, 'Good Health and Well-Being']"
claytonrcarter/cropplanning,Crop Planning Software for Small Farms and Serious Gardeners,"# NO LONGER MAINTAINED / MAINTAINER WANTED

It was great while it lasted, folks, but I no longer use this software and
don't have the time to maintain and support it. For the most part, it still
works fine and many people still use it on a regular basis.

If you are interested in taking over the maintenance and support of this project, please get in touch with me.

# CropPlanning

**Crop Planning Software** for small farmers and serious gardeners.
Plan for and manage the myriad crops, varieties and plantings required to keep a modern, intensive market garden producing throughout the growing season. This is a cross-platform (Windows, Mac, Unix, etc) desktop application that allows small farmers and gardeners to: Create, duplicate and delete crops and plantings; inherit data from other plantings and crops; key in a few data and let the program calculate the rest for you; sort and filter your plantings. These are all possible and new features are being added all the time. The software has been released as **open source, free software** and therefore is and will always be available **free of charge**. Those who [**donate**](Donate), however, will be thanked profusely.

### Latest Version
Version: **0.7.0** 
Released: **Feb 8, 2016** 
[Release Notes](https://github.com/claytonrcarter/cropplanning/wiki/ReleaseNotes070) 
[Installation Instructions](https://github.com/claytonrcarter/cropplanning/wiki/SetupAndInstallation) 

[Download for Windows](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0-Win.zip)  
[Download for Mac](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0-Mac.zip)
(for MacOS 10.8+: [read this](https://github.com/claytonrcarter/cropplanning/wiki/MacGatekeeper))  
[Download for Linux](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0.zip)

## Start Here

* Download the software. [Windows](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0-Win.zip) [Mac](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0-Mac.zip) [Linux and everyone else](https://github.com/claytonrcarter/cropplanning/releases/download/v0.7.0/CropPlanning-0.7.0.zip)
* See the program in action, including weekly planting lists and screencasts (New!).
* [Learn how to use the program](https://github.com/claytonrcarter/cropplanning/wiki/ExamplesAndScreenShots), including it's advanced features.
* Contact us at [cropplanning@gmail.com](mailto:cropplanning@gmail.com) with questions, concerns, suggestions, and such.
* Help us! Right now, the best way to help us is to try out the software and [tell us](mailto:cropplanning@gmail.com) what you think about it.

## Features

* familiar, spreadsheet-like interface
* easily create and print weekly field planting and GH seeding lists
* sort and filter your crop plans so you only see what you want to see
* powerful database makes it easier to work with your plans and keep data linked together
* easily keep track of whether something has been planted, transplanted and harvested
* easily keep track of the actual and planned dates upon which something is planted, transplanted and harvested
* data can be ""linked"" from crops ==> varieties ==> individual plantings
* enter certain values and have others calculated automatically; for example:
 * enter a number of beds or row-feet to plant and the program can calculate how many transplants you'll need
 * enter a desired yield quantity and the program can tell you how many row-feet or beds you should plant
 * enter your desired date of harvest and the program can estimate when to plant
* import & export data
* bed and row lengths can be set for every field or even for every bed
* Missed a planting? Don't delete it, just ignore it (ie, ""skip"" it) to get it out of the way.
* Plantings can be explicitly marked as direct seeded or transplanted and can have different planting data accordingly.
* Help you estimate how much seed you'll need to execute your plan
* Support for US and metric measurements
* charts and stats that show:
 * beds in use in each field by week
 * trays in the greenhouse by week
 * how many trays needed (tallied up by size) for the season
 * how many beds have ""requirements"" (ie, black plastic, reemay, etc) for the season

### Support Provided By

MOFGA, Fedco Seeds, Northeast SARE

### Free Software

This project is released as free, open source software, which means that the computer source code that makes it work is and will always be available for free. In addition, this project is built upon and couldn't exist without these other free, open source projects:

| HSQLDB | Persist | Glazed Lists | iText | Twinkle | GitX |
| ------ | ------- | ------------ | ----- | ------- | ---- |
| DBMS (Database) | ORM (Super easy interface to database) | Super easy and fast list sorting, filtering and more! | PDF creation | Sparkle-style updates for Java! | Great git app for Mac |

### Important Notice: Beta Software

This software is currently under development and is changing rapidly. Please DO download it, try it out and let us know what you think; but please keep in mind that it's not finished. Is is great? Does it suck? Does something about it really annoy you or can you simply not farm without it? Please let us know!

## Developing and Contributing Code

To download the code yourself simply

    git clone https://github.com/claytonrcarter/cropplanning.git

or of course fork it into your own github account.

You will need:

*  Netbeans and Java 1.5
*  A bundle of libraries
    * [Download libraries](http://sourceforge.net/projects/cropplanning/files/Developer%20Files/devel-libs.zip/download)

If you unzip the library bundle in the parent folder of the cloned `cropplanning` repo, NetBeans should find all of the libraries automatically.
",,2019-05-02T13:19:58Z,1,73,16,"('claytonrcarter', 151)","[12, 'Responsible Consumption and Production']"
publiclab/plots2,a collaborative knowledge-exchange platform in Rails; we welcome first-time contributors! :balloon:,"PublicLab.org
======

[![Code of Conduct](https://img.shields.io/badge/code-of%20conduct-green.svg)](https://publiclab.org/conduct)
[![Build Status](https://github.com/publiclab/plots2/workflows/tests/badge.svg?branch=main)](https://github.com/publiclab/plots2/actions)
[![first-timers-only-friendly](https://img.shields.io/badge/first--timers--only-friendly-blue.svg?style=flat-square)](https://code.publiclab.org#r=all)
[![Join the chat at https://publiclab.org/chat](https://img.shields.io/badge/chat-in%20different%20ways-blue.svg)](https://publiclab.org/chat)
[![Code Climate](https://codeclimate.com/github/publiclab/plots2/badges/gpa.svg)](https://codeclimate.com/github/publiclab/plots2)
[![codecov](https://codecov.io/gh/publiclab/plots2/branch/main/graph/badge.svg)](https://codecov.io/gh/publiclab/plots2)
[![View performance data on Skylight](https://badges.skylight.io/typical/GZDPChmcfm1Q.svg)](https://oss.skylight.io/app/applications/GZDPChmcfm1Q)
[![Newcomers welcome](https://img.shields.io/badge/newcomers-welcome-pink.svg)](https://code.publiclab.org) [![GitHub license](https://img.shields.io/github/license/publiclab/plots2?logo=gpl)](https://github.com/publiclab/plots2/blob/main/LICENSE)
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/publiclab/plots2/)

The content management system for the Public Lab research community, the `plots2` web application is a combination of a group research blog -what we call ""research notes""-and a wiki. Read more about the [data model here](https://github.com/publiclab/plots2/blob/main/doc/DATA_MODEL.md).

Begin running (and contributing to) this codebase immediately with [GitPod](https://gitpod.io):

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/publiclab/plots2)

It showcases a variety of features that help the Public Lab community collaborate on environmental technology, design, documentation, and community organizing. Originally a Drupal site, it was rewritten in 2012 with Ruby on Rails and has since extended but [not yet entirely replaced](https://github.com/publiclab/plots2/issues/956) the legacy Drupal data model and database design. We ❤️ Open Source and actively participate in various OSS programs such as [Google Summer of Code(GSoC)](https://publiclab.org/wiki/gsoc), Rails Girls Summer of Code (RGSoC), Outreachy, and Google Code-In (GCI).
Some key features include:

* A [Q&A portal](https://publiclab.org/questions) for peer-based problem solving
* A rich text and Markdown [editor](https://github.com/publiclab/PublicLab.Editor)
* [Wiki editing](https://publiclab.org/wiki) and revision tracking
* Tagging and [topically-based groups and content organization](https://publiclab.org/tags)
* Email notification subscriptions for topics and comments
* A search interface built out of [our growing API](https://github.com/publiclab/plots2/blob/main/doc/API.md)
* A user dashboard [presenting recent activity](https://publiclab.org/dashboard)
* A privacy-sensitive, Leaflet-based [location tagging system](https://github.com/publiclab/leaflet-blurred-location/) and [community map](https://publiclab.org/people)

## Roadmap

We are developing a draft Roadmap for `plots2` and our broader Public Lab code projects; [read more and comment here](https://publiclab.org/notes/warren/05-22-2019/draft-of-a-public-lab-software-roadmap-comments-welcome).

A full description of the features, audiences, inter-relationships, and goals of Public Lab software projects can be found here: https://publiclab.org/software-overview

## Table of Contents

1. [What Makes This Project Different](#what-makes-this-project-different)
2. [Data model](#data-model)
3. [Contributing](#contributing)
4. [Prerequisites](#prerequisites)
5. [Installation](#installation)
    - [Standard Installation](#standard-installation)
    - [Windows Installation](#windows-installation)
    - [Windows Subsystem for Linux 2 Installation](#windows-subsystem-for-linux-2-installation)
    - [Redis Installation](#redis-installation)
6. [SSL in Development](#ssl-in-development)
7. [Login](#login)
8. [Testing](#testing)
9. [Maintainers](#maintainers)
10. [How to start and modify cron jobs](#how-to-start-and-modify-cron-jobs)
11. [Bundle Exec](#bundle-exec)
12. [Reply-by-email](#reply-by-email)
13. [Bugs and Support](#bugs-and-support)
14. [Recaptcha](#recaptcha)
15. [Internationalization](#internationalization)
16. [Security](#security)
17. [Developers](#developers)
18. [First Time?](#first-time)
19. [Hacktoberfest](#hacktoberfest)
****

## What makes this project different

The people who create our platform make very different design and technology decisions from other projects. This stems from our deep belief that, to see a change in the world, we must build and maintain systems that **reflect our values and principles.**

From design to system architecture to basic vocabulary and communication patterns, our systems have grown organically since 2010 to support a powerful, diverse, and cooperative network of people capable of taking on environmental problems that affect communities globally. The platform we have built together speaks to this shared history in many ways, big and small. It reflects input from people facing serious health issues, on-the-ground organizers, policy specialists, hardware hackers, educators, and civil servants.

This broad community and the Public Lab team have facilitated a space where we can discuss, break down, construct, prototype, and critique  real-world projects. Together we have shaped a platform that incorporates familiar pieces but ultimately looks and feels quite different from anything else on the internet. Despite the growth of our platform, it remains committed to hearing the voices of others, mutual respect and support, an awareness of the barriers and challenges presented by gaps in expertise and knowledge, and a sensitivity to the inequalities and power imbalances perpetuated by many mainstream modes of knowledge production and technological and scientific development.

Our mutual aim to democratize inexpensive and accessible do-it-yourself techniques has allowed us to create a collaborative network of practitioners who actively re-imagine the human relationship with the environment. Our goals are supported and facilitated by a system which questions and even challenges how collaborative work can happen.

## Data Model

![Diagram](https://user-images.githubusercontent.com/24359/50705765-d84ae000-1029-11e9-9e4c-f166a0c0d5d1.png)

_(Above: draft of our [Data model](https://github.com/publiclab/plots2/blob/main/doc/DATA_MODEL.md))_

## Contributing

We welcome contributions, and are especially interested in welcoming [first time contributors](#first-time). Read more about [how to contribute](#developers) below! We especially welcome contributions from people belonging to groups under-represented in free and open source software!

### Code of Conduct

Please read and abide by our [Code of Conduct](https://publiclab.org/conduct); our community aspires to be a respectful place both during online and in-­person interactions.

## Prerequisites

For installation, prerequisites include sqlite3 and rvm. [Click here for a complete list and instructions](https://github.com/publiclab/plots2/blob/main/doc/PREREQUISITES.md).

## Installation

### Standard Installation

1. Fork our repo from https://github.com/publiclab/plots2.
2. In the console, download a copy of your forked repo with `git clone https://github.com/your_username/plots2.git` where `your_username` is your GitHub username.
3. Enter the new **plots2** directory with `cd plots2`.
4. Set the upstream remote to the original repository url so that git knows where to fetch updates from in future: `git remote add upstream https://github.com/publiclab/plots2.git`
5. Steps to install gems:
    * You may need to first run `bundle install` if you have older gems in your environment from previous Rails work. If you get an error message like `Your Ruby version is 2.x.x, but your Gemfile specified 2.7.3` then you need to install the ruby version 2.7.3 using `rvm` or `rbenv`.
	    * Using **rvm**: `rvm install 2.7.3` followed by `rvm use 2.7.3`
	    * Using **rbenv**:  `rbenv install 2.7.3` followed by `rbenv local 2.7.3`
    * Run this `bundle config set without 'production mysql'` from the rails root folder to set your project to exclude libraries only needed in production.
    * Install gems with `bundle install` from the rails root folder.
6. Run `cp db/schema.rb.example db/schema.rb` to make a copy of `db/schema.rb.example` in `db/schema.rb`.
7. You could choose to use mysql2 or sqlite3 as your database. *We **recommend** using `sqlite3` as your plots2 database as some of our contributors have reported issues while using `mysql2`*.
    * If mysql2,  run `cp config/database.yml.mysql.example config/database.yml` to make a copy of `config/database.yml.mysql.example` in `config/database.yml`
    * If sqlite3, run `cp config/database.yml.sqlite.example config/database.yml` to make a copy of `config/database.yml.sqlite.example` in `config/database.yml`.
_kindly note if you choose to use sqlite some tests may fail. The project was setup initially to use mysql and some tests are tailored for mysql db. No need for alarm, we are working to fix these and this will not interfere with your development process_
8. Run `rake db:setup` to set up the database
9. Install static assets (like external javascript libraries, fonts) with `yarn install`
10. Setup React & webpacker by running `rails webpacker:install && rails webpacker:install:react && rails generate react:install`(for local SSL work, see [SSL](#ssl-in-development) below)
   * If you get any prompt to overwrite files in this step please choose no. The prompt will be something like _""Overwrite /home/plots2/config/webpacker.yml? (enter ""h"" for help) [Ynaqdhm]""_ :-  type ""n"" and enter.
11. Start the server with `passenger start` and navigate to `http://localhost:3000/` on your browser.
12. Wheeeee! You're up and running! Log in with test usernames ""user"", ""moderator"", or ""admin"", and password ""password"".
13. Run `rails test` to confirm that your install is working properly. You can also run `rails test:system` for system tests. (_Note: if you chose sqlite as your database, some tests may fail; Please ignore these, we are working to fix this. If your server starts correctly, you are all set_)

### Windows Installation

We recommend you either work in a virtual environment, or on a dual booted system to avoid dependencies issues as Unix systems tend to work smoother with Ruby and Rails. This will not only benefit you now for plots2, but also in the future while working on other Ruby projects, a Linux or Mac based OS will make your development much easier.
1. [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10) (recommended)
2. [Dual Booting](https://www.tecmint.com/install-ubuntu-alongside-with-windows-dual-boot/amp/), [option2](https://askubuntu.com/questions/1031993/how-to-install-ubuntu-18-04-alongside-windows-10), [video guide](https://www.youtube.com/watch?v=qNeJvujdB-0&fbclid=IwAR0APhs89jlNR_ENKbSwrp6TI6P-wxlx-a0My9XBvPNAfwtADZaAXqcKtP4)
3. [Setting up a Linux virtual env](https://itsfoss.com/install-linux-in-virtualbox/)

### Windows Subsystem for Linux 2 Installation

Before continuing with the installation steps in this README, users of Windows Subsystem for Linux 2 (WSL 2) should open the WSL 2 Terminal and type out the commands below.

1. Install Dependencies required from Ruby Source with the following commands:
  * `sudo apt update`
  * `sudo apt install curl g++ gcc autoconf automake bison libc6-dev`
  * `sudo apt install libffi-dev libgdbm-dev libncurses5-dev libsqlite3-dev libtool`
  * `sudo apt install libyaml-dev make pkg-config sqlite3 zlib1g-dev libgmp-dev`
  * `sudo apt install libreadline-dev libssl-dev`
2. Add GPG Key & Install RVM:
  * Install gnupg2 if you haven't already:  `sudo apt install gnupg2`
  * `gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB`
  * `curl -sSL https://get.rvm.io | bash -s stable`
3. Load the Script environment variables using source command:
  * `source ~/.rvm/scripts/rvm`
4. Install and use specific Ruby version:
  * `rvm install ruby-x.x.x (this projects Ruby version)`
  * `rvm --default use ruby-x.x.x (replace x.x.x with this project's Ruby version)`
5. For further reference, [read here](https://linuxize.com/post/how-to-install-ruby-on-ubuntu-20-04/)

### Redis Installation

Public Lab uses Redis and may be required for some functionality when running the application locally.
1. Install Redis if you haven't already:
  * Using **MacOS**: `brew install redis`
  * Using **Linux**: `sudo yum -y install redis`
  * Using **Ubuntu WSL2**:
  	* update & upgrade Ubuntu: `sudo apt update && apt upgrade`
	* install redis: `sudo apt install Redis-server`
	* open Redis.config file: `sudo nano /etc/redis/redis.conf`
	* update the file by changing the supervised no line to supervised systemd(ubuntu uses systemd)
	* start Redis: `sudo service redis-server start`
	* open Redis CLI: `redis-cli`
	* test Redis: type `ping` & response should be `pong`
	* exit cli: type `quit`
	* Awesome :thumbsup: All done :white_check_mark:
2. Run Redis server:
  * Using **MacOS**: `brew services start redis`
  * Using **Linux**: `redis-server`
3. Run SideKiq: `bundle exec sidekiq`
4. If SideKiq started correctly Redis is now configured and working!

## SSL in Development

At Public Lab we use the [openssl](https://github.com/ruby/openssl) gem to provide SSL (Secure Sockets Layer) for a secure connection (https) in the development mode. You can run the https connection on localhost through the following steps:
1. Use `passenger start --ssl --ssl-certificate config/localhost.crt --ssl-certificate-key config/localhost.key --ssl-port 3001`.
2. Open up https://localhost:3001.
3. Add security exceptions from the advance settings of the browser.
You can also use http (unsecure connection) on the port number 3000 by going to 'http://localhost:3000'. We use port number 3001 for 'https' and port number 3000 for 'http' connection.
Secure connection is needed for OAuth authentication etc.

## Login

Once you complete the installation, use any of these credentials to login into the PL website in your local development/testing environment to gain additional permissions for logged in users only. Each one comes with its own set of permissions; nevertheless, the experience across them is pretty much the same.

**username**: `admin`, `moderator`, or `user`

**password**: `password`

For more on the login systems, see [this page](https://github.com/publiclab/plots2/blob/b1c57446d016f8cd0ec149a75298711270e1643e/doc/LOGIN_SYSTEMS.md#how-to-setup-login-modal-on-various-locations)

## Testing

Click [here](https://github.com/publiclab/plots2/blob/main/doc/TESTING.md) for a comprehensive description of testing and [here](SYSTEM_TESTS.md) to learn about system tests.

## Maintainers

+ See [/doc/MAINTAINERS.md](https://github.com/publiclab/plots2/blob/main/doc/MAINTAINERS.md) for Public Lab's policy on feature maintainers!

## How to start and modify cron jobs

1. We are using [Whenever](https://github.com/javan/whenever) gem to schedule cron jobs.
2. All the cron jobs are written in easy ruby syntax using this gem and can be found in config/schedule.rb.
2. Go to the config/schedule.rb file to create and modify the cron jobs.
3. [Click here](https://github.com/javan/whenever) to learn more about how to write cron jobs.
4. After updating config/schedule.rb file run the command `whenever --update-crontab` to update the cron jobs.
5. To see the installed list of cron jobs use command `crontab -l`
6. For more details about this gem, visit the official repository of the [Whenever](https://github.com/javan/whenever) gem.


## Bundle exec

For some, it will be necessary to prepend your gem-related commands with `bundle exec`.
For example, `bundle exec passenger start`.
Adding `bundle exec` ensures you're using the version of passenger you just installed with Bundler.
`bundle exec rake db:setup`, `bundle exec rake db:seed` are other examples of where this might be necessary.


## Reply-by-email

Public Lab now supports ""reply by email to comment"" feature. For more details, go to the [email documentation](https://github.com/publiclab/plots2/blob/main/doc/EMAIL.md)


## Bugs and support

To report bugs and request features, please use the [GitHub issue tracker](https://github.com/publiclab/plots2/issues)

For additional support, join the Public Lab website and mailing list at http://publiclab.org/lists.
For urgent requests, email web@publiclab.org

## Recaptcha

This application uses RECAPTCHA via the recaptcha gem in production only. For more information, [click here](https://github.com/publiclab/plots2/blob/main/doc/RECAPTCHA.md).

## Internationalization

Publiclab.org now supports Internationalization and localization, though we are in the initial stages. This has been accomplished with [rails-I8n](https://github.com/svenfuchs/rails-i18n).

To see it in action, click on the 'Language' drop-down located in the footer section of the page. All the guidelines and best practices for I18n can be found [here](http://guides.rubyonrails.org/i18n.html).

Translations are arranged in the YAML files [here](https://github.com/publiclab/plots2/tree/main/config/locales), which are
set in a similar way to [views](https://github.com/publiclab/plots2/tree/main/app/views) files. An example for adding translations can be found [here](http://guides.rubyonrails.org/i18n.html#adding-translations).

Since the implementation of our new [Translation system](https://github.com/publiclab/plots2/issues/5737), we now use the `translation()` helper, [found here](https://github.com/publiclab/plots2/blob/438b649669b2029d01437bec9eb2826cf764851b/app/helpers/application_helper.rb#L141-L153). This provides some extra translation features such as inserting a prompt visible to site visitors if no translation exists yet. You can learn more about our translation system by reading our [Translation system docs](https://github.com/publiclab/plots2/blob/main/doc/Translation_System.md).

To add new languages or for additional support, please write to plots-dev@googlegroups.com

## Security

To report security vulnerabilities or for questions about security, please contact web@publiclab.org. Our Web Working Group will assess and respond promptly.

## Developers

Help improve Public Lab software!

* Join the plots-dev@googlegroups.com discussion list to get involved
* Look for open issues at https://github.com/publiclab/plots2/labels/help-wanted
* We're specifically asking for help with issues labelled with [help-wanted](https://github.com/publiclab/plots2/labels/help-wanted) tag
* Find lots of info on contributing at http://publiclab.org/wiki/developers
* Review specific contributor guidelines at http://publiclab.org/wiki/contributing-to-public-lab-software
* Some devs hang out in the [irc webchat](http://publiclab.org/chat)
* Join our [gitter chat](https://gitter.im/publiclab/publiclab)
* Try out some [supportive tasks](https://github.com/publiclab/plots2/wiki/Supportive-Tasks)
* Get involved with our weekly community check-ins. For guidelines: [https://github.com/publiclab/plots2/tree/main/doc/CHECKINS.md
](https://github.com/publiclab/plots2/tree/main/doc/CHECKINS.md)
* You can help us by opening first timers issues or fto. The template for opening an issue can be found at https://github.com/publiclab/plots2/blob/main/.github/ISSUE_TEMPLATE/--first-timers-only.md

## First Time?

New to open source/free software? Here is a selection of issues we've made **especially for first-timers**. We're here to help, so just ask if one looks interesting : https://code.publiclab.org

[Here](https://publiclab.org/notes/warren/11-22-2017/use-git-and-github-to-contribute-and-improve-public-lab-software) is a link to our Git workflow.

## Hacktoberfest
Wishing to contribute to Publiclab as part of Hacktoberfest? Check out our [Hacktoberfest contributing docs](https://github.com/publiclab/plots2/blob/main/HACKTOBERFEST.md)

## Let the code be with you.
### Happy opensourcing. :smile:





#### [Platforms that :heart: OSS](./doc/SUPPORTERS.md)

[![Twitter Follow](https://img.shields.io/twitter/follow/PublicLab?style=social)](https://twitter.com/PublicLab)



","'beginner-friendly', 'collaboration', 'environment', 'first-timers', 'hacktoberfest', 'help-wanted', 'javascript', 'publiclab', 'rails', 'ruby', 'wiki'",2024-04-22T16:53:28Z,60,951,67,"('jywarren', 2296), ('dependabotbot', 427), ('dependabot-previewbot', 400), ('ujithaperera', 227), ('david-days', 173), ('ananyo2012', 131), ('icarito', 115), ('SidharthBansal', 107), ('grvsachdeva', 81), ('Tlazypanda', 72), ('sagarpreet-chadha', 69), ('namangupta01', 62), ('ViditChitkara', 57), ('CleverFool77', 54), ('noi5e', 51), ('btbonval', 48), ('cesswairimu', 45), ('jiteshjha', 39), ('Manasa2850', 36), ('Souravirus', 35), ('nstjean', 33), ('lalithr95', 31), ('milaaraujo', 30), ('RuthNjeri', 29), ('VladimirMikulic', 28), ('siaw23-retired', 28), ('keshavsethi', 26), ('oorjitchowdhary', 25), ('Uzay-G', 25), ('IshaGupta18', 25), ('ryzokuken', 24), ('sirMackk', 22), ('stefannibrasil', 22), ('imajit', 21), ('gautamig54', 20), ('KarishmaVanwari', 19), ('divyabaid16', 18), ('mridulnagpal', 18), ('TildaDares', 18), ('sssash18', 17), ('17sushmita', 17), ('sashadev-sky', 16), ('kevinzluo', 16), ('shubhscoder', 15), ('Rishabh-Kumar-Bothra', 15), ('StlMaris123', 15), ('NitinBhasneria', 14), ('500swapnil', 14), ('seafr', 13), ('anirudhprabhakaran3', 12), ('aspriya', 11), ('Sreyanth', 11), ('digitaldina', 10), ('seabl', 9), ('Diksha2008', 9), ('sukhbir-singh', 9), ('sakshi-2412', 8), ('jonxuxu', 8), ('chen-robert', 8), ('kunalvaswani123', 7)","[13, 'Climate Action']"
letsdoitworld/World-Cleanup-Day,☀️ World Cleanup Day: App (React Native) & Platform (Node). Join us in building software for a cleaner planet! PRs welcome!,"![World Cleanup Day](https://www.letsdoitworld.org/wp-content/uploads/2017/04/header.png)
# World's Largest Civic Action: World Cleanup Day

Over 20 million trash heroes have already cleaned up more than 500 000 tonnes of trash in over 100 countries! The movement against pollution is growing fast and we're now building the World Cleanup App and Platform for the hundreds of millions of people people joining the World Cleanup Day on **15th of September 2018**. 

Together we will get rid of trash once and for all, save lives, improve health & reduce costs. Read below on how you can participate! Let's do it! 😃 

## 1: Download World Cleanup and Start Mapping Trash





Watch this video on how mapping works:

<a href=""https://www.youtube.com/watch?feature=player_embedded&v=YnPvOVzbQpA
"" target=""_blank""><img src=""https://s3.eu-central-1.amazonaws.com/lets-do-it-world/world-cleanup-day-app-video.png"" 
alt=""Let's Do It: World Cleanup Day!"" width=""100%"" border=""0"" />

![World Cleanup Day](https://www.letsdoitworld.org/wp-content/uploads/2017/04/content.png)

## 2: Help us Build the App

We need your help to make the app better! If you're a developer you can join us now by building the Share feature described below. Download the code and submit your ideas and pull requests. We're very happy to welcome you to the team!

![World Cleanup Day](https://s3.eu-central-1.amazonaws.com/letsdoitworld-gfx/github_poster.jpg)

Thank you for considering contributing to World Cleanup Day Mobile App! See the [CONTRIBUTING.md](https://github.com/letsdoitworld/World-Cleanup-Day/blob/master/CONTRIBUTING.md) for more details and please request access to the World Cleanup Day Asana.

## 3: Tell Your Friends!
Join the movement and let's clean up the World together!

* Facebook: https://www.facebook.com/worldcleanupday2018/
* Twitter: https://twitter.com/letsdoitworld/
* Instagram: https://www.instagram.com/worldcleanupday2018/
* YouTube: https://www.youtube.com/letsdoitworld/
* Flickr: https://www.flickr.com/letsdoitworld/
* Wikipedia: https://en.wikipedia.org/wiki/Let%27s_Do_It!_World
* Homepage: https://www.worldcleanupday.org/

## API
See the [API docs](http://ldiw-api.s3-website.eu-central-1.amazonaws.com/)

## Translations
See [translate.worldcleanupday.org](https://translate.worldcleanupday.org/ )

## Roadmap
See [Roadmap.md](https://github.com/letsdoitworld/World-Cleanup-Day/blob/master/ROADMAP.md)

## License

[GPL-3.0 license](https://opensource.org/licenses/GPL-3.0)
","'android', 'azure-storage', 'cleanup', 'cleanup-day', 'couchdb', 'environmentalism', 'ios', 'javascript', 'microservices-architecture', 'movement', 'ngo', 'nodejs', 'pollution', 'react', 'react-native', 'senecajs', 'swagger2', 'trash'",2022-08-19T04:56:08Z,16,110,16,"('krishaamer', 654), ('Volodymyr-Zhuravlov', 123), ('KucherenkoIhor', 122), ('BroshkovD', 59), ('mihaisaru', 32), ('bednyak', 11), ('lynxlynxlynx', 11), ('vsheremet92', 6), ('georg199041', 5), ('zuavra', 5), ('yalantisgeorge', 3), ('daniel-jirca', 2), ('densa', 1), ('gitter-badger', 1), ('SlavaPanevskiy', 1), ('severianremi', 1)","[11, 'Sustainable Cities and Communities']"
empirical-org/Quill-NLP-Tools-and-Datasets,Quill's library of open source NLP algorithms and data sets.,"# Background: NLP at Quill

At Quill, our mission is to enhance students' writing skills. Our focus is on two key areas: firstly, creating automated tools to analyze and identify the reasoning and argumentation in students' writing; secondly, developing systems to assess and correct grammar in student sentences. To achieve these objectives, we use Natural Language Processing (NLP), a specialized branch of Artificial Intelligence dedicated to the automated analysis and understanding of written language.

### Grammar Correction

In Quill's [Reading for evidence](https://www.quill.org/tools/evidence) tool, students read a nonfiction text and are asked to support a series of claims with evidence sourced from the text. First we want to check their responses for grammatical correctness. We're focusing in particular on a range of grammar errors that we frequently see in students’ writings, such as confusion between _it’s_ and _its_, between _than_ and _then_, between a possessive form (_year’s_) and a plural form of the same word (_years_), and subject-verb agreement errors. Our goal is to automatically spot these errors, so that we can inform students about them and ask them to correct the errors.

We develop machine learning models capable of automatically tagging words in a text with specific labels. For grammar correction, we train these models by exposing them to thousands of sentences where grammatical errors are pre-identified. The effectiveness of these models is then assessed based on their ability to detect errors in new, unlabeled sentences.

Machine learning models tend to perform better with more data, but the number of human-evaluated training sentences is limited. To deal with this challenge, we work with so-called synthetic data &mdash; sentences from sources like Wikipedia, where we’ve programmatically replaced a word with an incorrect alternative. For example, by replacing _it’s_ by _its_ in the sentence _it’s a sunny day_, we’ve automatically created a grammar error and we can teach our model what word in the sentence is incorrect.

Since around 2012, neural networks are the standard machine learning technique for solving this type of task. In the last few years, transformer models have emerged as the most popular type of neural network for language tasks. To train such models, we use spaCy, one of the most popular open-source NLP libraries.

This repository contains our code for generating synthetic data with the types of grammatical errors that we care about, and for creating a spaCy transformer model to find these errors automatically.

### Feedback

We are investigating generative AI models to help students develop strong argumentation skills. These models should give custom, targeted feedback to the arguments in students' responses, so that students strengthen their reading comprehension and hone their writing skills. This repository contains the code for our experiments with OpenAI's GPT models in particular. These experiments are focused on both prompt engineering, where we feed GPT a custom prompt with elaborate instructions and examples, and model finetuning, where we finetune a custom GPT model to give relevant feedback to students.

# Setup

All scripts have been tested with Python 3.11.6 and pip 23.2.1.

Different scripts in this repo rely on different pip packages. We currently use python's `virtualenv` standard library to manage dependencies.

Here's how to set up the (currently) two virtual envs:

```shell
python -m venv env-grammar
python -m venv env-gpt
```

Here's how to use a virtualenv in the context of running a script:

```shell
source env-myEnvName/bin/activate
python myScript
deactivate
```

# Grammar Correction: Technical Details

This repository contains the scripts for creating synthetic data and training a grammar model with spaCy.

### Grammar

Quill has developed a grammar pipeline that labels sentences with frequent grammar errors, such as subject-verb agreement errors and plural-possessive errors.
The goal is to give students feedback on their writing, so that they can correct grammatical errors.
This pipeline is a combination of simple rules and a statistical machine learning modeling model that is trained on a mix real student data along with synthetic data generated from grammar errors. This repository has the code for creating such synthetic grammar errors and preparing a training corpus for spaCy.

#### 1. Data

Option 1: Get existing training data

All grammar errors in the grammar model that are identified with a machine learning model already have synthetically generated data.
This data is stored in a Google Cloud bucket and can be pulled with our DVC account:

```
> dvc pull
```

The training data will be downloaded to the `data/training` directory of this repository.

Option 2: Generate synthetic data

Alternatively, it is possible to create new synthetic training data. Every grammar error has an `ErrorGenerator`
that takes an input sentence and inserts a synthetic error in that sentence (if possible). For example, the `SubjectVerbAgreementWithSimpleNounErrorGenerator`
takes a sentence and replaces the present verb by another verb form if the subject contains a simple noun. The README file in the directory `scripts/quillgrammar` contains more detailed information about this synthetic data generation.

The error generators can be run with the script `create_grammar_training_corpus.py`:

```bash
> export PYTHONPATH=.
> python scripts/quillgrammar/create_grammar_training_corpus.py \
path_to_newsoftheworld_corpus
```

It will generate a synthetic training file for each of the error generators that is called in the script.

Add this training data to the directory `data/training` and upload it to the Google Cloud with

```
> dvc commit
> dvc push
```

#### 2. SpaCy training corpus

We train our grammar model as a spaCy pipeline. As a result, we need to prepare a training and development corpus
that spaCy can work with. This is done in the script `prepare_spacy_grammar_corpus`.
This takes as its only argument the directory to which the corpus files will be written:

```bash
> export PYTHONPATH=.
> python scripts/quillgrammar/prepare_spaCy_grammar_corpus.py output_path
```

The list of synthetic error files that will be used for the corpus can be adapted in `scripts/quillgrammar/grammar_files.csv`.
This csv file contains a list of the error files that will be used, together with the number of training items and the number
of dev/test items that will be taken from the file. The more difficult the error, the more training (and dev/test) files we
collect.

This script has the following output:

- `/dev.spacy`: a development file on which the grammar model will be tested repeatedly during training
- `/test.spacy`: a test file that can be used for testing the grammar model after training
- `/train/*.spacy`: one or more training files on which the grammar model will train

#### 3. Training

Now the grammar model can be trained with spaCy's standard training command:

```
spacy train config_distilbert.cfg --output output_path \
--paths.train /train \
--paths.dev /dev.spacy \
--gpu-id 0
```

With `config_distilbert.cfg` as a configuration file, this command trains a model from scratch with the training corpus in `paths.train`. With `config_distilbert_add.cfg` as a configuration file, spaCy will load an already trained model from the directory `quillgrammar/models/current`, and continue training on the data in `paths.train`. This is convenient if an
existing model needs to be updated with some additional (e.g. manually labelled Quill) data. To create a training corpus
with new Quill data, format the data like the other training files (see for example `data/training/quill_labels_20231101_train.ndjson`), and rerun the previous step (`prepare_spacy_training_corpus.py`) with only the new
files in `grammar_files.csv`.

# Large Language Models for Student Feedback: Technical Details

Second, this repository contains all data and scripts for our experiments with Large Language Models for student feedback.
The goal of this task is to provide automatic feedback on the content of student responses.
The files with examples of human feedback are in `data/automl`, organized by passage and prompt. The scripts are in `scripts/gpt`.

### GPT scripts

There are several scripts for our experiments with GPT:

- `finetune.py`: finetune a GPT model with Quill's feedback
- `test_openai_for_feedback.py`: evaluate the output of a large language model against Quill's feedback
- `moderate_feedbac.py`: moderate GPT feedback by an additional GPT step that removes undesired elements

#### 1. Finetuning script

First, this repo contains a script to finetune a GPT-3.5-turbo model with Quill's human feedback. This can be done with the script `finetune.py`:

```
> pip install -r requirements-gpt.txt
> export OPENAI_API_KEY=
> python scripts/gpt/finetune.py .json
```

#### 2. Evaluation script

Second, it is possible to evaluate GPT-3.5, GPT-4 or a finetuned GPT model by comparing their feedback to Quill's human feedback, using `test_openai_for_feedback.py`:

```
> pip install -r requirements-gpt.txt
> export OPENAI_API_KEY=
> python scripts/gpt/test_openai_for_feedback.py  
```

For example:

```
> python scripts/test_openai_for_feedback.py gpt-3.5-turbo gpt3-5-turbo
```

#### 3. Moderation script

Finally, the moderation script is a basic script that calls a GPT model to moderate automatic feedback. This moderation step can be necessary when GPT gives feedback that does not focus on argumentation: comments on spelling or grammar, clarity or conciceness, or when GPT gives away the correct answer. The moderation script takes one or more pieces of feedback as input, asks the GPT model to remove any undesired elements, and writes the output to a file. It is used in the following way:

```
> python scripts/moderate_feedback.py   --verbose 
```

For example:

```
> python scripts/moderate_feedback.py gpt-4 feedback_output.csv --verbose False
```
",,2024-03-25T14:09:57Z,9,52,10,"('buckmaxwell', 429), ('yvespeirsman', 256), ('catherinealvarado', 67), ('Etang21', 14), ('ddmck', 13), ('dandrabik', 4), ('petergault', 2), ('happythenewsad', 2), ('brendanshean', 1)","[4, 'Quality Education']"
xiaoyudi-China/Yuudee-api,,"# Yuudee-api
",,2022-04-19T02:32:51Z,2,1,1,"('xiaoyudi-China', 13), ('liubiao1102', 1)","[4, 'Quality Education']"
GliaX/pulseox,Pulse oximetry,"Pulse Oximeter
==============

This project aims to create a research-validated pulse oximeter whose plans are available freely and openly. The goal is for the device to cost under USD$25 to produce.

Currently, **THE PULSE OXIMETER IS NOT USABLE AND SHOULD NOT BE USED IN A CLINICAL SETTING.** The current version can be used for laboratory testing only.


Status
======
The device is undergoing clinical calibration as of 2019 OCTOBER 2.

Installation
============
Clone the repository and then do the following to initialize the submodules:

```
git submodule init
git submodule update
```

Licensing notes
===============
As per our understanding, hardware is not covered by copyright. However, we present
our work under the TAPR OHL license insofar as it applies. All software is covered by the GNU aGPL V3

",,2019-10-11T20:36:50Z,6,43,39,"('tareko', 15), ('SpencerChambers', 14), ('kliment', 8), ('carewake', 3), ('awpavlos', 2), ('esteph4', 1)","[3, 'Good Health and Well-Being']"
opennorth/represent-canada,"Point or postcode to electoral district service for Canada, its provinces and municipalities","# Represent

[![Dependency Status](https://gemnasium.com/opennorth/represent-canada.png)](https://gemnasium.com/opennorth/represent-canada)

[Represent](https://represent.opennorth.ca) is the open database of Canadian elected officials and electoral districts. It provides a [REST API](https://represent.opennorth.ca/api/) to boundary, representative, and postcode resources.

This repository contains a master Django project, documentation, and a demo app. Code for the individual components of the API is in separate packages, which this project depends on:

* [represent-boundaries](https://github.com/opennorth/represent-boundaries)
* [represent-reps](https://github.com/opennorth/represent-reps)
* [represent-postcodes](https://github.com/opennorth/represent-postcodes)

There's also a package to provide colourful district map tiles:

* [represent-maps](https://github.com/tauberer/represent-maps)

## Getting Started

The following instructions are to setup your own instance of Represent. If you just want access to data, [please read our API documentation](https://represent.opennorth.ca/api/).

Follow the instructions in the [Python Quick Start Guide](https://github.com/opennorth/wiki/wiki/Python-Quick-Start%3A-OS-X) to install Homebrew, Git, Python, virtualenv, GDAL and PostGIS. The [deployment](deployment/) uses Python 3.5, PostgreSQL 9.6, and PostGIS 2.3.

Create a database and enable PostGIS (commands should be issued as user `postgres`):

    createdb represent
    psql -c ""CREATE EXTENSION postgis;"" represent

Install the project:

    pyvenv represent-env
    source represent-env/bin/activate
    git clone https://github.com/opennorth/represent-canada.git
    cd represent-canada
    pip install -r requirements.txt

Configure the `DATABASES` Django setting and and create the database tables:

    cp represent/settings.py.example represent/settings.py
    $EDITOR represent/settings.py
    python manage.py migrate

You can launch a development server with:

    python manage.py runserver

## Adding Data

[Download the data](https://github.com/opennorth/represent-canada-data), and then symlink `represent-canada-data` into the project directory:

    mkdir data
    ln -s /path/to/represent-canada-data/ data/shapefiles

To load the data into the API, see the boundaries, representatives, and postcodes packages.

## Maintenance

Read the [wiki](https://github.com/opennorth/represent-canada/wiki) and [deployment/](deployment/).

## Bugs? Questions?

This repository is on GitHub: [https://github.com/opennorth/represent-canada](https://github.com/opennorth/represent-canada), where your contributions, forks, bug reports, feature requests, and feedback are greatly welcomed.

Copyright (c) 2017 Open North Inc., released under the MIT license
",,2024-04-17T05:16:02Z,5,65,12,"('michaelmulley', 91), ('onyxfish', 30), ('jpmckinney', 19), ('jmejmejme', 6), ('dependabotbot', 5)","[16, 'Peace, Justice and Strong Institutions']"
public-accountability/littlesis-main,LittleSis development setup w/ docker & deployment instructions w/ ansible,"# littlesis-main

Welcome to the LittleSis project! LittleSis has been tracking powerful people and organizations since 2009. The repo [littlesis-main](https://github.com/public-accountability/littlesis-main) contains instructions for setting up a development environment for LittleSis.org.

Our code can be found under the github organization [public-accountability](https://github.com/public-accountability). [littlesis-rails](https://github.com/public-accountability/littlesis-rails) is the central rails repository for the project. [oligrapher](https://github.com/public-accountability/oligrapher) is our javascript/react mapping tool. [littlesis-browser-addon](https://github.com/public-accountability/littlesis-browser-addon) is a chrome browser extension to add relationships to the database. [pai-core-functionality](https://github.com/public-accountability/pai-core-functionality), [pai-packages](https://github.com/public-accountability/pai-packages), [pai](https://github.com/public-accountability/pai), [littlesis-packages](https://github.com/public-accountability/littlesis-packages), [littlesis-news-theme](https://github.com/public-accountability/littlesis-news-theme), and [littlesis-core-functionality](https://github.com/public-accountability/littlesis-core-functionality) are the themes and functionality for our two wordpress sites: [news.littlesis.org](https://news.littlesis.org) and [public-accountability.org](https://public-accountability.org/).

Our code is all open source, licensed with the General Public License version 3.0.

## Project History

Matthew Skomarovsky co-founded LittleSis and was the initial developer behind the project. When it started in 2009, LittleSis was a PHP [see here](https://github.com/littlesis-org/littlesis) for the original PHP application). The port to Ruby on Rails began in 2013 and finished in 2017.

Ziggy ([@aepyornis](https://github.com/aepyornis)) joined in 2016 and currently maintains the project.

Along the way, Austin ([@aguestuser](https://github.com/aguestuser)) worked on on oligrapher and the rails codebase. Liz ([@lizstarin](https://github.com/lizstarin)) helped port PHP code to rails and
developed the chrome extension. Pea ([@misfist](https://github.com/misfist)) coded our wordpress sites. Since 2020, Rob [@robjlucas](https://github.com/robjlucas) has contributed to the rails application.

LittleSis is a project of [The Public Accountability Initiative](https://public-accountability.org/), a non-profit public interest research organization focused on corporate and government accountability.

## Software Overview

|             |               |
|:-----------:|:-------------:|
| Application | Ruby on Rails |
| Database    | Postgresql    |
| Web Server  | Puma, Nginx   |
| Search      | Manticore     |
| Cache       | Redis         |
| Blog        | Wordpress     |
| OS          | Debian        |

Development requirements:

* docker, docker-compose, bash, git, gzip
* free disk space (> 20gb)

To install these requirements on debian use: `apt install bash git docker.io docker-compose gzip`

## Helper program: ./littlesis

There's a helper bash program that will let you easily interact with the docker containers to do common tasks such as starting the rails console, running tests, starting sphinx, and viewing logs without having to remember esoteric docker & bash commands.

To see the script's features run: ```./littlesis help ```

Although not necessary, it is suggested to create symlink for `littlesis`: `sudo ln -s ./littlesis /usr/local/bin/littlesis`

## Installation Steps

1) Clone this repo. `git clone https://github.com/public-accountability/littlesis-main`.

2) Clone the rails repo: `littlesis setup-clone`

3) Build the docker images  `littlesis build`

4) Start the docker containers: `littlesis up`

5) Setup the database:  `littlesis setup-database`

6) Load a copy of the development database `zcat littlesis_developemnt_db.sql.gz | littlesis psql`

7) Setup the testing database: `littlesis reset-test-db`

8) Compile test assets: `littlesis --test rake assets:precompile`

9) Run the tests: ` littlesis test `

The app is accessible at `localhost:8080` and `localhost:8081`. 8080 goes to directly to puma. 8081 is nginx.

The configurations for nginx and mariadb are in the  folder _docker/config_.

## Subsequent runs

Start app: `littlesis up`

Open rails console: `littlesis console`

*Manticore*
    - status: `littlesis rake ts:status`
    - start: `littlesis rake ts:start`
    - index: `littlesis rake ts:index`
    - reconfigure: `littlesis rake ts:rebuild`

Clear logs:  `littlesis rake log:clear`

Clear cache: `littlesis runner Rails.cache.clear`

Start DelayedJob: `littlesis delayed_job start`

Login as system user:

* username: `user1@email.com`
* password: `password`

Create new user: `littlesis script create_example_user.rb`

Reset user password:  `User.find_by(email: ).send_reset_password_instructions`

Update Network Map Collections: `littlesis rake maps:update_all_entity_map_collections`


To give yourself easy database access:

``` sql
create role  login;
grant all privileges on database littlesis to ;
```
","'ansible', 'docker', 'littlesis'",2022-01-07T15:37:45Z,5,12,7,"('aepyornis', 612), ('robjlucas', 12), ('lovemedicine', 7), ('aguestuser', 3), ('kconn', 1)","[16, 'Peace, Justice and Strong Institutions']"
LLK/scratchjr,"With ScratchJr, young children (ages 5-7) can program their own interactive stories and games.","## Overview
This is the official git repository hosting the source code for the
[ScratchJr](http://scratchjr.org/) project.

ScratchJr can be built both for iOS and Android.
A pure-web version is planned to follow at some point in the future.

Platform | Status
-------- | -------------
iOS      | Released in App Store
Android  | Released in Google Play

## Architecture Overview
The diagram below illustrates the architecture of ScratchJr and
how the iOS (functional), Android (functional) and pure HTML5 (future)
versions share a common client.

![Scratch Jr. Architecture Diagram](doc/scratchjr_architecture.png)


## Directory Structure and Projects
This repository has the following directory structure:

* src/ - Shared JavaScript code for iOS and Android common client. This is where most changes should be made for features, bug fixes, UI, etc.
* editions/ - Assembly directories for each ""flavor"" of ScratchJr. These symlink to src for common code, and could diverge in settings and assets.
  * free/ - Free edition JavaScript, including all shared code for all releases
* android/ - Android port of Scratch Jr. (Java, Android Studio Projects)
  * ScratchJr/ - Android Studio Project for ScratchJr Android Application
* bin/ - Build scripts and other executables
* doc/ - Developer Documentation
* ios/ - Xcode project for iOS build. (Make sure to open ScratchJr.xcworkspace not ScratchJr.xcodeproj)

## Building ScratchJr

### Initial setup

Regardless of whether you are doing iOS development or Android development, you should do these steps.

*These instructions assume you are building both versions on Mac OSX, with [Homebrew](http://brew.sh) installed.*

1. Clone or update the code for this repo
2. Ensure you have node and npm [installed](https://www.npmjs.com/get-npm).
3. Run sudo easy_install pysvg to install python svg libraries
4. Run brew install librsvg to install commandline `rsvg-convert`
5. Run brew install imagemagick to install commandline `magick`
6. In the top level of the scratchjr repo directory, install npm dependencies for bundling the JavaScript: npm install

### Analytics
ScratchJr uses the Firebase SDK to record analytics for the app. Scratch Team developers should look for
the configuration files in the Scratch Devs Vault. If you're not on the Scratch Team, then you'll need to
set up your own [app analytics](https://firebase.google.com/products/analytics) with Google Firebase. It's free. Firebase will generate the configuration files for you to download.

1. Place the `google-services.json` file in `editions/free/android-resources`
2. Place the `GoogleService-Info.plist` file in `editions/free/ios-resources`

### iOS

1. To build the iOS version, you need to have a Mac with Xcode
2. Run brew install cocoapods to install CocoaPods
3. Run pod install to install the Firebase Analytics dependencies
4. Open Xcode
5. In Xcode, open ios/ScratchJr.xcworkspace

### Android

1. Install or update Android Studio
2. In Android Studio, open the project android/ScratchJr
3. Choose the appropriate flavor/build variant in Android Studio

*Note: you can still do Android development on Ubuntu. Instead of the install commands above, run:*

1. sudo easy_install pysvg to install python svg libraries
2. sudo apt-get install librsvg2-bin to install rsvg-convert
3. sudo apt-get install imagemagick to install ImageMagick

## Where and how to make changes

All changes should be made in a fork. Before making a pull request, ensure all changes pass our linter:
* npm run lint

For more information, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Code credits
ScratchJr would not be possible without free and open source libraries, including:
* [Snap.svg](https://github.com/adobe-webplatform/Snap.svg/)
* [JSZip](https://github.com/Stuk/jszip)
* [Intl.js](https://github.com/andyearnshaw/Intl.js)
* [Yahoo intl-messageformat](https://github.com/yahoo/intl-messageformat)

## Acknowledgments
ScratchJr is a collaborative effort between:

* [Tufts DevTech Research Group](http://ase.tufts.edu/devtech/)
* [Lifelong Kindergarten group at MIT Media Lab](http://llk.media.mit.edu/)
* [Playful Invention Company](http://www.playfulinvention.com/)
* [Two Sigma Investments](http://twosigma.com)
",,2023-05-01T20:20:03Z,10,31,7,"('chrisgarrity', 213), ('tmickel', 133), ('yueyuzhao', 43), ('transifex-integrationbot', 38), ('benjiwheeler', 22), ('thisandagain', 12), ('kerrtravers', 3), ('Murkantilism', 2), ('yida921', 1), ('apple502j', 1)","[4, 'Quality Education']"
eKitabu/Accessible-EPUB-Toolkit,For new input entry data files and real-time frames and deliverables over the course of the project.,"# Resources for eKitabu Accessible Toolkit

This repository holds the software tools, EPUB templates, and EPUB samples to support eKitabu's Accessible EPUB Toolkit, locatated at http://toolkit.ekitabu.com 
Please visit the toolkit site for a Step by Step guide to creating accessible EPUBs and other resources. The Toolkit and related work in
this repository were made possible through the generous support of the All Children Reading: A Grand Challenge for Development (ACR GCD)
Partners (the United States Agency for International Development (USAID), World Vision, and the Australian Government) and Pearson. It was
prepared by eKitabu, LLC and does not necessarily reflect the views of Pearson and the ACR GCD Partners.
",,2021-10-14T16:31:54Z,2,3,6,"('brianagina', 153), ('mutterback', 15)","[4, 'Quality Education']"
heatseeknyc/heatseeknyc,HeatSeekNYC -- a web-enabled hardware platform to detect heating violations in NYC,"# Heat Seek NYC

## Description

We’re a team of New Yorkers who believes no New Yorker should have to spend the winter in a frigid apartment. The reality is, this happens to thousands of us every year, creating a public health hazard and some serious animosity between tenants and landlords.

At Heat Seek NYC, we’re using temperature sensors to empower tenants, landlords, community organizations, and the justice system to tackle our city’s heating crisis. We:

- Provide unbiased evidence to verify heating code abuse claims in housing court
- Help landlords heat their buildings more effectively while reducing costs
- Create transparency in heating data to educate the community and inform housing policy
- Our affordable temperature sensors can be installed in any number of apartments per building. They talk to each other via mesh network to periodically collect and transmit ambient temperature data to Heat Seek NYC’s servers. Our powerful web app integrates this data with public 311 heating complaint information to deliver a better picture of New York City’s heating crisis than ever before.

We are working closely with community organizations, landlords, and the HPD to make our technology available to thousands of New Yorkers in time for the cold.

## The Process

Heat Seek staff and volunteers install the temperature sensors at the beginning of heat season (Oct 1 - May 31), and they remain in place throughout the winter. The temperature sensors monitor the temperature by taking a reading once per hour. Readings are transmitted via 3G internet to our web app, where they are recorded in the tenant’s account. The web app incorporates the outdoor temperature, the time of day, and time of year in order to identify whether or not a building is in violation of NYC housing code. Tenants and their advocates can access our web app at any time to view their readings and can download heat logs for use in tenant-landlord negotiations and/or housing court.

## Screenshots

![dashboard screenshot](app/assets/images/readme-assets/dashboard_screenshot.png)

![search screenshot](app/assets/images/readme-assets/dashboard_advocate.png)

## Development/Contribution

If you'd like to contribute to the code base, please submit a pull request. See our [Contribution Guide](CONTRIBUTING.md).

## Development Environment Setup

Be sure you have Postgres installed and running on your machine, then run these commands from the project's root directory:

```bash
bundle install
createuser -s -r twinedb_development_user
rake db:create
rake db:migrate
rake db:seed
```

You can login as a Super User to view admin pages:

```
email: super-user@heatseeknyc.com
password: 33west26
```

or you can login as Jane Doe for the more limited tennant experience:

```
email: jane@heatseeknyc.com
password: 33west26
```

## Adding new sensor IDs

Example of adding new sensor IDs 251-400.

### On the rails app

```
heroku run rails c -a heatseek-prod
(251..400).each{|i| puts ""feather0#{i.to_s}""; Sensor.create(name: ""feather0#{i.to_s}"")};0
```

### On the relay server

```
heroku pg:psql -a hs-relay-prod

hs-relay-prod::DATABASE-> DO $$
BEGIN
FOR counter IN 251..400 LOOP
insert into cells values (concat('feather0', counter::text), 'n/a');
END LOOP;
END; $
$
;
```

## Rake Tasks

There are a couple rake tasks that require API tokens.

The data scrubber task requires a Google Geocode API token, which can be generated [here](https://developers.google.com/maps/documentation/geocoding/get-api-key).
It also requires a Geoclient App id and token, which can be generated [here](https://developer.cityofnewyork.us/api/geoclient-api).

```
rake data:scrub_data
```

The weather update task requires a Wunderground API token, which can be generated [here](https://www.wunderground.com/weather/api).

```
rake weather:update
```

This project uses the [Dotenv](https://github.com/bkeepers/dotenv) gem to store environment variables so add a `.env.development.local` file to the root of this project with the following:

```
GEOCODE_API_KEY=
WUNDERGROUND_KEY=
GEOCLIENT_APP_ID=
GEOCLIENT_APP_KEY=
```

## License

This application is MIT Licensed. See [LICENSE](LICENSE.txt) for details.
",,2023-04-14T16:46:22Z,18,31,12,"('williamcodes', 817), ('tsiege', 285), ('bolandrm', 172), ('ramillim', 87), ('eozelius', 38), ('kronosapiens', 13), ('jayhamm', 11), ('ericskiff', 9), ('gabrie30', 7), ('hrldcpr', 7), ('alexgraffeocohen', 5), ('kthffmn', 4), ('hoffm', 3), ('cromulus', 1), ('jhack32', 1), ('mapmeld', 1), ('danaavesar', 1), ('joanieS', 1)","[10, 'Reduced Inequalities']"
CenterForOpenScience/osf.io,Facilitating Open Science,"# OSF

The code for [https://osf.io](https://osf.io).

- `master` ![Build Status](https://github.com/CenterForOpenScience/osf.io/workflows/osf.io/badge.svg?branch=master)
- `develop` ![Build Status](https://github.com/CenterForOpenScience/osf.io/workflows/osf.io/badge.svg?branch=develop)
- Versioning Scheme:  [![CalVer Scheme](https://img.shields.io/badge/calver-YY.MINOR.MICRO-22bfda.svg)](http://calver.org)
- Issues: https://github.com/CenterForOpenScience/osf.io/issues?state=open
- COS Development Docs: http://cosdev.readthedocs.org/

## Running the OSF For Development

To run the OSF for local development, see [README-docker-compose.md](https://github.com/CenterForOpenScience/osf.io/blob/develop/README-docker-compose.md).

Optional, but recommended: To set up pre-commit hooks (will run
formatters and linters on staged files):

```
pip install pre-commit

pre-commit install --allow-missing-config
```

## More Resources

The [COS Development Docs](http://cosdev.readthedocs.org/) provide detailed information about all aspects of OSF development.
This includes style guides, process docs, troubleshooting, and more.
","'django', 'django-rest-framework', 'javascript', 'openscience', 'python', 'science'",2024-05-03T14:51:13Z,30,663,57,"('sloria', 12286), ('pattisdr', 3564), ('mfraezz', 3072), ('jmcarp', 2629), ('erinspace', 2530), ('samchrisinger', 2403), ('brianjgeiger', 2284), ('caneruguz', 2220), ('chrisseto', 1972), ('icereval', 1835), ('chennan47', 1728), ('samanehsan', 1552), ('Johnetordoff', 1407), ('Ghalko', 716), ('rliebz', 707), ('billyhunt', 686), ('lyndsysimon', 667), ('alexschiller', 651), ('cslzchen', 558), ('fabianvf', 496), ('abought', 461), ('laurenbarker', 449), ('caseyrollins', 421), ('HarryRybacki', 413), ('fabmiz', 395), ('huangginny', 334), ('felliott', 283), ('atelic', 226), ('kushG', 157), ('JeffSpies', 157)","[17, 'Partnerships for the Goals']"
open-contracting/standard,Documentation of the Open Contracting Data Standard (OCDS),"# Open Contracting Data Standard

Visit [standard.open-contracting.org](https://standard.open-contracting.org) to read the standard's documentation.

Visit the [OCDS Standard Development Handbook](https://ocds-standard-development-handbook.readthedocs.io/en/latest/standard/) for developer documentation about the standard.

## Maintenance

Install [OCDS Kit](https://pypi.org/project/ocdskit/)

Update the examples in `docs/examples/merging`:

```shell
cat docs/examples/merging/deletions/field_tender*.json | ocdskit --pretty compile --package --versioned --schema schema/release-schema.json --published-date 2013-07-30T09:00:10.000Z > docs/examples/merging/deletions/field_record.json
cat docs/examples/merging/deletions/object_tender*.json | ocdskit --pretty compile --package --versioned --schema schema/release-schema.json > docs/examples/merging/deletions/object_record.json
cat docs/examples/merging/deletions/array_award*.json | ocdskit --pretty compile --package --versioned --schema schema/release-schema.json > docs/examples/merging/deletions/array_record.json
```

Update the examples in `docs/examples/change_history`:

```shell
cat docs/examples/change_history/tender.json | ocdskit --pretty compile --published-date 2010-03-15T09:30:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/tender.json
cat docs/examples/change_history/{tender,tenderUpdate}.json | ocdskit --pretty compile --published-date 2010-03-20T09:45:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/tenderUpdate.json
cat docs/examples/change_history/{tender,tenderUpdate,award}.json | ocdskit --pretty compile --published-date 2010-05-10T09:30:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/award.json
cat docs/examples/change_history/{tender,tenderUpdate,award,contract}.json | ocdskit --pretty compile --published-date 2010-06-10T10:30:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/contract.json
cat docs/examples/change_history/{tender,tenderUpdate,award,contract,implementation}.json | ocdskit --pretty compile --published-date 2011-01-10T09:30:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/implementation.json
cat docs/examples/change_history/{tender,tenderUpdate,award,contract,implementation,contractAmendment}.json | ocdskit --pretty compile --published-date 2011-04-05T13:30:00Z --uri https://standard.open-contracting.org/examples/records/ocds-213czf-000-00001.json --package --versioned --schema schema/release-schema.json > docs/examples/change_history/records/contractAmendment.json
```
",,2024-04-16T15:34:10Z,30,135,36,"('jpmckinney', 1353), ('duncandewhurst', 371), ('yolile', 302), ('Bjwebb', 240), ('JachymHercher', 219), ('ColinMaudry', 150), ('practicalparticipation', 130), ('timgdavies', 127), ('odscjen', 92), ('michaeloroberts', 42), ('kindly', 35), ('romifz', 31), ('odscrachel', 26), ('edugomez', 25), ('pindec', 14), ('choldgraf', 11), ('Ravf95', 9), ('nativaldezt', 8), ('dincamihai', 8), ('mrshll1001', 7), ('Camilamila', 4), ('odscjames', 4), ('mgax', 3), ('avian2', 3), ('EricReese15', 2), ('foobacca', 2), ('aborruso', 1), ('gabelula', 1), ('Inova-MPRJ', 1), ('ngm', 1)","[16, 'Peace, Justice and Strong Institutions']"
OpenEnergyDashboard/OED,Open Energy Dashboard (OED),"# Open Energy Dashboard #

![Github Build](https://github.com/OpenEnergyDashboard/OED/workflows/Build/badge.svg)

Open Energy Dashboard is a user-friendly way to display energy information from smart energy meters or uploaded via CSV files. It is available to anyone and is optimized for non-technical users with a simple interface that provides access to your organization's energy data. To learn more, see [our website](https://openenergydashboard.github.io/).

Open Energy Dashboard is available under the Mozilla Public License v2, and contributions, in the form of bug reports, feature requests, and code contributions, are welcome.

## Installation and Usage ##

See [USAGE.md](USAGE.md).

## Built With ##

Plotly.js - JavaScript library used to generate data charts ([plotly.com](https://plotly.com/javascript/))

PostgreSQL - Database management system ([postgresql.org](https://www.postgresql.org))

Node.js - JavaScript runtime environment ([nodejs.org](https://nodejs.org/en/))

and lots of other software/packages.

## Authors ##

This application has been developed by many volunteer developers (mostly students) and is an independent open source project.

For a list of contributors, [click here](https://github.com/OpenEnergyDashboard/OED/graphs/contributors)

## Licensing ##

This project is licensed under the MPL version 2.0.

See the full licensing agreement [here](LICENSE.txt)

## Contributions ##

We welcome others to contribute to this project by writing code for submission or collaborating with us. Before contributing, please sign our [Contributor License Agreement](https://openenergydashboard.github.io/developer/cla.html). Web pages with [information for developers](https://openenergydashboard.github.io/developer/) is available. If you have any questions or concerns feel free to at engage@OpenEnergyDashboard.org.

## Code of Conduct ##

OED is based on the idea of sharing so everyone benefits from our combined efforts. To benefit everyone, we need to maintain a welcoming and appropriate community. 
Toward that end, OED has as a [code of conduct](CODE_OF_CONDUCT.md) that follows the [Contributor Covenant](https://www.contributor-covenant.org/) used by many
open source projects. It specifies
our expectations and how to report any concerns. If, for any reason, you do not feel comfortable with any aspect of the OED project then you are encouraged to 
contact the OED leadership so we can work to make this a welcoming community. This includes formal complaints, informal concerns and suggestions for how OED can
improve what it does. We commit to a timely response to input that clearly articulates what actions may
be taken and why. Whatever our decision, you will be informed. We want everyone to know that we take having a welcoming community seriously and will work for and
respond appropriately to any concern or ideas for improvement.

## Security Concerns ##

If you think there is a security concern in the OED software, the please visit our [security reporting page](SECURITY.md) for information on reporting it to the OED project.

## Contact ##

To contact us, see our [contact web page](https://openenergydashboard.github.io/contact.html), send an email to info@OpenEnergyDashboard.org or open an issue on GitHub.
","'climate-change', 'education', 'environmental', 'javascript', 'nodejs', 'open-source', 'plotly', 'postgresql'",2024-05-02T16:10:19Z,30,70,18,"('huss', 1385), ('lindavin', 383), ('johndiiorio', 344), ('NoraCodes', 331), ('jameeters', 197), ('ChrisMart21', 183), ('simonbtomlinson', 170), ('truongdd03', 165), ('buip', 146), ('jina2k', 145), ('ak476519', 119), ('spearec', 98), ('truonghh99', 87), ('dependabotbot', 83), ('LucasFares', 75), ('andrew-cline', 63), ('LouisLu705', 53), ('Deelane', 51), ('wootent', 49), ('TrongQuocLe', 44), ('LukeLavan', 42), ('RagingOcelot', 41), ('limh0228', 38), ('carlsonrob', 37), ('Jake-Bodin', 37), ('JustLe56', 31), ('alfarj83', 29), ('ngan-cassie', 27), ('wadoodalam', 25), ('kooolbreez1', 25)","[7, 'Affordable and Clean Energy']"
onaio/mspray,Mspray,"mSpray
=======

mSpray is a mobile based IRS tool developed by Akros with Ona

Setup
-----

**INSTALLATION**

------------------------------------------------------------------------

**Get the Code**

------------------------------------------------------------------------

```
git clone git@github.com:onaio/mspray.git

```

**Docker**

------------------------------------------------------------------------

You need to have Docker and Docker Compose installed on your machine. 
Run the following commands to start the containers in the background and leave them running.
The option -d runs in detach mode(runs the containers in the background).

```

docker-compose up -d

```


Running docker-compose logs -f, could be useful when you need to follow the log output from services.

```

docker-compose logs -f

```


You can use `docker exec` command to start a shell inside the running container. For example:


```

docker exec -it mspray_web_1 bash


```  

The above command starts the bash shell inside the `mspray_web_1` container. 


It should now be accessible via:

```

http://localhost:8000.

```

**For Development Use-cases** 

The following packages need to be installed to set up a development environment. 
Note, this is unnecessary for those who do not need to run tests while setting up.


For you to run the following command, you must have Pipenv installed:

```

pipenv install --dev

```


Use `python manage.py test` to now run tests.


**Setting up Postgres**

Once you have docker running and are able to get into a container shell environment, you will need to apply the superuser privileges for the mspray database user. 
Use the `postgres` user to grant privileges to the `mspray` user.

```

psql -h db -U postgres

```

Within on your database configuration, the postgres user must have sufficient privileges to run tests i.e. the user needs to have the ability to create testing databases. 

The following command grants the postgres user role with superuser priviledges. 

```

ALTER USER mspray WITH SUPERUSER;

```

You can  now load some fixtures to your mspray dashboard, this gives you the chance to explore the mspray dashboard.

```

python manage.py load_location_shape_file mspray/apps/main/tests/fixtures/Lusaka/districts/Lusaka.shp NAME district --parent-skip=yes


python manage.py load_location_shape_file mspray/apps/main/tests/fixtures/Lusaka/HF/Mtendere.shp HFC_NAME RHC --parent=DISTRICT --parent-level=district


python manage.py load_location_shape_file mspray/apps/main/tests/fixtures/Lusaka/SA/Akros_1.shp SPRAYAREA ta --parent=HFC_NAME --parent-level=RHC --structures=HOUSES


```

The above three commands will load the districts, health facility catchment areas and spray area locations from the shape files provided.


```

python manage.py load_osm_hh mspray/apps/main/tests/fixtures/Lusaka/OSM/

```

The load_osm_hh command Loads OSM files as households into the Household model. This then loads the structures found in the region.


Finally use this command to update location structure numbers for districts/regions within the target areas.

```

python manage.py update_locations_structures

```



Data Flow
---------

- An IRS Form submission is sent from ODK Collect to ona.io
- The IRS Form in ona.io has a web service hook configured to transmit the data to the dashboard e.g zambia-2016.mspray.onalabs.org which runs this codebase.
- On receiving the submission:
    - the submission is saved in the SprayDay table
    - the submission will then be linked to a location, these are the options considered in order
        - the OSM file is downloaded and parsed, the OSM information is looked up to see which spray area it falls within, when found the location is the spray area of the submission
        - If that fails, we determine if the area is a new structure, if it is a new structure we lookup the GPS to see which spray area it falls in, if found that becomes the spray area for the submission
        - If that fails, we look up the district and spray area from the submission data corresponding fields, this will then be used for the location of the spray area
        - If that fails, then the submission cannot be linked to a spray area
    - Once a submission has been linked to spray area, a unique record will be created using the spray area, the OSMID or the GPS in the event the submission only captured a GPS.
    - In the event a unique record already existed, the spray status will be checked, if the spray status was 'not sprayed', the new submission will be linked to the unique record, the old record therefore does not become part of the unique record.

![data_flow](https://user-images.githubusercontent.com/11174326/49940782-9bd80880-fef1-11e8-8ad4-f3ec41c5c00e.png)


Spray Area Calculations
-----------------------

|**Variable** | **Aggregations**         |
| ------------- | ----------- |
| Enumerated Structures             | The number of structures as they are in the spray area shape file.|
| Not sprayable structures          | The number of structures in the field that were found not to be sprayable.     |
| Duplicate sprayed structures      | When a structure has been reported more than one time with the spray status sprayed (if                                         ID XYZ appears twice as sprayed, the count of duplicates will be 1, if it appears 3 times                                       the count will be 2, ... etc.)     |
| Structures on the ground     | Enumerated structures subtract the number of not sprayable structures + number of new                                          structures + number of duplicate structures that have been sprayed.     |
| Found     | Number of all unique records that are sprayable add Number of structures that were sprayed that are not part of                 the unique record set (i.e the number of duplicates.)     |
| Visited Sprayed     | Number of all unique records that have the spray status ""sprayed"".     |
| Spray Effectiveness     | The percentage of  Visited Sprayed / Structures on the ground.     |
| Found Coverage     | The percentage of Found / Structures on the ground.     |
| Sprayed Coverage     | The percentage of Visited Sprayed / Found.     |

",,2019-06-20T22:16:24Z,9,3,32,"('ukanga', 1101), ('moshthepitt', 380), ('ivermac', 202), ('wmucheru', 111), ('WinnyTroy', 76), ('mberg', 10), ('bmarika', 1), ('mlberg', 1), ('pld', 1)","[3, 'Good Health and Well-Being']"
kobotoolbox/kpi,"kpi is the (frontend) server for KoboToolbox. It includes an API for users to access data and manage their forms, question library, sharing settings, create reports, and export data. ","# KPI

[![Python Build Status](https://github.com/kobotoolbox/kpi/workflows/pytest/badge.svg)](https://github.com/kobotoolbox/kpi/actions?query=workflow%3Apytest)
[![Python Coverage Status](https://coveralls.io/repos/github/kobotoolbox/kpi/badge.svg?branch=master)](https://coveralls.io/github/kobotoolbox/kpi?branch=master)
[![JavaScript Build Status](https://github.com/kobotoolbox/kpi/workflows/npm-test/badge.svg)](https://github.com/kobotoolbox/kpi/actions?query=workflow%3Anpm-test)

We're open for [contributions](./CONTRIBUTING.md)!

## Important notice when upgrading from any release older than [`2.020.18`](https://github.com/kobotoolbox/kpi/releases/tag/2.020.18)

Prior to release [`2.020.18`](https://github.com/kobotoolbox/kpi/releases/tag/2.020.18), this project (KPI) and [KoBoCAT](https://github.com/kobotoolbox/kobocat) both shared a common Postgres database. They now each have their own. **If you are upgrading an existing single-database installation, you must follow [these instructions](https://community.kobotoolbox.org/t/upgrading-to-separate-databases-for-kpi-and-kobocat/7202)** to migrate the KPI tables to a new database and adjust your configuration appropriately.

If you do not want to upgrade at this time, please use the [`shared-database-obsolete`](https://github.com/kobotoolbox/kpi/tree/shared-database-obsolete) branch instead.

## Python Dependencies

Python dependencies are managed with `pip-compile` and `pip-sync` from the [`pip-tools`](https://github.com/jazzband/pip-tools/) package. The dependencies are listed in [`dependencies/pip/`](./dependencies/pip/), with core requirements in [`dependencies/pip/requirements.in`](./dependencies/pip/requirements.in). You may use `pip` directly with one of the compiled `dependencies/pip/*.txt` files, but consider using instead the `pip-sync`. **Do not** add new dependencies directly to the *compiled* `dependencies/pip/*.txt` files; instead, update the relevant the *source* `dependencies/pip/*.in` file(s), and execute `./pip-compile.sh` after any changes. You can pass arguments to `pip-compile.sh` with e.g. `./pip-compile.sh --upgrade-package=xlwt`.

## Downloading and compiling the translations

* Pull the submodule into the `locale` directory with `git submodule update`.
* Optionally configure transifex to pull the latest translations into the `locale` directory with `tx pull --all`
* Run `python manage.py compilemessages` to create `.mo` files from the `.po` files.
* To test out locales in the interface, double click ""account actions"" in the left navbar, use the dropdown to select a language, and refresh.

## Searching

Results from the `tags` and `api/v2/assets` endpoints can be filtered by a
Boolean query specified in the `q` parameter. For example:
`api/v2/assets?q=owner__username:meg AND name__icontains:quixotic` would return
assets whose owner has the username ""meg"" (case sensitive) and whose name
contains ""quixotic"" anywhere (case insensitive). For more details about the
syntax, see the documentation at the top of
[kpi/utils/query_parser/query_parser.py](./kpi/utils/query_parser/query_parser.py).

## Admin reports

There are several types of data reports available to superusers. 
* Full list of users including their details provided during signup, number of deployed projects (XForm count), number of submissions, date joined, and last login: `/superuser_stats/user_report/`. File being created is a CSV, so don't download immediately to wait for server to be finished writing to the file (it will download even if incomplete).
* Monthly aggregate figures for number of forms, deployed projects, and submissions (from kobocat): `//superuser_stats/`

## Django Admin Interface

As this is a Django project, you may find the admin panel at `/admin` useful, e.g. to configure user accounts or log in as other users without knowing their passwords. You must use a superuser account to access the admin panel.

## Icons

All project icons are kept in `jsapp/svg-icons/`. Adding new icon requires adding new `svg` file here and regenerating icons with `npm run generate-icons`. Filenames are used for icon font classnames, e.g. `.k-icon-arrow-last` for `arrow-last.svg` (please use kebab-case). You can see all available icons in Storybook by running `npm run storybook`.

## Supported Browsers

See [browsers list config](./.browserslistrc)
",,2024-05-03T02:46:58Z,29,122,30,"('magicznyleszek', 4795), ('noliveleger', 2772), ('jnm', 2118), ('dorey', 1313), ('duvld', 1075), ('pmusaraj', 979), ('LMNTL', 683), ('joshuaberetta', 536), ('JacquelineMorrissette', 363), ('p2edwards', 354), ('bufke', 348), ('RuthShryock', 166), ('srartese', 93), ('jamesrkiger', 49), ('arsea', 43), ('tinok', 21), ('Yann-J', 14), ('paulschreiber', 6), ('jeverling', 4), ('m4tx', 4), ('gushil', 3), ('ksamuel', 3), ('rhunwicks', 2), ('avdata99', 1), ('shyamkumaryadav', 1), ('mdestaubin', 1), ('russbiggs', 1), ('shadimh', 1), ('wetbadger', 1)","[17, 'Partnerships for the Goals']"
codeforamerica/etpl-search,Mobile-optimized site for job seekers to find and compare training programs that are eligible for tuition assistance from the government workforce system.,"# 👨🏻‍🏫👩🏿‍🚒👩🏽‍⚕️&nbsp; Find the Best Training

### Mobile-optimized site to help job seekers find high-quality training that is eligible for publicly-funded tuition assistance.

This project is an experiment to increase usability and data presentation of the [New Jersey ETPL](http://www.njtrainingsystems.org/) in order to support job seekers in choosing a training program.

⚡️ [View Live Site](https://findtraining.org)

![screenshot](https://github.com/codeforamerica/etpl-search/blob/master/readme/screenshot-compressed.png)

Every state provides a list of training programs that a job seeker can get help paying for with a subsidy from the public workforce system. This list is generally called an Eligible Training Provider List (ETPL).

For people struggling to escape the cycle of low-wage work, subsidized training or paid apprenticeships can dramatically increase opportunities for higher-wage work.

States publish different amounts of data about programs on their ETPL. Some include employment rates, wages, and long-term wage growth, while others provide just titles and descriptions. For example, the [New Jersey ETPL](http://www.njtrainingsystems.org/) exposes a lot of data about each program.","'codeforamerica-workforce', 'codeforamerica-workforce-2018', 'jobs', 'workforce-development'",2022-02-15T21:28:54Z,3,2,6,"('leomancini', 32), ('bengolder', 6), ('bensheldon', 3)","[8, 'Decent Work and Economic Growth']"
sozialhelden/wheelmap-classic,":wheelchair: Legacy ""classic"" wheelmap.org (deprecated)","# Classic Wheelmap.org (deprecated)

**This repository is not maintained anymore. The new GitHub repository is here: https://github.com/sozialhelden/wheelmap-react-frontend**

[![Build Status](https://travis-ci.org/sozialhelden/wheelmap.svg?branch=master)](https://travis-ci.org/sozialhelden/wheelmap)
[![Stories in Next](https://badge.waffle.io/sozialhelden/wheelmap.png?label=next&title=Next)](https://waffle.io/sozialhelden/wheelmap)

Wheelmap.org is an online map to search, find and mark wheelchair-accessible places. Get involved by marking public places like bars, restaurants, cinemas or supermarkets!

This repository is the source code of the old website [classic.wheelmap.org](http://classic.wheelmap.org/).

### Development Environment

A `Vagrantfile` is bundled for convenience. It is the easiest way to get started testing or developing wheelmap. It will set up the application entirely and configure it for use inside the box. Please note it imports data for Berlin only to say time.

To use this just navigate into the Wheelmap directory and run `vagrant up`. After it is completed you can enter the box with `vagrant ssh`. You'll find a current copy of the repository at `/vagrant`, `cd` into it then you can start the server with `bundle exec rails server`, visit it at `localhost:3000`.

**If you use the `vagrant` box you do not need to worry about the rest of the setup below It is all done for you.**

## I. Installation

### Requirements

- Ruby 2.2.2
- Bundler
- MySQL 5.6 (< 5.7)
- ImageMagick
- PhantomJS
- Node >= 4.1
- npm


Before you start, please make sure you have [Homebrew](http://brew.sh/) for Mac or [aptitude](http://packages.ubuntu.com/search?keywords=aptitude) for Ubuntu installed.

#### Install essential software packages:

MacOS

```
$ brew install git wget
```

Ubuntu

```
$ sudo apt-get update
$ sudo apt-get install -y git wget curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt1-dev libcurl4-openssl-dev python-software-properties libffi-dev libgeos-dev libproj-dev libgdal-dev
```

#### Install Ruby 2.2.2:

MacOS / Ubuntu

First, install the Ruby Package Manager of your choice:

- [rbenv](https://github.com/rbenv/rbenv)
- [rvm](https://rvm.io/rvm/install)

Dependent on your choice, please install Ruby with either:

```
$ rbenv install 2.2.2
$ rbenv rehash
$ rbenv local 2.2.2
```

or:

```
$ rvm install 2.2.2
$ rvm use 2.2.2
```

Note: If you need more infos about how to install rbenv, rvm or Ruby, please take a look at our [wiki page](https://github.com/sozialhelden/wheelmap/wiki/Install-rbenv,-rvm-&-ruby).


#### Install Bundler:

MacOS / Ubuntu

```
$ gem install bundler
```

#### Install MySQL:

MacOS

```
$ brew install mysql
```

Ubuntu

```
$ sudo apt-get update
$ sudo apt-get install -y libmysqlclient-dev mysql-server-5.6

```
Note: Please make sure to install a mysql-server version that is before `v5.7` due to some geometry issues.

#### Install ImageMagick:

MacOS

```
$ brew install imagemagick
```

Ubuntu
```
$ sudo apt-get install -y imagemagick
```

#### Install PhantomJS:

PhantomJS is a testing framework for headless testing.

MacOS

```
$ brew install phantomjs   # via Homebrew

or:
$ port install phantomjs   # via MacPorts
```

Ubuntu

```
$ sudo npm install -g phantomjs-prebuilt
```

#### Clone the app from Github:

```
$ git clone https://github.com/sozialhelden/wheelmap.git --depth 1
$ cd wheelmap
$ bundle install --path vendor/bundle
```

## II. Getting started

##### Copy the example `secrets.yml`:

```
$ cp config/secrets.sample.yml config/secrets.yml
```

##### Copy the example openstreetmap config:

```
$ cp config/open_street_map.SAMPLE.yml config/open_street_map.yml
```

##### Copy the example environment variable config file:

```
$ cp .env.sample .env
```

##### Copy the example database config and edit accordingly:

```
$ cp config/database.SAMPLE.yml config/database.yml
```

##### Edit `database.yml` to reflect your current database settings.

##### Now lets create the actual database and prepare minimal data:

```
$ bundle exec rake db:create:all
```

##### Then log into the mysql server and pipe the `structure.sql` file into the database you want to use, for example:

```
$ mysql -u root wheelmap_development < db/structure.sql
```

##### Run the rake task to seed data:

```
$ bundle exec rake db:seed
```

##### And get some POI data into the database:

```
$ wget http://download.geofabrik.de/europe/germany/berlin-latest.osm.bz2
$ bzcat berlin-latest.osm.bz2 | bundle exec rake osm:import
```

##### Install all JavaScript dependencies:

MacOS

```
$ brew install node
$ npm install -g npm
$ npm install
```

Ubuntu

```
$ sudo apt-get install -y nodejs
$ sudo apt-get install -y npm
$ sudo npm install -g npm
```

##### Finally startup a local rails server

```
$ bundle exec rails server
```

And visit the website in your browser: `http://0.0.0.0:3000`

## III. Documentation

Please also check our [wiki](https://github.com/sozialhelden/wheelmap/wiki), if you need more informations to specific topics and can't find them here, e.g. how to generate a sprite or how to test our app.


## IV. Code of Conduct

We refer to the [Berlin Code of Conduct](http://berlincodeofconduct.org/) and friendly ask all contributors and people involved to comply with it.

## V. License

The Wheelmap Software is released under the [GNU Affero General Public License v3.0](/LICENSE).
","'accessibility', 'javascript', 'map', 'open-source', 'rails', 'ruby', 'wheelchair'",2022-02-26T01:34:34Z,16,47,13,"('christoph-buente', 2202), ('lennerd', 588), ('1000miles', 161), ('freundchen', 83), ('opyh', 28), ('juni0r', 26), ('hagenburger', 24), ('joustava', 22), ('holgerd', 9), ('spacekookie', 8), ('justahero', 7), ('yeralin', 2), ('skade', 2), ('BriocheBerlin', 1), ('Svenyo', 1), ('thermann78', 1)","[10, 'Reduced Inequalities']"
LibreHealthIO/lh-ehr,LibreHealth EHR - Free Open Source Electronic Health Records,"# LibreHealth EHR

[![pipeline status](https://gitlab.com/librehealth/ehr/lh-ehr/badges/master/pipeline.svg)](https://gitlab.com/librehealth/ehr/lh-ehr/commits/master)

LibreHealth EHR is a free and open-source electronic health records and medical practice management application.

The mission of LibreHealth is to help provide high quality medical care to all people, regardless of race, socioeconomic status, or geographic location, by providing medical practices and clinics across the globe access to free of charge medical software. That same software is designed to save clinics both time and money, which gives practitioners more time to spend with individual patients, thereby supplying patients with higher quality care.

We are current and former contributors to OpenEMR and thank that community for years of hard work. We intend to honor that legacy by allowing this new community to leverage the good things in OpenEMR, share what we create and not be afraid to break backward compatibility in the name of forward progress and modern development models.

We are collaborating closely with the [LibreHealth Project](http://LibreHealth.io), an umbrella organization for health IT projects with similar goals.

# Contributing code

Code contributions are very welcome! We encourage newcomers to browse the [issue tracker](https://github.com/LibreHealthIO/LibreEHR/issues) for open issues and/or if you have found a bug in LibreEHR, please [create a new issue](https://github.com/LibreHealthIO/LibreEHR/issues/new) for same. You may open a [pull request](https://github.com/LibreHealthIO/LibreEHR/pulls) to contribute your code to an issue, from your fork of the [LibreEHR repository](https://github.com/LibreHealthIO/LibreEHR).

# Docker Support

Docker images are now pushed to [GitLab's container registry](https://gitlab.com/librehealth/ehr/lh-ehr/container_registry) built using [GitLab CI](https://gitlab.com/librehealth/ehr/lh-ehr/pipelines).

Currently the image uses PHP 7.2 and Apache. We plan to support more than one version of PHP, which will be published at a later date.

For the database in the docker-compose files, we use MariaDB

See [the Docker README](./docker/README.md) for more information.

# Installation

For detailed, step-by-step instructions, refer to [Installation Instructions](/INSTALL.md)

## On Windows:

1. First off, make sure that you have the [WAMP](http://www.wampserver.com/en/) or [XAMPP](https://www.apachefriends.org/index.html) server installed and that the time zone is set correctly.

2. Make the following changes in `php.ini` file. You can find the `php.ini` file by looking at the following destination:
..* In the case of WAMP :
`C:/WAMP/BIN/PHP/php.ini` OR (left click)  wampmanager icon -> PHP -> php.ini
..* In the case of XAMPP:
`C:\xampp\php\php.ini.`.
In Linux, it is located in:
`/etc/php/7.0/php.ini` or something similar.

Make the following changes in your php.ini file:
(Search for the following and make necessary changes)

```
max_execution_time = 600
max_input_time = 600
max_input_vars = 3000
memory_limit = 512M
post_max_size = 32M
upload_max_filesize = 32M
session.gc_maxlifetime = 14400
short_open_tag = On
display_errors = Off
upload_tmp_dir is set to a correct default value that will work on your system
error_reporting = E_ALL & ~E_DEPRECATED & ~E_STRICT & ~E_NOTICE
```

3. Make sure you have disabled strict mode in MySQL.

### How to disable Mysql strict mode?

Make the following changes in the `my.ini/my.cnf`:
Find it here `C:\WAMP\BIN\MYSQL\MySQL Server 5.6\my.ini` OR `C:\xampp\mysql\bin\my.ini`
OR (left click ) wampmanager icon -> MYSQL -> my.ini
In Linux it's typically located in /etc/mysql

    1.  Look for the following line:
        sql-mode = STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION
        or sometimes it maybe sql_mode

    2.  Change it to:
        sql-mode="""" (Blank)

    3. Restart the MySQL service.


4. Restart WAMPP/XAMPP Server.

(XAMPP)
 If you don't find this parameter (sql-mode/sql_mode) in the my.ini file, you should run the server, open http://localhost/phpmyadmin/, click on the ""variables"" tab, search for ""sql mode"", and then set it to: """"

5. You can fork & clone the repository for local development. To get started you need to:
 - Fork the [LibreEHR repository](https://github.com/LibreHealthIO/LibreEHR).
 - Clone your fork of LibreEHR repository to your local machine. Your fork would be on, as an example, `https://github.com/your-github-username/LibreEHR`
 - Open LibreEHR directory and run index.php file, which will then redirect to the setup page! Follow the [instructions](/INSTALL.md/#windows-setup) and you are done!

Note: Sometimes , installation may take more time than usual on some systems. In that case, you would need to increase `max_execution_time` in your php.ini file and then restart your server.

# License

LibreHealth EHR is primarily licensed under Mozilla Public License Version 2. The code inherited from OpenEMR is licensed under GPL 2 or higher. This project is a part of the [Software Freedom Conservancy](http://sfconservancy.org) family.

***Thank you for your support!***
","'openemr', 'php'",2023-09-09T09:33:00Z,30,225,36,"('bradymiller', 696), ('teryhill', 338), ('sunsetsystems', 207), ('aethelwulffe', 178), ('yehster', 125), ('azwreith', 70), ('muarachmann', 60), ('kchapple', 57), ('nileshprasad137', 45), ('apooravc', 36), ('Ngai-E', 35), ('robertogagliotta', 34), ('Trodrige', 34), ('MigDinny', 32), ('kevmccor', 31), ('naveen17797', 29), ('dependabotbot', 26), ('Dev2-PracticeProvider', 22), ('Jdew192837', 19), ('rreddy70', 17), ('robbyoconnor', 17), ('jonathanzhang53', 17), ('stephen-smith', 17), ('tajemo', 15), ('epsdky', 14), ('punwai', 14), ('arnabnaha', 14), ('stephenwaite', 12), ('Perseus', 12), ('apmuthu', 11)","[3, 'Good Health and Well-Being']"
OptiKey/OptiKey,OptiKey - Full computer control and speech with your eyes,"# OptiKey

OptiKey is an on-screen keyboard that is designed to help Motor Neuron Disease (MND) patients interact with Windows computers. When used with an eye-tracking device, OptiKey's on-screen keyboard allows MND patients to complete tasks, such as email composition, using only their eyes. OptiKey can also be used with a mouse or webcam. Unlike expensive and unreliable Alternative and Augmentative Communication (AAC) products that are difficult to use, Optikey is free, reliable, and easy to use.

# Getting Started

[**The OptiKey Wiki**](https://github.com/OptiKey/OptiKey/wiki) contains OptiKey's user guides, installation and system requirements, and additional support information. OptiKey's Windows installer can be downloaded from the [latest release](https://github.com/JuliusSweetland/OptiKey/releases/latest). To get an understanding of OptiKey's use, users should watch [Optikey's introduction video](https://www.youtube.com/watch?v=HLkyORh7vKk).

# Supported Platforms

OptiKey uses the .Net 4.6 Framework and is designed to run on Windows 8 / 8.1 / 10.

# Problems?

If users encounter an issue with OptiKey, such as a software crash or an unexpected behaviour, users should add an issue ticket to [OptiKey's issue tracker](https://github.com/OptiKey/OptiKey/issues). Users are encouraged to check if their issue is already being tracked by OptiKey before creating a new issue ticket.

The following information should be specified in an issue ticket:

* How OptiKey was being used
* What the user expected to happen
* What the user actually experienced
* Steps to reproduce the issue
* Any other information that helps to describe and/or reproduce the problem (ex. screenshots)

# License

Licensed under the GNU GENERAL PUBLIC LICENSE (Version 3, 29th June 2007)

# Contributing

Contributions to this project are always welcome. If you'd like to help translate Optikey to another language, fix bugs or add new features, check out our [Contributing page](https://github.com/OptiKey/OptiKey/wiki/Contribute). 

# Contact

To ask a question, or to discuss information that is not on the [**OptiKey Wiki**](https://github.com/JuliusSweetland/OptiKey/wiki/), please use  or  to contact Julius.
","'aac', 'accessibility', 'accessible', 'als', 'assistive-technology', 'c-sharp', 'communication', 'disabilities', 'disabilty', 'eye-tracker', 'eye-tracking', 'eyes', 'eyetracking', 'mnd', 'optikey', 'screen-keyboard'",2024-05-03T10:45:43Z,30,4253,204,"('JuliusSweetland', 2240), ('kmcnaught', 727), ('zelmon64', 147), ('AdamRoden', 97), ('Chopinsky', 66), ('kevinlin18', 30), ('annakirkpatrick', 25), ('samgled', 24), ('illialarka', 21), ('mostlyjason', 18), ('alexandre-mbm', 18), ('petermorlion', 17), ('andylamp', 15), ('ninehundred1', 14), ('Ar2rZ', 13), ('N1Official', 13), ('SamPlusPlus', 13), ('rschiefer', 12), ('hugodahl', 12), ('PeterOeClausen', 10), ('ervacaninb', 9), ('LeeCampbell', 8), ('feugy', 8), ('baljakbratislav', 7), ('utacub', 6), ('A01221624', 6), ('alandbm', 6), ('bswearingen', 5), ('dptucunduva', 5), ('TheGreenAirplane', 5)","[3, 'Good Health and Well-Being']"
OpenAgricultureFoundation/openag-device-software,Software for running controlled grow environments on Linux / OSX machines and embedded linux devices such as Beaglebones and Raspberry Pis.,"# OpenAg Device Software
Software for running controlled grow environments on embedded linux devices such Raspberry Pi and Beaglebone. 

## Overview
This software is designed to be used on any embedded linux devices. 
It currently supports the Beaglebone, Raspberry Pi, and Standalone Linux-machines with a usb-to-i2c dongle.
It can easily be adaped to a new platform such as a Dragonboard 410C or an Orange Pi.
The two main parts of the code base are the device threads and on-device app. 
The device threads coordinate recipes, control loops, and peripheral (sensor/actuator) interactions. 
The on-device django-based app coordinates the interactions with the on-device database and hosts a local device UI and API. 
There is also an MQTT-based IoT manager for communication with the OpenAg cloud service.

## Introductory Videos
1. [Introduction](https://www.youtube.com/watch?v=RByKZJ7bDx8&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=1)
2. [Getting Started](https://www.youtube.com/watch?v=M3rPBoFnRuo&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=2)
3. [Architecture Overview](https://www.youtube.com/watch?v=tYYAANnXESI&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=3) 
4. [Device Overview](https://www.youtube.com/watch?v=lotOETQ6RsQ&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=4)
5. [App Overview](https://www.youtube.com/watch?v=2YWZdtC_ApQ&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=5)
6. [Data Overview](https://www.youtube.com/watch?v=DeByYZ-9yeI&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=6)
7. [Scripts Overview](https://www.youtube.com/watch?v=glc1fmoQOr4&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=7)
8. [Tests, Type Checks, Coding Conventions](https://www.youtube.com/watch?v=USms_7X83aE&list=PL7dmhIGxrpXE0EEOFxz7wbVOLJXYAMqE0&index=8)

## Installation Instructions
 - [Raspberry Pi 3 Production](docs/install/install_raspberry_pi_3_production.md)
 - [Raspberry Pi 3 Development](docs/install/install_raspberry_pi_3_development.md)
 - [Raspberry Pi 3 Source](docs/install/install_raspberry_pi_3_source.md)
 - [Beaglebone Black Wireless Production](docs/install/install_beaglebone_black_wireless_production.md)
 - [Beaglebone Black Wireless Development](docs/install/install_beaglebone_black_wireless_development.md)
 - [Beaglebone Black Wireless Source](docs/install/install_beaglebone_black_wireless_source.md)
 - [Linux Machine (e.g. Ubuntu Laptop or Fanless PC)](docs/install/install_linux_machine.md)

## Image Creation Instructions
 - [Create Image for Raspberry Pi 3](docs/install/create_raspberry_pi_3_image.md)
 - [Create Image for Beaglebone Black Wireless](docs/install/create_beaglebone_black_wireless_image.md)

## Contributing Instructions
See [Contributing](docs/contributing.md) for links to our forum and wiki.

## Design Documents
 - [Code Structure Diagram](docs/code_structure.png)
 - [System Architecture Diagram](docs/iot_architecture.jpg)
 - [Light Control](docs/light/overview.md)
 - [Aeration](docs/aeration.md)

## Development
See [USB to I2C communication cable](docs/usb_i2c_cable/USB-I2C.md) for I2C development notes.
",,2022-06-21T21:30:03Z,3,193,42,"('rbaynes', 136), ('srmoore', 96), ('jakerye', 1)","[12, 'Responsible Consumption and Production']"
openfoodfoundation/openfoodnetwork,"Connect suppliers, distributors and consumers to trade local produce.","[![Build](https://github.com/openfoodfoundation/openfoodnetwork/actions/workflows/build.yml/badge.svg)](https://github.com/openfoodfoundation/openfoodnetwork/actions/workflows/build.yml)

# Open Food Network

The Open Food Network is an online marketplace for local food. It enables a network of independent online food stores that connects farmers and food hubs (including co-ops, online farmers markets, independent food businesses, etc) with individuals and local businesses. It gives farmers and food hubs an easier and fairer way to distribute their food.

Supported by the Open Food Foundation and a network of global affiliates, we are proudly open source and not-for-profit - we're trying to seriously disrupt the concentration of power in global agri-food systems, and we need as many smart people working together on this as possible.

We're part of global movement - get involved!

* Join the conversation [on Slack][slack-invite]. Make sure you introduce yourself in the #general channel and join #dev for all tech-related topics.
* Head to [https://openfoodnetwork.org](https://openfoodnetwork.org) for more information about the global OFN project.
* Check out the [User Guide](https://guide.openfoodnetwork.org/) for a list of features and tutorials.
* Join our [discussion forum](https://community.openfoodnetwork.org).

## Contributing

If you are interested in contributing to the OFN in any capacity, please introduce yourself [on Slack][slack-invite], and have a look through the [OFN Handbook][ofn-handbook].

Our [GETTING_STARTED](GETTING_STARTED.md) and [CONTRIBUTING](CONTRIBUTING.md) guides are the best place to start for developers looking to set up a development environment and make contributions to the codebase.

### Hacktoberfest :tada:

Are you participating in [Hacktoberfest](https://hacktoberfest.digitalocean.com/)? Go check out our [Welcome New Developers project board][welcome-dev]! We have curated all issues we consider to be a good starting point for new members of the community and categorized them by skills and level of complexity. 
Have a look and pick the one you would prefer working on!

## Provisioning

If you're interested in provisioning a server, see [ofn-install][ofn-install] for the project's Ansible playbooks.

We also have a [Super Admin Guide][super-admin-guide] to help with configuration of new servers.

## Testing

If you'd like to help out with testing, please introduce yourself on the #testing channel on [Slack][slack-invite]. Also, do have a look in our [Welcome New QAs board][welcome-qa] for some good first issues, both on manual and automated testing (RSpec/Capybara).

We use [BrowserStack](https://www.browserstack.com/) as a manual testing tool. BrowserStack provides open source projects with unlimited and free of charge accounts. A big thanks to them!


We use [KnapsackPro](https://knapsackpro.com/) for optimal parallelisation of our automated tests. KnapsackPro offers unlimited plans for non-commercial open source projects, like ours - a big thanks to them!

![image](https://user-images.githubusercontent.com/49817236/201330047-e64147a7-d91c-4c10-bd4d-ca519d8fe945.png)


## Licence

Copyright (c) 2012 - 2024 Open Food Foundation, released under the AGPL licence.

[survey]: https://docs.google.com/a/eaterprises.com.au/forms/d/1zxR5vSiU9CigJ9cEaC8-eJLgYid8CR8er7PPH9Mc-30/edit#
[slack-invite]: https://join.slack.com/t/openfoodnetwork/shared_invite/zt-9sjkjdlu-r02kUMP1zbrTgUhZhYPF~A
[ofn-handbook]: https://ofn-user-guide.gitbook.io/ofn-handbook/
[ofn-install]: https://github.com/openfoodfoundation/ofn-install
[super-admin-guide]: https://ofn-user-guide.gitbook.io/ofn-super-admin-guide
[welcome-dev]: https://github.com/orgs/openfoodfoundation/projects/5
[welcome-qa]: https://github.com/orgs/openfoodfoundation/projects/6
","'farmers', 'food', 'food-hubs', 'hacktoberfest', 'nonprofit', 'rails', 'ruby', 'sustainable-consumption'",2024-05-03T11:47:19Z,30,1061,71,"('luisramos0', 3629), ('Matt-Yorkley', 3528), ('mkllnk', 3314), ('RohanM', 2657), ('oeoeaio', 2428), ('sauloperez', 1773), ('jibees', 1739), ('Transifex-Openfoodnetwork', 1275), ('dependabotbot', 1221), ('filipefurtad0', 1066), ('willrjmarshall', 994), ('dacook', 938), ('summerscope', 812), ('andrewpbrett', 803), ('rioug', 462), ('kristinalim', 405), ('abdellani', 291), ('drummer83', 233), ('cillian', 199), ('rafaqz', 194), ('stveep', 172), ('dependabot-previewbot', 170), ('macanudo527', 166), ('apricot12', 121), ('HugsDaniel', 110), ('alexs333', 103), ('RachL', 99), ('andrewspinks', 96), ('bingxie', 95), ('chahmedejaz', 93)","[12, 'Responsible Consumption and Production']"
energy-data/market-opportunities,The Offgrid Market Opportunities tool developed by IFC and the World Bank.,"# Offgrid Market Opportunities tool
Identify market opportunities for off-grid energy services in Sub-Saharan Africa.

## Development environment
To set up the development environment for this website, you'll need to install the following on your system:

- Node (v4.2.x) & Npm ([nvm](https://github.com/creationix/nvm) usage is advised)

> The versions mentioned are the ones used during development. It could work with newer ones.

After these basic requirements are met, run the following commands in the website's folder:

```
$ npm install
```

## Gulp for building

There are two commands, both run via npm.

- `npm run build` or `gulp` - clean & build everything and put it into dist folder
- `npm run serve` or `gulp serve` - Compiles the sass files, javascript, and launches the server making the site available at `http://localhost:3000/`. The system will watch files and execute tasks whenever one of them changes. The site will automatically refresh since it is bundled with livereload.

## standard for linting
We're using [standard](https://github.com/feross/standard) for linting.

- `npm run lint` - will run linter and warn of any errors.


### Other commands

- `npm run collections` - create icon font",,2017-04-27T16:28:08Z,5,8,7,"('drewbo', 110), ('ricardoduplos', 57), ('olafveerman', 52), ('anandthakker', 7), ('danielfdsilva', 3)","[13, 'Climate Action']"
ctt-gob-es/clienteafirma,Cliente @firma,"# Cliente @firma

El Cliente @firma es uno de los productos de la Suite @firma de soluciones de identificación y firma electrónica. Se proporciona de a las Administraciones Públicas para que dispongan de los instrumentos necesarios para implementar la autenticación y firma electrónica avanzada de una forma rápida y efectiva.

El cliente de firma es una herramienta de firma electrónica en entornos de escritorio y dispositivos móviles, que funciona en forma de Applet de Java integrado en una página Web mediante JavaScript, como aplicación de escritorio, o como aplicación móvil, dependiendo del entorno del usuario.

Es software libre con licencia GPL 2+ y EUPL 1.1. Puede consular más información y el código del producto en la forja del CTT.

## Construcción del Cliente @firma

Los módulos del Cliente @firma se encuentran preparados para su compilación y empaquetado mediante Apache Maven. A continuación se indican los distintos parámetros a utilizar para construir sus artefactos según el uso que se desee dar.

A cualquiera de los comandos que se indican se le puede agregar el parámetro `-DskipTests` para omitir los tests JUnit.

### Módulos básicos

Los módulos del Cliente @firma incluidos en este repositorio se pueden construir mediante el siguiente comando de Maven.

`mvn clean install`

Este comando generará todos los módulos básicos del proyecto.

### Artefactos desplegables y aplicaciones

Para la construcción de AutoFirma (JAR) y los servicios que utiliza será necesario usar el perfil `env-install`. Este se puede activar mediante el comando:

`mvn clean install -Denv=install`

Con esto, se podrán construir los artefactos:

* `afirma-server-triphase-signer`: WAR con el servicio para la generación de firmas trifásicas.
* `afirma-signature-retriever`: WAR con el servicio de recuperación de datos del servidor intermedio.
* `afirma-signature-storage`: WAR con el servicio de guardado de datos en el servidor intermedio.
* `afirma-simple`: JAR autoejecutable de AutoFirma (`AutoFirma.jar`).
* `afirma-ui-simple-configurator`: JAR autoejecutable del configurador necesario para la instalación de AutoFirma (`AutoFirmaConfigurador.jar`).

### Despliegue en repositorio de artefactos

Para el despliegue de los distintos módulos en un repositorio de artefactos, además de la construcción de los los propios artefactos, es necesario aportar el código fuente de la aplicación, su JavaDoc y firmar los distintos artefactos. Para evitar generar estos recursos y realizar la firma de los artefactos para la operativa ordinaria de compilación y empaquetado se ha creado un perfil `env-deploy` para que se utilice sólo cuando se va a proceder al despliegue de los artefactos en un repositorio. Se puede hacer eso mediante el comando:

`mvn clean deploy -Denv=deploy`

## Módulos del proyecto

El proyecto está formado por múltiples módulos, algunos de los cuales se utilizan en varias de las aplicaciones del Cliente @firma. Otros son los módulos de las propias aplicaciones o con recursos necesarios para su construcción o su uso.

### Módulos vigentes

A continuación, se muestra un listado de los distintos módulos actualmente en uso en el proyecto:

* `afirma-core`: Módulo con los componentes principales del proyecto.
* `afirma-core-keystores`: Módulo con las clases de gestión de almacenes de claves de usuario.
* `afirma-core-massive`: Módulo con funcionalidades para la ejecución de operaciones masivas de firma.
* `afirma-crypto-batch-client`: Módulo con el componente cliente para la invocación de las operaciones de firma de lote en servidor.
* `afirma-crypto-cades`: Módulo con la lógica de generación de las firmas CAdES (excluidas cofirmas y contrafirmas) y ASiC-CAdES.
* `afirma-crypto-cades-multi`: Módulo con la lógica de generación de las cofirmas y contrafirmas CAdES.
* `afirma-crypto-cadestri-client`: Módulo con lógica de invocación para la generación de firmas trifásicas CAdES en servidor.
* `afirma-crypto-cms`: Módulo con la lógica de generación de las firmas CMS.
* `afirma-crypto-core-pkcs7`: Módulo con la lógica básica de estructuras PKCS#7, necesarias para la generación de firmas ASN.1 (CAdES, PAdES, etc.).
* `afirma-crypto-core-xml`: Módulo con la lógica básica de estructuras XML, necesarias para la generación de firmas XML (XAdES, ODF, OOXML, etc.).
* `afirma-crypto-odf`: Módulo con la lógica de generación de las firmas ODF.
* `afirma-crypto-ooxml`: Módulo con la lógica de generación de las firmas OOXML.
* `afirma-crypto-padestri-client`: Módulo con lógica de invocación para la generación de firmas trifásicas PAdES en servidor.
* `afirma-crypto-pdf`: Módulo con la lógica de generación de las firmas PAdES.
* `afirma-crypto-pdf-common`: Módulo con recursos comunes utilizados en los módulos que operan sobre firmas PDF.
* `afirma-crypto-validation`: Módulo con la lógica de verificación de la integridad de las firmas CAdES, PAdES y XAdES (no incluye la comprobación de la validez de los certificados).
* `afirma-crypto-xades`: Módulo con la lógica de generación de las firmas XAdES, ASiC-XAdES y FacturaE.
* `afirma-crypto-xadestri-client`: Módulo con lógica de invocación para la generación de firmas trifásicas XAdES y de FacturaE en servidor.
* `afirma-crypto-xmlsignature`: Módulo con la lógica de generación de las firmas XMLdSig.
* `afirma-keystores-filters`: Módulo con los filtros de certificados utilizados por AutoFirma.
* `afirma-keystores-mozilla`: Módulo para la gestión del almacén de claves de Mozilla Firefox.
* `afirma-server-triphase-signer`: Módulo principal del servicio de firma trifásica y de lotes.
* `afirma-server-triphase-signer-cache`: Módulo con la interfaz que define las operaciones de guardado y recuperación de datos de caché del servidor trifásico.
* `afirma-server-triphase-signer-core`: Módulo con la funcionalidad básica de firma trifásica CAdES, PAdES, XAdES y de FacturaE.
* `afirma-server-triphase-signer-document`: Módulo con la interfaz que define las operaciones de guardado y recuperación de documentos para firmar del servidor trifásico.
* `afirma-signature-retriever`: Módulo principal del servicio de recuperación del servidor intermedio.
* `afirma-signature-storage`: Módulo principal del servicio de guardado del servidor intermedio.
* `afirma-simple`: Módulo principal de la aplicación AutoFirma.
* `afirma-simple-installer`: Módulo con los componentes para la generación de los instaladores de AutoFirma.
* `afirma-simple-plugin-hash`: Módulo con el plugin de AutoFirma para generación y validación de hashes.
* `afirma-simple-plugin-hash-exe`: Módulo de la aplicación EXE para el registro de las entradas de generación y validación de hashes en el menú contextual de Windows.
* `afirma-simple-plugin-validatecerts`: Módulo con el plugin de AutoFirma para validación de firmas.
* `afirma-simple-plugins`: Módulo con los recursos base para la implementación de plugins de AutoFirma.
* `afirma-ui-core-jse`: Módulo con las interfaces gráficas genéricas usadas por las distintas aplicaciones del Cliente @firma.
* `afirma-ui-core-jse-keystores`: Módulo con la interfaz gráfica del diálogo de selección de certificados.
* `afirma-ui-miniapplet-deploy`: Módulo principal para el desarrollo de AutoScript.
* `afirma-ui-simple-configurator`: Módulo principal de la aplicación de configuración ejecutada durante la instalación de AutoFirma.

### Módulos sin mantenimiento

La lista de módulos obsoletos y/o sin soporte que se conservan en el repositorio son los siguientes:

* `afirma-crypto-cipher`: __Obsoleto.__ Módulo con las clases para el cifrado sincrono y asíncrono de datos usado en el antiguo Applet de @firma y StandAlone.
* `afirma-crypto-cms-enveloper`: __Obsoleto.__ Módulo con la lógica para la generación de sobre digitales CMS utilizada en los antiguos Applet de @firma y StandAlone.
* `afirma-crypto-core-pkcs7-tsp`: __Sin soporte.__ Módulo con la lógica para agregar sellos de siempre a firmas PKCS#7 
* `afirma-crypto-jarverifier`: __Obsoleto.__ Módulo para la comprobación de la integridad de un JAR utilizada en el antiguo Applet de @firma.
* `afirma-crypto-pdf-enhancer`: __Obsoleto.__ Módulo con un cliente SOAP para el envío de peticiones a @firma para la actualización de PDF a formatos longevos.
* `afirma-keystores-capiaddressbook`: __Obsoleto.__ Módulo con la lógica de acceso a la libreta de direcciones de Windows.
* `afirma-keystores-single`: __Obsoleto.__ Módulo con un proveedor criptográfico para la gestión de certificados sueltos como si fuesen almacenes.
* `afirma-miniapplet-report`: __Obsoleto.__ Módulo para la generación de informes de las pruebas del antiguo MiniApplet.
* `afirma-miniapplet-store-testdata`: __Obsoleto.__ Módulo para el guardado de los datos de los informes de las pruebas del antiguo MiniApplet.
* `afirma-report-fail-tests`: __Obsoleto.__ Módulo para la notificación de errores de las pruebas del antiguo MiniApplet.
* `afirma-server-simple-webstart`: __Obsoleto.__ Módulo principal del servicio para la generación del JNLP para la ejecución de AutoFirma WebStart.
* `afirma-standalone`: __Obsoleto.__ Módulo principal de la antigua herramienta de escritorio StandAlone.
* `afirma-standalone-installer`: __Obsoleto.__ Módulo con los componentes para la generación del instalador de la antigua herramienta de escritorio StandAlone.
* `afirma-ui-applet`: __Obsoleto.__ Módulo principal del antiguo Applet de @firma.
* `afirma-ui-applet-deploy`: __Obsoleto.__ Módulo con el JavaScript de despliegue del antiguo Applet de @firma.
* `afirma-ui-miniapplet`: __Obsoleto.__ Módulo principal del antiguo MiniApplet.
* `afirma-ui-simple-webstart`: __Obsoleto.__ Módulo principal del antiguo empaquetado de AutoFirma como aplicación WebStart,
* `afirma-windows-store`: __Obsoleto.__ Módulo principal del antiguo cliente de firma para Windows 8.

No se ofrece ningún tipo de mantenimiento ni soporte sobre estos módulos.
",,2024-04-30T15:44:32Z,17,243,40,"('Gamuci', 1165), ('clawgrip', 487), ('borillo', 119), ('Jose2601', 93), ('mrobledo', 68), ('mglago', 48), ('jjrodgom', 25), ('sergiomrico1', 18), ('albfernandez', 9), ('dependabotbot', 9), ('dmlambea', 4), ('ssaavedra', 4), ('jose-vano', 4), ('rasputino', 2), ('EchedelleLR', 1), ('dantefff', 1), ('victorjss', 1)","[16, 'Peace, Justice and Strong Institutions']"
WattTime/pyiso,Python client libraries for ISO and other power grid data sources.,"pyiso
============

[![Build Status](https://travis-ci.org/WattTime/pyiso.svg?branch=master)](https://travis-ci.org/WattTime/pyiso)
[![Coverage Status](https://coveralls.io/repos/WattTime/pyiso/badge.svg?branch=master)](https://coveralls.io/r/WattTime/pyiso?branch=master)
[![PyPI version](https://badge.fury.io/py/pyiso.svg)](https://badge.fury.io/py/pyiso)

pyiso provides Python client libraries for [ISO](https://www.epsa.org/industry/primer/?fa=rto) and other power grid data sources.
It powers the WattTime API (https://api.watttime.org/), among other things.

Documentation: https://pyiso.readthedocs.io/

User group: https://groups.google.com/forum/#!forum/pyiso-users

Upcoming Changes
----------------
* Add changes here

Changelog
---------
* 0.4.0: Added BCH (trade-only), EIA, IESO, NBPower, NSPower, AESO, PEI, SASK, NLHydro and YUKON authorities. Added `ccgt` as fuel type. Removed
         `get_lmp` function (backward-incompatible change).
* 0.3.19: Fix bug with `Biomass/Fossil` fuel type for BPA
* 0.3.18: Fix bug with PJM date parsing
* 0.3.17: Fix bug with `Black Liquor` fuel type for PJM
* 0.3.16: Implement ISONE get_morningreport and get_sevendayforecast
* 0.3.15: Minor bugfixes to CAISO get_generation. 
* 0.3.14: Minor bugfixes to ISONE, PJM, and ERCOT. 
* 0.3.13: Major feature: generation mix in PJM (RTHR market only). Minor change: SSL handling in BPA.
* 0.3.12: Bugfix: fixed EU authentication, thanks @frgtn!
* 0.3.11: Changes: `timeout_seconds` kwarg to client constructor; do not remember options from one `get_*` call to the next.
* 0.3.10: Changes: Historical DAHR LMP data in NYISO is not available using `market='DAHR'`; error is raised when trying to access historical RT5M LMP data in PJM.
* 0.3.9: Fixes breaking error with BeautifulSoup. Minor fixes: closes issues #79, #84.
* 0.3.8: Minor feature: Historical NYISO LMP data available farther into the past.
* 0.3.7: Change: For CAISO historical generation, defaults to DAHR market instead of RTHR if no market is provided.
* 0.3.6: Change: If `forecast=True` is requested without specifying `start_at` or `end_at`, `start_at` will default to the current time; previously it defaulted to midnight in the ISO's local time. Bugfixes: times outside the `start_at`-`end_at` range are no longer returned for ISONE generation and load, CAISO DAHR generation.
* 0.3.5: Minor feature: all tasks can accept strings for `start_at` and `end_at` kwargs.
* 0.3.2: Minor feature: `get_lmp` task. Minor bugfixes: safer handling of response errors for load (BPA, ERCOT, MISO, NVEnergy, PJM) and generation (BPA, CAISO, ERCOT, ISONE, NYISO); clean up LMP tests.
* 0.3.1: Minor changes for PJM real-time load data: fall back to OASIS if Data Snapshot is down, round time down to nearest 5 min period. Major feature: SVERI back up.
* 0.3.0: Major features: Add LMP to all ISOs, license change. Please contact us for alternative licenses. Bugfixes: SVERI has a new URL. Minor features: CAISO has 15-minute RTPD market.
* 0.2.23: Major fix: ERCOT real-time data format changed, this release is updated to match the new format. Minor fixes to excel date handling with pandas 0.18, and MISO forecast.
* 0.2.22: Feature: LMP in NYISO, thanks @ecalifornica! Bug fixes for DST transition.
* 0.2.21: Major feature: generation mix in NYISO. Bug fix: time zone handling in NYISO.
* 0.2.18: Minor change: enforce pandas version 0.17 or higher.
* 0.2.17: Minor change: Limit retries in `base.request`, and increase time between retries.
* 0.2.16: Major fix: PJM deprecated the data source that was used in previous releases. This release uses a new data source that has load and tie flows, but not wind. So PJM generation mix has been deprecated for the moment--hopefully it will return in a future release.
* 0.2.15: Minor changes: enforce pandas 0.16.2 and change NYISO index labelling to fix NYISO regression in some environments.
* 0.2.14: Major features: forecast load in ERCOT, MISO, NYISO, PJM; forecast genmix in MISO; forecast trade in MISO. Minor changes: fixed DST bug in BPA, refactored several to better use pandas.
* 0.2.13: Minor bugfix: Better able to find recent data in NVEnergy.
* 0.2.12: Major features: EU support, support for throttling in CAISO. Minor upgrades: Improve docs, dedup logging messages.
* 0.2.11: Minor bugfixes. Also, made a backward-incompatible change to the data structure that's returned from `get_ancillary_services` in CAISO.
* 0.2.10: Fixed bug in CAISO LMP DAM.
* 0.2.9: Added load and generation mix for SVERI (AZPS, DEAA, ELE, HGMA, IID, GRIF, PNM, SRP, TEPC, WALC)
* 0.2.8: Added lmp in ISONE. Also, made a backward-incompatible change to the data structure that's returned from `get_lmp` in CAISO.
* 0.2.7: Added load and trade in Nevada Energy (NEVP and SPPC)
* 0.2.1: Added load (real-time 5-minute and hourly forecast) in ISONE
* 0.2.0: Maintained Python 2.7 support and added Python 3.4! Thanks @emunsing
",,2024-02-02T05:23:31Z,14,236,43,"('aschn', 426), ('r24mille', 264), ('ajdonnison', 57), ('andydevlinsmith', 42), ('gordonmslai', 21), ('marcpare', 14), ('ndavis6', 6), ('marcparewatttime', 4), ('hangtwenty', 2), ('bendichter', 1), ('dzimmanck', 1), ('ezeagwulae', 1), ('frgtn', 1), ('teschmitt', 1)","[7, 'Affordable and Clean Energy']"
pwyf/aid-transparency-tracker,A data quality measurement tool for international aid data.,"# Aid Transparency Tracker

A data quality measurement tool for international aid data.

## Installation

**N.B. If you would like to develop locally with vagrant follow these [instructions](./vagrant/vagrant-readme.md)**

Run the following commands to bootstrap your environment:

``` bash
git clone --recursive https://github.com/pwyf/aid-transparency-tracker.git
cd aid-transparency-tracker
```

Setup a virtual environment, and install dependencies:

``` bash
python3 -m venv .ve
source .ve/bin/activate
pip install ""setuptools<58""
pip install -r requirements.txt
```

Install and run a postgres database. If you have docker-compose, you can do this:

```
docker-compose -f docker-compose-postgres.yml up -d
```

(You may need to run `docker compose` instead of `docker-compose` if you have docker v2 installed).

You can login to the PostgreSQL server (which might be useful to check things are working) using `psql` with the following command (using password from dockerfile; or using port and password you set, if not using the docker compose setup):

```commandline
psql -h localhost -p 5433 -U postgres
```


Copy and if necessary edit the config.py.tmpl. (If you installed postgres using docker-compose you shouldn't need to edit anything.)

``` bash
cp config.py.tmpl config.py
```

If you're running this for the first time, edit `tests/organisations_with_identifiers.csv` to have less organisations. e.g. For just one:

```
cp tests/organisations_with_identifiers_sample_1_org.csv tests/organisations_with_identifiers.csv
```

Finally, run the setup script to populate your database:

``` bash
flask setup
```

This will prompt you to create a new admin user (this will be the username & password you use to login to the ATT web app, once it is up and running--see 'Running' section below).

## Fetching and testing data

1. You can download a dump of today’s IATI data with:
    ``` bash
    flask download_data
    ```
   The data will be downloaded to the `__iatikitcache__` directory by default, or you can add an `iatikit.ini` file to specify a different location.


2. The relevant data (according to the organisations in your database) should then be moved into place using:
    ``` bash
    flask import_data
    ```
   This will move files into the `IATI_DATA_PATH` specified in your config.py


3. Tests are run on this data using:
    ``` bash
    flask test_data
    ```
   The complete output of this is stored as CSV files in the `IATI_RESULT_PATH` specified in your config.py


4. Finally, you can refresh the aggregate data shown in the tracker using:
    ``` bash
    flask aggregate_results
    ```
   This step will destructively populate the `aggregateresult` table of your database.

## Running

You can run a development server with:
``` bash
flask run
```
## Survey component


## Reinitialise

If at any time you need to reset, you can drop all tables using:

``` bash
flask drop_db
```
Then follow the installation instructions to reinitialise.

## Code formatting

We are currently incrementally adding files to black and isort code formatting.

To ensure the relevant files are formatted correctly, run:

```
black conftest.py config_test.py unittests/test_web.py integration_tests/
isort conftest.py config_test.py unittests/test_web.py integration_tests/
```

These will be checked by GitHub Actions.

## Data files required by the tests

The PWYF tests which assess the quality of the data use various files which are stored
in the `tests/` folder. Some of these files need to be updated from the following
locations:

**`orgid.json`**: http://org-id.guide/download.json

**`publishers.json`**: https://iatiregistry.org/publisher/download/json

**`CRSChannelCode.json`**: https://iatistandard.org/reference_downloads/203/codelists/downloads/clv3/json/en/CRSChannelCode.json (see https://iatistandard.org/en/iati-standard/203/codelists/crschannelcode/ for more information)

",,2024-04-17T09:33:49Z,9,10,8,"('mk270', 1914), ('markbrough', 1481), ('andylolz', 617), ('shreyabasu', 133), ('Bjwebb', 130), ('simon-20', 36), ('kindly', 24), ('siwhitehouse', 16), ('dependabotbot', 5)","[16, 'Peace, Justice and Strong Institutions']"
EvictionLab/eviction-maps,Data visualization tool for the Eviction Lab at Princeton University,"# Eviction Lab Map Tool

[![Build Status](https://travis-ci.org/EvictionLab/eviction-maps.svg?branch=master)](https://travis-ci.org/EvictionLab/eviction-maps)

Data visualization tool for [the Eviction Lab at Princeton University](https://evictionlab.org) displaying information on evictions and demographics for the United States.

## Setup

This project uses [Angular](https://angular.io/), and can be installed by cloning a local copy of the repo and running `npm install`. Use the Angular CLI to run a local development server with `ng serve` and access the site in your browser at `http://localhost:4200`.

## Deployment

The app is deployed as a static site served over AWS S3 and CloudFront. Mapbox vector tiles provide all of the source data, and are built using the pipeline in [EvictionLab/eviction-lab-etl](https://github.com/EvictionLab/eviction-lab-etl). Exports are generated using serverless AWS Lambda functions in [EvictionLab/eviction-lab-exports](https://github.com/EvictionLab/eviction-lab-exports), with map screenshots for the PowerPoint export being generated by an auto-scaling ECS microservice [EvictionLab/map-screenshot-gl](https://github.com/EvictionLab/map-screenshot-gl).

## Contributing

If you're interested in contributing to the project check out the [contributing](CONTRIBUTING.md) page.

## License

This application is open source code under the [MIT License](LICENSE).

",,2023-07-14T16:19:00Z,8,27,4,"('Lane', 829), ('pjsier', 803), ('lougroshek', 33), ('coreytegeler', 18), ('hoshmn', 8), ('james-minton', 8), ('Peterlollo', 5), ('angular-cli', 1)","[1, 'No Poverty']"
uptake/autofocus,Deep learning computer vision for classifying wildlife in camera trap images,"[![Travis Build Status](https://img.shields.io/travis/uptake/autofocus.svg?label=travis&logo=travis&branch=master)](https://travis-ci.org/uptake/autofocus)

# Autofocus

![coyote](./gallery/coyote1.jpg)

THIS PROJECT IS INACTIVE, AND ITS APP AND DATASETS ARE NO LONGER AVAILABLE.

This project uses deep learning computer vision to label images taken by motion-activated ""camera traps"" according to the animals they contain. Accurate models for this labeling task can address a major bottleneck for wildlife conservation efforts.

## Further Reading

- [Uptake.org Autofocus Case Study](https://www.uptake.org/impact/special-projects)
- [Machine Learning Meets Wildlife Conservation](https://www.lpzoo.org/blog/machine-learning-meets-wildlife-conservation)

## Getting the App

If you just want to get labels for your images, you can use the following steps to run a service that passes images through a trained model.

1. Make sure [Docker](https://www.docker.com/get-started) is installed and running.
2. ~Run `docker pull gsganden/autofocus_serve:1.2.3` to download the app image. (Note that it takes up about 4GB of disk space.)~ This dataset is no longer available.
3. Run `docker run -p 8000:8000 gsganden/autofocus_serve:1.2.3` to start the app.
4. Make POST requests against the app to get predictions.

For instance, with the base of this repo as the working directory you can send the image `fawn.JPG` to the app with this command:

```bash
curl -F ""file=@./gallery/fawn.JPG"" -X POST http://localhost:8000/predict
```

Or send the zipped `gallery` directory to the app with this command:

```bash
curl -F ""file=@gallery.zip"" -X POST http://localhost:8000/predict_zip
```

See `autofocus/predict/example_post.py` and `autofocus/predict/example_post.R` for example scripts that make requests using Python and R, respectively.

For a single image, the app will respond with a JSON object that indicates the model's probability that the image contains an animal in each of the categories that it has been trained on. For instance, it might give the following response for an image containing raccoons:

```json
{
  ""beaver"": 7.996849172335282e-16,
  ""bird"": 6.235780460883689e-07,
  ""cat"": 9.127776934292342e-07,
  ""chipmunk"": 4.231552441780195e-09,
  ""coyote"": 2.1184381694183685e-05,
  ""deer"": 3.6601684314518934e-06,
  ""dog"": 1.4745426142326323e-06,
  ""empty"": 0.0026697132270783186,
  ""fox"": 2.7905798602890358e-14,
  ""human"": 1.064212392520858e-05,
  ""mink"": 2.7622977689933936e-13,
  ""mouse"": 4.847318102463305e-09,
  ""muskrat"": 6.164089044252078e-16,
  ""opossum"": 9.763967682374641e-05,
  ""rabbit"": 2.873173616535496e-05,
  ""raccoon"": 0.9986177682876587,
  ""rat"": 4.3888848111350853e-10,
  ""skunk"": 4.078452775502228e-07,
  ""squirrel"": 1.2888597211713204e-06,
  ""unknown"": 0.0004612557531800121,
  ""woodchuck"": 1.2980818033154779e-14
}
```

The model generates each of these probabilities separately to allow for the possibility e.g. that an image contains both a human and a dog, so they will not sum to 1 in general.

The `/predict_zip` endpoint returns a JSON object mapping file paths to model probabilities formatted as above.

During development, it is convenient to run the app in debug mode with the local directory mounted to the Docker container so that changes you make locally are reflected in the service immediately:

```
docker run \
    -it \
    -v ""${PWD}/autofocus/predict:/image_api"" \
    -p 8000:8000 \
    gsganden/autofocus_serve python app/app.py
```

## Getting the Model

The app described above uses a multilabel fast.ai model. You can download that model directly with the following command. This command was written to run from the repo root. 

```
This dataset is no longer available.
```

`autofocus/train_model/train_multilabel_model.ipynb` contains the code that was used to train and evaluate this model.

## Getting the Data

The model described above was trained on a set of images provided by the Lincoln Park Zoo's Urban Wildlife Institute that were taken in the Chicago area in mid-summer 2016 and 2017. If you wish to train your own model, you can use the instructions below to download that dataset and other related datasets.

If necessary, create an AWS account, install the AWS CLI tool (`pip install awscli`), and set up your AWS config and credentials (`aws configure`). All of the commands below are written to run from the repo root.

Use this commend to download a preprocessed version of the Lincoln Park Zoo 2016-2017 dataset to `autofocus/data/` (you can change the destination directory if you like):

```
No longer available
```

Unpack the tarfile:

```bash
mkdir $(pwd)/data/lpz_2016_2017/
tar -xvf $(pwd)/data/${FILENAME} -C $(pwd)/data/lpz_2016_2017/
```

Delete the tarfile:

```bash
rm $(pwd)/data/${FILENAME}
```

This dataset contains approximately 80,000 images and a CSV of labels and image metadata. It occupies 17.1GB uncompressed, so you will need about 40GB free for the downloading and untarring process. The images have been preprocessed by trimming the bottom 198 pixels (which often contains a metadata footer that could only mislead a machine learning model) and resizing to be 512 pixels along their shorter dimension. In addition, the labels have been cleaned up and organized.

~If you would like to work with data that has not been preprocessed as described above, replace `FILENAME=lpz_2016_2017_processed.tar.gz` with `FILENAME=data_2016_2017.tar.gz`. You will need to have about 100GB free to download and untar the raw data.~ This dataset is no longer available. `autofocus/build_dataset/lpz_2016_2017/process_raw.py` contains the code that was used to generate the processed data from the raw data.

~A second dataset from the Lincoln Park Zoo's Urban Wildlife Institute contains approximately 75,000 images (227 x 227 pixels) and a CSV of labels and image metadata from the Chicago area in 2012-2014. It takes up 7.9GB uncompressed. To get this data, follow the same steps as for the 2016-2017 dataset, but replace `FILENAME=lpz_2016_2017_processed.tar.gz` with `FILENAME=lpz_2012-2014.tar.gz`, and use this command to unpack the tarfile.~ This dataset is no longer available.

```bash
tar -xvf $(pwd)/data/${FILENAME} -C $(pwd)/data/
```

A third dataset from the Lincoln Park Zoo's Urban Wildlife Institute contains unlabeled three-image bursts from 2018. It takes up 5.7GB uncompressed. ~To get this data, follow the same steps as for the 2012-2014 dataset, but replace `FILENAME=lpz_2016_2017_processed.tar.gz` with `FILENAME=lpz_2018.tar.gz`.~ This dataset is no longer available.

## Running Tests

To test the app, run `pip install -r requirements-dev.txt` and then `pytest`. The tests assume that the app is running locally on port `8000` according to the instructions above.``

## Example Images

![buck](./gallery/buck.jpeg)

![fawn](./gallery/fawn.JPG)

![racoons](gallery/raccoons.jpeg)
","'camera-traps', 'computer-vision', 'conservation-bio', 'deep-learning', 'fastai', 'pytorch'",2023-04-16T00:05:16Z,11,54,12,"('gsganden', 233), ('Manusreekumar', 14), ('parsing-science', 7), ('jameslamb', 5), ('jayqi', 3), ('rahulgurnani', 2), ('Chronocook', 1), ('bburns632', 1), ('davidwilby', 1), ('mfidino', 1), ('sourcery-ai-bot', 1)","[15, 'Life On Land']"
ShelterTechSF/askdarcel-web,"The ""Ask Darcel"" web app.","# ShelterTech Web App [![Travis CI Status](https://travis-ci.org/ShelterTechSF/askdarcel-web.svg?branch=master)](https://travis-ci.org/ShelterTechSF/askdarcel-web)

## Sauce Labs Browser Test Status

[![Sauce Test Status](https://saucelabs.com/browser-matrix/askdarcel-web-master.svg)](https://saucelabs.com/u/askdarcel-web-master)

## Onboarding Instructions

[Dev Role Description](https://www.notion.so/sheltertech/Developer-Engineer-Role-Description-ShelterTech-AskDarcel-SFServiceGuide-Tech-Team-7fd992a20f864698a43e3882a66338bb)

[Technical Onboarding & Team Guidelines](https://www.notion.so/sheltertech/Technical-Onboarding-and-Team-Guidelines-a06d5543495248bfb6f17e233330249e)

## Docker-based Development Environment (Recommended)

### Requirements

Docker Community Edition (CE) >= 17.06
Docker Compose >= 1.18

Follow the [Docker installation instructions](https://www.docker.com/community-edition#/download) for your OS.

### Set up the project

This is not a full guide to Docker and Docker Compose, so please consult other
guides to learn more about those tools.

The docker-compose.yml is configured to mount the git repo on your host
filesystem into the Docker container so that any changes you make on your host
machine will be synced into the container and vice versa.

#### Creating the `config.yml` file

All config should be added in a file called `config.yml`. A sample `config.example.yml` is provided, you need to copy it and edit any parts that ask you to enter in your own information.

```sh
$ cp config.example.yml config.yml

# Open it in your preferred text editor
```

##### Algolia

[Algolia](https://www.algolia.com/doc/guides/getting-started/what-is-algolia/) is used as our search engine and in order for it to operate properly for everyone, we each need our own [index](https://www.algolia.com/doc/guides/indexing/indexing-overview/).

- in `config.yml` set _your_ github username as the value for `ALGOLIA_INDEX_PREFIX`. This will point to the search index matching your local environment.

#### Building and running the application

```sh
# Install node dependencies
$ docker-compose run --rm web npm install

# Build static assets bundle
$ docker-compose run --rm web npm run build

# Run dev server
$ docker-compose up
```

You should be able to view the web app in your browser at http://localhost:8080.

By default, this assumes that you have also set up askdarcel-api project using
the Docker setup instructions and that the API server is running. If you want to
target a different instance of askdarcel-api, you can modify the `API_URL`
environment variable in docker-compose.yml.

#### Fully tearing down environment

In case you ever need to fully tear down your local development environment,
such as to do a fresh setup from a clean slate, you will need to run extra
commands to remove state that is stored in Docker. Removing the git repository
and re-cloning is _insufficient_ because some of the state is stored in Docker.

In particular, for performance reasons, we save NPM modules in a `node_modules/`
directory that is mounted from a Docker volume rather than bind mounting the
`node_modules/` directory from the host operating system (e.g. macOS). To delete
all the installed NPM modules, you will have to remove the Docker volume.

The following command will stop all running Docker containers, delete them, and
remove their volumes:

```sh
$ docker-compose down --remove-orphans --volumes
```

Note: When you run that command, you may get an error message about removing
networks:

```
ERROR: error while removing network: network askdarcel id
4c4713d7f42173843437de3b0051a9d7e7bc81eb18123993975c3cd5a9e0a38e has active
endpoints
```

If this happens, then you need to run `docker-compose stop` in the askdarcel-api
application first before running the `docker-compose down` command above.

## Non-Docker Development Environment

### Installing Node.js and npm

We recommend using [nvm](https://github.com/creationix/nvm) (Node Version
Manager) or Docker to ensure that the versions of Node.js and npm are the same
across development, Travis CI, staging, and production environments.

After installing nvm, to install both Node.js and npm run from the top of the
git repo:

```sh
$ nvm install  # Reads from .nvmrc
```

### Installing npm dependencies

To install the dependencies, from the top directory run

```sh
npm install
```

To build the bundled script with webpack run

```sh
npm run build
```

And to run the dev server, run

```sh
npm run dev
```

## End to end testing

#### Quick summary of what TestCafe is and how it works

It's a framework for running end-to-end tests (read: real browser tests) that injects your tests onto an existing web page. Architecturally, they spin up a lightweight proxy server that wraps your web page, and when you connect a browser to the proxy server, it serves the requested page with the test driver injected into it.

It's essentially an alternative to writing Selenium tests, and I've found it nice to use because it mimics many of the common HTML5 DOM APIs and because they've added a lot of reasonable default behavior that Selenium lacks, such as properly waiting for events to finish running and for elements to appear before running your assertions.

#### How to run

If you are not using Docker and all the services are bound to localhost, then you should just be able to run:

```
$ npm run testcafe -- --skip-js-errors chrome testcafe/*.js
```

Note: Make sure you have the dev server running (`npm run dev`) before you try running the above

If you are using Docker, then you'll need to run it somewhat like this:

```
$ docker-compose run --rm -p 1337:1337 -e BASE_URL=http://web:8080 web npm run testcafe -- --skip-js-errors remote --skip-js-errors --hostname localhost --ports 1337,1338 ./testcafe/
```

This will spin up a web server at http://localhost:1337/ and print out a URL to use. You should manually enter it into your browser to start the tests.
",,2024-05-01T19:13:53Z,46,27,6,"('lexholden', 373), ('schroerbrian', 260), ('richardxia', 227), ('twolfe2', 136), ('lgarofalo', 115), ('AnnaKalkanis', 43), ('derekfidler', 41), ('alexanderturinske', 29), ('drcaramelsyrup', 29), ('achadha', 17), ('jjfreund', 14), ('tpetersen0308', 11), ('trucnguyen', 11), ('Akhtam', 11), ('jfhamlin', 10), ('cliffcrosland', 9), ('JacobDFrank', 8), ('quanhuynh', 8), ('dependabotbot', 7), ('davidagustin', 7), ('quanwin', 7), ('Maxastuart', 6), ('Raawr', 4), ('genovese28', 4), ('nknavkal', 4), ('jmpainter', 3), ('ZanderSparrow', 3), ('candywang', 3), ('devbrianschroer', 2), ('azhelle16', 2), ('zjipsen', 2), ('shadycat', 2), ('ruochenhuang', 1), ('akatrent', 1), ('yuidavidson', 1), ('tabithahsia', 1), ('PPzhen000', 1), ('kris10brady', 1), ('iopkelvin', 1), ('katerina-kossler', 1), ('GeorgeCloud', 1), ('fayceltouili', 1), ('dfloo', 1), ('daniel-owens', 1), ('chrisco23', 1), ('ashleyabrooks', 1)","[1, 'No Poverty']"
coralproject/ask,"A better way for journalists to manage forms, submissions, and galleries. Because journalism needs everyone.","# Ask

*Ask is no longer supported. Docs / installation may not work, install at your own risk.*

Ask was a tool that enabled editors to create embeddable calls for contributions. Editors can manage high volumes of contributions, and display the best ones in a gallery. It was developed by The Coral Project, as part of its work within the Mozilla Foundation.

Ask was designed to facilitate engagement among all readers, journalists, editors, and publishers, in order to promote user-generated content that improves the quality and relevancy of journalism within communities.

## Installation
To install Ask, [follow the instructions detailed here.](https://docs.coralproject.net/ask/products/ask/) You can also [download the precompiled binary here.](https://github.com/coralproject/ask-install)
",,2021-03-01T17:14:30Z,8,42,26,"('jtnelson', 27), ('jde', 8), ('losowsky', 7), ('gabelula', 4), ('kgardnr', 3), ('wyattjoh', 3), ('davisshaver', 2), ('alexbyrnes', 1)","[8, 'Decent Work and Economic Growth']"
sugarlabs/sugar,Sugar GTK shell,"Sugar
=====

Sugar is the desktop environment component of a worldwide effort to
provide every child with an equal opportunity for a quality
education. Available in more than twenty-five languages, Sugar
Activities are used every school day by children in more than forty
countries.

Originally developed for the One Laptop per Child XO-1 netbook, Sugar
can run on most computers.

Sugar is free/libre and open-source software.

https://www.sugarlabs.org/

Installing on Debian or Ubuntu
------------------------------

```
sudo apt install sucrose
```

Then log out, and log in with the Sugar desktop selected.

See also [Debian](docs/debian.md) or [Ubuntu](docs/ubuntu.md).

Installing on Fedora
--------------------

```
sudo dnf groupinstall sugar-desktop
```

Then restart your computer.  At the *Sign in* select the *Sugar*
desktop.

See also [Fedora](docs/fedora.md).

Building
--------

Sugar follows the [GNU Coding
Standards](https://www.gnu.org/prep/standards/).

Install all dependencies, especially [`sugar-artwork`](https://github.com/sugarlabs/sugar-artwork), [`sugar-datastore`](https://github.com/sugarlabs/sugar-datastore),
and [`sugar-toolkit-gtk3`](https://github.com/sugarlabs/sugar-toolkit-gtk3).

Clone the repository, run `autogen.sh`, then `make` and `make
install`.

See also [Setup a development
environment](docs/development-environment.md).
",,2024-04-04T21:53:49Z,85,248,45,"('pootle-sugarlabs', 683), ('tomeuv', 605), ('dcbw', 597), ('erikos', 493), ('quozl', 220), ('godiard', 170), ('samdroid-apps', 135), ('dnarvaez', 126), ('smcv', 123), ('tchx84', 116), ('worr', 93), ('walterbender', 79), ('devAbnull', 55), ('benzea', 43), ('sayamindu', 43), ('i5o', 40), ('silbe', 27), ('heeres', 22), ('chimosky', 17), ('mdengler', 16), ('manuq', 14), ('cscott', 13), ('edsiper', 12), ('doublea', 11), ('mstone', 10), ('rhl-bthr', 10), ('ezequielpereira', 9), ('codefrau', 8), ('marcopg', 8), ('leonardcj', 8), ('edudev', 7), ('humitos', 7), ('icarito', 7), ('iamutkarshtiwari', 6), ('migonzalvar', 5), ('Saumya-Mishra9129', 5), ('srevinsaju', 5), ('mpdmanash', 4), ('ana-balica', 4), ('timClicks', 4), ('Aniket21mathur', 3), ('shaansubbaiah', 3), ('codewiz', 3), ('ywwg', 3), ('kipply', 3), ('0xdaf', 3), ('FGrose', 3), ('mslg', 3), ('khaledhosny', 3), ('aperezbios', 2), ('yuanchao', 2), ('kennym', 2), ('franrogers', 2), ('HoboPrimate', 2), ('cjb', 2), ('ujdhesa', 2), ('uwog', 1), ('nwolisaemmanuel2-zz', 1), ('ortegaps', 1), ('rbuj', 1), ('DarkPrince304', 1), ('seberg', 1), ('surajgillespie', 1), ('nswarup14', 1), ('architagrawal', 1), ('cheekujodhpur', 1), ('commonsolutionmd', 1), ('deepsource-autofixbot', 1), ('korakurider', 1), ('pupp3tc0m', 1), ('sukhdeepg', 1), ('kartikperisetla', 1), ('nemesiscodex', 1), ('JuiP', 1), ('joausaga', 1), ('jonassmedegaard', 1), ('goutamnair7', 1), ('garycmartin', 1), ('The-Compiler', 1), ('davelab6', 1), ('Clytie', 1), ('ChristoferR', 1), ('cdelorme', 1), ('bhulsken', 1), ('Ovalelephant35', 1)","[4, 'Quality Education']"
instedd/surveda,InSTEDD Surveda,"# Surveda

## Dockerized development
You can setup your local environment following different approaches.

### Approach #1: without Verboice nor Nuntium
If you don't need to connect to Verboice nor Nuntium, you can move forward with this setup.

1. You need to use [dockerdev](https://github.com/waj/dockerdev) to access the web app at `app.surveda.lvh.me` and ngrok at `ngrok.surveda.lvh.me`.  Just follow the project's readme.  **WARNING:** You should install `dockerdev` _before_ creating your stack's network in Docker. If you have already run `./dev-setup.sh`, you may want to run `docker compose down -v` to **delete every container, data and other artifacts** from the project and start from scratch _after_ running `dockerdev`.
2. Clone this repository.
3. In the project's root, execute `./dev-setup.sh`.
4. Start the app with `docker compose up`.
5. Once the app is up and running, you can visit [`http://app.surveda.lvh.me/`](http://app.surveda.lvh.me/) from your browser.
6. You will be able to Create an Account and Login with a built-in authentication.  However, you might fail to receive the needed emails that are sent during this flow.  You can check the CLI console where you'll be able to see logs with the email content, including the links that you need to complete the workflow.

If needed, you can change the authentication mechanism to Guisso.  This will take your local app 1 step closer to similarity with the production environment.  The section ""Guisso"" in this document, explains how to achieve this.

### Approach #2: with Verboice and Nuntium
Here we have 2 options regarding the connection of your local Surveda with Verboice and Nuntium:

1. You can install and setup Verboice and Nuntium [locally on your computer](https://github.com/instedd/surveda/blob/main/docs/dev-setup-local.md).

2. You can use Verboice and Nuntium from [the cloud in the Staging environment](./docs/dev-setup-cloud.md).

Both guides include setting up Guisso, so you can skip the following GUISSO section.

### Useful commands
We can open a shell in a service. For example the **app** service:

```console
$ docker compose exec app bash
```
or the **db** service:

```console
$ docker compose exec db bash
```

For a list of all available services, we can run:

```console
$ docker compose ps --services
```

Also we can run [Interactive Elixir](https://elixir-lang.org/getting-started/introduction.html#interactive-mode) like this:

```console
$ docker compose exec app iex
```

And if we want to start an `Interactive Elixir` in the context of our running Phoenix app:

```console
$ docker compose exec app iex -S mix
```

#### Tests
We can [run a one-time command](https://docs.docker.com/compose/reference/run/) to execute all the `backend` tests:

```console
$ docker compose run --rm app mix test
```

For the `client side`, we can open a terminal:
```console
$ docker compose run --rm webpack bash
```

and then we can run all JavaScript tests:
```console
$ yarn test
```

we can also check JavaScript `types` with [Flow](https://flow.org/):
```console
$ yarn flow
```

and finally run a static analysis/style guide with [ESLint](https://eslint.org/):
```console
$ yarn eslint
```

## NGROK

Ngrok is a service that exposes local servers over secure channels.
From a Dev perspective, it's very useful avoiding the need of having Guisso, Verboice and Nuntium running locally. That could be inconvenient for the daily work.
The `docker-compose.yml` has instructions to get the service working. You'll need to have an account and get your auth token to use the service.

## GUISSO

You need GUISSO to access Verboice and/or Nuntium channels.

Get a working GUISSO instance (online, or hosted on your development machine) and create a new Application. If it's a local Guisso instance, use `app.surveda.lvh.me` as the domain, and this two redirect URIs:

```
http://app.surveda.lvh.me/session/oauth_callback
http://app.surveda.lvh.me/oauth_client/callback
```

To work with a cloud GUISSO, make sure your `ngrok` service is running (`docker compose up ngrok`), and get your ngrok domain visiting `http://ngrok.surveda.lvh.me`. Fill the Application information as for the local case, but using the ngrok domain instead. When you restart your `ngrok` service, you will need to update this information before approving new authorizations in GUISSO.

On your local surveda directory, create a `config/local.exs` file like below, including the client ID & secret from your Application in GUISSO:

```
use Mix.Config

config :alto_guisso,
  enabled: true,
  base_url: ""http://web.guisso.lvh.me"", # or https://login-stg.instedd.org for a cloud GUISSO
  client_id: """",
  client_secret: """",

config :ask, AskWeb.Endpoint,
  url: [host: ""app.surveda.lvh.me""] # or ""abcd123.ngrok.io"" for a cloud GUISSO
```

## IVR channels with Verboice

To setup a channel you need to create it using the console. For that you need to create a channel with the folowing settings:

```
%Ask.Channel{name: ""Channel name"",
  provider: ""verboice"",
  settings: %{
    ""channel"" => ""Verboice Channel name in Verboice"",
    ""username"" => ""Your Verboice username"",
    ""password"" => ""Your Verboice password"",
    ""url"" => ""http://verboice.instedd.org""
  },
  type: ""ivr"",
  user_id: your ask user id
}
```

In order for it to work, that Verboice channel must be associated to a dummy flow of a dummy Verboice project. Otherwise it will fail and won't log anything.


## Verboice Channel

Once you have GUISSO enabled on Surveda, you can connect a Verboice instance that's already registered with GUISSO by adding this fragment to your `config/local.exs`:

```
config :ask, Verboice,
  base_url: ""http://web.verboice.lvh.me"", # or the URL for your Verboice instance
  channel_ui: true,
  base_callback_url: ""http://abcd123.ngrok.io"", # specify the base URL to use on channel callbacks if it's not the same as the host
  guisso: [
    base_url: ""http://web.guisso.lvh.me"", # or the URL for your GUISSO
    client_id: """",
    client_secret: """",
    app_id: ""web.verboice.lvh.me"" # or your Verboice APP ID in GUISSO
  ]
```

## Coherence

### Upgrading

We're using Coherence to support registration, authorization, and other user management flows.
If you need to upgrade the version of Coherence that Ask uses, there are some steps that you need to mind.
Please check them out here: https://github.com/smpallen99/coherence#upgrading

### Coherence Mails

Coherence uses Swoosh as it's mailer lib. In development, we use Swoosh's local adapter, which
mounts a mini email client that displays sent emails at `{BASE_URL}/dev/mailbox`. That comes handy
to test flows which depend on email without having to send them in development.

## Intercom

Surveda supports Intercom as its CRM platform. To load the Intercom chat widget, simply start Surveda with the env variable `INTERCOM_APP_ID` set to your Intercom app id (https://www.intercom.com/help/faqs-and-troubleshooting/getting-set-up/where-can-i-find-my-workspace-id-app-id).

Surveda will forward any conversation with a logged user identifying them through their email address. Anonymous, unlogged users will also be able to communicate.

If you don't want to use Intercom, you can simply omit `INTERCOM_APP_ID` or set it to `''`.

To test the feature in development, add the `INTERCOM_APP_ID` variable and its value to the `environment` object inside the `app` service in `docker-compose.yml`.

## InSTEDD's url shortener

Surveda uses InSTEDD's [shorter](https://github.com/instedd/shorter) for sending urls to respondents when web-mobile mode is used.

Is necessary to configure an api-key in surveda to use this service. If no api-key is provided, surveda works fine but
full-urls are sent to respondents

For editing/creating a new api-key:
1. Go to AWS console
2. Go to API Gateway service
3. Select Usage-Plans
4. Select ""Surveda Shorter"" plan
5. Edit or create under ""API Keys"" tab

## Screen resolutions

The minimum supported screen resolution is 1366x768.
Mobile devices and screen resolutions less than 1366x768 are not supported.


## Linting and Formatting

To help us keep a consistent coding style, we're using StandardJS. Follow their instructions to install it: http://standardjs.com/#install

If you're using Sublime, you can setup a Build System that will use StandardJS to format your code when you hit `Ctrl+B`. To do so:

1. In Sublime, go to `Tools -> Build System -> New Build System...`
2. A file will open, replace its contents with:

```
{
  ""cmd"": [""standard"", ""--fix"", ""$file""],
  ""selector"": ""source.js""
}
```
3. Save. That's it. When you want to format, just hit `Ctrl+B`. Note that the formatter is a bit slow, so it's not a good idea to format on save.
",,2024-05-03T15:29:34Z,23,15,8,"('waj', 656), ('asterite', 598), ('macoca', 536), ('lmatayoshi', 451), ('juanboca', 357), ('fgasperij', 83), ('ysbaddaden', 79), ('anaPerezGhiglia', 51), ('matiasgarciaisaia', 39), ('devduarte', 32), ('aeatencio', 24), ('leandroradusky', 21), ('manumoreira', 19), ('spalladino', 17), ('NEKRON', 16), ('bcardiff', 14), ('danimiba', 14), ('hdf1996', 13), ('ftarulla', 10), ('diegoliberman', 3), ('mmuller', 2), ('nthiad', 2), ('dependabotbot', 1)","[17, 'Partnerships for the Goals']"
openstreetmap/iD,🆔 The easy-to-use OpenStreetMap editor in JavaScript.,"# iD - friendly JavaScript editor for [OpenStreetMap](https://www.openstreetmap.org/)

[![build](https://github.com/openstreetmap/iD/workflows/build/badge.svg)](https://github.com/openstreetmap/iD/actions?query=workflow%3A%22build%22)

## Basics

* iD is a JavaScript [OpenStreetMap](https://www.openstreetmap.org/) editor.
* It's intentionally simple. It lets you do the most basic tasks while not breaking other people's data.
* It supports all popular modern desktop browsers: Chrome, Firefox, Safari, Opera, and Edge.
* iD is not yet designed for mobile browsers, but this is something we hope to add!
* Data is rendered with [d3.js](https://d3js.org/).

## Participate!

* Read the project [Code of Conduct](CODE_OF_CONDUCT.md) and remember to be nice to one another.
* Read up on [Contributing and the code style of iD](CONTRIBUTING.md).
* See [open issues in the issue tracker](https://github.com/openstreetmap/iD/issues?state=open)
if you're looking for something to do.
* [Translate!](https://github.com/openstreetmap/iD/blob/develop/CONTRIBUTING.md#translating)
* Test a prerelease version of iD:
  * Stable mirror of `release` branch: https://ideditor-release.netlify.app
  * Development mirror of `develop` branch + latest translations: https://ideditor.netlify.app

Come on in, the water's lovely. More help? Ping `Martin Raifer`/`tyr_asd` or `bhousel` on:
* [OpenStreetMap US Slack](https://slack.openstreetmap.us/) (`#id` channel)
* [OpenStreetMap Discord](https://discord.gg/openstreetmap) (`#id` channel)
* [OpenStreetMap IRC](https://wiki.openstreetmap.org/wiki/IRC) (`irc.oftc.net`, in `#osm-dev`)
* [OpenStreetMap `dev` mailing list](https://wiki.openstreetmap.org/wiki/Mailing_lists)

## Installation

Follow the steps in the [how to get started guide](https://github.com/openstreetmap/iD/wiki/How-to-get-started#build-and-test-instructions) on how to install, build and run iD locally.

## License

iD is available under the [ISC License](https://opensource.org/licenses/ISC).
See the [LICENSE.md](LICENSE.md) file for more details.

iD also bundles portions of the following open source software.

* [D3.js (BSD-3-Clause)](https://github.com/d3/d3)
* [CLDR (Unicode Consortium Terms of Use)](https://github.com/unicode-cldr/cldr-json)
* [editor-layer-index (CC-BY-SA 3.0)](https://github.com/osmlab/editor-layer-index)
* [Font Awesome (CC-BY 4.0)](https://fontawesome.com/license)
* [Maki (CC0 1.0)](https://github.com/mapbox/maki)
* [Temaki (CC0 1.0)](https://github.com/ideditor/temaki)
* [Röntgen icon set (CC-BY 4.0)](https://github.com/enzet/map-machine#r%C3%B6ntgen-icon-set)
* [Mapillary JS (MIT)](https://github.com/mapillary/mapillary-js)
* [iD Tagging Schema (ISC)](https://github.com/openstreetmap/id-tagging-schema)
* [name-suggestion-index (BSD-3-Clause)](https://github.com/osmlab/name-suggestion-index)
* [osm-community-index (ISC)](https://github.com/osmlab/osm-community-index)


## Thank you

Initial development of iD was made possible by a [grant of the Knight Foundation](https://www.mapbox.com/blog/knight-invests-openstreetmap/).
","'d3', 'editor', 'hacktoberfest', 'javascript', 'mapping', 'openstreetmap'",2024-05-02T21:30:52Z,331,3248,169,"('bhousel', 4511), ('quincylvania', 3013), ('jfirebaugh', 1882), ('tmcw', 1274), ('tyrasd', 703), ('ansis', 701), ('samanpwbb', 505), ('dependabotbot', 287), ('kymckay', 161), ('greenkeeperbot', 154), ('kepta', 131), ('mbrzakovic', 125), ('greenkeeperio-bot', 116), ('JamesKingdom', 113), ('thomas-hervey', 104), ('1ec5', 102), ('aaronlidman', 64), ('maxgrossman', 49), ('westnordost', 48), ('vershwal', 46), ('systemed', 42), ('jguthrie100', 42), ('willemarcel', 38), ('k-yle', 36), ('Psigio', 32), ('hikemaniac', 32), ('manfredbrandl', 31), ('nickplesha', 29), ('peternewman', 28), ('noenandre', 23), ('RudyTheDev', 22), ('animesh-007', 21), ('boothym', 21), ('cschwarz', 20), ('tordans', 20), ('mapmeld', 16), ('ferdibiflator', 14), ('mourner', 14), ('pnorman', 14), ('Xavier-J-Ortiz', 13), ('rowanhogan', 13), ('jgravois', 12), ('M1dgard', 12), ('matkoniecz', 11), ('bkil', 11), ('mmd-osm', 11), ('morray', 11), ('ingalls', 11), ('AndreasHae', 10), ('wvanderp', 10), ('nyurik', 10), ('Firefishy', 10), ('ebrelsford', 10), ('bdon', 9), ('HolgerJeromin', 9), ('Vanuan', 9), ('wonga00', 9), ('richlv', 9), ('LorenMueller', 8), ('tpetillon', 8), ('MKuranowski', 8), ('chadrockey', 7), ('kratico', 7), ('dkniffin', 7), ('arno974', 7), ('paulmach', 6), ('Andygol', 6), ('bagage', 6), ('teymour-aldridge', 6), ('tanerochris', 6), ('homersimpsons', 6), ('waldyrious', 6), ('jleedev', 6), ('PaulAnnekov', 6), ('KathleenLD', 6), ('arch0345', 5), ('tomhughes', 5), ('TheAdventurer64', 5), ('stragu', 5), ('nlehuby', 5), ('nisargshh', 5), ('CarycaKatarzyna', 5), ('Stalfur', 5), ('iandees', 5), ('Bonkles', 5), ('laigyu', 4), ('mikini', 4), ('alperdincer', 4), ('tastrax', 4), ('til-schneider', 4), ('hodigabi', 4), ('sguinetti', 4), ('ogbeche77', 4), ('mikenath223', 4), ('booo', 4), ('ToastHawaii', 4), ('geohacker', 4), ('Nmargolis', 4), ('asolove', 4), ('lxbarth', 4), ('nadyafebi', 4), ('edpop', 4), ('g-k', 4), ('fakeharahman', 4), ('jgscherber', 4), ('nontech', 4), ('xmile1', 3), ('ToeBee', 3), ('koaber', 3), ('shawnaparadee', 3), ('mojodna', 3), ('zbycz', 3), ('ogadaki', 3), ('Neogeografen', 3), ('gaoxm', 3), ('YuliiaVeklycheva', 3), ('abdeldjalil09', 3), ('david082321', 3), ('palewire', 3), ('programistka', 3), ('umarpreet1', 3), ('nyampire', 3), ('simonpoole', 3), ('Abbe98', 3), ('obama', 3), ('blackboxlogic', 3), ('alphagamer7', 3), ('AviralSingh-code', 3), ('althio', 3), ('jaller94', 3), ('kreed', 3), ('davidchouse', 3), ('Dimitar5555', 3), ('guillaume', 3), ('NateGrobe', 3), ('mchlp', 3), ('mertemin', 3), ('mbrickn', 3), ('MateoV', 3), ('huonw', 3), ('Zverik', 3), ('iriman', 3), ('karmanya007', 3), ('leighghunt', 3), ('LaszloEr', 3), ('Supaplextw', 2), ('skorasaurus', 2), ('yohanboniface', 2), ('ajlomagno', 2), ('tristen', 2), ('DzikowskiW', 2), ('enighter', 2), ('hackily', 2), ('hlaw', 2), ('jgpacker', 2), ('jmespadero', 2), ('mstn', 2), ('ricloy', 2), ('slhh', 2), ('sun-geo', 2), ('thefifthisa', 2), ('tohaklim', 2), ('wingra2', 2), ('mondeja', 2), ('cbeddow', 2), ('humanforklift', 2), ('ramith-kulal', 2), ('renancleyson-dev', 2), ('bsvensson', 2), ('biswajit-k', 2), ('beaugunderson', 2), ('Dgleish', 2), ('briandaviddavidson', 2), ('sashazykov', 2), ('brianhatchl', 2), ('brianreavis', 2), ('duemir', 2), ('eliasp', 2), ('Giselle-MS', 2), ('gmaclennan', 2), ('simon04', 2), ('SatyaSudheer', 2), ('NopMap', 2), ('Nimisha94', 2), ('mpetroff', 2), ('51114u9', 2), ('JeeZeh', 2), ('iperdomo', 2), ('hchho', 2), ('jmandel1027', 2), ('Haves1001', 2), ('ltog', 2), ('smellman', 1), ('district10', 1), ('Teiron', 1), ('TagaSanPedroAko', 1), ('Sushil642', 1), ('Stormheg', 1), ('simonbilskyrollins', 1), ('SeanBarber', 1), ('sandykurniawan19', 1), ('slibby', 1), ('RoPP', 1), ('Rewinteer', 1), ('rbuffat', 1), ('thibaultmol', 1), ('Thue', 1), ('datendelphin', 1), ('sulfo', 1), ('linfindel', 1), ('naveensrinivasan', 1), ('CorruptComputer', 1), ('answerquest', 1), ('olafveerman', 1), ('ozcan-durak', 1), ('MindFreeze', 1), ('dobratzp', 1), ('schwindp', 1), ('pgiraud', 1), ('prateeklal', 1), ('prayagverma', 1), ('ProtD', 1), ('rtepowers', 1), ('ramunasd', 1), ('Appeltabak', 1), ('mrshu', 1), ('mxxcon', 1), ('n42k', 1), ('zzkt', 1), ('npmcdn-to-unpkg-bot', 1), ('paulklie', 1), ('pwelby', 1), ('rene78', 1), ('riQQ', 1), ('rugk', 1), ('sabas', 1), ('saleiva', 1), ('soshial', 1), ('yyazdi13', 1), ('zstadler', 1), ('cicku', 1), ('jidanni', 1), ('Vonter', 1), ('Wikiwide', 1), ('rivermont', 1), ('wcedmisten', 1), ('Yogurt4', 1), ('aaditya0000', 1), ('alanb43', 1), ('bryceco', 1), ('castriganoj', 1), ('danieldegroot2', 1), ('danielwu830', 1), ('endro', 1), ('faebebin', 1), ('henry4442', 1), ('irevenko', 1), ('m0rix', 1), ('mtmail', 1), ('brycenesbitt', 1), ('clkao', 1), ('moshen', 1), ('CommanderRoot', 1), ('frewsxcv', 1), ('CloCkWeRX', 1), ('tiziodcaio', 1), ('daguar', 1), ('davidgilbertson', 1), ('ENT8R', 1), ('ewnh', 1), ('emacgillavry', 1), ('Eric-Sparks', 1), ('e-n-f', 1), ('evansiroky', 1), ('excitablesnowball', 1), ('FrikanRw', 1), ('furkanmutlu', 1), ('0xAnon0602', 1), ('ajashton', 1), ('sashashura', 1), ('scaidermern', 1), ('andrewharvey', 1), ('AndrewHain', 1), ('ankit-m', 1), ('JackNUMBER', 1), ('AntonKhorev', 1), ('TAQ2', 1), ('arka-nitd', 1), ('artembert', 1), ('Asif-Sheriff', 1), ('bencostamagna', 1), ('wopfel', 1), ('bjornstar', 1), ('bradparker', 1), ('brandonreavis', 1), ('jleh', 1), ('kmpoppe', 1), ('demonshreder', 1), ('Zaczero', 1), ('Klumbumbus', 1), ('kriscarle', 1), ('lefuturiste', 1), ('lucymk', 1), ('LuisGC', 1), ('Elefant-aus-Wuppertal', 1), ('MaciejWWojcik', 1), ('manaswinidas', 1), ('Marc-marc-marc', 1), ('mvexel', 1), ('mbrunenieks-proofit', 1), ('mattiapezzotti', 1), ('mangerlahn', 1), ('McKaneAndrus', 1), ('furkanmutlu-tomtom', 1), ('GPSpilot', 1), ('guillaumep', 1), ('grischard', 1), ('guyarad', 1), ('Raubraupe', 1), ('hkirat', 1), ('haroldb', 1), ('harry-wood', 1), ('hirako2000', 1), ('hugovk', 1), ('jasonghent98', 1), ('jasonharrison', 1), ('joao', 1), ('oeon', 1), ('Jonobennett', 1), ('jonnybarnes', 1), ('jtracey', 1)","[17, 'Partnerships for the Goals']"
HospitalRun/hospitalrun-frontend,Frontend for HospitalRun,"# HospitalRun Frontend



![Status](https://img.shields.io/badge/Status-developing-brightgree) [![Release](https://img.shields.io/github/release/HospitalRun/hospitalrun-frontend.svg)](https://github.com/HospitalRun/hospitalrun-frontend/releases) [![Version](https://img.shields.io/github/package-json/v/hospitalrun/hospitalrun-frontend)](https://github.com/HospitalRun/hospitalrun-frontend/releases)
[![GitHub CI](https://github.com/HospitalRun/frontend/workflows/GitHub%20CI/badge.svg)](https://github.com/HospitalRun/frontend/actions) [![Coverage Status](https://coveralls.io/repos/github/HospitalRun/hospitalrun-frontend/badge.svg?branch=master)](https://coveralls.io/github/HospitalRun/hospitalrun-frontend?branch=master) [![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/HospitalRun/hospitalrun-frontend.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/HospitalRun/hospitalrun-frontend/context:javascript) ![Code scanning](https://github.com/HospitalRun/hospitalrun-frontend/workflows/Code%20scanning/badge.svg?branch=master) [![Documentation Status](https://readthedocs.org/projects/hospitalrun-frontend/badge/?version=latest)](https://hospitalrun-frontend.readthedocs.io)
[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-frontend.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-frontend?ref=badge_large) [![Commitizen friendly](https://img.shields.io/badge/commitizen-friendly-brightgreen.svg)](http://commitizen.github.io/cz-cli/)
![dependabot](https://api.dependabot.com/badges/status?host=github&repo=HospitalRun/hospitalrun-frontend) [![Slack](https://hospitalrun-slack.herokuapp.com/badge.svg)](https://hospitalrun-slack.herokuapp.com)



React frontend for [HospitalRun](http://hospitalrun.io/): free software for developing world hospitals.

---

# Are you a user? If yes...

[Visit this page for general information on the HospitalRun application](https://github.com/HospitalRun/hospitalrun/blob/master/README.md) including:

- How can I deploy 1.0.0-beta?
- Where do I report a bug or request a feature?
- How can I contribute? (There are several other ways besides coding)
- What is the project structure?
- What is the application infrastructure?
- Who is behind HospitalRun? etc.

# Would you like to contribute? If yes...

[Get started by checking out the Frontend Contributing Guide](https://github.com/HospitalRun/hospitalrun-frontend/blob/master/.github/CONTRIBUTING.md) for:
- What's the tech stack?
- Where can I become familiar with the technologies?
- Where do I browse issues?
- How do I set up my local environment?
- How do I run tests locally?
- How do I submit my changes?
- etc.

# License

Released under the [MIT license](LICENSE).
","'couchdb', 'emr', 'frontend', 'hacktoberfest', 'hospitalrun', 'hospitalrun-frontend', 'javascript', 'open-healthcare', 'pouchdb', 'react', 'redux'",2023-01-09T08:00:58Z,233,6790,329,"('jkleinsc', 1463), ('matteovivona', 650), ('dependabot-previewbot', 473), ('jackcmeyer', 315), ('tangollama', 309), ('JacobMGEvans', 157), ('nobrayner', 148), ('greenkeeperbot', 118), ('dependabotbot', 115), ('MatthewDorner', 90), ('codyarose', 90), ('stukalin', 86), ('greenkeeperio-bot', 85), ('fox1t', 74), ('emma-r-slight', 52), ('chadian', 47), ('Turbo87', 44), ('billybonks', 42), ('ocBruno', 31), ('donaldwasserman', 30), ('juhanakristian', 29), ('mkly', 28), ('tigerabrodi', 27), ('akshay-ap', 26), ('oliv37', 23), ('taras', 23), ('WinstonPoh', 21), ('btecu', 20), ('marcosvega91', 20), ('blestab', 20), ('yosephAHMED', 17), ('baoqchau', 16), ('merodiro', 15), ('oizuldan', 15), ('M-BenAli', 13), ('Aprillion', 12), ('erdeno', 12), ('mpeyper', 11), ('vault-developer', 10), ('alti21', 10), ('reidmeyer', 10), ('kiranjd', 10), ('adeolabadmus', 10), ('gmanou', 9), ('Proful', 9), ('falexsandro', 8), ('aboma', 8), ('hsorellana', 8), ('tobireuen', 8), ('anyapawar', 7), ('cynthiachen7', 7), ('weijentu', 7), ('sotous', 6), ('gnowoel', 6), ('turboMaCk', 6), ('DrewGregory', 6), ('internets', 6), ('RigoOnRails', 6), ('archwheeler', 6), ('agusbrand', 6), ('tchan', 6), ('ibiBgOR', 6), ('gaweki', 5), ('wwbarros', 5), ('rsousaj', 5), ('janmarkusmilan', 5), ('Adarsh710', 5), ('morsh', 5), ('Tomastomaslol', 5), ('gmanousaridis', 5), ('AlexTan331', 5), ('kilcorse-michael', 4), ('rubencgt', 4), ('SimonHFrost', 4), ('jeffrimko', 4), ('kognise', 4), ('jrmkim50', 4), ('atochef', 4), ('alexpelan', 4), ('anthonyaperez', 4), ('JDarke', 4), ('mattkuzco', 4), ('msalahz', 4), ('distributedlock', 4), ('mqchau', 4), ('nzidol', 3), ('pete-the-pete', 3), ('amyrlam', 3), ('lifeart', 3), ('semantic-release-bot', 3), ('riiniii', 3), ('akong', 3), ('connorlurring', 3), ('darrylpargeter', 3), ('dapierce', 3), ('EugenioAvila', 3), ('cpondoc', 3), ('Raul6469', 3), ('robrighter', 3), ('michaelkramer', 3), ('masanori1102', 3), ('martimfj', 3), ('Emalsha', 2), ('Kasahs', 2), ('sourabbanka22', 2), ('stoyan-ekupov', 2), ('tometo-dev', 2), ('FanciestW', 2), ('Chima1707', 2), ('danilojha', 2), ('eei34', 2), ('Dooris', 2), ('ziedtouibi', 2), ('la83lynx', 2), ('morrme', 2), ('brunorrr', 2), ('shiwangi20', 2), ('zinyando', 2), ('clettenberg', 2), ('Brahyt', 2), ('Alonski', 2), ('dastgirp', 2), ('sweeneydavidj', 2), ('DerekTBrown', 2), ('dev-script', 2), ('FalkF', 2), ('Hrugved', 2), ('TheGallery', 2), ('jcyang43', 2), ('LucasBN', 2), ('MarvinJWendt', 2), ('mleralec', 2), ('codecounselor', 2), ('locks', 2), ('richchurcher', 2), ('rodolfoghi', 2), ('MitchellCash', 1), ('napon', 1), ('nwmandel', 1), ('nisarhassan12', 1), ('nischayv', 1), ('ATLCTO', 1), ('jajodiaraghav', 1), ('Revln9', 1), ('nclBaz', 1), ('RichardLitt', 1), ('hd-genius', 1), ('SiddharthaSarma', 1), ('mellisdesigns', 1), ('mamoses', 1), ('PhearZero', 1), ('fuji939', 1), ('xmudrii', 1), ('vanakema', 1), ('marcorosas1991', 1), ('manaswinidas', 1), ('LoicB', 1), ('lisaychuang', 1), ('G07cha', 1), ('ArtificialQualia', 1), ('kartik95', 1), ('saksham93', 1), ('rabbihossain', 1), ('pjbass', 1), ('natbityou', 1), ('boredcity', 1), ('melissahie', 1), ('captain-enjoyable', 1), ('Mani-07', 1), ('linus345', 1), ('imalexsq', 1), ('gmmoraes', 1), ('gdemu13', 1), ('fossabot', 1), ('bmoore235', 1), ('aszx87410', 1), ('abc985', 1), ('ArturoDeVigo', 1), ('UmairKamran', 1), ('tma-tma', 1), ('alvesjtiago', 1), ('stephjs', 1), ('kud04rk', 1), ('samgaudr', 1), ('ryanauj', 1), ('esbanarango', 1), ('Epsilonnnn', 1), ('emadehsan', 1), ('dorianm', 1), ('sukima', 1), ('devinrhode2', 1), ('dvehar', 1), ('unobe', 1), ('TheBanditDave', 1), ('dbradf', 1), ('dannisonarias', 1), ('dorekhov1', 1), ('sericaia', 1), ('cfiorini74', 1), ('bspaulding', 1), ('heybereket', 1), ('ming13', 1), ('anko', 1), ('andrew-werdna', 1), ('alessioprestileo', 1), ('biasao', 1), ('omoabobade', 1), ('aeke', 1), ('Julio-Angel', 1), ('julienroulle', 1), ('juanjcsr', 1), ('n7down', 1), ('chibchombiano26', 1), ('ho1234c', 1), ('jonathanihm', 1), ('jmabry111', 1), ('James1x0', 1), ('jamesinsf-git', 1), ('jkingsman', 1), ('jscottchapman', 1), ('ImgBotApp', 1), ('hemantpandey17', 1), ('gustavoplenamente', 1), ('guiug', 1), ('gwal86', 1), ('gonzalompp', 1), ('giulianovarriale', 1), ('HugoGuiroux', 1), ('Fibii', 1), ('FergusInLondon', 1), ('fabriciofrontarolli', 1), ('b0nn13', 1), ('fabianschwarzfritz', 1)","[3, 'Good Health and Well-Being']"
inasafe/inasafe,InaSAFE - QGIS plugin for estimating impact from natural disasters,"InaSAFE
=======

[![InaSAFE Screenshot](http://inasafe.org/wp-content/uploads/2017/06/InaSAFE-screenshot-lores-1.jpg)](http://www.inasafe.org)

InaSAFE is free software that allows disaster managers to study realistic
natural hazard impact scenarios for better planning, preparedness and
response activities. InaSAFE is a plugin for [QGIS](http://qgis.org).

For more information about InaSAFE and its documentation please visit [inasafe.org](http://www.inasafe.org).

The latest source code is available at
[https://github.com/inasafe/inasafe](https://github.com/inasafe/inasafe),
which contains modules for risk calculations, GIS functionality and
functions for impact modelling.


* Current Travis test status master: [![Travis Build Status](https://travis-ci.org/inasafe/inasafe.svg?branch=master)](https://travis-ci.org/inasafe/inasafe)
* Current Landscape code health master: [![Landscape Code Health](https://landscape.io/github/inasafe/inasafe/master/landscape.svg?style=flat)](https://landscape.io/github/inasafe/inasafe/master)
* Current Travis test status develop: [![Travis Build Status](https://travis-ci.org/inasafe/inasafe.svg?branch=develop)](https://travis-ci.org/inasafe/inasafe)
* Current Landscape code health develop: [![Code Health](https://landscape.io/github/inasafe/inasafe/develop/landscape.svg?style=flat)](https://landscape.io/github/inasafe/inasafe/develop)
* PyPi Downloads for InaSAFE: [![PyPI version](https://badge.fury.io/py/inasafe-core.svg)](https://badge.fury.io/py/inasafe-core)
* PyPi Downloads for the old 'safe' library (currently unmaintained): [![PyPI version](https://badge.fury.io/py/python-safe.svg)](https://badge.fury.io/py/python-safe)
* Github download: [![Github All Releases](https://img.shields.io/github/downloads/inasafe/inasafe/total.svg)]()

Quick Installation Guide
========================

You first need to have [QGIS](http://qgis.org/) installed. Grab your free copy from [the QGIS download page](http://download.qgis.org).

To install the InaSAFE plugin, use the plugin manager in [QGIS](http://qgis.org):

  Plugins → Manage and Install Plugins

Then search for ""InaSAFE"", select it and click the install button.
The plugin will now be added to your plugins menu.

**Note:** You may need to restart QGIS if upgrading from a prior version.

Participation
=============

We work under the philosophy that stakeholders should have access to the
development and source code, and be able to participate in every level of the 
project - we invite comments, suggestions and contributions.  See
[our milestones list](https://github.com/AIFDR/inasafe/issues/milestones) and
[our open issues list](https://github.com/inasafe/inasafe/issues?q=is%3Aopen+is%3Aissue+no%3Amilestone)
for known bugs and outstanding tasks. You can also chat live with our developers
and community members using the link below.

[![Join the chat at https://gitter.im/AIFDR/inasafe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/AIFDR/inasafe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


System Requirements
-------------------

 - A standard PC with at least 4GB of RAM running Windows, Linux or Mac OS X
 - The QGIS Open Source Geographic Information System (http://www.qgis.org).
   Although InaSAFE will probably run on any version of QGIS version above 2.0
   or newer, our supported version is the latest Long Term Release version
   of QGIS.

Running tests
-------------------

 - InaSAFE is using the Unittest Python framework.
 - Different ways that you can run tests:
   - On Travis using a Pull Request
   - In QGIS Desktop if you enable `developer mode` in Plugins -> InaSAFE -> Options -> Advanced, restart QGIS and then click the run tests button.
   - Using docker `make docker-test`
   - Using PyCharm if you set up the dev environment with the qgis_prefix_path

History
=======

* In March 2011, Risk In A Box (the predecessor to InaSAFE) was built as a web 
  application running as a GeoNode Plugin.
* InaSAFE development as a QGIS plugin started in January 2012.
* In October 2012, version 1.0 of InaSAFE was launched at the [5th Asian ministerial conference on disaster risk reduction (AMCDRR)](http://www.unisdr.org/we/inform/events/23540) where it was demonstrated to 
  the then president of Indonesia, Susilo Bambang Yudhoyono who praised the project.
* On 30 Jan 2013, InaSAFE was awarded the prestigious [Black Duck Open Source Rookie of the Year Award](https://www.blackducksoftware.com/open-source-rookies).
* Version 2.0 of InaSAFE was released in January, 2014.
* Version 3.0 was released in March 2015 which added support
  for using the QGIS API within impact functions.
* Version 4.0 was released in March 2017.
* Version 5.0 was released in November 2018 which add support for QGIS 3.x

Disclaimer
==========

InaSAFE has been jointly developed by the Indonesian Government-BNPB, the
Australian Government, the World Bank-GFDRR and independent contributors.
These agencies and the individual software developers of InaSAFE take no
responsibility for the correctness of outputs from InaSAFE or decisions 
derived as a consequence.


License
=======

InaSAFE is free software: you can redistribute it and/or modify it
under the terms of the GNU General Public License version 3 (GPLv3) as
published by the Free Software Foundation.

The full GNU General Public License is available in LICENSE.txt or
http://www.gnu.org/licenses/gpl.html


Disclaimer of Warranty (GPLv3)
==============================

There is no warranty for the program, to the extent permitted by
applicable law. Except when otherwise stated in writing the copyright
holders and/or other parties provide the program ""as is"" without warranty
of any kind, either expressed or implied, including, but not limited to,
the implied warranties of merchantability and fitness for a particular
purpose. The entire risk as to the quality and performance of the program
is with you. Should the program prove defective, you assume the cost of
all necessary servicing, repair or correction.


Limitation of Liability (GPLv3)
===============================

In no event unless required by applicable law or agreed to in writing
will any copyright holder, or any other party who modifies and/or conveys
the program as permitted above, be liable to you for damages, including any
general, special, incidental or consequential damages arising out of the
use or inability to use the program (including but not limited to loss of
data or data being rendered inaccurate or losses sustained by you or third
parties or a failure of the program to operate with any other programs),
even if such holder or other party has been advised of the possibility of
such damages.

","'disasterresponse', 'inasafe', 'python', 'qgis', 'qgis-plugin', 'risk'",2024-01-19T12:32:25Z,48,252,54,"('timlinux', 4230), ('ismailsunni', 3847), ('uniomni', 1541), ('Gustry', 1360), ('akbargumbira', 1213), ('mbernasocchi', 909), ('lucernae', 479), ('borysiasty', 229), ('dichapabe', 191), ('cchristelis', 186), ('myarjunar', 173), ('KolesovDmitry', 124), ('elpaso', 119), ('alexbruy', 118), ('ingenieroariel', 86), ('bungcip', 84), ('mach0', 82), ('Samweli', 81), ('misugijunz', 67), ('ivanbusthomi', 63), ('nyalldawson', 63), ('assefay', 50), ('wonder-sk', 49), ('vanpuk', 47), ('dynaryu', 40), ('easmetz', 34), ('rudithiede', 11), ('gvallarelli', 9), ('manombawa', 9), ('maning', 7), ('MariaSolovyeva', 6), ('Charlotte-Morgan', 5), ('ted-dunstone', 4), ('tomchadwin', 4), ('tomkralidis', 4), ('vdeparday', 3), ('danylaksono', 3), ('gubuntu', 2), ('mbasa', 2), ('za', 2), ('olivierdalang', 2), ('anitanh', 1), ('lrntct', 1), ('simgislab', 1), ('nicolas-raoul', 1), ('gitter-badger', 1), ('waldoj', 1), ('faizalprbw', 1)","[13, 'Climate Action']"
getodk/build,ODK Build is a drag-and-drop form designer for ODK XForms. Thousands of users around the world depend on it for their data collection campaigns. Contribute and make the world a better place! ✨📝✨,"## ⚠️ ODK Build is no longer being updated. Please use [XLSForm](https://docs.getodk.org/xlsform/) instead. ⚠️

# ODK Build

[![docker](https://github.com/getodk/build/actions/workflows/docker.yml/badge.svg)](https://github.com/getodk/build/actions/workflows/docker.yml)

ODK Build is a web-based, drag-and-drop service for creating forms used with data collection tools such as [ODK Collect](https://docs.getodk.org/collect-intro/) and [ODK Central](https://docs.getodk.org/central-intro/). 
ODK Build is part of ODK, a free and open-source set of tools which help organizations author, field, and manage mobile data collection solutions. Learn more about the ODK project and its history [here](https://getodk.org/) and read about example ODK deployments [here](https://forum.getodk.org/c/showcase).

## Get started
Start right now building your own forms at [https://build.getodk.org](https://build.getodk.org).
Read the [user manual](https://docs.getodk.org/build-intro/) and post on the [ODK forum](https://forum.getodk.org/c/support) if you get stuck.

If you want to build forms while offline, Build offers an [offline version](docs/offline.md).

## Contribute
Contributions, big and small, are welcome! 
Create an [issue](https://github.com/getodk/build/issues) if you found a bug or want to suggest a new feature, 
and review the [roadmap](https://github.com/getodk/build/projects/1) to see what's being worked on.

Read the [contribution guide](docs/contribute.md) to get started.

Further information for developers and maintainers:

* The [architecture](docs/architecture.md) provides a high-level overview of how Build works.
* The [developer guide](docs/develop.md) guides you through the steps to work on Build.
* The [deployment guide](docs/deploy.md) shows how to deploy build to a production server.

## License
Build is licensed under the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) license.

","'data-collection', 'form-design', 'global-development', 'global-health', 'javascript', 'mobile-data-collection', 'odk', 'ruby', 'social-impact', 'xforms'",2023-11-02T00:27:47Z,12,110,26,"('issa-tseng', 404), ('yanokwa', 37), ('lognaturel', 34), ('dmac', 12), ('florianm', 7), ('rockydcoder', 2), ('ukanga', 2), ('moogacs', 1), ('danbjoseph', 1), ('erikhowe', 1), ('JamieSlome', 1), ('trendspotter', 1)","[17, 'Partnerships for the Goals']"
owid/owid-grapher,A platform for creating interactive data visualizations,"# owid-grapher

[![Actions Status](https://github.com/owid/owid-grapher/workflows/Continuous%20Integration/badge.svg)](https://github.com/owid/owid-grapher/actions)
[![Test coverage](https://owid.github.io/badges/coverage.svg)](https://owid.github.io/coverage/)
[![Storybook](https://raw.githubusercontent.com/storybookjs/brand/master/badge/badge-storybook.svg)](https://owid.github.io/stories/)

The monorepo we use at [Our World in Data](https://ourworldindata.org) to create and publish embeddable, interactive visualizations like this one:

[![A Grapher chart showing world-wide life expectancy at birth. Click for interactive.](https://ourworldindata.org/grapher/exports/life-expectancy.svg)](https://ourworldindata.org/grapher/life-expectancy)

## ✋ Disclaimer

This repo is currently not well-designed for reuse as a visualization library, nor for reproducing the full production environment we have at Our World in Data, as our tools are tightly coupled with our database structure.

We're gradually making steps towards making our work more reusable, however we still prioritize [needs specific to our project](#why-did-we-start-this-project) that can be at odds with making our tools reusable.

You are still very welcome to reuse and adapt any of our code for your own purposes, and we welcome [contributions](CONTRIBUTING.md)!

## 🏎 Quick start

To quickly get a version of the site running for developing Grapher features, we recommend following the [local development setup](docs/docker-compose-mysql.md) guide.

[Additional setup options](docs/setup-options-overview.md) are also available for other use cases.

## 🗂 Overview

Multiple projects are maintained in this repo:

### [Grapher](packages/%40ourworldindata/grapher/)

A client-side interactive data visualization library used by almost every chart on Our World in Data.

All grapher data is stored in a MySQL database that contains both JSON configuration objects for individual charts as well as the data values that they ingest.

The Grapher project is built with [Lerna](https://github.com/lerna/lerna/) and [tsup](https://github.com/egoist/tsup).

### [Explorer](explorer/)

A Grapher-based tool that creates more complex [data visualization user interfaces](https://ourworldindata.org/explorers/migration).

Each explorer can be configured via a [panel](explorerAdminServer/) in the admin client. Their config files are stored in [a separate repository](https://github.com/owid/owid-content/tree/master/explorers).

### Grapher Admin

-   A [client-side](adminSiteClient/) project that provides a user interface for configuring graphers, explorers, and managing and uploading data.

-   A [server-side](adminSiteServer/) project that manages the MySQL database used by graphers.

### [WordPress](wordpress/)

The CMS we use to manage articles published on Our World in Data. It's a relatively stock setup, with a custom plugin to provide additional blocks for the Gutenberg editor.

Our Wordpress content and configuration is stored in a MySQL database, which currently isn't shared publicly.

We are currently in the process of migrating off of WordPress to a publishing flow based on [ArchieML](https://archieml.org) with Google Docs. See the [Site README](site/README.md) for more information.

### [Baker](baker/)

A [PM2](https://github.com/Unitech/pm2) project that builds a static copy of the Our World in Data website by merging the content authored in Wordpress with the grapher charts created in Grapher Admin.

### [Site](site/)

The React code for rendering our content in pages, used by the Grapher Admin and Baker.

As of March 2023, code exists for rendering both WordPress posts and Google Docs as we work on the transition away from WordPress.

## Tooling

Much of our code is based around [reactive programming](https://en.wikipedia.org/wiki/Reactive_programming) using [React](https://reactjs.org/) and [Mobx](http://github.com/mobxjs/mobx).

All non-WordPress code is written in [TypeScript](https://www.typescriptlang.org/).

If you want to enable pre-commit hooks, run `yarn husky`.

[Visual Studio Code](https://code.visualstudio.com/) is recommended for autocompletion and other awesome editor analysis features enabled by static typing.

## Why did we start this project?

The following is an excerpt explaining the origin of this repo and what the alternatives tried were (source: [Max Roser's Reddit AMA on Oct 17, 2017](https://www.reddit.com/r/dataisbeautiful/comments/76yknx/hi_reddit_i_am_max_roser_founder_of_the_online/doicj1j?utm_source=share&utm_medium=web2x&context=3))

> We built the Grapher because there is no similar external tool available. Datawrapper, Tableau, Plotly, various libraries based on d3 are out there but nothing is similar to what the Grapher does for our project.
>
> Before we developed this tool, we built interactive web visualizations by hand through a difficult process of preparing individual spreadsheets of data and then writing custom HTML and JavaScript code to process the contents for each individual visualization. That was pretty painful and it took me hours sometimes to built a chart.
>
> The owid-grapher solves this problem by using a single visualization codebase and crucially a single database into which all of our data is placed. Once the data has been imported, the process of creating a visualization is reduced to simply choosing what kind of visualization is needed and then selecting the relevant variables in the Grapher user interface. The result may then be customized, and is published to the web with the press of a button.
>
> Using our own system has very important advantages:
>
> -   **Integration with our global development database**: Our database of global development metrics is integrated into our visualization tool so that when we add and update empirical data the visualizations are all updated. (In contrast to this, a pre-existing tool would make the exploration of a database impossible and would require the preparation of each dataset separately for each visualisation.)
> -   **Flexibility**: We can use automation to change our entire system all at once. For example, if we decide we want to use a different source referencing style, we could easily update this across hundreds of charts. This makes it possible to scale our publication and to sustainably improve our work without starting from scratch at each round.
> -   **Risk mitigation**: We hope(!) that Our World in Data is a long-term project and we want the visualizations we produce to continue to be useful and available years from now. An external web service may be shut down or change for reasons we cannot control. We have had this experience in the past and learned our lesson from it.
> -   **Keeping everything up-to-date**: Because we want to be a useful resource for some time we make sure that we have a technology in place that allows us to keep all of our work up-to-date without starting from scratch each time. We have our global development database directly integrated in the Grapher and as soon as new data becomes available (for example from a UN agency) we can run a script that pulls in that data and updates all the visualizations that present that data.

---

Cross-browser testing provided by  BrowserStack

Client-side bug tracking provided by 
","'data-visualization', 'react', 'typescript'",2024-05-03T13:46:20Z,45,1321,31,"('mlbrgl', 2674), ('danielgavrilov', 2407), ('marcelgerber', 2254), ('ikesau', 1624), ('danyx23', 1322), ('breck7', 1309), ('sophiamersmann', 1144), ('zdenekhynek', 785), ('larsyencken', 276), ('shaahmad', 221), ('samizdatco', 147), ('Marigold', 136), ('aaldaber', 128), ('jasoncrawford', 83), ('mathisonian', 82), ('dependabotbot', 81), ('sherin', 60), ('dependabot-previewbot', 52), ('lucasrodes', 47), ('bnjmacdonald', 38), ('HannahRitchie', 26), ('MahmoudHamdy02', 17), ('markledwich2', 15), ('rakyi', 11), ('owidbot', 10), ('pabloarosado', 7), ('BhCh7051', 7), ('shafy', 4), ('edomt', 4), ('zamakkat', 3), ('toni-sharpe', 3), ('parthiv360', 3), ('aral', 2), ('Anubhav-2003', 2), ('wcox13', 1), ('ianxul', 1), ('Kalli', 1), ('madhums', 1), ('jazzido', 1), ('PremBharwani', 1), ('SukkaW', 1), ('swaptr', 1), ('tom-wvs', 1), ('YannyTwoo', 1), ('xammamax', 1)","[17, 'Partnerships for the Goals']"
Flowminder/FlowKit,FlowKit: Flowminder CDR analytics toolkit,"# FlowKit - CDR Analytics Toolkit

[![CircleCI](https://img.shields.io/circleci/build/gh/Flowminder/FlowKit.svg?logo=CircleCI&style=flat-square)](https://circleci.com/gh/Flowminder/FlowKit)  [![codecov](https://img.shields.io/codecov/c/github/Flowminder/FlowKit.svg?logo=Codecov&style=flat-square)](https://codecov.io/gh/Flowminder/FlowKit) [![License: MPL 2.0](https://img.shields.io/github/license/Flowminder/FlowKit.svg?style=flat-square)](https://opensource.org/licenses/MPL-2.0) [![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/python/black)  [![DOI](https://zenodo.org/badge/155638125.svg)](https://zenodo.org/badge/latestdoi/155638125)

## What is FlowKit ?

FlowKit is a platform for analysis of Call Detail Records (CDR) and other data. CDR data is created by mobile network operators (MNOs) primarily for generating subscriber bills and settling accounts with other carriers.

FlowKit is designed to extend CDR data analysis to meet many other applications beyond billing. Some examples include disaster response, precision epidimiology and transport and mobility, more examples can be found here.

CDRs constitute a highly sensitive data set, so FlowKit is designed with privacy protection in mind. It includes the FlowAuth framework to enable fine-grained authorization with extensive access logging, making it an important tool for deployment of a GDPR compliant CDR analysis system.

### Documentation

The FlowKit documentation is available [here](https://flowminder.github.io/FlowKit/).

### Development status and installation

FlowKit is under ongoing development. The list of releases can be found [here](https://github.com/Flowminder/FlowKit/releases). Until FlowKit reaches full stable release status, we recommend installing the latest version based on the Github master branch. For details see the installation instructions [here](https://flowminder.github.io/FlowKit/install/).

### Benchmarks

There is a suite of benchmarks for FlowKit at https://github.com/Flowminder/FlowKit-benchmarks.
The benchmark results can be seen at https://flowminder.github.io/FlowKit-benchmarks.
","'analysis', 'cdr', 'cdr-analytics-toolkit', 'cdr-data', 'flowkit', 'mobile', 'mobility'",2024-05-03T15:20:16Z,15,86,12,"('greenape', 3408), ('mergifybot', 2611), ('maxalbert', 1503), ('jc-harrison', 1337), ('dependabotbot', 1187), ('Thingus', 789), ('dependabot-previewbot', 553), ('dependabot-support', 311), ('BhavinPanch', 237), ('danwilliams', 99), ('OwlHute', 59), ('flowstef', 49), ('gzagatti', 38), ('chrisjbrooks', 2), ('gitter-badger', 1)","[11, 'Sustainable Cities and Communities']"
OperationCode/operationcode-pybot,Operation Code's Official Slackbot,"[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Twitter Follow](https://img.shields.io/twitter/follow/operation_code.svg?style=social&label=Follow&style=social)](https://twitter.com/operation_code)
[![Code-style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://contributor-covenant.org/)

# OperationCode-Pybot

OperationCode PyBot is a Python [Slack](https://api.slack.com) Bot utilizing [Slack Bolt](https://github.com/SlackAPI/bolt-python).

## Resources
* [Slack Web API Methods](https://api.slack.com/methods) - used to interact with Slack beyond the built-in Slack Bolt capabilities
* [Slack Block Kit](https://api.slack.com/block-kit) - used to build the blocks used in various requests and responses
* [Slack Bolt](https://slack.dev/bolt-python/tutorial/getting-started) - the underlying framework of the bot
* [Slack Bolt API Reference](https://slack.dev/bolt-python/api-docs/slack_bolt/index.html)
* [Slack Python SDK API Reference](https://slack.dev/python-slack-sdk/api-docs/slack_sdk/index.html)


## Contributing
[Bug reports](https://github.com/OperationCode/operationcode-pybot/issues) and [pull requests](https://github.com/OperationCode/operationcode-pybot/pulls) are welcome on [our Github repo](https://github.com/OperationCode/operationcode-pybot).
This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [Contributor Covenant](http://contributor-covenant.org) code of conduct.
The best place to get assistance with OperationCode-Pybot is on [Slack](https://operationcode.org/join) in the `#oc-python-project` channel.

## Quick Start
Recommended versions of tools used within the repo:
- `python@3.10` or greater
- `pipenv@2021.5.29` or greater - [pipenv](https://github.com/pypa/pipenv) is a package manager similar to poetry that utilizes `pip` to manage project dependencies, along with creating new virtual environments
and deterministic builds

```bash
# Ensure you have pipenv already installed
pipenv install --dev

# Start up your virtual environment
pipenv shell

# Run the test suite
pytest

# Run the code formatter
black .
```

## How to Test Integration with SlackAPI

In order to test the new methods and interactions you may have created already,
you'll need an ""app configuration token"". In order to get one of those, you'll need to create [a new
issue](https://github.com/OperationCode/operationcode-pybot/issues). Please use the `type: config token request`
label and make the title "" Requests an App Config Token"". For example: `Judson Stevens Requests an App Config Token`.

Once you have created your issue, one of the maintainers of this repository will get in touch and give you your token.


## How to Test Integration With Slack

After having developed some new feature, or having in hand what you believe is
a fix for an existing bug, how do you test it out in a real system in order to
make sure that your changes do all that you hope they do? The answer; bring up
the application in your own environment and hook it up to Slack!

In order to do this, you'll want to tackle the following items in order:

1. Setup your own Slack workspace.
2. Grab a signing secret from Slack that pybot can utilize.
3. Launch pybot locally, passing it your Slack signing secret.
4. Attach your pybot instance to the public internet so that Slack can speak
   with it.
5. Point Slack at your running pybot instance, and properly configure it.

The following sections will guide you through each of these stages.

### 1 - Setup Your Own Slack Workspace

To start, you'll want to visit Slack's [Getting
Started](https://slack.com/get-started) page. From this page, follow the steps
required to create a new workspace. The names/options you configure during
creation don't matter so much, but make sure you associate it with an email
address you have access to. Once complete it should present you with an option
to login to the new workspace, make sure you go ahead and do that.

If you're having a hard time figuring this out, try checking out the following
Slack article [Create a Slack Workspace](https://slack.com/intl/en-ca/help/articles/206845317-Create-a-Slack-workspace).

#### Create expected channels
Several of Pybot's features involve sending messages to specific channels - in order
for this to work in your personal Slack workspace you'll need to create the following channels:
- mentors-internal
- greetings
- moderators
- oc-tech


### 2 - Create a pybot App in Your Slack Workspace

The next step is to create a new bot application in your workspace. While still
logged in, visit the [App Management](https://api.slack.com/apps) page and
choose to create a new app. During this process, make sure to copy down the
signing secret key that gets generated for your app, as you'll need it later,
following this, follow the guidelines for creating a bot app as laid out in the
[Enabling interactions with bots](https://api.slack.com/bot-users) article. When
you get to the stage of creating the bot user, make sure to write down the bot
user OAuth access token that is presented, as you'll need to use it later.

On the `OAuth & Permissions` page configure the Pybot app with the following scopes

- channels:manage
- chat:write
- chat:write.public
- commands
- users:read

### 3 - Launch pybot Locally, Passing in Your Signing Secret

With your Slack workspace, app and bot user created, and your app signing secret
and bot user OAuth access token in hand, you should now be ready to configure
pybot to integrate with your new Slack workspace. To do this, you'll first want
to setup the proper configuration in pybot.

pybot configuration is specified completely through environment variables. When
running locally, you can configure the _./docker/pybot.env_ file with the
environment variable name/value pairings, which will get evaluated on
application start. Otherwise, make sure to export or pass in the correct
environment variables through your shell when launching pybot.

Here's an example of configuring these through the _pybot.env_ file:

```bash
SLACK_BOT_SIGNING_SECRET=APP-SIGNING-SECRET
BOT_USER_OAUTH_ACCESS_TOKEN=BOT-USER-OAUTH-TOKEN
```

**NOTE**: More configuration settings than these may be specified. Please see
the _Known Configuration Settings_ section near the bottom of this document
for details on other settings that can be set.

### 4 - Attach Your pybot Instance to the Public Internet

With an instance of pybot running, you now need to expose this instance to the
public internet so Slack can send in API requests. You can easily utilize ngrok
for this purpose if you wish. To do so; download ngrok from https://ngrok.com/download
and set up a tunnel like so:

```bash
ngrok http 5000
```

Pay attention to copy out the response you get and keep this command running.
Here's an example output from the command:

```bash
ngrok by @inconshreveable                                                                        (Ctrl+C to quit)
Session Status                online                                                                             
Session Expires               7 hours, 56 minutes                                                                
Version                       2.3.35                                                                             
Region                        United States (us)                                                                 
Web Interface                 http://127.0.0.1:4040                                                              
Forwarding                    http://9d73595a7aac.ngrok.io -> http://localhost:5000                              
Forwarding                    https://9d73595a7aac.ngrok.io -> http://localhost:5000                             
Connections                   ttl     opn     rt1     rt5     p50     p90                                        
                              0       1       0.00    0.00    0.00    0.00                                       
HTTP Requests 
```

With this done, ngrok will now expose the instance of pybot running locally
on port 5000 via the ""Forwarding"" address it returns.  Be sure to use the URL
beginning with http**s**.

### 5 - Point Slack at Your Running pybot Instance

With the initial Slack configuration complete and your instance of pybot
running on the public internet, it is now the perfect time to fully configure
Slack to interact with your bot. Depending on the interactions you're wanting to
play with, there are various configurations you can specify, which can be
broken down into the following parts:

*  Event Subscriptions - this allows pybot to respond to various events that may
   occur in your Slack workspace.
*  Slash Commands - this allows a user to invoke various commands from any
   channel in your workspace to interact with pybot.
*  Interactive Components - this allows various options to be exposed when
   right clicking on a message, or, when the bot presents various user
   elements that can be interacted with, instructs Slack on where to send the
   results for such interactions.

High level steps for configuring each of these can be found in the following
sub-sections; note that you don't need to necessarily configure all of these,
it all depends on what areas of pybot you're wanting to play with.

#### Event Subscriptions

You can follow the instructions (and read helpful related information) on the
[Events API](https://api.slack.com/events-api) page on Slack to setup event
subscriptions. When configuring your events URI; make sure you pass in the
Base-URI that pybot is listening on followed by the text _/slack/events_. For
example:

    https://123_random_code_321.ngrok.io/slack/events

Additional setup may be needed depending on the type of events pybot is subscribing to. 
For example, in order to work on the app's functionality on a `team_join` event, you need to:

* Add `team_join` to workspace event
* Make sure `greetings` channel exists and ensure the app is invited to the channel
* Add necessary OAuth scopes to the app e.g. `users:read`, `chat:write`, etc.

In the section which says ""Subscribe to events on behalf of users"", you must add the following events:

| Event Name            | Required OAuth Scope         |
|-----------------------|------------------------------|
| member_joined_channel | channels:read or groups:read |
| message.channels      | channels:history             |
| message.groups        | groups:history               |
| message.im            | im:history                   |
| team_join             | users:read                   |

#### Slash Commands

You can follow the instructions (and read helpful related information) on the
[Enabling interactivity with Slash Commands](https://api.slack.com/interactivity/slash-commands)
page on Slack to setup pybot slash commands. When configuring a Slash command,
make sure you configure the request URL to match the Base-URI that pybot is
listening on followed by the text _/slack/commands_. For example:

    https://123_random_code_321.ngrok.io/slack/commands
   
You'll use the same URI for each command. Here's a table listing of currently
supported commands along with some suggested configuration text:

| Command           | Description                    | Usage Hint                           |
|-------------------|--------------------------------|--------------------------------------|
| /lunch            | find lunch suggestions nearby  | &lt;zip code> &lt;distance in miles> |
| /mentor           | request mentoring              |                                      |
| /mentor-volunteer | offer to mentor others         |                                      |
| /repeat           | parrot canned messages         | &lt;10000                            |ask|ldap|merge|firstpr|channels|resources>
| /report           | report something to the admins |                     |
| /roll             | roll x dice with y sides       |                                 |
| /ticket           | submit ticket to admins        | (text of ticket)                     |

**👋 IMPORTANT!**

The `/lunch` command requires a valid Yelp API token stored in the `YELP_TOKEN` 
environment variable. See https://www.yelp.com/developers/faq

Similarly, the `/mentor` and `/mentor-volunteer` commands require access to an Airtable
environment with a specific configuration.  If you're planning on working with the mentor
functionality please reach out to the `#oc-python-projects` channel for help getting set up.  

#### Interactive Components

You can follow the instructions (and read helpful related information) on the
[Handling user interaction in your Slack apps](https://api.slack.com/interactivity/handling)
page on Slack to setup Slack interactive component configuration. When
configuring the request URL, you'll want to set it to the Base-URI that pybot
is listening on followed by the text _/slack/actions_. For example:

    https://123_random_code_321.ngrok.io/slack/actions

You'll also want to make sure to configure the report message action with the
following parameters:

| Name           | Description                   | Callback ID    |
|----------------|-------------------------------|----------------|
| Report Message | Report this message to admins | report_message |

## License
This package is available as open source under the terms of the [MIT License](http://opensource.org/licenses/MIT).



## Notes
Option 1 - Create your own Slack workspace to use for testing.
Follow [this guide](https://slack.dev/bolt-python/tutorial/getting-started-http)


Start the application with WebSockets instead of HTTP for better development experience? Requires the use of the SLACK_APP_TOKEN.
Would need to set an environment variable to determine if we were in development or staging/production.

Database to store history of events or just use logging? Probably best to use a database to store history of requests and responses?
Easier to track interactions that way. 

Utilizing FastAPI allows us to take advantage of things like Pydantic, inherent typing, models, and a better handler
for the HTTP requests themselves.

Utilizing [FastAPI](https://fastapi.tiangolo.com/) and [Slack-Bolt](https://slack.dev/bolt-python/tutorial/getting-started-http).

All the interactive elements of this bot were built using the Slack [Block Kit Builder](https://app.slack.com/block-kit-builder/).
The example JSON for each interactive element can be found in the `modules/slack/blocks/block_kit_examples` folder.","'hacktoberfest', 'hacktoberfest2017', 'hacktoberfest2018', 'hacktoberfest2019', 'hacktoberfest2020', 'python', 'slack-api', 'slackbot'",2024-04-03T15:55:44Z,21,32,8,"('AllenAnthes', 181), ('JudsonStevens', 43), ('apex-omontgomery', 22), ('aaron-junot', 5), ('AshTemp', 5), ('chynh', 4), ('kylemh', 4), ('garyray-k', 3), ('dependabot-previewbot', 3), ('dependabotbot', 3), ('Vaishnavi-cyber-blip', 2), ('Adeola-Adesoba', 1), ('wendeee', 1), ('harithesolo', 1), ('MDeanLindsay', 1), ('vyaspranjal33', 1), ('RochelleLynn-programmer', 1), ('techytushar', 1), ('vatsalsharma376', 1), ('ptrstr', 1), ('cskinner74', 1)","[10, 'Reduced Inequalities']"
synthetichealth/synthea,Synthetic Patient Population Simulator,"# SyntheaTM Patient Generator ![Build Status](https://github.com/synthetichealth/synthea/workflows/.github/workflows/ci-build-test.yml/badge.svg?branch=master) [![codecov](https://codecov.io/gh/synthetichealth/synthea/branch/master/graph/badge.svg)](https://codecov.io/gh/synthetichealth/synthea)

SyntheaTM is a Synthetic Patient Population Simulator. The goal is to output synthetic, realistic (but not real), patient data and associated health records in a variety of formats.

Read our [wiki](https://github.com/synthetichealth/synthea/wiki) and [Frequently Asked Questions](https://github.com/synthetichealth/synthea/wiki/Frequently-Asked-Questions) for more information.

Currently, SyntheaTM features include:
- Birth to Death Lifecycle
- Configuration-based statistics and demographics (defaults with Massachusetts Census data)
- Modular Rule System
  - Drop in [Generic Modules](https://github.com/synthetichealth/synthea/wiki/Generic-Module-Framework)
  - Custom Java rules modules for additional capabilities
- Primary Care Encounters, Emergency Room Encounters, and Symptom-Driven Encounters
- Conditions, Allergies, Medications, Vaccinations, Observations/Vitals, Labs, Procedures, CarePlans
- Formats
  - HL7 FHIR (R4, STU3 v3.0.1, and DSTU2 v1.0.2)
  - Bulk FHIR in ndjson format (set `exporter.fhir.bulk_data = true` to activate)
  - C-CDA (set `exporter.ccda.export = true` to activate)
  - CSV (set `exporter.csv.export = true` to activate)
  - CPCDS (set `exporter.cpcds.export = true` to activate)
- Rendering Rules and Disease Modules with Graphviz

## Developer Quick Start

These instructions are intended for those wishing to examine the Synthea source code, extend it or build the code locally. Those just wishing to run Synthea should follow the [Basic Setup and Running](https://github.com/synthetichealth/synthea/wiki/Basic-Setup-and-Running) instructions instead.

### Installation

**System Requirements:**
SyntheaTM requires Java JDK 11 or newer. We strongly recommend using a Long-Term Support (LTS) release of Java, 11 or 17, as issues may occur with more recent non-LTS versions.

To clone the SyntheaTM repo, then build and run the test suite:
```
git clone https://github.com/synthetichealth/synthea.git
cd synthea
./gradlew build check test
```

### Changing the default properties


The default properties file values can be found at `src/main/resources/synthea.properties`.
By default, synthea does not generate CCDA, CPCDA, CSV, or Bulk FHIR (ndjson). You'll need to
adjust this file to activate these features.  See the [wiki](https://github.com/synthetichealth/synthea/wiki)
for more details, or use our [guided customizer tool](https://synthetichealth.github.io/spt/#/customizer).



### Generate Synthetic Patients
Generating the population one at a time...
```
./run_synthea
```

Command-line arguments may be provided to specify a state, city, population size, or seed for randomization.
```
run_synthea [-s seed] [-p populationSize] [state [city]]
```

Full usage info can be printed by passing the `-h` option.
```
$ ./run_synthea -h     

> Task :run
Usage: run_synthea [options] [state [city]]
Options: [-s seed]
         [-cs clinicianSeed]
         [-p populationSize]
         [-r referenceDate as YYYYMMDD]
         [-g gender]
         [-a minAge-maxAge]
         [-o overflowPopulation]
         [-c localConfigFilePath]
         [-d localModulesDirPath]
         [-i initialPopulationSnapshotPath]
         [-u updatedPopulationSnapshotPath]
         [-t updateTimePeriodInDays]
         [-f fixedRecordPath]
         [-k keepMatchingPatientsPath]
         [--config*=value]
          * any setting from src/main/resources/synthea.properties

Examples:
run_synthea Massachusetts
run_synthea Alaska Juneau
run_synthea -s 12345
run_synthea -p 1000
run_synthea -s 987 Washington Seattle
run_synthea -s 21 -p 100 Utah ""Salt Lake City""
run_synthea -g M -a 60-65
run_synthea -p 10 --exporter.fhir.export=true
run_synthea --exporter.baseDirectory=""./output_tx/"" Texas
```

Some settings can be changed in `./src/main/resources/synthea.properties`.

SyntheaTM will output patient records in C-CDA and FHIR formats in `./output`.

### SyntheaTM GraphViz
Generate graphical visualizations of SyntheaTM rules and modules.
```
./gradlew graphviz
```

### Concepts and Attributes
Generate a list of concepts (used in the records) or attributes (variables on each patient).
```
./gradlew concepts
./gradlew attributes
```

# License

Copyright 2017-2023 The MITRE Corporation

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
","'fhir', 'health-data', 'simulation', 'synthea', 'synthetic-data', 'synthetic-population'",2024-05-03T14:11:58Z,30,2004,73,"('jawalonoski', 1606), ('dehall', 882), ('eedrummer', 379), ('RobertScalfani', 324), ('hadleynet', 286), ('nanowizard', 185), ('brianandersonmitre', 144), ('mickohanlon23', 68), ('shabiel', 49), ('mzwong', 47), ('laambrosino', 42), ('pmadria', 40), ('cmbear37', 38), ('mdk6jd', 38), ('cjduffett', 35), ('Abdulrahims', 34), ('cmoesel', 33), ('mosiermt', 26), ('kghoreshi', 26), ('jeffeastman', 24), ('johngrimes', 20), ('tiloc', 19), ('afansi', 19), ('casey7083', 15), ('glow-mdsol', 14), ('jcgeer', 10), ('andrequina', 10), ('BrianKolowitz', 9), ('swohlever', 7), ('babraham33', 5)","[3, 'Good Health and Well-Being']"
tidepool-org/platform,The Tidepool Platform API,"# Platform

The Tidepool Platform API.

[![Build Status](https://travis-ci.com/tidepool-org/platform.png)](https://travis-ci.com/tidepool-org/platform)
[![Code Climate](https://codeclimate.com/github/tidepool-org/platform/badges/gpa.svg)](https://codeclimate.com/github/tidepool-org/platform)
[![Issue Count](https://codeclimate.com/github/tidepool-org/platform/badges/issue_count.svg)](https://codeclimate.com/github/tidepool-org/platform)

# Setup

1. Install Go version 1.11.4 or later
1. Install mongodb (if it is not already installed, or run it from Docker)

    The tests assume that mongodb is listening on 127.0.0.1:27017.
    1. Configure mongodb replica sets (required for tests to pass)

        A single node is all that's required. It can be as simple as simple adding `--replSet rs0` when running mongd, or the equivalent config file change.
1. Start mongodb (if it is not already running)
    1. Initiate the replica set

	    Something like: `mongosh rs.initiate()`
1. Clone this repo
1. Change directory to the path you cloned the repo into
1. Source the `env.sh` file
1. Execute `make buildable` to install the various Go tools needed for building and editing the project

For example:

```
brew install go
brew install mongo
brew services start mongodb
git clone https://github.com/tidepool-org/platform.git
cd platform
. ./env.sh
make buildable
```

# Execute

1. Setup the environment, as above.
1. Build the project.
1. Execute a service.

In addition to the setup above, for example:

```
make build
_bin/services/data/data
```

Use `Ctrl-C` to stop the executable. It may take up to 60 seconds to stop.

> **Note:** For testing and development, services are generally run on a local Kubernetes cluster through the [development repo](https://github.com/tidepool-org/development#developing-tidepool-services).

# Makefile

* To setup your Go environment for building and editing the project:

```
make buildable
```

* To build the executables:

```
make build
```

All executables are built to the `_bin` directory in a hierarchy that matches the locations of executable source files.

The environment variable `BUILD` indicates which executables to build. If not specified, then all executables are built. For example, to build just the executables found in the `services` directory:

```
BUILD=services make build
```

* To run all of the tests manually:

```
make test
```

The environment variable `TEST` indicates which package hierarchy to test. If not specified, then all packages are tested. For example,

```
TEST=user make test
```

* To run all of the tests automatically after any changes are made, in a separate terminal window:

```
make test-watch
```

The environment variable `WATCH` indicates which package hierarchy to test. If not specified, then all packages are tested. For example,

```
WATCH=user make test-watch
```

* To run `gofmt`, `goimports`, `go vet`, and `golint`:

```
make pre-commit
```

* To clean the project of all build files:

```
make clean
```

# Sublime Text

If you use the Sublime Text editor with the GoSublime plugin, open the `platform.sublime-project` project to ensure the `GOPATH` and `PATH` environment variables are set correctly within Sublime Text. In addition, the recommended user settings are:

```
{
  ""autocomplete_builtins"": true,
  ""autocomplete_closures"": true,
  ""autoinst"": false,
  ""fmt_cmd"": [
    ""goimports""
  ],
  ""fmt_enabled"": true,
  ""fmt_tab_width"": 4,
  ""use_named_imports"": true
}
```

# Upgrade Golang Version

## Prepare

**Before** you update this repository to use a newer version of Golang, please perform these checks:

- Review the release notes for **all** Golang versions, major and minor, from the current Golang version to the target Golang version. The entire Golang release history can be found at https://golang.org/doc/devel/release.html.
  - For major revisions, if any change described in the release notes could have a negative impact upon this repository, follow up and review any associated issues and the updated code. Make note of this change in order to explicitly test after upgrading.
  - For minor revisions, review all issues included in the associated GitHub milestone issue tracker. These can be found in the minor revision release notes. If any issue could have a negative impact upon this repository, review the updated code. Make note of this issue in order to explicitly test after upgrading.
- Install `gimme`(https://github.com/travis-ci/gimme) via `brew`. Execute `gimme -k`. Ensure that the target Golang version is listed. The `gimme` tool is used by Travis CI to manage Golang versions. If the version is not listed, then the Travis CI build will not succeed.
- Browse to https://hub.docker.com/_/golang and ensure the target Golang version in an Alpine Linux image is available. For example, if the target version is `1.11.4`, then ensure that the `1.11.4-alpine` image tag is available. If the image tag is not avaiable, then the Travis CI build will not succeed.

## Upgrade

Ensure you are using the target Golang version locally.

Change the version in `.travis.yml` and all `Dockerfile.*` files.

Add an entry in `CHANGELOG.md` and commit.

## Test

Ensure the `ci-build` and `ci-test` Makefile targets pass using the target Golang version.

If you previously noted any changes or issues of concern, perform any explicit tests necessary.

# Upgrade Dependencies

## Upgrade

```
go get -u    # e.g. go get -u github.com/onsi/gomega
go mod tidy
go mod vendor
```

## Review

Review all pending changes to all dependencies. If any changes could have a negative impact upon this repository, make note of this change to explicitly test afterwards.

## Test

Ensure the `ci-build` and `ci-test` Makefile targets pass using the target Golang version.

If you previously noted any changes or issues of concern, perform any explicit tests necessary.
",'golang',2024-05-02T14:20:19Z,16,27,21,"('darinkrauss', 384), ('toddkazakov', 255), ('jh-bate', 229), ('Roukoswarf', 208), ('derrickburns', 64), ('clintonium-119', 58), ('jamesraby', 39), ('ewollesen', 23), ('james-mux', 14), ('pazaan', 11), ('gniezen', 5), ('lostlevels', 5), ('tjotala', 4), ('jebeck', 4), ('Benderr-TP', 3), ('adinhodovic', 2)","[3, 'Good Health and Well-Being']"
Intelehealth/intelehealth-fhw-mobileapp,Intelehealth's FHWs Mobile App,"
# Intelehealth FHW MobileApp

## About

Over 400 million people all over the world lack access to basic healthcare services. The reality for people living in rural areas for most of the developing world is that there is no doctor nearby. As a result, they have to travel long distances and spend considerable time and money in order to get to the nearest clinic. With the increasing availability of mobile networks and mobile internet even in rural areas we now have new opportunities to provide healthcare.

Telemedicine provides an opportunity by connecting doctors in urban areas to patients in rural areas to provide healthcare. But health organizations that want to set up telemedicine programs do not have a lot of technology options to choose from. A robust software platform is the backbone of a good telemedicine program. The platform needs to be low cost, work even over an unreliable low bandwidth data connection and needs to be appropriately designed for health workers or nurses who may have never used a smartphone before.


Intelehealth is an open source telemedicine platform that empowers local community health workers (CHWs) in rural communities to facilitate tele-consultations with remote doctors. It enables them to gather high quality clinical history and conduct clinically-relevant physical exams. Intelehealth can be used by hospitals, community health programs & governments to expand the reach of health services to ensure that vital primary care reaches remote and rural populations.

## How it works
* Intelehealth Android application runs on low bandwidth mobile data connection so it can operate in the remotest of settings
* The mobile org uses intelligent data-gathering flowcharts so that CHWs gather comprehensive clinical history and conduct clinically relevant physical exams
* The org independently guides the CHW through a series of questions and exams based on the patient’s presenting symptoms and medical history
* The CHW uploads the gathered clinical data to a cloud server which houses the patients Electronic Health Record (EHR) system
* The remote doctor reviews uploaded data, offer medical advice and prescription, and refer patient if required

  [Disclaimer](https://github.com/Intelehealth/Intelehealth-FHW-MobileApp/blob/master-2.0/Healthcare%20disclaimer.md)

## Credits
### Icons used:
* Happy by Alina Oleynik from the Noun Project
* Unhappy by Alina Oleynik from the Noun Project
* Embarrassed by Alina Oleynik from the Noun Project
* Shocked by Alina Oleynik from the Noun Project
","'chw', 'health-workers', 'healthcare', 'intelehealth-android-client', 'patients', 'remote-doctors', 'rural-areas', 'telemedicine'",2024-05-02T14:18:37Z,15,31,11,"('prajwalmw', 475), ('kranthi-mahiti', 93), ('nishitagoyal', 66), ('nehav39', 48), ('manojpedvi', 43), ('SagarS23', 41), ('RIntelehealth', 36), ('venugopalaswamy-Android', 27), ('linconcandoit', 21), ('manishyadav2021', 15), ('arpansircardevelopment', 7), ('GokulWakchaure', 7), ('raviratansingh', 5), ('sanjeet-intelehealth', 2), ('satyadeep-ih', 1)","[3, 'Good Health and Well-Being']"
unicef/magicbox-open-api,An API to serve open data aggregated by administrative boundaries,"Magic Box API
=============

[![Chat on Gitter](https://badges.gitter.im/unicef-innovation-dev/Lobby.png)](https://gitter.im/unicef-innovation-dev/Lobby)
[![Build Status](https://travis-ci.org/unicef/magicbox-open-api.svg?branch=master)](https://travis-ci.org/unicef/magicbox-open-api)
[![Maintainability](https://api.codeclimate.com/v1/badges/d36cba5a7e783ffd8970/maintainability)](https://codeclimate.com/github/unicef/magicbox-open-api/maintainability)

[Magic Box](https://github.com/unicef/magicbox/wiki) is an open-source platform that is intended to use real-time information to inform life-saving humanitarian responses to emergency situations. It’s composed of multiple github repositories designed to ingest, aggregate, and serve data.

### Install the API that serves the data

Magic Box API serves information useful to the data science team at the Office Of Innovation at UNICEF. This section describes how to install a local instance. It comes with sample data, but you can follow links below for code to download and aggregate many of the open data sets.

Types of data currently available to the public include:

- population
- mosquito prevalence
- Paho Zika case data
- School location and connectivity

...aggregated at municipal, state, and national levels.


### Dependencies:
#### NVM
	curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash
#### Node.js
	nvm install 8
## Setup

    git clone https://github.com/unicef/magicbox-open-api.git
    cd magicbox-open-api
    cp config-sample.js config.js
    npm install
    npm run build
    npm run start

Now browse to: localhost:8000/docs

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/expand_pop.gif)

The first endpoint: /api/v1/population/countries, returns a list of codes for countries for which we have population data:
````
[ ‘afg’, ‘ago’, ‘arg’ … ‘zwe’]
````

The second endpoint returns population data for a single country. For instance, to fetch the population for Afghanistan at the district level, browse to: localhost:8000/api/v1/population/countries/afg

	[
 	  { admin_id: ‘afg_1_5_50_gadm2–8’, value: 165297 },
	  { admin_id: ‘afg_1_4_33_gadm2–8’, value: 175117 },
	  { admin_id: ‘afg_1_7_57_gadm2–8’, value: 49994},
	  { admin_id: ‘afg_1_11_102_gadm2–8’, value: 50304 },

	  … 228 more items
	]

### What does this mean?
The value points to number of people. But, what swath of land does each admin_id refer to?

Answer: An admin ID points to a specific shape in a shapefile that represents an individual country.

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/afg_shapefile.png)

To understand where afg_1_11_102-gadm2–8 points to, first note that Afghanistan has three levels of administrative boundaries:

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/admin_levels.png)

The admin_id has three integers, one per admin level:

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/admin_levels_arrows.png)

The first integer is 1 because Afghanistan is the first country in the collection of 254 available at gadm.org.

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/afg_thru_zwe.png)

As for the second and third integers, Afghanistan has 34 shapes at admin level one (each shape is assigned an ID), and 320 shapes in admin 2.

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/admin_1_and_2v2.png)

Thus, afg_1_11_102-gadm2–8 indicates that any population value attached to it is related to:

- The first country in the gadm collection.
- The 11th shape in the admin 1 level shapefile.
- The 102nd shape in the admin 2 level shapefile.

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/admin_id_explain_all.png)

### Mosquito Prevalence (University of Oxford)

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/mos_endpoints.png)

- Currently, the API serves prevalence scores at both a national and district/province level per country. Scores range from 0 to 1. Browse to localhost:8000/api/v1/mosquito/kinds to see what mosquito types we have data for:

````
[ ‘aegypti’, ‘albopictus’ ]
````
- Browse to localhost:8000/api/v1/mosquito/kinds/aegypti for a country by country list:
````
	{
	  abw: 0.92733,
	  afg: 0.12469,
	  …
	  zwe: 0.54493
	}
````

Similar to Population, you can also use:

- /api/v1/mosquito/kinds/aegypti/countries to retrieve a list of country_codes
- /api/v1/mosquito/kinds/aegypti/countries/afg/ to get mosquito prevalence scores per district.

### Zika Case Data (Paho)

The API serves Zika case data for the Americas (national level) as published by the Pan American Health Organization in excel files each epi week. In order to overlay with travel data given to us by Amadeus, we’ve also used a *really* simple algorithm to group the cases by ISO week.

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/epi_iso.png)

To fetch all zika case data to date for either week type, use these end points:
- localhost:8000/api/v1/cases/kinds/zika/weekTypes/iso
- localhost:8000/api/v1/cases/kinds/zika/weekTypes/epi

![Screenshot](https://github.com/unicef/magicbox-open-api/blob/master/public/images/case_output.png)

#### To serve the same data as the [live API](http://magicbox-open-api.azurewebsites.net/docs), follow the [Magic Box Wiki](https://github.com/unicef/magicbox/wiki)!
","'api', 'api-server', 'data', 'data-science', 'emergency-response', 'geospatial', 'geospatial-data', 'humanitarian', 'javascript', 'magicbox', 'mosquito-prevalence', 'nodejs', 'paho', 'population', 'population-data', 'schools', 'serve-data', 'shapefile', 'unicef'",2022-07-06T19:59:29Z,7,27,15,"('mikefab', 232), ('coolPrat', 55), ('jwflory', 17), ('thoat', 13), ('carloscdias', 13), ('alfredoxyanez', 6), ('dependabotbot', 3)","[10, 'Reduced Inequalities']"
sahana/eden,"Sahana Eden is an Open Source Humanitarian Platform which can be used to provide solutions for Disaster Management, Development, and Environmental Management sectors.. Please sign CLA when submitting pull requests: http://bit.ly/SSF-eCLA","# Sahana Eden

Sahana Eden is an Emergency Development Environment - an Open Source framework to rapidly build powerful applications for Emergency Management.

It is a web based collaboration tool that addresses the common coordination problems during a disaster from finding missing people, managing aid, managing volunteers, tracking camps effectively between Government groups, the civil society (NGOs) and the victims themselves.

Please see the website for more details: 
+ http://eden.sahanafoundation.org/

Note to developers -- get started here:
+ http://eden.sahanafoundation.org/wiki/Develop

Before your first pull request, sign the Contributor's License Agreement, which protects your rights to your code, while allowing it to be distributed and used in Sahana Eden:
+ http://bit.ly/SSF-eCLA
",,2023-10-27T10:40:09Z,30,383,48,"('flavour', 7157), ('nursix', 5655), ('michaelhowden', 278), ('graeme-f', 177), ('biplovbhandari', 130), ('somayjain', 61), ('trendspotter', 35), ('tirgil', 34), ('ashwyn', 31), ('raj454raj', 29), ('ptressel', 25), ('aviraldg', 22), ('hitesh96db', 21), ('hemebond', 19), ('VishrutMehta', 18), ('arnavsharma93', 17), ('arnavkagrawal', 14), ('waidyanatha', 13), ('gauravmittal1995', 11), ('liezl200', 11), ('nownikhil', 8), ('coldblooded01', 8), ('govind-menon', 7), ('hallamoore', 7), ('PeterDaveHello', 7), ('coder006', 6), ('lifeeth', 6), ('eimmirzi', 6), ('energy7', 6), ('devinbalkind', 6)","[13, 'Climate Action']"
WFP-VAM/HRM,High Resolution Mapping of Food Security,"# High Resolution Mapping of Food Security [![Build Status](https://travis-ci.org/WFP-VAM/HRM.svg?branch=master)](https://travis-ci.org/WFP-VAM/HRM)
for information on the project, please refer to the [GitHub page](https://wfp-vam.github.io/HRM/).

The application takes as input geo-referenced survey data, then for every survey _cluster_:
  - downloads relevant satellite images from the Google Maps Static API and Sentinel-2 from Google Earth Engine API. The class that handles this is the [img_lib.py](https://github.com/WFP-VAM/HRM/blob/master/Src/img_lib.py)
  - extract features from the images using neural networks trained [here](https://github.com/WFP-VAM/HRM_NN_Training). Class that handles it is the [nn_extractor.py](https://github.com/WFP-VAM/HRM/blob/master/Src/nn_extractor.py)
  - extract features as distance to hospital and school from OpenStreetMap using the [Overpass API](http://wiki.osm.org/wiki/Overpass_API). The [OSM_extactor.py[](https://github.com/WFP-VAM/HRM/blob/master/Src/osm.py) handles that part.
  - extract remote sensing indices from Sentinel 2, namely NDVI, NDBI and NDWI. [S2_indexes.py](https://github.com/WFP-VAM/HRM/blob/master/Src/rms_indexes.py) handles this part.
  - pull information from [ACLED](https://www.acleddata.com/) on violent events.
  - use ridge regression to infer indicator's value from the extarcted features. 
  
 All of the _training_ is coordinated by the [scripts/master.py](https://github.com/WFP-VAM/HRM/blob/master/scripts/master.py). 
 Predictions for an area are made with [scripts/score_area.py](https://github.com/WFP-VAM/HRM/blob/master/scripts/score_area.py).
  
The trained models can then be used for making predictions in areas where no data is available. Use the [scripts/score_area.py](https://github.com/WFP-VAM/HRM/blob/master/scripts/score_area.py) for that. Work is in progress in the `application` directory for taking the method to produciton. 
  
### How to run the code:
#### File-system
Make sure to have the following file-system in place:
 ```
config
└── example_config.yaml 
Data
├── datasets
│   └── processed_survey.csv
├── Features
├── Geofiles
│   ├── ACLED
│   ├── NDs
│   ├── nightlights
│   ├── OSM
│   └── Rasters
│       └── base_layer.tif
└── Satellite   
    ├── Sentinel
    └── Google   
Models/
env.list
  ```
 The mandatory files are: 
 
`Data/datasets/processed_survey.csv` this is your survey data! should contain at least 3 columns: ""gpsLongitude"",""gpsLatitude"" and one indicator. You can either work with individual survey data or aggregate the surveys to some geographic level. 
  
`Data/Geofiles/Rasters/base_layer.tif` is a raster file that containing the area of interest and the population density. Survey points will be snapped to its grid and the pulled layers over-laid.Please use 100x100m resolution WorldPop's rasters, available [here](https://www.worldpop.org/geodata/listing?id=16). 
 
`config/example_config` is the config file that you should fill in. Please use the template provided, fields list in there. 

`env.list` this should contain the key to access the [Google Maps Static API](https://developers.google.com/maps/documentation/maps-static/intro). 
After you get yours, add it to the file if you will be using Docker or to your environment variables if you run with Python. The format should be `Google_key=`. 

#### Google Earth Engine API credentials
Because you will be pulling Sentinel-2 and Nightlights data from Google Earth Engine, you will need to set up some credentials. Not so easy because of Google's OAuth2.
Please follow [this link](https://developers.google.com/earth-engine/python_install_manual) to create your credentials file.
 
### Train Model
To run the app that trains the model on your survey data you can either set up your python environment (install libraries listed in `environment.yml`) or use docker.
#### With Python
To run the training with Python simply run the `/scripts/master.py`:
```
python master.py args 
```
where args is one or more `example_config.yaml`. Each `.yaml` should be space separated. Please run from the root directory of the application. 
For example to trigger for configs config_1.yaml, config_2.yaml and config_3.yaml do:
```
python master.py config_1.yaml config_2.yaml config_3.yaml > log.txt &
```
This will:
* download the relevant satellite images. (if not there already)
* extract the features for each image. (if no features for that id)
* pull and vectorize data from OSM, ACLED, Sentinel-2 and NOAA
* train the model on the existing data.
* write r2 Pearson scores to the `Results/` directory on a 5-fold cross validation loop.
* save the full predictions on the left-out data, aslo in `Results/`.
* save the trained model.

#### With Docker
If you want to use docker, build the image with ``` docker build -t hrm . ``` then run with:
```
docker run -v ~/Desktop/HRM/HRM/Data:/app/Data -v ~/.config/earthengine:/root/.config/earthengine --env-file ./env.list hrm ../config/example_config.yaml
```
First `-v` flag maps local directory `Data` to the same directory in the container. Second `-v` maps the earth engine credentials. 
The `--env-file ./env.list` adds the `Google_key` environment variable to the container.
### Contacts
For more info to collaborate, use or just to know more reach us at jeanbaptiste.pasquier@wfp.org and lorenzo.riches@wfp.org or submit an issue.
",,2019-09-08T08:33:43Z,3,21,9,"('lorenzori', 249), ('pasquierjb', 58), ('gsVAM', 2)","[2, 'Zero Hunger']"
magma/magma,Platform for building access networks and modular network services,"
    


Connecting the Next Billion People


    
    
    
    
    
    
    


Magma is an open-source software platform that gives network operators an open, flexible and extendable mobile core network solution. Magma enables better connectivity by:

- Allowing operators to offer cellular service without vendor lock-in with a modern, open source core network
- Enabling operators to manage their networks more efficiently with more automation, less downtime, better predictability, and more agility to add new services and applications
- Enabling federation between existing MNOs and new infrastructure providers for expanding rural infrastructure
- Allowing operators who are constrained with licensed spectrum to add capacity and reach by using Wi-Fi and CBRS

## Magma Architecture

The figure below shows the high-level Magma architecture. Magma is 3GPP generation (2G, 3G, 4G or upcoming 5G networks) and access network agnostic (cellular or WiFi). It can flexibly support a radio access network with minimal development and deployment effort.

Magma has three major components

- **Access Gateway.** The Access Gateway (AGW) provides network services and policy enforcement. In an LTE network, the AGW implements an evolved packet core (EPC), and a combination of an AAA and a PGW. It works with existing, unmodified commercial radio hardware.

- **Orchestrator.** Orchestrator is a cloud service that provides a simple and consistent way to configure and monitor the wireless network securely. The Orchestrator can be hosted on a public/private cloud. The metrics acquired through the platform allows you to see the analytics and traffic flows of the wireless users through the Magma web UI.

- **Federation Gateway.** The Federation Gateway integrates the MNO core network with Magma by using standard 3GPP interfaces to existing MNO components.  It acts as a proxy between the Magma AGW and the operator's network and facilitates core functions, such as authentication, data plans, policy enforcement, and charging to stay uniform between an existing MNO network and the expanded network with Magma.

![Magma architecture diagram](docs/readmes/assets/magma_overview.png?raw=true ""Magma Architecture"")

## Documentation

- [Magma Website](https://magmacore.org/): Project landing page
- [Docs](https://magma.github.io/magma/docs/basics/introduction.html): Deployment, configuration and usage information
- [Code](https://github.com/magma): Source code
- [Contributing](https://github.com/magma/magma/wiki/Contributor-Guide): Contributor Guide
- [Wiki](https://wiki.magmacore.org/): Meeting notes and project team resources
- [Rewards Program](REWARDS_PROGRAM.md): How to participate in the Magma rewards program

## Join the Magma community

See the [Community](https://magmacore.org/join-the-open-source-community/) page for entry points.

Start by joining the community on Slack: [magmacore workspace](https://slack.magmacore.org/).

Direct specific questions to the [GitHub Discussions page](https://github.com/magma/magma/discussions). Your question might already have an answer!

## License

Magma is BSD License licensed, as found in the LICENSE file.

The EPC originates from OAI (OpenAirInterface Software Alliance) and is offered under the same BSD-3-Clause License.

## Risks

The Magma materials are provided in accordance with the licenses made available in the LICENSE file. Prior to using the materials, it is highly recommended that you test and verify that the materials meet your specific requirements, including, without limitation, any and all security and performance requirements.

## Security

Responsible disclosures from independent researchers are gratefully accepted. See the [Security Policy](/magma/magma/security/policy) for submission details and [Security Overview for Contributors](https://github.com/magma/magma/wiki/Security-Overview-for-Contributors) to learn about other ways of contributing.

We wish to acknowledge valuable disclosures by the following security researchers:

- Guarang Maheta
- Phi Trần
","'3gpp', '4g', '5g', 'bazel', 'c', 'golang', 'hacktoberfest', 'magma', 'mno', 'mobile', 'networks', 'python', 'vagrant'",2024-05-03T09:34:53Z,30,1651,84,"('themarwhal', 1028), ('xjtian', 453), ('koolzz', 411), ('pshelar', 410), ('AmitArbel', 399), ('Scott8440', 356), ('andreilee', 354), ('uri200', 323), ('emakeev', 265), ('ardzoht', 264), ('karthiksubraveti', 262), ('ssanadhya', 217), ('idoshveki', 210), ('alexsn', 207), ('rckclmbr', 200), ('hcgatewood', 196), ('electronjoe', 173), ('ulaskozat', 172), ('LKreutzer', 160), ('rsarwad', 152), ('tmdzk', 150), ('mpfirrmann', 144), ('vdorfman', 140), ('nstng', 138), ('Neudrino', 132), ('pruthvihebbani', 125), ('quentinDERORY', 118), ('sebathomas', 110), ('VinashakAnkitAman', 108), ('ymasmoudi', 99)","[9, 'Industry, Innovation and Infrastructure']"
saycel/Saycel.Phone,,"# Webph.one server

This is a WebApp for connecting internet enabled devices to community cellular networks.

This repository will easen newcomers the setup of the whole infrastructure;

The system is composed of a set of server modules and a client app.

The server modules use several Open Source tools to achieve the desired result, namely:
* Kamailio for SIP signaling
* RTPEngine as RTP Server (for audio tunneling between clients)
* apache as Web Server
* webpush-server: Push notifications to announce incoming calls
* allocatenumber-server: Server that manages new number assignments
* MySQL: to manage all data
* LetsEncrypt SSL certificates

The client module is a Progressive-web-app (PWA) that is served by apache.

# Architecture

![architecture diagram](https://raw.githubusercontent.com/saycel/Saycel.Phone/master/docs/images/architecture.png)

This describes the main relations between the modules of the system:
* The apache web server, using the LetsEncrypt SSL Certificate, exposes a public secure web service that serves the webph.one PWA in a certain DNS domain (for example https://example.webph.one/).
* When a user gets to the url https://example.webph.one/ using his web browser, he installs the PWA as an app in his phone.
* If it is the PWA's first run, it will request the allocatenumber-server a phone number for that device, so he can use that to call and be called.
* The allocatenumber-server will talk to Kamailio and save all information regarding new numbers registered and the credentials the phone uses to identify them.
* The PWA will connect to the Kamailio SIP Server and register with Google Push Notifications Service to receive notifications of incoming calls.
* The Kamailio SIP Server will talk with the MySQL Database to check the credentials of the connected user.
* When the user triggers a call, a message is sent to Kamailio to notify the other callee that is being called.
* If the call goes to a phone that is connected to a web client, webpush-server is used to send the push notification through Google's push notification service.
* If the call goes to a gsm phone belonging to a Kamalio configured community, Kamailio will forward the call to the Community's SIP Server.
* If the callee accepts the call, Kamailio coordinates with the clients and RTPEngine to create an RTP Media Tunnel between the clients to pass the audio through it.

# Requirements

In order to run the system, you will need docker-compose installed.

# Installation

Clone the repository by doing:

```bash
git clone https://github.com/saycel/Saycel.Phone.git
```

The configuration of the system is stored in a file called .env.
A dot-env file is provided as example, please copy it to .env and customize before running.
You need to provide a ./certificates directory with the SSL certs like this:
```
./certificates/privkey.pem
./certificates/fullchain.pem

```

```bash
cp dot-env .env
```

To start the system run:

```bash
docker-compose up
```

# References

This system is based on many components:
* https://github.com/saycel/webph.one
* https://github.com/saycel/webpush-server
* https://github.com/saycel/kamailio-config
* https://github.com/saycel/allocatenumber-server
",,2018-03-05T18:05:10Z,3,2,4,"('nicopace', 27), ('JonMor26756', 22), ('dmehrotra', 1)","[9, 'Industry, Innovation and Infrastructure']"
rapidpro/rapidpro,RapidPro allows organizations to visually build scalable interactive messaging applications.,"# RapidPro 

[![Build Status](https://github.com/rapidpro/rapidpro/workflows/CI/badge.svg)](https://github.com/rapidpro/rapidpro/actions?query=workflow%3ACI) 
[![codecov](https://codecov.io/gh/rapidpro/rapidpro/branch/main/graph/badge.svg)](https://codecov.io/gh/rapidpro/rapidpro)

RapidPro is a platform for visually building interactive messaging applications.
To learn more, please visit the project site at http://rapidpro.github.io/rapidpro.

### Stable Versions

The set of versions that make up the latest stable release are:

 * [RapidPro 8.0.1](https://github.com/rapidpro/rapidpro/releases/tag/v8.0.1)
 * [Mailroom 8.0.0](https://github.com/rapidpro/mailroom/releases/tag/v8.0.0)
 * [Courier 8.0.2](https://github.com/nyaruka/courier/releases/tag/v8.0.2)
 * [Archiver 8.0.0](https://github.com/nyaruka/rp-archiver/releases/tag/v8.0.0)
 * [Indexer 8.0.0](https://github.com/nyaruka/rp-indexer/releases/tag/v8.0.0)

### Versioning

Major releases are made every four months on a set schedule. We target November 1st
as a major release (`v7.0.0`), then March 1st as the first stable dot release (`v7.2.0`) and July 1st
as the second stable dot release (`v7.4.0`). The next November would start the next major release `v8.0.0`.

Unstable releases have odd minor versions, that is versions `v7.1.*` would indicate an unstable or *development*
version. Generally we recommend staying on stable releases.

To upgrade from one stable release to the next, you should first install and run the migrations
for the latest stable release you are on, then every stable release afterwards. If you are
on version `v6.0.12` and the latest stable release on the `v6.0` series is `v6.0.14`, you should
first install `v6.0.14` before trying to install the next stable release `v6.2.5`.

Generally we only do bug fixes (patch releases) on stable releases for the first two weeks after we put
out that release. After that you either have to wait for the next stable release or take your
chances with an unstable release.
",,2024-04-30T01:12:06Z,30,838,68,"('rowanseymour', 9673), ('nicpottier', 4419), ('ericnewcomer', 3358), ('norkans7', 3139), ('teehamaral', 552), ('dodobas', 451), ('nullcode', 350), ('hudsonbrendon', 189), ('smn', 114), ('susanm74', 100), ('tybritten', 77), ('vctrferreira', 77), ('jcbalmeida', 64), ('johncordeiro', 62), ('ewheeler', 60), ('musamusa', 43), ('jofomah', 42), ('edudouglas', 37), ('Robi9', 26), ('ycleptkellan', 19), ('awensaunders', 18), ('matmsa27', 18), ('koallann', 17), ('jpaidoussi', 12), ('transifex-integrationbot', 10), ('rasoro', 9), ('Cloves23', 9), ('erikh360', 8), ('xkmato', 7), ('alexmuller', 7)","[16, 'Peace, Justice and Strong Institutions']"
code-dot-org/blockly,Blockly (Code Studio fork),"# Blockly (Code Studio fork)

[![npm version](https://img.shields.io/npm/v/@code-dot-org/blockly.svg)](https://www.npmjs.com/package/@code-dot-org/blockly)
[![CircleCI](https://circleci.com/gh/code-dot-org/blockly.svg?style=shield)](https://circleci.com/gh/code-dot-org/blockly)

This is a fork of [Blockly](https://code.google.com/p/blockly/), an open source visual programming environment.

Major additions and changes in this fork:

* Modal editor for function blocks ([function_editor.js](./core/ui/function_editor.js)) 
* Scrolling improvements:
  * auto-scroll on block drag ([scroll_on_block_drag_handler.js](./core/ui/block_space/scroll_on_block_drag_handler.js))
  * scroll on mouse wheel ([scroll_on_wheel_handler.js](./core/ui/block_space/scroll_on_wheel_handler.js))
* New in-toolbox trashcan
* Addition of ""Functional blocks"" and ""Contract/Variable Editor"" ([contract_editor/](./core/ui/contract_editor/)) for use in the CS in Algebra curriculum
  * Blocks have ""Block Value Types"" ([block_value_type.js](./core/utils/block_value_type.js))
* Support for ""block limits"" ([block_limits.js](./core/ui/block_space/block_limits.js)), toolbox blocks which allow only a certain number of instances in the block space
* Support for new block properties ([block.js](./core/ui/block.js)): invisible, un-deletable, immovable, specify-able via a context menu when `Blockly.editBlocks` is set
* Structure: add folders in [core/](./core) to further categorize classes
* New field types:
  * Image Dropdown ([field_image_dropdown.js](./core/ui/fields/field_image_dropdown.js))
* BlockSpace (Workspace) Refactoring:
  * moved many static properties and methods from `blockly.js` into a prototype class `BlockSpaceEditor` which can be instantiated multiple times on the same page
  * renamed `Workspace` to `BlockSpace` to disambiguate from higher-level `BlockSpaceEditor`
  * improved support for multiple blockspaces on a single page
* Playground: add dependency cache and generation script ([tests/update_test_dependencies.sh](./tests/update_test_dependencies.sh)), to allow for testing changes without re-building
  * support for [goog.ui.tweak](https://google.github.io/closure-library/source/closure/goog/demos/tweakui.html)s to configure playground page behavior
  * debug drawing helpers for block bumping 
* Support for a special UI for unattached blocks
* Improvements to block arrangement on initialization
* Testing: add phantomjs-based test runner [test.sh](./test.sh). Tested in CI at root level of this repository.

## Installation

1. Download and install JDK version 8 from [here](https://www.oracle.com/technetwork/es/java/javase/downloads/jdk8-downloads-2133151.html)
2. run the following commands:

```
cd blockly
npm install
npm run build
```

## Usage

### Playground manual testing page

There is a playground manual testing page at [tests/playground.html](./tests/playground.html), which requires no build step or server running.

`open tests/playground.html`

### Building with apps

This is the most typical use case for code-dot-org fork development.

[Apps (aka Code Studio)](https://github.com/code-dot-org/code-dot-org/tree/staging/apps) is a set of blockly apps built on top of blockly, which installs and references this package via NPM. The easiest pathway for local development is to use [npm-link](https://docs.npmjs.com/cli/link):

```
cd {blockly repo directory}
npm link
cd {code-dot-org repo directory}/apps
npm link @code-dot-org/blockly
```

Apps will now reference your local blockly repository rather than the npm package. If you then make local changes to your repo, you can simply rebuild blockly (via `npm run build` in this repo) and then apps (via `npm run build` in code-dot-org/apps) to communicate those changes to apps.

### Publishing changes

To publish a new version to npm switch to the main branch, use `npm login` to sign in as an account with access to the `@code-dot-org` scope, then `npm version [major|minor|patch|premajor|preminor|prepatch]` for the appropriate version bump.  This will do the following:

* Run linting and tests to verify your local repo.
* Rebuild the release package.
* Bump the version, adding a corresponding commit and version tag.
* Push the commit and tag to github.
* Publish the new release package to npm.

#### Testing changes

There are a set of utility and integration tests included in `tests/blockly_test.html`, and a playground manual testing page at `tests/playground.html`.

After adding any new files, you will need to run `./update_test_dependencies.sh` to update the test dependency map, which caches file dependencies so edits can be tested and played with without any re-build time.

There are three ways the test suites can be run:

1. `./test.sh` will run the tests in phantomjs
1. To debug failures, you can open the test page in your browser, e.g. `open tests/blockly_test.html` 
1. `./deploy.sh` will also run `./test.sh` at the end of its full rebuild.

##### Other tests covering this package

[Blockly apps](https://github.com/code-dot-org/code-dot-org/tree/staging/apps) contains many tests that target features of blockly in the context of the code.org curriculum apps.

Additionally, [Dashboard's UI tests](https://github.com/code-dot-org/code-dot-org/tree/staging/dashboard/test/ui) cover certain features of blockly through Cucumber / Selenium scenarios.
",,2024-02-09T18:45:12Z,30,60,44,"('bcjordan', 497), ('joshlory', 366), ('Hamms', 306), ('deploy-code-org', 227), ('ajpal', 167), ('balderdash', 151), ('Bjvanminnen', 138), ('tanyaparker', 98), ('islemaster', 83), ('breville', 32), ('sureshc', 20), ('mikeharv', 19), ('nkiruka', 19), ('davidsbailey', 18), ('jmkulwik', 14), ('marcd123', 7), ('levadadenys', 7), ('cpirich', 6), ('wjordan', 6), ('tim-dot-org', 6), ('bethanyaconnor', 5), ('laurelfan', 5), ('dju90', 4), ('Erin007', 4), ('ebeastlake', 3), ('geoffrey-elliott', 3), ('JillianK', 3), ('caleybrock', 3), ('PhantomMike', 3), ('mgc1194', 3)","[4, 'Quality Education']"
tidepool-org/tideline,Library for Tidepool's timeline-style diabetes data visualization(s) used in Blip,"tideline
========

[![Build Status](https://img.shields.io/travis/com/tidepool-org/tideline.svg)](https://travis-ci.com/tidepool-org/tideline)
[![Coverage Status](https://img.shields.io/coveralls/tidepool-org/tideline/master.svg)](https://coveralls.io/r/tidepool-org/tideline)

This repository is a self-contained module library for [Tidepool](http://tidepool.org/ 'Tidepool')'s timeline-style diabetes data visualization(s).

This module is currently under construction; check back often for updates!

More information is also available in [the wiki](https://github.com/tidepool-org/tideline/wiki).

## Dependencies and Installation

### Front-end dependencies

- [Crossfilter](http://square.github.io/crossfilter/ 'Crossfilter')
- [D3.js](http://d3js.org/ 'D3')
- [Duration.js](https://github.com/icholy/Duration.js 'Duration.js')
- [Lo-Dash](http://lodash.com/ 'Lo-Dash')
- [Moment](http://momentjs.com/ 'Moment')
- [Bows](https://github.com/latentflip/bows 'Bows')

**Fonts**: Tideline should be used with the [Open Sans](https://www.google.com/fonts#UsePlace:use/Collection:Open+Sans) font.

Development-only dependencies: See the `package.json`.

Install for use in your own web application using:

```bash
$ npm install --save tideline
```

## Usage

You can use the library directly with [Webpack](http://webpack.github.io/ 'Webpack'):

```javascript
var tideline = require('tideline');

// load styles
require('tideline/css/tideline.less');
```

For information on building charts using tideline components, see [Using Tideline](https://github.com/tidepool-org/tideline/wiki#using-tideline).

## Development

To run the tests you will need to have a couple of tools installed. Everything you need can be installed via `npm`:

```bash
$ npm install
```

### Testing

To run the tests in Chrome using [Mocha](http://mochajs.org/ 'Mocha') and the [testem](https://github.com/airportyh/testem 'Test'em') test runner:

```bash
$ npm test
```

To run the unit tests in watch, use:

```bash
$ npm run test-watch
```

#### Lint

Run JSHint with:

```bash
$ npm run lint
```

You can also watch files for changes and re-run automatically by starting:

```bash
$ npm run lint-watch
```

## Code Philosophy and Organization

Tideline is designed to be highly modular and framework-independent. It is currently being used in conjunction with [React](http://facebook.github.io/react/ 'React') in Tidepool's first application [blip](https://github.com/tidepool-org/blip 'blip').

The main functionality tideline provides is modules for building out various visualizations of multi-typed data sets aligned on various timescales. At present, there is a module (`oneday.js`) for creating a horizontal scrolling timeline that shows twenty-four hours of data at a time, a module (`twoweek.js`) for creating a vertical scrolling timeline that shows two weeks of data at a time, and a module (`settings.js`) for creating an HTML table view of insulin pump settings.

**Jargon:** The horizontal sections comprising sub-units of visualization plotted against the same x-axis are referred to in this repository as *pools*.

### Philosophy

Almost all of the main tideline components (found in `js/`) hew to at least some (but rarely all) of the suggestions in Mike Bostock's [Towards Reusable Charts](http://bost.ocks.org/mike/chart/ 'Mike Bostock: Towards Reusable Charts'). The data-type specific plotting functions (found in `js/plot/`) hew most closely to the suggested pattern, while the higher-level components (i.e., `oneday.js`, `twoweek.js`, `settings.js`) do not, as their tasks are not quite the same.

The plotting functions in `js/plot/` critically depend on D3's [enter](https://github.com/mbostock/d3/wiki/Selections#wiki-enter) and [exit](https://github.com/mbostock/d3/wiki/Selections#wiki-exit) selections. If you need it, [this tutorial by Mike Bostock](http://mbostock.github.io/d3/tutorial/circle.html) includes a good introduction to these.

While tideline is quite specific to diabetes at the moment, it is designed to be as flexible and modular as possible. We plan to integrate data types not specific to diabetes (e.g., activity tracker data, calendar events, etc.), and it should be possible to create a visualization of any multi-typed dataset using a combination of the higher-level components and additional plotting modules. We welcome any and all contributions of new plotting modules, as well as contributions to the core library modules.

### SVG Philosophy

Tideline uses [D3.js](http://d3js.org/ 'D3') to create an [SVG](http://www.w3.org/Graphics/SVG/ 'SVG') data visualization. SVG is an extremely powerful graphics format, and there are often many, many ways to accomplish the same visualization task. For the purposes of the code in this repository, two related points of philosophy should be noted upfront:

- Tideline *loves* SVG group `` elements. **_Loves._**
- Relatedly, Tideline likes to use the `transform` attribute (usually just with a `translate(x,y)` definition) for positioning.

### Code Conventions

Tideline makes every attempt to adhere to standard coding conventions. In development, we use the same `.jshintrc` file as tideline's parent application [blip](https://github.com/tidepool-org/blip 'blip').

The only coding conventions unique to tideline are conventions of HTML and CSS ID and class names. All of the SVG elements comprising tideline use `camelCase` for IDs, with different parts of the ID separated by an underscore `_`. Class names, in contrast, are all lowercase, prefixed with `d3` and employ hyphen `-` as a separator. These conventions help tideline developers to keep IDs and classes distinct.

### Repository Organization

- `css/` contains the Less files that compile to tideline's CSS. `tideline.less` provides the styles and depends on `tideline-colors.less` for color variables. This makes it possible to customize tideline's color scheme by defining a different `tideline-colors.less` file.
- `dev/` contains a few tools that are (occasionally) useful for development.
    + `demodata/` contains a Python script for generating fake data for testing tideline during development. For usage information, run `python demo_data.py --help`.
    + `templates/` contains two module templates: `plottemplate.js` for a [plot module](https://github.com/tidepool-org/tideline/wiki/CreatingOneDay#plot-modules 'Tideline Wiki: Plot Modules') and `datautil.js` for a data utility analogous to those found in `js/data/`.
    + `testpage/` is a miniature JavaScript library for generating test data used in the tideline visualization integration tests. In contrast to the demo data generator, the test page data is extremely regular, with no randomization.
- `img/` contains the images used to plot certain types of data (i.e., notes).
- `js/` contains the tideline library. At the top level, `oneday.js`, `twoweek.js`, `settings.js`, and `pool.js` are the main components. `tidelinedata.js` defines the data object that the other core components expect to be passed. `index.js` exports the entire library, which can be used for creating a standalone tideline bundle with [browserify](http://browserify.org/).
   + `data/` contains a set of mini-modules for munging and calculating statistics around various types of diabetes data.
     - `util/` contains some common utilities that are used mainly in the `data/` modules, but `datetime.js`, `format.js` (for formatting the output of numerical calculations - that is, rounding and displaying numbers to the proper number of significant digits), and `tidelinecrossfilter.js`, which wraps the most common uses of [Crossfilter](http://square.github.io/crossfilter/ 'Crossfilter') in tideline, are also used outside of `data/`.
   + `plot/` contains mini-modules for plotting various types of data, mostly diabetes-specific. These mini-modules are called by `pool.js` when rendering data. Most of the data types are self-explanatory (at least to those who have some knowledge of type 1 diabetes), but 'cbg' and 'smbg' may require explanation. 'cbg' stands for **C**ontinuous **B**lood **G**lucose and refers to the readings generated by a [Dexcom](http://www.dexcom.com/ 'Dexcom') or [Medtronic](http://www.medtronicdiabetes.com/treatment-and-products/enlite-sensor 'Medtronic Enlite Continuous Glucose Monitoring') continuous glucose sensor. 'smbg' stands for **S**elf-**M**onitored **B**lood **G**lucose and refers to the readings generated by a traditional home fingerstick blood glucose meter.
      - `stats/` contains a special mini-module for creating a ""stats widget"" that updates on the fly as the user navigates along the tideline. This is essentially a special type of pool that is hierarchical itself, containing component ""puddles,"" where the relationship between `puddle.js` and `stats.js` is roughly equivalent to the relationship between `pool.js` and the `one-week.js` and `twoweek.js` main components.
     - `util/` contains a couple of small utility modules:
     	- within `annotations/`, `annotation.js` and `annotationdefinitions.js` generate data annotations.
        - within `axes/`, `dailyx.js` is a custom axis generator for the x-axis of the tideline one-day view; more custom axis generators will be added here in the future.
        - within `tooltips/`, `shapes.js` encodes the shapes for tideline's custom tooltips and `tooltip.js` provides methods for adding a tooltip on hover over a plotted datapoint.
     	- `bgboundary.js` provides a utility for determining the class (very-low, low, target, high, very-high) of a blood glucose value given the user's (or the default) target range.
     	- `commonbolus.js` provides a utility for getting information about boluses, whether these are `bolus` events are embedded inside `wizard` events.
     	- `drawbolus.js` provides plotting functions for boluses, whether these are `bolus` events are embedded inside `wizard` events.
        - `fill.js` generates the background fill for each data pool.
        - `legend.js` defines legend generators for all the pools that require a legend.
        - `scales.js` generates D3 scales for various diabetes data types. The functions in this utility module are at the moment specific to the plotting functions in `plot/`, not generally useful.
        - `shapeutil.js` provides methods for manipulating SVG shapes in various ways; it is required by modules in `annotations/` and `tooltips/`.
   + `validation` contains all the code necessary to perform client-side data validation, including schemas for all datatypes currently rendered by tideline, a small module `validate.js` providing validation functions, and our custom schema construction and validation tools in `validator/`.
- `plugins/` contains modules that do not properly belong in tideline's core functionality. These fall into two categories: application-specific modules and data(-specific) preprocessing modules.
     - `blip/` contains 'factories' for generating tideline data visualizations in Tidepool's first application [blip](https://github.com/tidepool-org/blip 'blip'). See this repository's [wiki](https://github.com/tidepool-org/tideline/wiki#using-tideline 'Tideline Wiki') for information on writing tideline chart factories.
- `test/` contains the tideline test suite. See [Test](#test) for instructions on running the test suite.
- `web/` contains the [GitHub Pages](http://pages.github.com/ 'GitHub Pages') branch for this repository, which sometimes hosts a gallery for proposed additions or enhancements to the tideline example being developed in `example/`. If you would like to add something to this gallery, feel free to submit your modifications to the files in `example/` (and elsewhere in tideline, if relevant) and open a pull request against `master` (although this is not where your changes will be merged). Please comment in the pull request that your changes are intended as an addition to the gallery. If you are also proposing changes to the tideline library (i.e., outside of `example/`), a separate pull request containing those changes alone is appreciated.

## Using Tideline

### The Core

#### Common Assumptions

`js/tidelinedata.js/` makes certain assumptions about the data that is passed to it. These assumptions are verified at runtime via the schema and validation code found in `js/validation/`. The most important of the requirements are the following:

- The data are valid JSON - specifically, an array of objects.
- Each object has a key `normalTime` which is an [ISO 8601](http://en.wikipedia.org/wiki/ISO_8601 'Wikipedia: ISO 8601') representation of the date and time at which the datapoint occurred, *formatted as UTC time*.
- The data are sorted by `normalTime`, in ascending order.

Both one-day and two-week charts also expect a [Node.js EventEmitter](http://nodejs.org/api/events.html 'Node.js API: Events') passed as an argument.

#### SVG Structure

As noted above, tideline *loves* SVG group `` elements. The basic structure of the one-day tideline chart is as follows:

```XML
|--
| |-
| | |-
| | |-
| | | |-
| | | | |-
| | | | |-
| | |-
| | | |-
| | | |-
| | |-
| | |-
| | |-
| | |-
```

And the two-week chart differs only minimally:

```XML
|--
| |-
| | |-
| | | |-
| | | | |-
| | | | | |-
| | | | | |-
| | | |-
| | | | |-
| | |-
| | |-
| | |-
| | |-
| | |-
| | |-
```

Because SVG has no concept of a [z-index](https://developer.mozilla.org/en-US/docs/Web/CSS/z-index 'CSS z-index'), elements are layered according to the order in which they appear in the SVG XML. One of the reasons tideline makes such liberal use of group elements is to control the layering through the order of the group elements. Thus, the ordering of the groups in the two outlines above is often significant.
",,2024-04-16T13:13:00Z,22,38,41,"('jebeck', 1695), ('clintonium-119', 538), ('GordyD', 170), ('krystophv', 121), ('jh-bate', 87), ('ianjorgensen', 68), ('cheddar', 53), ('courtenayhuffman', 51), ('hntrdglss', 46), ('nicolashery', 38), ('kentquirk', 32), ('gniezen', 24), ('anderspitman', 15), ('trhodeos', 6), ('pazaan', 4), ('ursooperduper', 2), ('dependabotbot', 2), ('BCabon', 1), ('darinkrauss', 1), ('dduugg', 1), ('mortonfox', 1), ('coyotte508', 1)","[3, 'Good Health and Well-Being']"
jembi/hearth,A fast FHIR-compliant server focused on longitudinal data stores.,"[![Build Status](https://travis-ci.org/jembi/hearth.svg?branch=master)](https://travis-ci.org/jembi/hearth) [![codecov](https://codecov.io/gh/jembi/hearth/branch/master/graph/badge.svg)](https://codecov.io/gh/jembi/hearth)

# Hearth
HEARTH (noun): the floor of a '[FHIR](http://hl7.org/fhir/)'place. A fast FHIR-compliant server focused on longitudinal data stores.

This project aims to provide a fast and lightweight FHIR server that also supports some of the FHIR-based IHE profiles. It is still in the early stages of development, follow the project to stay informed.

We do our best to update this project when we have projects with funding that are using it. Any contributions are welcomed and encouraged! Help us make this something great.

# Documentation
For more information regarding the capabilities of Hearth and how to get working with it please refer to the [wiki documentation](https://github.com/jembi/hearth/wiki)

# Usage

## Using docker compose

**Note:** Requires [docker](https://docs.docker.com/install/) and [docker-compose](https://docs.docker.com/compose/install/) to be installed

Download the [docker compose file from here](./docker-compose.yml), then execute to following in the directory you downloaded it to:

`docker-compose up`

Once started the fhir endpoint will be available on your system at this url: `http://localhost:3447/fhir/`

## For development
To run in development mode use the following commands. First Mongo needs to be available on your system. The easiest way to do this is through docker:

**Note:** Requires mongo 3.6+

```
docker run --name hearth-mongo -d -p 27017:27017 mongo
```
Install dependencies
```
yarn
```
Now start the server in dev mode (which uses a dev namespaced database)
```
yarn dev:start
```
otherwise for production just run:
```
yarn start
```

The default FHIR version is STU3 as set in the config files (we don't yet support R4), to change this either change the config files or make use of overriding config variable via environment variables:
```
server__fhirVersion=dstu2 yarn start
```

To run the tests:
```
yarn test
```

View the possible config fields [here](https://github.com/jembi/hearth/blob/master/config/default.json).

# Pro dev tips:
* To run only specific test files use `yarn test:these-files test/pdqm.js`.
* Run `yarn cov` to show coverage details in your browser.
","'fhir', 'health', 'mhd', 'ohie', 'openhie', 'pdqm', 'pixm'",2022-12-22T11:31:56Z,14,26,13,"('rcrichton', 253), ('napiergit', 163), ('bradsawadye', 59), ('hnnesv', 49), ('nthfloor', 38), ('bausmeier', 30), ('MattyJ007', 26), ('Zooloo2014', 23), ('tmvumbi2', 12), ('kaweesi', 9), ('tumijacob', 8), ('johnnynotsolucky', 3), ('lukeaduncan', 1), ('trevorgowing', 1)","[3, 'Good Health and Well-Being']"
huridocs/uwazi,"Uwazi is a web-based, open-source solution for building and sharing document collections","

![Uwazi Logo](https://uwazi.io/assets/16369950628097kcvfquj74a.svg)

![Uwazi CI](https://github.com/huridocs/uwazi/workflows/Uwazi%20CI/badge.svg)
[![Maintainability](https://api.codeclimate.com/v1/badges/8c98a251ca64daf434f2/maintainability)](https://codeclimate.com/github/huridocs/uwazi/maintainability)
[![Test Coverage](https://api.codeclimate.com/v1/badges/8c98a251ca64daf434f2/test_coverage)](https://codeclimate.com/github/huridocs/uwazi/test_coverage)

Uwazi is a flexible database application to capture and organise collections of information with a particular focus on document management. HURIDOCS started Uwazi and is supporting dozens of human rights organisations globally to use the tool.

[Uwazi](https://www.uwazi.io/) | [HURIDOCS](https://huridocs.org/)

Read the [user guide](https://uwazi.io/page/9852italrtk/support)

# Installation guide

- [Dependencies](#dependencies)
- [Production](#production)
- [Development](#development)

# Dependencies

Before anything else you will need to install the application dependencies:

- **NodeJs >= 18.16.1** For ease of update, use nvm: https://github.com/creationix/nvm.
- **ElasticSearch 7.17.6** https://www.elastic.co/downloads/past-releases/elasticsearch-7-17-6 Please note that ElasticSearch requires Java. Follow the instructions to install the package manually, you also probably need to disable ml module in the ElasticSearch config file:
  `xpack.ml.enabled: false`
- **ICU Analysis Plugin (recommended)** [installation](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu.html#analysis-icu) Adds support for number sorting in texts and solves other language sorting nuances. This option is activated by setting the env var USE_ELASTIC_ICU=true before running the server (defaults to false/unset).
- **MongoDB 4.2** https://docs.mongodb.com/v4.2/installation/ . If you have a previous version installed, please follow the instructions on how to [upgrade here](https://docs.mongodb.com/manual/release-notes/4.2-upgrade-standalone/). The new mongosh dependency needs to be added [installation](https://www.mongodb.com/docs/mongodb-shell/).
- **Yarn** https://yarnpkg.com/en/docs/install.
- **pdftotext (Poppler)** tested to work on version 0.86 but it's recommended to use the latest available for your platform https://poppler.freedesktop.org/. Make sure to **install libjpeg-dev** if you build from source.

# Production

[Install/upgrade procedure](./SELF_HOSTED_INSTRUCTIONS.md)

# Development

If you want to use the latest development code:

```
$ git clone https://github.com/huridocs/uwazi.git
$ cd uwazi
$ yarn install
$ yarn blank-state
```

If you want to download the Uwazi repository and also download the included git submodules, such as the `uwazi-fixtures`, which is used for running the end-to-end testing:

```
$ git clone --recurse-submodules https://github.com/huridocs/uwazi.git
$ cd uwazi
$ yarn install
```

If the main Uwazi repository had already been cloned/downloaded and now you want to load its sub-modules, you can run

```
$ git submodule update --init
```

There may be an issue with pngquant not running correctly. If you encounter this issue, you are probably missing the library **libpng-dev**. Please run:

```
$ sudo rm -rf node_modules
$ sudo apt-get install libpng-dev
$ yarn install
```

### Docker

Infrastructure dependencies (ElasticSearch, ICU Analysis Plugin, MongoDB, Redis and Minio (S3 storage) can be installed and run via Docker Compose. ElasticSearch container will claim 2Gb of memory so be sure your Docker Engine is alloted at least 3Gb of memory (for Mac and Windows users).

```shell
$ ./run start
```

### Development Run

```
$ yarn hot
```

This will launch a webpack server and nodemon app server for hot reloading any changes you make.

### Webpack server

```
$ yarn webpack-server
```

This will launch a webpack server. You can also pass `--analyze`to get detailed info on the webpack build.

### Testing

#### Unit and Integration tests

We test using the JEST framework (built on top of Jasmine). To run the unit and integration tests, execute

```
$ yarn test
```

This will run the entire test suite, both on server and client apps.

Some suites need MongoDB configured in Replica Set mode to run properly. The provided Docker Compose file runs MongoDB in Replica Set mode and initializes the cluster automatically, if you are using your own mongo installation Refer to [MongoDB's documentation](https://www.mongodb.com/docs/manual/tutorial/deploy-replica-set/#initiate-the-replica-set) for more information.

#### End to End (e2e)

For End-to-End testing, we have a full set of fixtures that test the overall functionality. Be advised that, for the time being, these tests are run ON THE SAME DATABASE as the default database (uwazi_developmet), so running these tests will DELETE any existing data and replace it with the testing fixtures. DO NOT RUN ON PRODUCTION ENVIRONMENTS!

Running end to end tests requires a running Uwazi app.

Running tests with Nightmare

```
$ yarn hot
```

On a different console tab, run

```
$ yarn e2e
```

Running tests with Puppeteer

```
$ DATABASE_NAME=uwazi_e2e INDEX_NAME=uwazi_e2e yarn hot
```

On a different console tab, run

```
$ yarn e2e-puppeteer
```

Note that if you already have an instance running, this will likely throw an error of ports already been used. Only one instance of Uwazi may be run in the same port at the same time.

### Default login

The application's default login is admin / change this password now

Note the subtle nudge ;)

## System Requirements

- For big files with a small database footprint (such as video, audio and images) you'll need more HD space than CPU or RAM
- For text documents you should consider some decent RAM as ElasticSearch is pretty greedy on memory for full text search

The bare minimum you need to be able to run Uwazi on-prem without bottlenecks is:

- 4 GB of RAM (reserve 2 for Elastic and 2 for everything else)
- 2 CPU cores
- 20 GB of disk space

For development:

- 8GB of RAM (depending on whether the services are running)
- 4 CPU cores
- 20 GB of disk space
",'documents',2024-05-03T13:51:44Z,30,209,28,"('daneryl', 4040), ('konzz', 3125), ('RafaPolit', 2082), ('mfacar', 927), ('danicatalan', 877), ('txau', 859), ('Zasa-san', 856), ('grafitto', 794), ('dependabotbot', 766), ('fnocetti', 670), ('habbes', 602), ('bdittes', 446), ('LaszloKecskes', 276), ('gabriel-piles', 176), ('elric-wamugu', 151), ('varovaro', 75), ('rustedgrail', 50), ('yacky', 50), ('vkozinec', 18), ('snyk-bot', 14), ('Simpanoi-95', 11), ('nestorsalceda', 8), ('vorburger', 5), ('whyfrycek', 4), ('duncanka', 4), ('elreplicante', 2), ('enigmatic-bacon', 1), ('samschaevitz', 1), ('RhnSharma', 1), ('omimakhare', 1)","[16, 'Peace, Justice and Strong Institutions']"
OperationCode/resources_api,Flask API for programming and cyber security learning resources,"# Operation Code Learning Resources API

## Vision

This project provides an API for storing and retrieving learning resources that might be helpful to members of [Operation Code](https://operationcode.org/). Ideally, this project will provide the backend for various interfaces for working with the data. The front end can be found at https://operationcode.org/resources.

## Getting Started

Sometimes these installs can be tricky.  If you get stuck ask for help in the Slack [#oc-python-projects](https://operation-code.slack.com/messages/C7NJLCCMB) channel!

1. If you are not a member of Operation Code, please sign up at https://operationcode.org/join
    - Provide your email, name, zip code, and a password of your choosing.
    - You will need this email and password later when creating your API key.
    - Look for a Slack invite email and join the Slack organization, and then the [#oc-python-projects](https://operation-code.slack.com/messages/C7NJLCCMB) channel.

1. Install [Git](https://git-scm.com/downloads).
    - Choose your OS from the website and follow the prompts.  This installs Git and the Bash Terminal on your machine.
    - Windows users: use the Git Bash Terminal for any of
      the commands in the remainder of this README.
      [The Bash Primer](http://www.compciv.org/bash-guide/)
    - Extra: [Git Documentation](https://git-scm.com/doc) for more information on Git.

1. Fork & Clone
    - [Fork a repository](https://help.github.com/articles/fork-a-repo/)
    - Create a local clone of your fork

1. Install Docker and ensure it is running
    - [Docker Desktop for Mac and Windows](https://www.docker.com/products/docker-desktop)
    - [Docker Engine for Linux](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
    - Additional step for Linux: install [docker compose](https://docs.docker.com/compose/install/#install-compose) as well.

1. [Install Make](http://gnuwin32.sourceforge.net/packages/make.htm) if you're on Windows. OSX already has it installed. Linux will tell you how to install it (i.e., `sudo apt-get install make`)

1. Run `make setup`

1. Run `make all` and then navigate to http://localhost:5000/

This should open the **Operation Code Resources API** webpage which has the documentation for the API. You can go through it to know about all available endpoints and understand how to use them.

If you encounter any errors, please open an issue or contact us on slack in #oc-python-projects.

## Authentication

 Routes that modify the database (e.g., `POST` and `PUT`) are authenticated routes. You need to include a header in your request with your API key. To generate an API key:

 1. Send a POST to http://localhost:5000/api/v1/apikey with the following JSON payload:

    ```json
    {
      ""email"": ""your@email.com"",
      ""password"": ""yoursupersecretpassword""
    }
    ```

    The email and password specified should be your login credentials for the Operation Code website. If you are not a member of Operation Code, please sign up at https://operationcode.org/join

    Example `curl` command:
    ```sh
    curl -X POST \
      http://localhost:5000/api/v1/apikey \
      -H 'Content-Type: application/json' \
      -d '{
            ""email"": ""your@email.com"",
            ""password"": ""yoursupersecretpassword""
          }'
    ```

 1. The response will have the following structure (but will contain your email and apikey):
    ```json
    {
        ""apiVersion"": ""1.0"",
        ""data"": {
            ""apikey"": ""yourapikey"",
            ""email"": ""your@email.com""
        },
        ""status"": ""ok""
    }
    ```
    - It may be helpful to save this apikey in a secure place like a password manager.
    - Do not hard-code your apikey in your own scripts. Always use an [environment variable](https://www.twilio.com/blog/2017/01/how-to-set-environment-variables.html)
    - You can always re-issue the POST to recover your apikey.
    - To rotate your apikey (getting a new apikey and making your old key invalid), issue an empty `POST` request to `/api/v1/apikey/rotate` with your apikey included in a header `x-apikey: yourapikey`:
        ```sh
        curl -X POST \
          http://localhost:5000/api/v1/apikey/rotate \
          -H 'x-apikey: 0a14f702da134390ae43f3639686fe26'
        ```

1. When you create a request to an authenticated route, you must include a header `x-apikey: yourapikey`

    Example curl request to an authenticated route:
    ```bash
    curl -X POST \
      http://localhost:5000/api/v1/resources \
      -H 'Content-Type: application/json' \
      -H 'x-apikey: 0a14f702da134390ae43f3639686fe26' \
      -d '{
            ""category"": ""Regular Expressions"",
            ""languages"": [""Regex""],
            ""name"": ""Regex101"",
            ""notes"": ""Regular Expression tester"",
            ""paid"": false,
            ""url"": ""https://regex101.com/""
    }'
    ```

## Development Notes

If you make changes to the models.py or other schemas, you need to run a migration and upgrade again:

```
make migrate
```

Before committing, please lint your code:

```
make lint
```

And make sure the tests pass:

```
make test
```

There are [many more make commands available](Makefile) you may find useful.

## History

This project began when I ([Aaron Suarez](https://github.com/aaron-suarez)) started to learn software development. I did a little searching for learning resources and started a list of resources I wanted to come back to when I had some time. Shortly after this, I joined [Operation Code](https://operationcode.org/join) and encountered several more learning resources that members were sharing. Soon enough, my list had grown to about 40 links. At some point, someone asked ""Hey, does anyone have any learning resources?"" and I said ""I have a list of about 40 links that I can DM to you if you like.""

Immediately, I received several requests from others asking for this list. I started to share it around via DMs, and someone suggested that I put it up on GitHub. I also got a lot of feedback about how it would be nice if there was more metadata, like a way to categorize the resources, organize them by language, and maybe even have some notes or a description (assuming the title isn't obvious). Meanwhile, the list continued to grow rapidly.

Someone got the bright idea to put it on the OC website, so I started working on a PR to incorporate it into OC, which at the time used Rails and YAML files to serve up list-like content. However, a redesign of the site was in progress, so my PR was never merged. Eventually I got out of the military and began doing software development full time. I kept maintaining the list as a little side project and sharing it around, but it hasn't made it onto the OC website. However, the [resources.yml](https://github.com/OperationCode/resources_api/blob/main/resources.yml) file is accessible via Slack with a slash command, so if you type `/repeat resources` in a public channel, the bot will link you to that file.

Eventually, the idea to use the data for multiple purposes was proposed. One really cool idea was for a Slack bot to ping relevant channels with random resources on a regular basis so that people could find awesome resources that had been shared without having to find them by searching the slack history. So it was decided that a full featured API would be important in order to really serve the needs of the OC community. This repo contains the source for the aforementioned API.

It took a lot of time and collaborative effort to put together the data and to build the API, and the work is not done yet! Please help us out, and feel free to use the learning resources to gain the knowledge required to contribute.
","'api', 'flask', 'hacktoberfest', 'hacktoberfest2017', 'hacktoberfest2018', 'hacktoberfest2019', 'hacktoberfest2020', 'learning', 'resources'",2023-12-10T21:37:47Z,30,65,12,"('aaron-junot', 135), ('dependabot-previewbot', 125), ('apex-omontgomery', 41), ('michizhou', 23), ('Jitsusama', 15), ('Digbigpig', 15), ('Kandeel4411', 13), ('veralake777', 12), ('hugovk', 9), ('chynh', 7), ('irvingpop', 6), ('soris-codes', 5), ('thinkingserious', 5), ('kylemh', 5), ('AllenAnthes', 4), ('lord-cant-even', 4), ('projectLewis', 4), ('stormybay', 4), ('mike-lloyd03', 3), ('Qlwentt', 3), ('Optimus-PrimaNocta', 3), ('McDuckerton', 3), ('platipo', 2), ('distributedlock', 2), ('king-11', 2), ('dprothero', 2), ('hydrosquall', 1), ('wendeee', 1), ('itsaudia', 1), ('jtmst', 1)","[10, 'Reduced Inequalities']"
demarches-simplifiees/demarches-simplifiees.fr,Dématérialiser et simplifier les démarches administratives ,"# demarches-simplifiees.fr

## Contexte

[demarches-simplifiees.fr](https://www.demarches-simplifiees.fr) est un site web conçu afin de répondre au besoin urgent de l'État d'appliquer la directive sur le 100 % dématérialisation pour les démarches administratives.

## Comment contribuer ?

demarches-simplifiees.fr est un [logiciel libre](https://fr.wikipedia.org/wiki/Logiciel_libre) sous licence AGPL.

Vous souhaitez y apporter des changements ou des améliorations ? Lisez notre [guide de contribution](CONTRIBUTING.md).

## Installation pour le développement

### Dépendances techniques

#### Tous environnements

- postgresql
- imagemagick et gsfonts pour générer les filigranes sur les titres d'identité ou générer des minitiatures d'images.

> [!WARNING]
> Pensez à restreindre la policy d'ImageMagick pour bloquer l'exploitation d'images malveillantes.
> La configuration par défaut est généralement insuffisante pour des images provenant du web.
> Par exemple sous debian/ubuntu dans `/etc/ImageMagick-6/policy.xml` :

```xml


    
    
    

```

Nous sommes en cours de migration de `delayed_job` vers `sidekiq` pour le traitement des jobs asynchrones.
Pour faire tourner sidekiq, vous aurez besoin de :

- redis

#### Développement

- rbenv : voir https://github.com/rbenv/rbenv-installer#rbenv-installer--doctor-scripts
- Bun : voir https://bun.sh/docs/installation

#### Tests

- Chrome
- chromedriver :
  * Mac : `brew install chromedriver`
  * Linux : voir https://developer.chrome.com/blog/chrome-for-testing

Si l'emplacement d'installation de Chrome n'est pas standard, ou que vous utilisez Brave ou Chromium à la place,
il peut être nécessaire d'overrider pour votre machine le path vers le binaire Chrome, par exemple :

```ruby
# create file spec/support/spec_config.local.rb

Selenium::WebDriver::Chrome.path = ""/Applications/Brave Browser.app/Contents/MacOS/Brave Browser""

# Must exactly match the browser version
Webdrivers::Chromedriver.required_version = ""103.0.5060.53""
```

Il est également possible de faire une installation et mise à jour automatique lors de l'exécution de `bin/update` en définissant la variable d'environnement `UPDATE_WEBDRIVER`. Les binaires seront installés dans le repertoire `~/.local/bin/` qui doit être rajouté manuellement dans le path. 

### Création des rôles de la base de données

Les informations nécessaire à l'initialisation de la base doivent être pré-configurées à la main grâce à la procédure suivante :

    su - postgres
    psql
    > create user tps_development with password 'tps_development' superuser;
    > create user tps_test with password 'tps_test' superuser;
    > \q


### Initialisation de l'environnement de développement

Sous Ubuntu, certains packages doivent être installés au préalable :

    sudo apt-get install libcurl3 libcurl3-gnutls libcurl4-openssl-dev libcurl4-gnutls-dev zlib1g-dev

Afin d'initialiser l'environnement de développement, exécutez la commande suivante :

    bin/setup

### Lancement de l'application

On lance le serveur d'application ainsi :

    bin/dev

L'application tourne alors à l'adresse `http://localhost:3000` avec en parallèle un worker pour les jobs et le bundler vitejs.

### Utilisateurs de test

En local, un utilisateur de test est créé automatiquement, avec les identifiants `test@exemple.fr`/`this is a very complicated password !`. (voir [db/seeds.rb](https://github.com/betagouv/demarches-simplifiees.fr/blob/dev/db/seeds.rb))

### Programmation des tâches récurrentes

    rails jobs:schedule

### Voir les emails envoyés en local

Ouvrez la page [http://localhost:3000/letter_opener](http://localhost:3000/letter_opener).

### Mise à jour de l'application

Pour mettre à jour votre environnement de développement, installer les nouvelles dépendances et faire jouer les migrations, exécutez :

    bin/update

### Exécution des tests (RSpec)

Les tests ont besoin de leur propre base de données et certains d'entre eux utilisent Selenium pour s'exécuter dans un navigateur. N'oubliez pas de créer la base de test et d'installer chrome et chromedriver pour exécuter tous les tests.

Pour exécuter les tests de l'application, plusieurs possibilités :

- Lancer tous les tests

        bin/rake spec
        bin/rspec

- Lancer un test en particulier

        bin/rake spec SPEC=file_path/file_name_spec.rb:line_number
        bin/rspec file_path/file_name_spec.rb:line_number

- Lancer tous les tests d'un fichier

        bin/rake spec SPEC=file_path/file_name_spec.rb
        bin/rspec file_path/file_name_spec.rb

- Relancer uniquement les tests qui ont échoué précédemment

        bin/rspec --only-failures

- Lancer un ou des tests systèmes avec un browser

        NO_HEADLESS=1 bin/rspec spec/system

- Afficher les logs js en error issus de la console du navigateur `console.error('coucou')`

        JS_LOG=error bin/rspec spec/system

- Augmenter la latence lors de tests end2end pour déceler des bugs récalcitrants

        MAKE_IT_SLOW=1 bin/rspec spec/system

### Ajout de taches à exécuter au déploiement

        rails generate after_party:task task_name

### Linting

Le projet utilise plusieurs linters pour vérifier la lisibilité et la qualité du code.

- Faire tourner tous les linters : `bin/rake lint`
- Vérifier l'état des traductions : `bundle exec i18n-tasks health`
- [AccessLint](http://accesslint.com/) tourne automatiquement sur les PRs

### Régénérer les binstubs

    bundle binstub railties --force
    bin/rake rails:update:bin

## Déploiement

Voir les notes de déploiement dans [DEPLOYMENT.md](doc/DEPLOYMENT.md)

## Tâches courantes

### Tâches de gestion des comptes super-admin

Des tâches de gestion des comptes super-admin sont prévues dans le namespace `superadmin`.
Pour les lister : `bin/rake -D superadmin:`.

### Tâches d’aide au support

Des tâches d’aide au support sont prévues dans le namespace `support`.
Pour les lister : `bin/rake -D support:`.

## Compatibilité navigateurs

L'application gère les navigateurs récents, parmis lequels Firefox, Chrome, Safari et Edge (voir `config/initializers/browser.rb`).

La compatibilité est testée par Browserstack.[](https://www.browserstack.com/)

## Performance

[![View performance data on Skylight](https://badges.skylight.io/status/zAvWTaqO0mu1.svg)](https://oss.skylight.io/app/applications/zAvWTaqO0mu1)

Nous utilisons Skylight pour suivre les performances de notre application.

Par ailleurs, nous utilisons [Yabeda](https://github.com/yabeda-rb/yabeda) pour exporter des metriques au format prometheus. L'activation se fait via la variable d'environnement `PROMETHEUS_EXPORTER_ENABLED` voir config/env.example.optional .
",,2024-05-03T14:40:07Z,30,182,15,"('tchak', 3697), ('gregoirenovel', 2331), ('kemenaran', 2255), ('LeSim', 2208), ('colinux', 1631), ('mmagn', 1037), ('krichtof', 834), ('kara22', 665), ('Keirua', 625), ('fredZen', 563), ('mfo', 509), ('lisa-durand', 431), ('E-L-T', 405), ('XjulI1', 375), ('TanguyPatte', 206), ('JCBlondel', 154), ('sebastiencarceles', 135), ('n-b', 107), ('julieSalha', 100), ('dependabotbot', 100), ('chaibax', 100), ('damienlethiec', 91), ('glazzara', 85), ('maatinito', 69), ('jpoulvel', 69), ('pengfeidong', 43), ('dzc34', 26), ('Bounga', 25), ('inseo', 21), ('ysbaddaden', 20)","[16, 'Peace, Justice and Strong Institutions']"
curiouslearning/Norad-Eduapp4syria, Eduapp4syria - https://www.norad.no/eduapp4syria,"# Updated Repo

Looking for the most up-to-date version of Feed The Monster? We've [moved](https://github.com/curiouslearning/FeedTheMonster)! The new repo has updated documentation on how to create localizations and includes many tools we have created to get Feed The Monster into as many languages as possible!

If you are interested in more information about Feed The Monster, feel free to [contact us](https://www.curiouslearning.org/contact). 

# Eduapp4syria

Almost 2.5 million Syrian children are out of school because of conflict. Many have to cope with traumas and high levels of stress, which also affects their learning ability. High availability of smartphones among war-affected Syrian families can be a means for reaching children with engaging and fun learning supplements.

This can help facilitate their continued learning and future reintegration into school. Norway and several partners have therefore since January 2016 been conducting an international innovation competition to develop an open source smartphone application that can help Syrian children learn how to read in Arabic and improve their psychosocial wellbeing. The winning games will be released on Google Play and App Store in March 2017.([www.norad.no/eduapp4syria](https://www.norad.no/eduapp4syria)).
",,2020-08-24T16:56:21Z,6,5,9,"('tzahi-cet', 3), ('burrage', 2), ('goopennow', 1), ('runesto', 1), ('lacabra', 1), ('ndla', 1)","[4, 'Quality Education']"
ConservationInternational/trends.earth,trends.earth - measure land change,"# Trends.Earth

[![Trends.Earth](https://s3.amazonaws.com/trends.earth/sharing/trends_earth_logo_bl_600width.png)](http://trends.earth)

[![Documentation Status](https://readthedocs.org/projects/trendsearth/badge/?version=latest)](https://trendsearth.readthedocs.io/en/latest/?badge=latest)
[![Zipfile Status](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml)
[![Update translations](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml)
[![Tests](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml)

`Trends.Earth` is a free and open source tool to understand land change: the how and why
behind changes on the ground. Trends.Earth allows users to draw on the best available
information from across a range of sources - from globally available data to customized
local maps. A broad range of users are applying Trends.Earth for projects ranging from
planning and monitoring restoration efforts, to tracking urbanization, to developing
official national reports for submission to the United Nations Convention to Combat
Desertification (UNCCD).

`Trends.Earth` is a [QGIS](http://www.qgis.org) plugin that supports monitoring of land
change, including trends in urbanization, and changes in productivity, land cover, and
soil organic carbon. The tool can support monitoring land degradation for reporting to
the Global Environment Facility (GEF) and United Nations Convention to Combat
Desertification (UNCCD), as well as tracking progress towards achievement of Sustainable
Development Goal (SDG) target 15.3, Land Degradation Neutrality (LDN).

`Trends.Earth` was produced by a partnership of Conservation International, Lund
University, and the National Aeronautics and Space Administration (NASA), with
the support of the Global Environment Facility (GEF). It was further developed
through a partnership with Conservation International, University of Bern,
University of Colorado in partnership with USDA and USAID, University of California -
Santa Barbara in partnership with University of North Carolina - Wilmington and Brown
University with additional funding from the Global Environment Facility (GEF).

## Documentation

See the [user guide](http://trends.earth) for information on how to use
the plugin.

## Installation of stable version of plugin

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/). However, It is also
possible to install the plugin manually from a zipfile, which can be useful to
access an old version of the plugin, or to install the plugin without internet.
Instructions for both of these possibilities are below.

### Stable version from within QGIS (recommended)

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/).

### Stable version from zipfile

Download a stable version of `Trends.Earth` from
[the list of available releases on
GitHub](https://github.com/ConservationInternational/trends.earth/releases). Then follow
the instructions below on [installing the plugin from a
zipfile](#installing-plugin-from-a-zipfile).

## Installation of unstable version of plugin

If you are interested in using the unstable (actively under development) version of the
plugin, with the very latest (but not as well tested) features, or in contributing to
the development of it, you will want to install the development version. There are
two ways to install the development version:

- Using a packaged version (zipfile)

- Cloning the github repository and installing from that code

It is easier to install the plugin from a zipfile than from github, so this
option is recommended unless you are interested in contributing to development
of the plugin.

### Development version from zipfile

[Download the latest `Trends.Earth` zipfile](https://s3.amazonaws.com/trends.earth/sharing/LDMP_main.zip) (or use the
[the zipfile from the develop branch
](https://s3.amazonaws.com/trends.earth/sharing/LDMP_develop.zip) for the absolute
latest version). Then follow the instructions below on [installing the plugin
from a zipfile](#installing-plugin-from-a-zipfile).

QGIS3+ is required for the latest versions of Trends.Earth. The QGIS2 version is no
longer supported (support ended in March 2020). If you want to use a previous version of
`Trends.Earth` (e.g. versions that work with QGIS2), please refer to this
[repository](https://github.com/ConservationInternational/trends.earth/releases) where
all `Trends.Earth` releases are available.

### Development version from source

Open a terminal window and clone the latest version of the repository from
Github:

```
git clone https://github.com/ConservationInternational/trends.earth
```

Navigate to the root folder of the newly cloned repository, and install
`invoke`, a tool that assists with installing the plugin:

```
pip install invoke
```

Now run the setup task with `invoke` to pull in the external dependencies needed
for the project:

```
invoke plugin-setup
```

then you can install the plugin using invoke:

```
invoke plugin-install --profile=
```

If you modify the code, you need to run `invoke plugin-install` to update the
installed plugin in QGIS. You only need to rerun `invoke plugin-setup` if you
change or update the plugin dependencies. After reinstalling the plugin you
will need to restart QGIS or reload the plugin. Install the ""Plugin reloader""
plugin if you plan on making a log of changes
(https://github.com/borysiasty/plugin_reloader).

## Installing plugin from a zipfile

While installing `trends.earth` directly from within QGIS is recommended, it
might be necessary to install the plugin from a zipfile if you need to install
it offline, or if you need the latest features.

To install from a zipfile, first download a zipfile of the
[stable](#stable-version-from-zipfile) or
[development](#development-version-from-zipfile) version. The zipfile might be
named `LDMP.zip`or `LDMP_QGIS3.zip` depending on what
version you are installing.

When using QGIS3.10.3 or greater versions it is possible to install `trends.earth`
directly from a zipfile. To install `trends.earth` from a zipfile, open QGIS3.10.3
(or greater) and click on ""Plugins"" then on ""Manage and install plugins"" and
choose the option ""Install from ZIP"". Browse to the folder in which the zipfile
has been saved, select the zipfile and click on 'Install Plugin'.
It is not necessary to unzip the file.

Please, note that the latest version of `trends.earth` is only supported for
QGIS3.10.3 or greater versions.

Start QGIS, and click on ""Plugins"" then ""Manage and install plugins"". In the
plugins window that appears, click on ""Installed"", and then make sure there is
a check in the box next to ""Land Degradation Monitoring Tool"". The plugin is
now installed and activated. Click ""Close"", and start using the plugin.

## Getting help

### General questions

If you have questions related to the methods used by the plugin, how to use a
particular dataset or function, etc., it is best to first check the [user
guide](http://trends.earth/docs/en) to see if your question is already
addressed there. The [frequently asked questions (FAQ)
page](http://trends.earth/docs/en/about/faq.html) is another good place to
look.

If you don't find your answer in the above pages, you can also [contact the
discussion group](https://groups.google.com/forum/#!forum/trends_earth_users).

### Reporting an issue (error, possible bug, etc.)

If you think you have found a bug in Trends.Earth, report your issue to our
[issue
tracker](https://github.com/ConservationInternational/trends.earth/issues) so
the developers can look into it.

When you report an issue, be sure to provide enough information to allow us to
be able to reproduce it. In particular, be sure to specify:

- What you were doing with the plugin when the problem or error occurred (for
  example ""I clicked on 'Download Results' and got an error messaging saying
  `describe what the message said`"".
- The operating system you are using, version of the plugin are you using, and
  version of QGIS that you are using
- If you are emailing about an error or problem that occurred when downloading
  results, tell us your username, and the task start time that is listed in the
  download tool for the task you are referring to
- If the error occurred while processing data with the plugin, tell us the
  location you were analyzing with the tool (for example: ""I selected Argentina
  from the dropdown menu""). If you used your own shapefile, please send us the
  file you used.
- If you got a message saying ""An error has occurred while executing Python
  code"", send us either the text of the message, or a a screenshot of the error
  message. **Also, send us the content of the Trends.Earth log messages
  panel.** To access the Trends.Earth log messages panel, select ""View"", then
  ""Panels"", then ""Log Messages"" from within QGIS. Copy and paste the text from
  that panel and include it in your issue report. It will make it easiest for
  us to track things down (as there will be fewer log messages) if you do this
  after first starting a new QGIS session and immediately reproducing the
  error.

## License

`Trends.Earth` is free and open-source. It is licensed under the GNU General
Public License, version 2.0 or later
","'climate', 'climate-change', 'conservation', 'conservation-international', 'earth', 'gef', 'land', 'land-degradation', 'ldn', 'natural-climate-solutions', 'python', 'qgis-plugin', 'remote-sensing', 'sdgs', 'sustainable-development-goals', 'toolbox', 'trends', 'unccd'",2024-04-29T20:36:56Z,21,105,19,"('azvoleff', 2819), ('MLNoon', 138), ('luipir', 102), ('Samweli', 67), ('gkahiu', 66), ('gdaldegan', 62), ('alexbruy', 24), ('github-actionsbot', 22), ('vermeulendivan', 20), ('pre-commit-cibot', 20), ('gabrieldaldegan', 18), ('mgroglich', 10), ('dependabotbot', 7), ('omahs', 2), ('elpaso', 1), ('zacharlie', 1), ('eliaswilz', 1), ('jmertic', 1), ('nyalldawson', 1), ('mentaljam', 1), ('vikineema', 1)","[13, 'Climate Action']"
publiclab/leaflet-environmental-layers,"Collection of different environmental map layers in an easy to use Leaflet library, similar to https://github.com/leaflet-extras/leaflet-providers#leaflet-providers","# Leaflet Environmental Layers (LEL)
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/publiclab/leaflet-environmental-layers/)
[![npm version](https://badge.fury.io/js/leaflet-environmental-layers.svg)](https://badge.fury.io/js/leaflet-environmental-layers) [![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](http://standardjs.com)
[![Code of Conduct](https://img.shields.io/badge/code-of%20conduct-green.svg)](https://publiclab.org/conduct)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)


A leaflet plugin that has a collection of layers containing environmental data pulled in from different sources. See this [demo page](https://publiclab.github.io/leaflet-environmental-layers/example/index.html#lat=43.00&lon=-4.07&zoom=3&layers=Standard) for a simple demonstration of the plugin.

## Table of Contents
1. [What is LEL](#leaflet-environmental-layers-lel)
2. [Installation](#installation)
3. [Usage](#usage)
4. [Dependencies](#dependencies)
5. [Getting started](#getting-started)
6. [Features](#features)
7. [Layers](#layers)
8. [Adding LEL features individually](#adding-LEL-features-individually)
9. [Adding layers individually](#adding-layers-individually)
10. [Contributing](#contributing)
11. [Reach out to the maintainers](#reach-out-to-the-maintainers)
12. [About PublicLab](#about-publicLab)

## Installation
  
Install using NPM with: `npm install leaflet-environmental-layers`

See [Dependencies](https://github.com/publiclab/leaflet-environmental-layers/#dependencies) below for what you need to include for basic usage and for individual layers.

## Usage

Instantiate a collection of environmentally related Leaflet layers with the `L.LayerGroup.EnvironmentalLayers(options)` function:

```js
L.LayerGroup.EnvironmentalLayers({
  // simpleLayerControl: true,
  addLayersToMap: true,
  include: ['odorreport', 'clouds', 'eonetFiresLayer', 'Unearthing', 'PLpeople'], // display only these layers
  // exclude: ['mapknitter', 'clouds'], // layers to exclude (cannot be used at same time as 'include'
  // display: ['eonetFiresLayer'], // which layers are actually shown as opposed to just being in the menu
  hash: true,
  embed: true,
  // hostname: 'domain name goes here'
}).addTo(map);
```

When specifying layers to include or exclude, use [their names as listed in the table below](https://github.com/publiclab/leaflet-environmental-layers/#layers).

### Options

| Option              | Type    |Default                | Description |
|---------------------|---------|-----------------------|-------------|
| `baseLayers`          | Object  | -                     | Passed in as `{ 'Standard': baselayer }` where `'Standard'` is the name given to the layer and `baselayer` is the variable containing the base tile layer(`L.tileLayer()`). It can have more than one base layer. At least one base layer should be added to the map instance. If no baseLayers are provided it is defaulted to a grey-scale base map. |
| `simpleLayerControl`  | Boolean | false                 | If set to true, it will replace LEL's layer menu with leaflet's default layers control. |
| `addLayersToMap`      | Boolean | false                 | If set to true, adds all layers in the `include` option to the map by default. |
| `include`             | Array   | Array                 | If provided, adds the given layers to the layer menu or layers control. If not provided, adds all the layers to the layer menu or layers control. |
| `exclude`             | Array   | -                     | If provided, excludes the given layers from the layer menu or layers control. |
| `display`             | Array   | -                 | If provided, displays the given layers by default on the map. |
| `hash`                | Boolean | false                 | If true, provides hash support for the map. |
| `embed`               | Boolean | false                 | If true, adds an embed control that generates code to the map for embedding the map on other sites. |
| `hostname`            | String  | 'publiclab.github.io' | Uses the value in place of hostname in the URL generated in the embed code. |

## Dependencies

- Install Bootstrap(Required for the layers menu)
- Install @fortawesome/fontawesome-free
- Add the following to the head of the HTML file that would contain the map

```html






































```

## Getting started

### _Installation Instructions_

1. Clone this repository to your local environment.
2. Run `npm install` to install all the necessary packages required.
3. Open `examples/index.html` in your browser to look at the preview of the library.

### _Instructions for a developer_

1. Install grunt - https://gruntjs.com/installing-grunt.
2. Make the changes you are working on in the respective /src files.
3. Run `grunt build` to generate files in the /dist directory.
4. Run `grunt transpile` to transpile es6 code and copy files needed to run the tests to the /dist directory.
5. Run `grunt jasmine` to run tests on the LEL layers and ensure they pass.
6. Run `npm run start` to start a local server.
7. Run `npm run cy:run:chrome` to run e2e and integration tests.
8. Test your changes on a browser by opening `examples/index.html`.

#### Testing in GitPod

To run Cypress tests in GitPod, you'll need to do `npm install -g cypress` and then use: `npm run start:ci & cypress run --browser electron`

## Features

### _Zoom or Pan_

Click and drag the map to pan it.

### _Change the Base Map and Overlay layers_

Use the button on right-most corner to change the way the background of the map looks.

### _See More Data_

- Toggle certain layers on and off using the Layers button in the toolbar. 
- Layers with near-real-time or real-time data will have the 'NRT/RT' mark on them.
- More information on the layer data will be available when clicking the 'i' button on the layer
- Layers that allow contributions will have a `report` button or `contribute` button.
- Layers will be visible on the menu only when the map view intersects with the layer's bounds or zoom levels.
- A badge displays the number of new layers in the map view when the map intersects with new layers

Read more about the layers menu [here](https://publiclab.org/notes/christie_reni/01-29-2020/new-features-in-leaflet-environmental-layers#Layers+Menu).

### _Click on a Point_

Click on a point or marker on the map to learn more about it.

### _Minimal mode_

Click on the button group on the left, below the zoom controls, to change between default markers mode and minimal markers mode. Use minimal markers mode for a smoother experience when using multiple layers with many markers.

Read more about this feature [here](https://publiclab.org/notes/christie_reni/01-29-2020/new-features-in-leaflet-environmental-layers#Minimal+mode).

### _URL Hash_

The map page's URL hash updates on map movement and when a layer is added or removed from a map. This helps preserve map state when refreshing or copying the URL to another page.

### _Embed Code_

Click on the button at the bottom on the left side of a map to generate an embed code so that the map page can be embedded in other sites.

## Layers
The information of each layer can be found here: [Layer Information](https://publiclab.org/notes/sagarpreet/06-06-2018/leaflet-environmental-layer-library)


| Layer Name                    | Color     |
| ----------------------------- | --------- |
| `PLpeople`                    | N/A       |
| `wisconsin`                   | N/A       |
| `fracTrackerMobile`           | N/A       |
| `purpleLayer`                 | `#8b0000` |
| `purpleairmarker`             | `#800080` |
| `skytruth`                    | `#ff0000` |
| `fractracker`                 | `#ffff00` |
| `pfasLayer`                   | `#00ff00` |
| `toxicReleaseLayer`           | `#008000` |
| `odorreport`                  | `#ff00ff` |
| `mapknitter`                  | `#D50039` |
| `Power`                       | `#ffc0cb` |
| `Telecom`                     | `#0000ff` |
| `Petroleum`                   | `#a52a2a` |
| `Water`                       | `#4B0082` |
| `income`                      | `#006400` |
| `americanIndian`              | `#800000` |
| `asian`                       | `#ffa500` |
| `black`                       | `#FFD700` |
| `multi`                       | `#ffc0cb` |
| `hispanic`                    | `#DCDCDC` |
| `nonWhite`                    | `#808080` |
| `white`                       | `#a52a2a` |
| `plurality`                   | `#800000` |
| `clouds`                      | `#80dfff` |
| `cloudsClassic`               | `#b3f0ff` |
| `precipitation`               | `#00ff55` |
| `precipitationClassic`        | `#00008b` |
| `rain`                        | `#8080ff` |
| `rainClassic`                 | `#1a1aff` |
| `snow`                        | `#80ffe5` |
| `pressure`                    | `#e62e00` |
| `pressureContour`             | `#ff3300` |
| `temperature`                 | `#ff3300` |
| `wind`                        | `#00008b` |
| `city`                        | `#b3ffff` |
| `windrose`                    | `#008000` |
| `Territories`                 | `#000000` |
| `Languages`                   | `#000000` |
| `Treaties`                    | `#000000` |
| `aqicnLayer`                  | `#000000` |
| `openaq`                      | `#000000` |
| `luftdaten`                   | `#000000` |
| `opensense`                   | N/A       |
| `osmLandfillMineQuarryLayer`  | N/A       |
| `eonetFiresLayer`             | `#78fffa` |
| `Unearthing`                  | N/A       |


## Adding LEL features individually
### _Add a legend_

In `src/legendCreation.js`, add `addLayerNameURLPair(layer_var, ""img_url"");`, where `layer_var` is consistent with the variable used in `example/index.html` and `img_url` is the source of the image to be used as the legend.

### _Add an embed control_

#### Creation

    // Assuming your map instance is in a variable called map
    L.control.embed(options).addTo(map);

The optional options object can be passed in with any of the following properties:
| Option    | Type    | Default     | Description |
|-----------|---------|-------------|-------------|
| position  | String  | 'topleft'   | Other possible values include 'topright', 'bottomleft' or 'bottomright' |
| hostname  | String  | 'publiclab.github.io'   | Sets hostname for the URL in the embed code |

### _Add hash support for easy sharing of map_    
#### Add link

        

#### Creation

        // Assuming your map instance is in a variable called map
        // Assuming an object with all the map layers is in a variable called allMapLayers
        var hash = new L.FullHash(map, allMapLayers);  

### _Add the layers menu_

#### Prerequisites

- Bootstrap
- jQuery

#### Dependencies

- Install Bootstrap(Required for the layers menu)
- Install @fortawesome/fontawesome-free
- Add the following to the head of the HTML file that would contain the map

```
 


 








```

#### Usage example
```js
  var baseMaps = {
    'Standard': L.tileLayer('TILE_LAYER_URL').addTo(map),
    'Dark': L.tileLayer('TILE_LAYER_URL')
  };

  var overlayMaps = {
    'wisconsin': Wisconsin_NM,  // Assuming 'Wisconsin_NM' is the variable that holds the wisconsin layer object
    'indigenousLands': {
      category: 'group', // Let's the control know if this should be rendered as a group
      layers: { // Layers making the group
        'Territories': IndigenousLandsTerritories,  // Assuming 'IndigenousLandsTerritories' is the variable that holds the respective layer object
        'Languages': IndigenousLandsLanguages,  // Assuming 'IndigenousLandsLanguages' is the variable that holds the respective layer object
        'Treaties': IndigenousLandsTreaties,  // Assuming 'IndigenousLandsTreaties' is the variable that holds the respective layer object
      },
    },
  };

  var leafletControl = new L.control.layersBrowser(baseMaps, overlayMaps);
  leafletControl.addTo(map);
```
#### Creation

    L.control.layersBrowser(baseMaps, overlayMaps).addTo(map);

- `baseMaps` and `overlayMaps` are object literals that have layer names as keys and [Layer](https://leafletjs.com/reference-1.6.0.html#layer) objects as values. Read more about [Leaflet's Control.Layers](https://leafletjs.com/reference-1.6.0.html#control-layers).
- `baseMaps` will be hidden if only one base map is provided
- The layer information displayed for each layer is stored in `info.json`
- The layer name(key) in the `overlayMaps` object should match the keys in `info.json`
- The layers are filtered according to the map view
- When there are new layers present in the map view when moving around a badge is displayed near the layer control icon on the top right showing the number of new layers in the view

### _Add minimal mode control_
#### Creation

    // Assuming your map instance is in a variable called map
    // Assuming your layers menu or layers control instance is in a variable called layersControl
    L.control.minimalMode(layersControl).addTo(map);

### _Add search control_

  LEL uses [leaflet-google-places-autocomplete](https://github.com/Twista/leaflet-google-places-autocomplete) for the search control feature.



## Adding layers individually

### _To use Wisconsin Non-Metallic Layer_
#### Add
      
      
#### Creation
      var Wisconsin_NM = wisconsinLayer(map) ;

### _To use Fractracker Mobile Layer_
      var FracTracker_mobile = L.geoJSON.fracTrackerMobile();

### _To use Purple Layer_
	
	

### _To use Unearthing Layer_
	

### _To use PLpeople Layer_
  These must be included in the file before /dist/LeafletEnvironmentalLayers.js:

        
        

### _Real Time Layers

#### city (by openWeather)

        var city = L.OWM.current({intervall: 15, minZoom: 3});

#### WindRose (by openWeather)
      
      

      var windrose = L.OWM.current({intervall: 15, minZoom: 3, markerFunction: myWindroseMarker, popup: false, clusterSize:       50,imageLoadingBgUrl: 'https://openweathermap.org/img/w0/iwind.png' });
      windrose.on('owmlayeradd', windroseAdded, windrose);

### _Open Infra Map_
#### OpenInfraMap_Power Layer

    var OpenInfraMap_Power = L.tileLayer('https://tiles-{s}.openinframap.org/power/{z}/{x}/{y}.png',{
        maxZoom: 18,
        attribution: '&copy; OpenStreetMap, About OpenInfraMap'
    });

#### OpenInfraMap_Petroleum Layer

    var OpenInfraMap_Petroleum = L.tileLayer('https://tiles-{s}.openinframap.org/petroleum/{z}/{x}/{y}.png', {
      maxZoom: 18,
      attribution: '&copy; OpenStreetMap, About OpenInfraMap'
    });

#### OpenInfraMap_Telecom Layer

    var OpenInfraMap_Telecom = L.tileLayer('https://tiles-{s}.openinframap.org/telecoms/{z}/{x}/{y}.png', {
      maxZoom: 18,
      attribution: '&copy; OpenStreetMap, About OpenInfraMap'
    });

#### OpenInfraMap_Water Layer

    var OpenInfraMap_Water = L.tileLayer('https://tiles-{s}.openinframap.org/water/{z}/{x}/{y}.png',{
      maxZoom: 18,
      attribution: '&copy; OpenStreetMap, About OpenInfraMap'
    });

### _Spreadsheet-based layers_

We can source locations from a spreadsheet in a format like this:

| Title  | Latitude | Longitude   | Notes             |
|--------|----------|-------------|-------------------|
| First	 | 29.671282 | -95.17829  | The first marker  |
| Second | 29.760371 | -95.504828 | The second marker |
| Third  | 29.917755 | -95.283494 | The third marker  |

The layer is constructed like this:

```js
var layer = L.SpreadsheetLayer({
  url: 'https://docs.google.com/spreadsheets/d/14BvU3mEqvI8moLp0vANc7jeEvb0mnmYvH4I0GkwVsiU/edit?usp=sharing', // String url of data sheet
  lat: 'Latitude', // name of latitude column
  lon: 'Longitude', // name of longitude column
  columns: ['Title', 'Notes'], // Array of column names to be used
  generatePopup: function() {
    // function used to create content of popups
  },
  // imageOptions: // optional, defaults to blank
  // sheetNum: // optional, defaults to 0 (first sheet)
});
layer.addTo(map);
```

Read more here: https://github.com/publiclab/leaflet-environmental-layers/blob/master/src/util/googleSpreadsheetLayer.js

We're going to try spinning this out into its own library; see: https://github.com/publiclab/leaflet-environmental-layers/issues/121

## Contributing
Please read [CONTRIBUTING.md](https://github.com/publiclab/leaflet-environmental-layers/blob/master/CONTRIBUTING.md) for details on our code of conduct, the process for submitting pull requests, and steps to add new layers.

## Reach out to the maintainers
Reach out to the maintainers here: https://github.com/orgs/publiclab/teams/maintainers

## About PublicLab
Public Lab is a community and non-profit democratizing science to address environmental issues that affect people.

[_^back to top_](#leaflet-environmental-layers-lel)
",,2023-03-04T02:39:41Z,30,99,18,"('sagarpreet-chadha', 201), ('crisner', 141), ('jywarren', 67), ('ananyaarun', 43), ('nstjean', 18), ('dependabotbot', 16), ('dependabot-previewbot', 12), ('rarrunategu1', 11), ('kevinzluo', 11), ('daemon1024', 7), ('rkpattnaik780', 6), ('ahmadkarlam', 4), ('anthony-zhou', 3), ('sidntrivedi012', 3), ('Rishabh570', 2), ('nkleinbaer', 2), ('faithngetich', 2), ('cryptoclidus', 2), ('saurabhkanswal', 2), ('gabrielrumiranda', 2), ('cesswairimu', 2), ('avsingh999', 2), ('z-arnott', 1), ('IZNYogendra', 1), ('llabake', 1), ('ephemeralwaves', 1), ('dewanhimanshu', 1), ('sssash18', 1), ('subhamskm', 1), ('Tlazypanda', 1)","[13, 'Climate Action']"
rufuspollock-okfn/reconcile-csv,A simple OpenRefine reconciliation service that runs on top of a CSV file,"# reconcile-csv

A OpenRefine reconciliation service that works from CSV files.

## Introduction

Someone just handed you some datasets they claim are related and you should
be able to mesh them up easily - no problem you're the data wizard. Except
there is no clear unique identifier you could use to join the dataset. All
you know is that if several columns are the same, it should be the same.
Enter typos and misspellings and here you go: A long night ahead.

Reconcile-CSV aims to ease your pain: It acts as a OpenRefine
reconciliation service and performs fuzzy matching to identify related
entries.  

## Usage

Create a column with Unique-ID's you will use to match.

Pre-compiled:
```
java -Xmx2g -jar reconcile-csv-0.1.2.jar   
```

With Leiningen:
```
lein run   
```

Then add ```http://localhost:8000/reconcile``` as a reconciliation service
to refine. You can add more columns through the reconcile-interface in
Refine. 

Reconcile away!

Then use:

```
cell.recon.match.id
```

to get the ID from the match.


## License

Copyright © 2013 Michael Bauer, Open Knowledge Foundation

Distributed under the BSD-2 Clause license. See LICENSE for details
",,2015-07-06T18:10:19Z,1,116,37,"('mihi-tr', 27)","[17, 'Partnerships for the Goals']"
GlobalFishingWatch/vessel-classification,Vessel classification: feature generation and model training/inference.,"# Global Fishing Watch Vessel Classification Pipeline.

[Global Fishing Watch](http://globalfishingwatch.org) is a partnership between [Skytruth](https://skytruth.org), [Google](https://environment.google/projects/fishing-watch/) and [Oceana](http://oceana.org) to map all of the trackable commercial fishing activity in the world, in near-real time, and make it accessible to researchers, regulators, decision-makers, and the public.

This repository contains code to build Tensorflow models to classify vessels and identify fishing behavior
based on [AIS](https://en.wikipedia.org/wiki/Automatic_identification_system) data.

(This is not an official Google Product).

## Overview

Use AIS, and possibly VMS data in the future, to extract various types of information including:
   
  - Vessel types

  - Vessel fishing activity

  - Vessel attributes (length, tonnage, etc)

The project consists of a convolutional neural networks (CNN) that infers vessel features.


### Neural Networks

We have two CNN in production, as well as several experimental nets. One net
predict vessel class (`longliner`, `cargo`, `sailing`, etc), as well as
vessel length and other vessel parameters, while the second predicts whether 
a vessel is fishing or not at a given time point.

*We initially used a single CNN to predict everything at once,
but we've moveed to having two CNN.  The original
hope was that we would be able to take advantage of transfer learning between
the various features. However, we did not see any gains from that, and using
a multiple nets adds useful flexibility.*

The nets share a similar structure, consisting of a large number (currently 9)
of 1-D convolutional layers, followed by a single dense layer. The net for 
fishing prediction is somewhat more complicated since it must predict fishing at
each point. To do this all of the layers of the net are combined, with upscaling
of the upper layers, to produce a set of features at each point. 
These design of these nets incorporates ideas are borrowed
from the ResNets and Inception nets, among other places, but adapted for the 1D environment.

The code associated with the neural networks is located in
`classification`. The models themselves are located
in `classification/models`. 

## Data layout

*The data layout is currently in flux as we move data generation to Python-Dataflow
managed by Airflow*

### Common parameters

In order to support the above layout, all our programs need the following common parameters:

* `env`: to specify the environment - either development or production.
* `job-name`: for the name (or date) of the current job.
* Additionally if the job is a dev job, the programs will read the $USER environment variable
  in order to be able to choose the appropriate subdirectory for the output data.


# Neural Net Classification

## Running Stuff

-  `python -m train.deploy_cloudml` -- launch a training run on cloudml. Use `--help` to see options

   If not running in the SkyTruth/GFW environment, you will need to edit `deploy_cloudml.yaml`
   to set the gcs paths correctly.

   For example, to run vessel classification in the dev environment with the name `test`:

      python -m train.deploy_cloudml \
              --env dev \
              --model_name vessel_characterization \
              --job_name test_deploy_v20200601 \
              --config train/deploy_v_py3.yaml \
              --feature_path gs://machine-learning-dev-ttl-120d/features/vessel_char_track_id_features_v20200428/features \
              --vessel_info char_info_tid_v20200428.csv \
              --fishing_ranges det_ranges_tid_v20200428.csv


   **IMPORTANT**: Even though there is a maximum number of training steps specified, the CloudML
   process does not shut down reliably.  You need to periodically check on the process and kill it
   manually if it has completed and is hanging. In addition, there are occasionally other problems
   where either the master or chief will hang or die so that new checkpoints aren't written, or
   new validation data isn't written out. Again, killing and restarting the training is the solution.
   (This will pick up at the last checkpoint saved.)

- *running training locally* -- this is primarily for testing as it will be quite slow unless you
  have a heavy duty machine:

        python -m classification.run_training \
            fishing_range_classification \
            --feature_dimensions 14 \
            --root_feature_path FEATURE_PATH \
            --training_output_path OUTPUT_PATH \
            --fishing_range_training_upweight 1 \
            --metadata_file VESSEL_INFO_FILE_NAME \
            --fishing_ranges_file FISHING_RANGES_FILE_NAME \
            --split {0, 1, 2, 3, 4, -1}
            --metrics minimal

- `python -m train.compute_metrics` -- evaluate results and dump vessel lists. Use `--help` to see options


* Inference is now run solely through Apache Beam. See README in pipe-features for details


## Local Environment Setup

* Python 3.7++
* Tensorflow version >1.14.0,<2.0 from (https://www.tensorflow.org/get_started/os_setup)
* `pip install google-api-python-client pyyaml pytz newlinejson python-dateutil yattag`






",,2023-10-10T18:30:39Z,8,82,14,"('d40cht', 176), ('bitsofbits', 116), ('seacourtaw', 80), ('hmoraldo', 9), ('enriquetuya', 9), ('smpiano', 8), ('andres-arana', 3), ('smcclellan7', 2)","[14, 'Life Below Water']"
healthsites/healthsites,Building an open data commons of health facility data with OpenStreetMap,"# Welcome to the healthsites code base!



[![Join the chat at https://gitter.im/healthsites/healthsites](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/healthsites/healthsites?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Healthsites is a framework for capturing, publishing and sharing critical
health and sanitation related data to help make these facilities more
accessible and relevant to the communities they serve. Our framework does not
limit our endeavours to these domains and in the future we plan to support
additional domains where it is helpful in humanitarian work.

**Please note that this project is in the early phase of its development.**

You can visit a running instance of healthsites at [healthsites.io](http://healthsites.io).

# Status

These badges reflect the current status of our development branch:

Tests status: [![Build Status](https://travis-ci.org/healthsites/healthsites.svg?branch=develop)](https://travis-ci.org/healthsites/healthsites)

Coverage status: [![Coverage Status](https://coveralls.io/repos/github/healthsites/healthsites/badge.svg?branch=develop)](https://coveralls.io/github/healthsites/healthsites?branch=develop)

Development status: [User stories in the backlog](https://github.com/healthsites/healthsites/projects/12)

# License

Data: [Open Database License](http://opendatacommons.org/licenses/odbl/)
Data credits : &copy; OpenStreetMap contributors 
Code: [3-clause BSD License](https://opensource.org/license/bsd-3-clause/)

Our intention is to foster widespread usage of the data and the code that we provide. Please use this code and data in the interests of humanity and not for nefarious purposes.

# Setup instructions

1. Copy the `.env` template to project root

```shell
cp deployment/.template.env .env
```

2. Copy docker-compose file

```shell
cp deployment/docker-compose.override.template.yml deployment/docker-compose.override.yml
```

3. Change the PBF file to smaller one, i.e. from Senegal

```Dockerfile
# change this
RUN wget https://planet.openstreetmap.org/pbf/planet-latest.osm.pbf  -O settings/country.pbf
# to this
RUN wget https://download.geofabrik.de/africa/senegal-and-gambia-latest.osm.pbf -O settings/country.pbf
```

4. Run `make`

Additional steps:

1. Because the imposm may fail as the database container is not ready, you may need to restart it.

```shell
docker restart healthsites_osm_imposm
```
","'coverage-status', 'django', 'electicity', 'facilities', 'healthcare', 'openstreetmap', 'python', 'sanitation'",2024-02-16T12:13:36Z,19,139,33,"('meomancer', 761), ('timlinux', 223), ('dodobas', 195), ('nixilla', 150), ('ismailsunni', 104), ('MariaSolovyeva', 83), ('myarjunar', 36), ('anitanh', 23), ('markherringer', 18), ('Gustry', 6), ('dimasciput', 4), ('milovanderlinden', 2), ('ericboucher', 1), ('jnicho02', 1), ('waffle-iron', 1), ('tyrasd', 1), ('gitter-badger', 1), ('danieldegroot2', 1), ('dependabotbot', 1)","[17, 'Partnerships for the Goals']"
giving-a-fuck-about-climate-change/carbon-inferno,A react.js front-end for climate change related data  :earth_africa:,"[![Build Status](https://travis-ci.org/giving-a-fuck-about-climate-change/carbon-inferno.svg?branch=master)](https://travis-ci.org/giving-a-fuck-about-climate-change/carbon-inferno)

[![GitHub issues](https://img.shields.io/github/issues/giving-a-fuck-about-climate-change/carbon-inferno.svg)](https://github.com/giving-a-fuck-about-climate-change/carbon-inferno/issues)
[![GitHub forks](https://img.shields.io/github/forks/giving-a-fuck-about-climate-change/carbon-inferno.svg)](https://github.com/giving-a-fuck-about-climate-change/carbon-inferno/network)
[![GitHub stars](https://img.shields.io/github/stars/giving-a-fuck-about-climate-change/carbon-inferno.svg)](https://github.com/giving-a-fuck-about-climate-change/carbon-inferno/stargazers)
[![GitHub stars](https://img.shields.io/github/watchers/giving-a-fuck-about-climate-change/carbon-inferno.svg)](https://github.com/giving-a-fuck-about-climate-change/carbon-inferno/watchers)
[![Gitter chat](https://badges.gitter.im/giving-a-fuck-about-climate-change/gitter.png)](https://gitter.im/giving-a-fuck-about-climate-change/Lobby)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

![CarbonDoomsDay Logo](https://i.imgur.com/jfj3CMs.png)

A [React.js] web application for visualising CO2 emissions since 1958.

[React.js]: https://github.com/facebookincubator/create-react-app

We're currently live over at [carbondoomsday.com]. This application is part of the [Carbon Doomsday project]:

[carbondoomsday.com]: http://www.carbondoomsday.com/
[Carbon Doomsday project]: http://datadrivenjournalism.net/featured_projects/carbon_doomsday_tracking_co2_since_1958

> Carbon Doomsday is a real-time API and chart of worldwide carbon dioxide
> levels. Developed as a community project, its goal is to be an open-source
> platform for climate data. Data for the API and chart comes from NOAA’s Earth
> System Research Lab in Mauna Loa, Hawaii. The project is rooted in principles
> of free software, open data access and a willingness to contribute to further
> education on the global climate issue.

We pull our data from [api.carbondoomsday.com] (powered by [carbondoomsday]) while [giving a fuck].

[api.carbondoomsday.com]: http://api.carbondoomsday.com/
[giving a fuck]: http://titojankowski.com/no-one-gives-a-fck-about-climate-change/
[carbondoomsday]: https://github.com/giving-a-fuck-about-climate-change/carbondoomsday

## How to run locally

  * `git clone` the project.
  * Install the dependencies using `npm i`.
  * Run `npm start`.
  * The app will run on `http://localhost:3000`.

## Linting

We are using [eslint] and [prettier], we recommend setting up your editor to
format on save. But we also have a git hook which will also do the prettier
formatting upon committing.

[eslint]: https://github.com/eslint/eslint
[prettier]: https://github.com/prettier/prettier

## Tests

To run the tests run `npm test`.

## Deploy to Production

Every merged pull request to master gets auto-magically deployed.

Check the [buildpack](https://github.com/mars/create-react-app-buildpack) that we are using.

## Graph Component

Our Chart component is an SVG component written in react.

We are using [react-svg-coordfuncs].

[react-svg-coordfunc]: https://github.com/grady-lad/react-svg-coordfuncs

## Contributing

We welcome all persons no matter what skill level you have!

If you're interested in contributing, we need you! Please check out the following:

  * [Wiki]: Read about the project!
  * [Gitter Lobby]: Come chat to us!
  * [Vision] Repository: Weigh in and let us know your opinion!

[Wiki]: https://github.com/giving-a-fuck-about-climate-change/carbondoomsday/wiki
[Gitter Lobby]: https://gitter.im/giving-a-fuck-about-climate-change/Lobby
[Vision]: https://github.com/giving-a-fuck-about-climate-change/vision

## Support

Support us with a monthly donation and help us continue our activities.

> [Become a backer!](https://opencollective.com/giving-a-fuck-about-climate-change)


",,2018-12-20T19:53:21Z,8,36,5,"('grady-lad', 178), ('stephanieinez', 58), ('decentral1se', 21), ('YPCrumble', 8), ('titojankowski', 6), ('lwm', 4), ('pwhisenhunt', 1), ('Ricardo-Silva91', 1)","[13, 'Climate Action']"
openfoodfacts/openfoodfacts-ios,Native (Swift) version of Open Food Facts for iOS. Coders & Decoders welcome 🤳🥫 😊,"


# (Old) Open Food Facts iPhone and iPad app

The new app is located at https://github.com/openfoodfacts/smooth-app. We would like to use some part of this [to create a Swift SDK for Open Food Facts](https://github.com/openfoodfacts/openfoodfacts-swift). Will you help us ?

[![Build Status](https://travis-ci.org/openfoodfacts/openfoodfacts-ios.svg?branch=master)](https://travis-ci.org/openfoodfacts/openfoodfacts-ios)
[![Project Status](https://opensource.box.com/badges/active.svg)](http://opensource.box.com/badges)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/openfoodfacts/localized.svg)](https://translate.openfoodfacts.org)
![TestFlight release](https://github.com/openfoodfacts/openfoodfacts-ios/workflows/TestFlight%20release/badge.svg)






## What is Open Food Facts? What can I work on ?

[Open Food Facts](https://world.openfoodfacts.org/) is a food products database made by everyone, for everyone.
Open Food Facts on iPhone and iPad has 0,5M users and 1,6M products. Each contribution you make will have a large impact on food transparency worldwide. Finding the right issue or feature will help you have even more more impact. Feel free to ask for feedback on the #android channel before you start work, and to document what you intend to code.

### Features you can work on
- [ ] [What can I work on ?](https://github.com/openfoodfacts/openfoodfacts-ios/issues/912)

## Join the team !

OpenFoodFacts [has a Slack chat room where we discuss and support each other](https://slack.openfoodfacts.org/), join the #iOS and #iOS-alerts channels. 

## Current features

- [x] Barcode scanning (including a simple offline mode)
- [x] NOVA, Nutri-Score and Eco-Score display (including in grey if we don't have them yet for the product)
- [x] Ingredient analysis with a simple way to get it if not available
- [x] Product page (needs revamping)
- [x] Search for products based on name
- [x] Allergen alerts (would need to be more discoverable)
- [x] Internationalised user interface & multilingual products handling (view & data addition)
- [x] Product addition & editing (incl. on-the-fly OCR of ingredients and labels, plus integration of the OFF AI)
- [x] Image upload
- [x] Night mode

## Code documentation
[Automatically generated code documentation on the wiki](https://github.com/openfoodfacts/openfoodfacts-ios/wiki/)

## Building

### Quick & automatic setup
The easiest way to setup the dependencies of the project and generate the Xcode project is to run `sh scripts/setup.sh` from the top of the repository, before opening the project in Xcode. 

#### Dependency Management - Carthage

We currently use [Carthage](https://github.com/Carthage/Carthage) for dependency management.
New to Carthage? Others have found the following resources helpful:
* [Ray Wenderlich's Carthage Tutorial](https://www.raywenderlich.com/416-carthage-tutorial-getting-started)
* [Chris Mendez's Carthage cheat sheet](https://www.chrisjmendez.com/2016/10/30/carthage-cheat-sheet/)

Before opening the project in Xcode, run 
`
brew install carthage
`

`
carthage bootstrap --platform iOS --cache-builds
`

To generate the Xcode project run `sh scripts/create-project.sh`.
In order to generate the Xcode project we use [XcodeGen](https://www.github.com/yonaskolb/XcodeGen).
  
### Fastlane
See the [fastlane/README.md](fastlane/README.md) for a list and description of all lanes. 

To launch a lane, you must have several env variable set. This can be done by creating a `.env` file in the `fastlane` folder, and fill it (see `.env.example`)

You can install [Fastlane](https://github.com/fastlane/fastlane) with Homebrew:
```
brew cask install fastlane
```
#### Generating screenshots
```
fastlane snapshot 
```
##### Roadmap on automatic screenshot generation:
* [Our priorities](https://github.com/openfoodfacts/openfoodfacts-ios/issues/913)
* [Configuration file](https://github.com/openfoodfacts/openfoodfacts-ios/blob/01ea37e5247978a52d491181bb7dd2fb384214af/Snapshots/SnapshotConfiguration.swift)

### Style and conventions - SwiftLint

A script runs when building the app that executes SwiftLint to enforce style & conventions to the code.

You can install [SwiftLint](https://github.com/realm/SwiftLint/) with Homebrew:
```
brew install swiftlint
```

### Error reporting - Sentry
[Track crashes](https://sentry.io/organizations/openfoodfacts/issues/?project=5276492)

### Translations

You can [help translate](https://translate.openfoodfacts.org) Open Food Facts (no technical knowledge required, takes a minute to signup).
","'crowdsourcing', 'food', 'food-additives', 'food-products', 'foodtracker', 'hacktoberfest', 'ios', 'iphone', 'openfoodfacts', 'swift', 'xcode'",2023-09-05T11:22:10Z,30,357,18,"('teolemon', 4949), ('tovkal', 398), ('aleene', 203), ('philippeauriach', 107), ('dependabotbot', 46), ('rudrankriyam', 45), ('github-actionsbot', 10), ('SuzGupta', 7), ('SpacyRicochet', 7), ('s4rv4d', 6), ('jncosideout', 4), ('roger-tan', 4), ('maleshchenko', 4), ('mike011', 4), ('aubincleme', 3), ('magauran', 3), ('tejuamirthi', 2), ('safiach', 2), ('yegorsch', 2), ('hfehrmann', 2), ('davidlamys', 2), ('damien-rivet', 2), ('acecilia', 1), ('chaubss', 1), ('cquest', 1), ('kant', 1), ('punss', 1), ('bigimot22', 1), ('mohammedsafwat', 1), ('raziqfarid', 1)","[12, 'Responsible Consumption and Production']"
foodcoops/foodsoft,"Web-based software to manage a non-profit food coop (product catalog, ordering, accounting, job scheduling).","Foodsoft
=========

[![Build Status](https://github.com/foodcoops/foodsoft/workflows/Ruby/badge.svg)](https://github.com/foodcoops/foodsoft/actions)
[![Coverage Status](https://coveralls.io/repos/foodcoops/foodsoft/badge.svg?branch=master)](https://coveralls.io/r/foodcoops/foodsoft?branch=master)
[![Docs Status](https://inch-ci.org/github/foodcoops/foodsoft.svg?branch=master)](http://inch-ci.org/github/foodcoops/foodsoft)
[![Code Climate](https://codeclimate.com/github/foodcoops/foodsoft.svg)](https://codeclimate.com/github/foodcoops/foodsoft)
[![Docker Status](https://img.shields.io/docker/cloud/build/foodcoops/foodsoft.svg)](https://hub.docker.com/r/foodcoops/foodsoft)
[![Documentation](https://img.shields.io/badge/yard-docs-blue.svg)](http://rubydoc.info/github/foodcoops/foodsoft)

Web-based software to manage a non-profit food coop (product catalog, ordering, accounting, job scheduling).

A food cooperative is a group of people that buy food from suppliers of their own choosing. A collective do-it-yourself supermarket. Members  order their products online and collect them on a specified day. And all put in a bit of work to make that possible. Foodsoft facilitates the process.

If you're a food coop considering to use foodsoft, please have a look at the [wiki page for foodcoops](https://github.com/foodcoops/foodsoft/wiki/For-foodcoops). When you'd like to experiment with or develop foodsoft, you can read [how to set it up](https://github.com/foodcoops/foodsoft/blob/master/doc/SETUP_DEVELOPMENT.md) on your own computer.

More information about using this software and contributing can be found on the [wiki](https://github.com/foodcoops/foodsoft/wiki).

Roadmap
-------

If you'd like to see what is currently bring prioritised for development, check [our roadmap](https://github.com/orgs/foodcoops/projects/1). If you'd like to influence the roadmap, please join our [monthly community call](https://forum.foodcoops.net/t/foodsoft-monthly-community-call/573/6). As of March 2023, Foodsoft has limited development capacity but we are trying to build this up once more. For now, we try to prioritise what we work on, in order to focus our efforts. If your proposed changes are waiting for some time without review, please join the community call to discuss.

Developing
----------

> Foodsoft development needs your help! If you want to hack/triage/organise to improve the software, please consider joining our monthly community calls which are announced on [this forum thread](https://forum.foodcoops.net/t/foodsoft-monthly-community-call/573/6). In these calls, we check in with each other, discuss what to prioritise and try to make progress with development and community issues together.

Get foodsoft [running locally](doc/SETUP_DEVELOPMENT.md),
then visit our [Developing Guidelines](https://github.com/foodcoops/foodsoft/wiki/Developing-Guidelines)
page on the wiki.

Get a foodsoft dev-environment running in the browser with Gitpod

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/foodcoops/foodsoft)

Follow these [instructions](doc/SETUP_DEVELOPMENT_GITPOD.md) to complete setup from within the Gitpod workspace. 

Deploying
---------

Setup foodsoft to [run in production](doc/SETUP_PRODUCTION.md), or join an existing
[hosting platform](https://foodcoops.net/foodsoft-hosting/).

License
-------

Foodsoft is licensed under the [AGPL](https://www.gnu.org/licenses/agpl-3.0.html)
license (version 3 or later). Practically this means that you are free to use,
adapt and redistribute the software, as long as you publish any changes you
make to the code.

For private use, there are no restrictions, but if you give others access to
Foodsoft (like running it open to the internet), you must also make your
changes available under the same license. This can be as easy as
[forking](https://github.com/foodcoops/foodsoft/fork) the project on Github and
pushing your changes. You are not required to integrate your changes back into
the main Foodsoft version (but if you're up for it that would be very welcome).

To make it a little easier, configuration files are exempt, so you can just
install and configure Foodsoft without having to publish your changes. These
files are marked as public domain in the file header.

If you have any remaining questions, please
[open an issue](https://github.com/foodcoops/foodsoft/issues/new) or open a new
topic at the [forum](https://forum.foodcoops.net).

Please see [LICENSE](LICENSE.md) for the full and authoritative text. Some
bundled third-party components have [other licenses](vendor/README.md).

Thanks to [Icons8](http://icons8.com/) for letting us use their icons.
","'cooperative', 'food', 'food-cooperative', 'foodcoop', 'foodsoft', 'membership', 'product-catalog'",2024-04-28T11:56:39Z,30,313,37,"('wvengen', 1027), ('paroga', 369), ('bennibu', 304), ('yksflip', 51), ('sandoz', 50), ('fsmanuel', 42), ('dependabotbot', 29), ('kidhab', 29), ('benni-as', 28), ('lentschi', 16), ('carchrae', 15), ('Viehlieb', 9), ('haraldreingruber', 8), ('JuliusR', 6), ('dokterbob', 6), ('mjavurek', 5), ('twothreenine', 5), ('mortbauer', 4), ('decentral1se', 3), ('MikeiLL', 3), ('mamhoff', 3), ('akirk', 2), ('TomStrom16', 2), ('tg-x', 2), ('nurp', 1), ('mariandl', 1), ('hamaryns', 1), ('acracia', 1), ('metawilm', 1), ('sjmulder', 1)","[12, 'Responsible Consumption and Production']"
microsoft/Smart-Energy-Foundation-Demo-Stack,"A demonstration solution that uses real-time Carbon Emissions and Weather data mining to show how data from several Web based APIs can be mined, visualized and acted upon in a Microsoft Azure solution","# Carbon Emissions Data Platform
Optimising energy consumption based on the real-time Marginal Emissions of an electricity market can substantially reduce the consumer's Carbon Emissions.
This is a demonstration solution to show how data from several Web based APIs can be mined, visualised and acted upon in a Microsoft Azure solution. This solution collects real-time Carbon Emissions data from the WattTime API (https://api.watttime.org/), and global Weather data and weather forecasts from the Wunderground API (https://www.wunderground.com/). It then visualises this data over time to allow the user to understand the relationship between the two. It demonstrates the ability to collect related pieces of data into a single place to allow automation to act upon the conclusions extracted from it. For example, automating devices via the Azure IoT service to minimise net Carbon Emissions.
For more information, and the source code of the solution, see the project's GitHub page at https://github.com/Microsoft/Smart-Energy-Foundation-Demo-Stack .

![Image](https://github.com/Microsoft/Smart-Energy-Foundation-Demo-Stack/blob/master/Images/MarginalEmissionsDashboard.PNG)

# Problem and Solution Space
[Video](https://www.youtube.com/watch?v=5VjkwxCsWv4) on the background on this Solution.

# Azure Subscription
To deploy this solution, you'll need an Azure subscription. You can sign up for a free Azure subscription to deploy this solution to at https://azure.microsoft.com/en-us/free . 

# Prerequisites
To enable the solution to mine Carbon Emissions data, you will need to register for a free WattTime API Key, and enter it when deploying the solution. Register for a key at https://api.watttime.org/accounts/register/ . When registering, tick 'I would like request access to Pro features (e.g., marginal carbon emissions data)'"">


# Components
This solution automatically provisions the required Azure infrastructure, and kicks off the data miner. The components created are: 
* A SQL Azure Server and Database
* An Azure Storage account
* An Azure Function to create the database tables and initial seeded data
* An Azure Function that runs every hour, mining carbon emissions data from the pre-selected markets
* A PowerBI Report showing your real-time electricity grid carbon emissions data, and whether now is a good, average or bad time to consume power in order to minimise carbon emissions. That dashboard will appear at https://functions-*MyFunctionNamespace*.azurewebsites.net/api/pbiweb 

# Customising the Solution
The code for this solution is available from the project's GitHub page at https://github.com/Microsoft/Smart-Energy-Foundation-Demo-Stack . From here, you can update the locations which the miner pulls in data for and republish the solution. You can also extend the solution as you see fit. For example, by pulling in additional data points which correlate with weather or Carbon Emissions in order to build Machine Learning Models that predict future values. 

# Configuring the Data Miner / Updating  the Regions Mined:
The DataMinerFunction reads where it should mine Weather and Emissions data from in the ApiDataMinerConfigs.xml file in the DataMinerWorkerRole project. The XML file contains a series of  elements, comprised of a \ and a \. A Region element can have one or both. Configure the details  of a region as such: 
```xml

  
    
      
        PJM
        40.348444276169
        -74.6428556442261
        Eastern Standard Time
        https://api.watttime.org/api/v1/
        **MyWattTimeAPIKey**
        AzureTableStorageCallRecollection
        200
      

      
        us/nj/princeton
        40.348444276169
        -74.6428556442261
        WundergroundPageSubUrl
        Eastern Standard Time
        http://api.wunderground.com/api/
        **MyWundergroundAPIKey**
        AzureTableStorageCallRecollection
        3
      
      
    

```
You can update the regions being mined in two places: 
A) Directly in the running Azure Function: 
	1. Opening the Resource group in the Azure Portal
	2. Hit the Azure Function and hit App Service Editor
	3. You'll see the ApiDataMinerConfigs.xml files listed in the ApiDataMinerConfigs folder under the wwroot
	4. You can update the content of the XML file and the updated contents will be picked up by the miner Azure Function the next time it runs. 

B) In the Visual Studio Solution: by updating the ApiDataMinerConfigs.xml XML file in the Azure Function project, before publishing the function to the Azure Function running on your Azure subscription

### GPS based weather data mining:
There are two options for mining weather data from the  Wunderground Service: 
	1. Using the suburl of the weather station you wise to mine (for example, us/nj/princeton to mine the data for Princeton New Jersey, located at https://www.wunderground.com/weather/us/nj/princeton on Wunderground). For this, set the  attribute in to ""WundergroundPageSubUrl"" (as in \WundergroundPageSubUrl\ )
	2. Based on GPS coordinates. In this case, the miner will first query Wunderground to locate the closest weather station to the GPS coordinates supplied, and then mine that weather station's weather data. There is the possibility that the weather station closest to the coordinates specified only records a subset of the full set of weather datapoints the major weather stations record. For this, set the  attribute in to ""GPS"" (as in \GPS\ )


# More Information
[The project's GitHub page](https://github.com/Microsoft/Smart-Energy-Foundation-Demo-Stack)
[WattTime.Org](http://watttime.org/)
",,2019-03-20T23:23:19Z,3,50,19,"('conork20', 26), ('microsoftopensource', 2), ('msftgits', 1)","[13, 'Climate Action']"
opengovfoundation/madison,"Madison is a platform for lawmakers to share legislation with their citizens, allowing the community to add comments and suggest improvements.","# Madison

[![CircleCI](https://circleci.com/gh/opengovfoundation/madison/tree/master.svg?style=svg)](https://circleci.com/gh/opengovfoundation/madison/tree/master)

Madison is an open-source document engagement and feedback platform.  While
Madison can be used to collaborate on many different kinds of documents, the
official version is being built legislative and policy documents in mind.

If you have questions about Madison, please open an issue and we will try to
respond as soon as possible.

Check out the [Madison Documentation](https://github.com/opengovfoundation/madison/tree/master/docs)
or jump right into the [Issue Log](https://github.com/opengovfoundation/madison/issues)
for more information on the project.

## Architecture

As of 4.0, Madison is a generally plain Laravel (PHP) application.

## User Guide

Visit our [Engagement Guide](https://mymadison.io/sponsors/guide)
for information on using Madison to run engagement rounds.

## How to Help

Open an issue, claim an issue, comment on an issue, or submit a pull request to
resolve one.
",,2022-12-06T14:34:14Z,20,656,50,"('cmbirk', 1280), ('sethetter', 418), ('rtsio', 186), ('doshitan', 119), ('coogle', 66), ('bryanconnor', 37), ('dkd903', 17), ('werdnanoslen', 9), ('mkmezz', 8), ('barryvdh', 7), ('ritvikgautam', 5), ('jraller', 2), ('st421', 2), ('denised', 1), ('joecohens', 1), ('zaenulhilmi', 1), ('spekulatius', 1), ('codemoe', 1), ('defvol', 1), ('Vanuan', 1)","[17, 'Partnerships for the Goals']"
oppia/oppia,"A free, online learning platform to make quality education accessible for all.","# [Oppia](https://www.oppia.org) [![End-to-End and Lighthouse CI performance tests](https://github.com/oppia/oppia/actions/workflows/e2e_lighthouse_performance_acceptance_tests.yml/badge.svg)](https://github.com/oppia/oppia/actions/workflows/e2e_lighthouse_performance_acceptance_tests.yml)

Oppia is an online learning tool that enables anyone to easily create and share interactive activities (called 'explorations'). These activities simulate a one-on-one conversation with a tutor, enabling students to learn by doing while getting feedback.

In addition to developing the Oppia platform, the team has developed free and effective [lessons](https://www.oppia.org/fractions) on basic mathematics, and we are planning to expand our educational offering to basic science and financial literacy. These lessons help learners who lack appropriate access to educational resources.

The Oppia web application is built using Python, Angular, and Google App Engine. See also:

- [Oppia.org community site](https://www.oppia.org)
- [User Documentation](https://oppia.github.io/)
- [Contributors' wiki](https://github.com/oppia/oppia/wiki)
- [GitHub Discussions](https://github.com/oppia/oppia/discussions)
- [Developer announcements](http://groups.google.com/group/oppia-dev)
- [File an issue](https://github.com/oppia/oppia/issues/new/choose)

You can also sign up to our [email newsletter](https://shorturl.at/CHPY6) for news and updates about the Oppia project.


  
    
  


## Installation

Please refer to the [Installing Oppia page](https://github.com/oppia/oppia/wiki/Installing-Oppia) for full instructions.

## Contributing

The Oppia project is built by the community for the community. We welcome contributions from everyone, especially new contributors.

You can help with Oppia's development in many ways, including art, coding, design and documentation.

- **Developers**: please see [this wiki page](https://github.com/oppia/oppia/wiki/Contributing-code-to-Oppia#setting-things-up) for instructions on how to set things up and commit changes.
- **All other contributors**: please see our [general contributor guidelines](https://github.com/oppia/oppia/wiki).

If you'd like to donate to support our work, you can do so [here](https://www.oppia.org/donate).

## Support

If you have any feature requests or bug reports, please log them on our [issue tracker](https://github.com/oppia/oppia/issues/new/choose).

Please report security issues directly to admin@oppia.org.

## License

The Oppia code is released under the [Apache v2 license](https://github.com/oppia/oppia/blob/develop/LICENSE).

## Keeping in touch

- [Discussion forum](https://github.com/oppia/oppia/discussions)
- [Announcements mailing list](http://groups.google.com/group/oppia-announce)

## Social Media

[][twitter] [][LinkedIn] [][Facebook] [][medium] [][oppia-org-youtube] [][dev-youtube]

[twitter]: https://twitter.com/oppiaorg
[linkedIn]: https://www.linkedin.com/company/oppia-org/
[medium]: https://medium.com/@oppia.org
[facebook]: https://www.facebook.com/oppiaorg/
[oppia-org-youtube]: https://www.youtube.com/channel/UC5c1G7BNDCfv1rczcBp9FPw
[dev-youtube]: https://www.youtube.com/channel/UCsrAX-oeqm0-NIQzQrdiUkQ
","'angular', 'angularjs', 'appengine', 'appengine-python', 'education', 'hacktoberfest', 'interactive', 'javascript', 'learning', 'nonprofit', 'python', 'sdg', 'sdg-4', 'sdg4', 'sdgs', 'teaching', 'tutor', 'tutorials', 'typescript', 'web'",2024-05-03T15:08:44Z,30,5616,241,"('seanlip', 4145), ('BenHenning', 442), ('kevintab95', 413), ('vojtechjelinek', 390), ('wxyxinyu', 321), ('brianrodri', 280), ('DubeySandeep', 251), ('aks681', 235), ('kevinlee12', 227), ('sfederwisch', 194), ('ankita240796', 186), ('makoscafee', 183), ('jacobdavis11', 175), ('U8NWXD', 146), ('czxcjx', 136), ('nithusha21', 124), ('giritheja', 122), ('Showtim3', 122), ('maitbayev', 118), ('kashida', 118), ('bansalnitish', 115), ('sbhowmik89', 115), ('526avijitgupta', 112), ('apb7', 110), ('pranavsid98', 107), ('ashutoshc8101', 101), ('1995YogeshSharma', 98), ('prasanna08', 91), ('nishantwrp', 90), ('marianadasilvadev', 79)","[4, 'Quality Education']"
instedd/verboice,"Open source toolkit for voice services; with special focus to the needs of medium- and low-income countries, scalable services, and interacting with vulnerable populations","Welcome to Verboice [![Build Status](https://github.com/instedd/verboice/actions/workflows/ci.yml/badge.svg)](https://github.com/instedd/verboice/actions/workflows/ci.yml)
=================

Voice is the most universal and inclusive means of communication, and it's an ideal way to expand the reach and impact of health and humanitarian technologies. Verboice is a free and open-source tool that makes it easy for anyone to create and run projects that interact via voice, allowing your users to listen and record messages in their own language and dialect or answer questions with a phone keypad. Verboice projects can start small and scale up, making it possible to improve lives even in communities previously closed off by literacy and technological barriers.


Getting Started
-------------
[Start using Verboice now](http://verboice.instedd.org)

[Verboice Help](https://github.com/instedd/verboice/wiki)

[What's coming next?](https://github.com/instedd/verboice/milestones)

[Installing your own server](https://github.com/instedd/verboice/wiki/Installing)

Docker development
------------------

We use [`dockerdev`](https://github.com/waj/dockerdev) to get domain names for the components of the app - so it can interoperate with other apps from the InSTEDD platform. Although it's optional, your first step to have Verboice running on Docker should be to install `dockerdev` - it needs to be running before creating any Verboice container, network and other objects.

Run the following commands to have a stable development environment.

```
$ docker compose run --rm --no-deps web bundle install
$ docker compose run --rm web bash
root@web_1 $ rake db:setup db:seed
$ docker compose up
```

You can also run the frontend unit tests inside the docker container. Here's how:

```
$ docker compose run --rm web rake db:test:prepare
$ docker compose run --rm web rspec
```

Testing with Zeus
-----------------

Compounding Docker and Rails load times makes for a terrible out of the box testing experience. To mitigate that, we use Zeus (https://github.com/burke/zeus). Zeus pre-loads your Rails application so you only pay the initialization cost once.

The `Dockerfile.dev` creates an image with Zeus already installed on it. To run tests with Zeus, first you need to:

1. From the app root, run: `zeus init`. This is a one time process that will create a couple of files `custom_plan.rb` and `zeus.json`.
1. To start Zeus, from the app root, run: `zeus start`.
1. From another terminal, run `zeus test spec` to run tests.

Deploying with Capistrano
-------------------------

Verboice is deployed with Capistrano. After `bundle install`ing, run:

```
$ cap -s branch=feature/my_branch deploy HOSTS=verboice-stg.instedd.org RVM=1
```

This will deploy `feature/my_branch` code to the host at `verboice-stg.instedd.org` using RVM. Your mileage may vary.

Intercom
--------

Verboice supports Intercom as its CRM platform. To load the Intercom chat widget, simply start Verboice with the env variable `INTERCOM_APP_ID` set to your Intercom app id (https://www.intercom.com/help/faqs-and-troubleshooting/getting-set-up/where-can-i-find-my-workspace-id-app-id).

Verboice will forward any conversation with a logged user identifying them through their email address. Anonymous, unlogged users will also be able to communicate.

If you don't want to use Intercom, you can simply omit `INTERCOM_APP_ID` or set it to `''`.

To test the feature in development, add the `INTERCOM_APP_ID` variable and its value to the `environment` object inside the `web` service in `docker-compose.yml`.

GUISSO
------

Verboice leverages the `alto_guisso` gem to connect with [GUISSO](https://github.com/instedd/guisso).

To set it up, you just have to register Verboice as a new Application in your GUISSO instance, and copy the `guisso.yml` file to your local `config/guisso.yml`.
",,2024-03-18T14:07:42Z,18,43,11,"('waj', 780), ('asterite', 342), ('spalladino', 149), ('ggiraldez', 117), ('carohadad', 79), ('matiasgarciaisaia', 37), ('macoca', 36), ('juanedi', 34), ('juanboca', 29), ('lmatayoshi', 16), ('dwilkie', 14), ('ysbaddaden', 8), ('pablobrusco', 6), ('nditada', 5), ('leandroradusky', 4), ('bcardiff', 3), ('pmallol', 3), ('aeatencio', 1)","[17, 'Partnerships for the Goals']"
guardianproject/haven,"Haven is for people who need a way to protect their personal spaces and possessions without compromising their own privacy, through an Android app and on-device sensors","[![Build Status](https://travis-ci.org/guardianproject/haven.svg)](https://travis-ci.org/guardianproject/haven)

# About Haven

Haven is for people who need a way to protect their personal areas and possessions without compromising their privacy. It is an Android application that leverages on-device sensors to provide monitoring and protection of physical areas. Haven turns any Android phone into a motion, sound, vibration and light detector, watching for unexpected guests and unwanted intruders. We designed Haven for investigative journalists, human rights defenders and people at risk of forced disappearance to create a new kind of herd immunity. By combining the array of sensors found in any smartphone, with the world's most secure communications technologies, like Signal and Tor, Haven prevents the worst kind of people from silencing citizens without getting caught in the act.

 
 
 

View our full [Haven App Overview](https://guardianproject.github.io/haven/docs/preso/) presentation for more about the origins and goals of the project.

## Announcement and Public Beta

We are announcing Haven today, as an open-source project, along with a public beta release of the app. We are looking for contributors who understand that physical security is as important as digital, and who have an understanding and compassion for the kind of threats faced by the users and communities we want to support. We also think it is cool, cutting-edge and making use of encrypted messaging and onion routing in whole new ways. We believe Haven points the way to a more sophisticated approach to securing communication within networks of things and home automation system.

Learn more about the story of this project at the links below:

* [Haven: Building the Most Secure Baby Monitor Ever?](https://guardianproject.info/2017/12/22/haven-building-the-most-secure-baby-monitor-ever/)
* [Snowden’s New App Uses Your Smartphone To Physically Guard Your Laptop](https://theintercept.com/2017/12/22/snowdens-new-app-uses-your-smartphone-to-physically-guard-your-laptop/)
* [Snowden's New App Turns Your Phone Into a Home Security System](https://www.wired.com/story/snowden-haven-app-turns-phone-into-home-security-system/)

## Project Team

Haven was developed through a collaboration between [Freedom of the Press Foundation](https://freedom.press) and [Guardian Project](https://guardianproject.info). Prototype funding was generously provided by FPF, and donations to support continuing work can be contributed through their site: https://freedom.press/donate-support-haven-open-source-project/

![Freedom of the Press Foundation](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/fopflogo.png)
![Guardian Project](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/gplogo.png)

## Safety through Sensors
Haven only records when triggered by sound and motion and stores everything locally on the device. You can position the device's camera to capture visible motion or place your phone somewhere discreet to listen for noises. Receive secure notifications of intrusion events instantly or access logs remotely later.

 
 
 

The following sensors are monitored for a measurable change, and then recorded to an event log on the device:

-   **Accelerometer**: phone's motion and vibration
-   **Camera**: motion in the phone's visible surroundings from front or back camera
-   **Microphone**: noises in the environment
-   **Light**: change in light from ambient light sensor
-   **Power**: detect device being unplugged or power loss  

## Building

The application can be built using Android Studio and Gradle. It relies on a number of third-party dependencies, all of which are free, open-source, and listed at the end of this document.

## Install

You can currently get the Haven BETA release in one of three ways:

* Download [Haven from Google Play](https://play.google.com/store/apps/details?id=org.havenapp.main)
* First, [install F-Droid](https://f-droid.org) the open-source app store, and second, add our Haven Nightly ""Bleeding Edge"" repository by scanning the QR Code below:

 

or add this repository manually in F-Droid's Settings->Repositories: [https://guardianproject.github.io/haven-nightly/fdroid/repo/](https://guardianproject.github.io/haven-nightly/fdroid/repo/)

* Grab the APK files from the [GitHub releases page](https://github.com/guardianproject/haven/releases)

You can, of course, build the app yourself, from source.

If you are an Android developer, you can learn more about how you can make use of F-Droid in your development workflow, for nightly builds, testing, reproducibility and more here: [F-Droid Documentation](https://f-droid.org/en/docs/)

## Why no iPhone Support?

While we hope to support a version of Haven that runs directly on iOS devices in the future, iPhone users can still benefit from Haven today. You can purchase an inexpensive Android phone for less than $100 and use it as your ""Haven Device""; leaving it behind whilst you keep your iPhone on you. If you run Signal on your iPhone you can configure Haven on Android to send encrypted notifications, with photos and audio, directly to you. If you enable the ""Tor Onion Service"" feature in Haven (requires installation of ""Orbot"" app as well) you can remotely access all Haven log data from your iPhone using the Onion Browser app.

So, no, iPhone users we didn't forget about you and we hope you will pick up an inexpensive Android burner today!

## Usage

Haven is meant to provide a smooth onboarding experience that walks users through configuring the sensors on their device to best detect intrusions into their environment. The current implementation has some of this implemented, but we are looking to improve this user experience dramatically.

### Main view

The application's main view allows the user to select which sensors to use along with their corresponding levels of sensitivity. A security code is required to disable monitoring, which must be provided by the user. A phone number can be set, to which a message will be sent if any of the sensors are triggered.

### Notifications

When one of the sensors is triggered (reaches the configured sensitivity threshold), notifications are sent through the following channels (if enabled):

- SMS: a message is sent to the number specified when monitoring started
- Signal: if configured, can send end-to-end encryption notifications via Signal

Note that it is not necessary to install the Signal app on the device that runs Haven. Doing so may invalidate the app's previous Signal registration and safety numbers. Haven uses normal APIs to communicate via Signal.

Notifications are sent through a service running in the background that is defined in class `MonitorService`.

### Remote Access

All event logs and captured media can be remotely accessed through a [Tor Onion Service](https://www.torproject.org/docs/onion-services). Haven must be configured as an Onion Service and requires the device to also have [Orbot: Tor for Android](https://guardianproject.info/apps/orbot) installed and running. 

## ATTRIBUTIONS

This project contains source code or library dependencies from the following projects:

* SecureIt project available at: https://github.com/mziccard/secureit Copyright (c) 2014 Marco Ziccardi (Modified BSD)
* libsignal-service-java from Open Whisper Systems: https://github.com/WhisperSystems/libsignal-service-java (GPLv3)
* Guardian Project fork of signal-cli from AsamK: https://github.com/AsamK/signal-cli , https://github.com/guardianproject/signal-cli-android (GPLv3)
* JayDeep's AudioWife: https://github.com/jaydeepw/audio-wife (MIT)
* AppIntro: https://github.com/apl-devs/AppIntro (Apache 2)
* Guardian Project's NetCipher: https://guardianproject.info/code/netcipher/ (Apache 2)
* NanoHttpd: https://github.com/NanoHttpd/nanohttpd (BSD)
* MaterialDateTimePicker from wdullaer: https://github.com/wdullaer/MaterialDateTimePicker (Apache 2)
* Fresco Image Viewer: https://github.com/stfalcon-studio/FrescoImageViewer (Apache 2)
* Facebook Fresco Image Library: https://github.com/facebook/fresco (BSD)
* Audio Waveform Viewer: https://github.com/derlio/audio-waveform (Apache 2)
* FireZenk's AudioWaves: https://github.com/FireZenk/AudioWaves (MIT)
* MaxYou's SimpleWaveform: https://github.com/maxyou/SimpleWaveform (MIT)
* Siralam's fork of CameraViewPlus: https://github.com/siralam/CameraViewPlus (Apache License 2.0)
* Halil Ozercan's BetterVideoPlayer: https://github.com/halilozercan/BetterVideoPlayer
* Reneto Silva's easyrs: https://github.com/silvaren/easyrs (MIT)
* Google's libphonenumber: https://github.com/googlei18n/libphonenumber (Apache License 2.0)
* Mike Penz's AboutLibraries: https://github.com/mikepenz/AboutLibraries (Apache License 2.0)
","'android', 'device-sensors', 'monitoring-service', 'prototype-android-application', 'sensor'",2022-10-26T22:43:24Z,30,6538,278,"('n8fr8', 531), ('archie94', 79), ('fat-tire', 56), ('lukeswitz', 52), ('mziccard', 37), ('percy-g2', 27), ('lohathe', 22), ('eighthave', 16), ('weblate', 13), ('louiswolfers', 11), ('AndrewR8', 7), ('btrice', 6), ('opticod', 6), ('mig5', 6), ('FREEWING-JP', 5), ('christxph', 5), ('Ajimi', 4), ('WaldiSt', 4), ('SantosSi', 4), ('AO-LocLab', 4), ('ACherepkov1989', 3), ('rockgecko-development', 3), ('wtuemura', 3), ('nicolabortignon', 3), ('oculushut', 3), ('samip5', 2), ('oliviakumar', 2), ('mansil', 2), ('1hiking', 2), ('IsaacYAGI', 2)","[11, 'Sustainable Cities and Communities']"
GreenButtonAlliance/OpenESPI-Common-java,Common library for Green Button Third Party and Data Custodian OpenESPI Applications,"[![CircleCI](https://circleci.com/gh/GreenButtonAlliance/OpenESPI-Common-java/tree/master.svg?style=svg)](https://circleci.com/gh/GreenButtonAlliance/OpenESPI-Common-java/tree/master)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=GreenButtonAlliance_OpenESPI-Common-java&metric=alert_status)](https://sonarcloud.io/dashboard?id=GreenButtonAlliance_OpenESPI-Common-java)


# OpenESPI-Common -- Archived January 29, 2024

NOTE: This repository is no longer maintained. The OpenESPI project has been archived and is no longer being maintained.

This is the Common module of the OpenESPI Green Button Data Custodian and Third Party implementation. It is a Spring application written in java and built on top of jpa for database access.

This Common run-time and test code is shared between stand-alone Data Custodian and Third Party applications. [OpenESPI-DataCustodian](https://github.com/greenbuttonalliance/OpenESPI-DataCustodian-java) and [OpenESPI-ThirdParty](https://github.com/greenbuttonalliance/OpenESPI-ThirdParty-java).

An operational sandbox with these services operating may be found at:
sandbox.greenbuttonalliance.org:8443

## Setup

First clone the project from github:

```bash
git clone https://github.com/greenbuttonalliance/OpenESPI-Common-java.git
cd OpenESPI-Common
```

Then install the OpenESPI-Common JAR in your local repository:
```bash
# The JUnit test have not been maintained since the original creation of the repository. 
# Any errors encountered here are due to not including the ""-Dmaven.test.skip=true"" portion of the command.
mvn -Dmaven.test.skip=true clean install

# or for a specific profile
mvn -P  -Dmaven.test.skip=true clean install
```

## IDE Setup

### Eclipse Setup

Open Eclipse and import a Maven project (File > Import... > Maven > Existing Maven Projects).

### Spring Tool Suite Setup

Open Spring Tool Suite and import a Maven project (File > Import... > Maven > Existing Maven Projects).

### IntelliJ Setup

Open IntelliJ and open the project (File > Open...).

## Testing

All testing of OpenESPI is performed using the [Test Harness](https://github.com/greenbuttonalliance/OpenESPI-GreenButtonCMDTest.git) project. See the [README](https://github.com/greenbuttonalliance/OpenESPI-GreenButtonCMDTest/blob/master/README.md) file for instructions.
","'data-custodian', 'java', 'openespi', 'pivotal', 'spring', 'third-party'",2024-01-29T21:16:16Z,7,29,23,"('dfcoffin', 366), ('jateeter', 288), ('MartyBurns', 180), ('RonPasquarelli', 6), ('AntiTyping', 4), ('mooreb', 1), ('nist-sgcps', 1)","[7, 'Affordable and Clean Energy']"
thinkingmachines/geomancer,Automated feature engineering for geospatial data,"![Geomancer Logo](https://storage.googleapis.com/tm-geomancer/assets/header.png)
---


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


Geomancer is a geospatial feature engineering library. It leverages geospatial
data such as [OpenStreetMap (OSM)](https://www.openstreetmap.org/) alongside a
data warehouse like BigQuery. You can use this to create, share, and iterate
geospatial features for your downstream tasks (analysis, modelling,
visualization, etc.). 


    


## Features

Geomancer can perform geospatial feature engineering for all types of vector data
(i.e. points, lines, polygons).

- Feature primitives for geospatial feature engineering
- Ability to switch out data warehouses (BigQuery, SQLite, PostgreSQL (*In Progress*))
- Compile and share your features using our SpellBook 

## Setup and Installation

### Installing the library

Geomancer can be installed using `pip`.

```
$ pip install geomancer
```

This will install **all** dependencies for every data-warehouse we support. If
you wish to do this only for a specific warehouse, then you can add an
identifier:

```
$ pip install geomancer[bq] # For BigQuery
$ pip install geomancer[sqlite] # For SQLite
$ pip install geomancer[psql] # For PostgreSQL
```

Alternatively, you can also clone the repository then run `install`.

```
$ git clone https://github.com/thinkingmachines/geomancer.git
$ cd geomancer
$ python setup.py install
```

### Setting up your data warehouse

Geomancer is powered by a geospatial data warehouse: we highly-recommend using
[BigQuery](https://cloud.google.com/bigquery/) as your data warehouse and
[Geofabrik's OSM catalog](https://www.geofabrik.de/data/download.html) as your
source of Points and Lines of interest. 

[![Geomancer architecture](https://storage.googleapis.com/tm-geomancer/assets/architecture.png
)](https://github.com/thinkingmachines/geomancer)

You can see the set-up instructions in [this link](https://geomancer.readthedocs.io/en/latest/setup.html#setting-up-your-data-warehouse)

## Basic Usage

All of the feature engineering functions in Geomancer are called ""spells"". For
example, you want to get the distance to the nearest supermarket for each
point.

```python
from geomancer.spells import DistanceToNearest

# Load your dataset in a pandas dataframe
# df = load_dataset()

dist_spell = DistanceToNearest(
    ""supermarket"",
    source_table=""ph_osm.gis_osm_pois_free_1"",
    feature_name=""dist_supermarket"",
    dburl=""bigquery://project-name"",
).cast(df)
```

You can specify the type of filter  using the format `{column}:{filter}`.  By
default, the `column` value is `fclass`. For example, if you wish to look for
roads on a bridge, then pass `bridge:T`:

```python
from geomancer.spells import DistanceToNearest

# Load the dataset in a pandas dataframe
# df = load_dataset()

dist_spell = DistanceToNearest(
    ""bridge:T"",
    source_table=""ph_osm.gis_osm_roads_free_1"",
    feature_name=""dist_road_bridges"",
    dburl=""bigquery://project-name"",
).cast(df)
```

Compose multiple spells into a ""spell book"" which you can export as a JSON file.

```python
from geomancer.spells import DistanceToNearest
from geomancer.spellbook import SpellBook

spellbook = SpellBook([
    DistanceToNearest(
        ""supermarket"",
        source_table=""ph_osm.gis_osm_pois_free_1"",
        feature_name=""dist_supermarket"",
        dburl=""bigquery://project-name"",
    ),
    DistanceToNearest(
        ""embassy"",
        source_table=""ph_osm.gis_osm_pois_free_1"",
        feature_name=""dist_embassy"",
        dburl=""bigquery://project-name"",
    ),
])
spellbook.to_json(""dist_supermarket_and_embassy.json"")
```

You can share the generated file so other people can re-use your feature extractions
with their own datasets.

```python
from geomancer.spellbook import SpellBook

# Load the dataset in a pandas dataframe
# df = load_dataset()

spellbook = SpellBook.read_json(""dist_supermarket_and_embassy.json"")
dist_supermarket_and_embassy = spellbook.cast(df)
```

## Contributing

This project is open for contributors! Contibutions can come in the form of
feature requests, bug fixes, documentation, tutorials and the like! We highly
recommend to file an Issue first before submitting a [Pull
Request](https://help.github.com/en/articles/creating-a-pull-request).

Simply fork this repository and make a Pull Request! We'd definitely appreciate:

- Implementation of new features
- Bug Reports
- Documentation
- Testing

Also, we have a
[CONTRIBUTING](https://github.com/thinkingmachines/geomancer/blob/master/CONTRIBUTING.rst)
and a [CODE_OF_CONDUCT](https://github.com/thinkingmachines/geomancer/blob/master/CODE_OF_CONDUCT.rst),
so please check that one out!

## License

MIT License © 2019, Thinking Machines Data Science
","'bigquery', 'feature-engineering', 'geospatial', 'machine-learning', 'openstreetmap'",2021-02-23T06:04:45Z,5,213,40,"('ljvmiranda921', 113), ('magtanggol03', 8), ('tm-ardie-orden', 5), ('marksteve', 2), ('jtmiclat', 1)","[11, 'Sustainable Cities and Communities']"
okfde/froide,Freedom Of Information Portal,"# Froide

[![Froide CI](https://github.com/okfde/froide/workflows/Froide%20CI/badge.svg)](https://github.com/okfde/froide/actions?query=workflow%3A%22Froide+CI%22)

Froide is a Freedom Of Information Portal using Django 3.2 on Python 3.8+.

It is used by the German and the Austrian FOI site, but it is fully
internationalized and written in English.

## Development on Froide

After clone, create a Python 3.8+ virtual environment and install dependencies:

```
python3 -m venv froide-env
source froide-env/bin/activate

# Install dev dependencies
pip install -r requirements-test.txt

# Install git pre-commit hook
pre-commit install
```

### Start services

You can run your own Postgres+PostGIS database and Elasticsearch service or run them with Docker.

You need [docker](https://www.docker.com/community-edition) and [docker-compose](https://docs.docker.com/compose/). Make sure Docker is running and use the following command:

```
docker-compose up
```

This will start Postgres and Elasticsearch and listen on port 5432 and 9200 respectively. You can adjust the port mapping in the `docker-compose.yml`.

### Setup database and search index, start server

If you need to adjust settings, you can copy the `froide/local_settings.py.example` to `froide/local_settings.py` and edit it. More steps:

```
# To initialise the database:
python manage.py migrate --skip-checks
# Create a superuser
python manage.py createsuperuser
# Create and populate search index
python manage.py search_index --create
python manage.py search_index --populate
# Run the Django development server
python manage.py runserver
```

### Run tests

Make sure the services are running.

```
# Run all tests
make test
# Run only unit/integration tests
make testci
# Run only end-to-end tests
make testui
```

### Development tooling

For Python code, we use flake8 following black code style. JavaScript, Vue and SCSS files are formatted and linted with ESLint and Prettier.

Make sure to have pre-commit hooks registered (`pre-commit install`). For VSCode, the [Python](https://code.visualstudio.com/docs/python/linting), [ESLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint) and [Vetur](https://marketplace.visualstudio.com/items?itemName=octref.vetur) extensions are helpful, with these workspace settings recommended:

```json
{
  ""eslint.format.enable"": true,
  ""eslint.packageManager"": ""yarn"",
  ""vetur.format.defaultFormatter.css"": ""prettier"",
  ""vetur.format.defaultFormatter.html"": ""prettier"",
  ""vetur.format.defaultFormatter.js"": ""prettier-eslint""
}
```

### Upgrade dependencies

```
# with pip-tools
pip-compile -U requirements.in
pip-compile -U requirements-test.in
```

## Docs

[Read the documentation](http://froide.readthedocs.org/en/latest/) including a [Getting Started Guide](http://froide.readthedocs.org/en/latest/gettingstarted/).

Froide is supported by the [Open Knowledge Foundation Germany](http://www.okfn.de/) and [Open Knowledge Foundation International](http://okfn.org/).

## License

Froide is licensed under the MIT License.
",'fragdenstaat',2024-04-27T09:29:30Z,29,343,34,"('stefanw', 6306), ('pajowu', 129), ('krmax44', 125), ('gambolputty', 89), ('MagdaN', 42), ('dependabotbot', 25), ('fin', 18), ('arnese', 17), ('missgreenwood', 7), ('felixebert', 6), ('robbi5', 3), ('milkcask', 3), ('rixx', 2), ('rufuspollock', 2), ('jdieg0', 2), ('alehaa', 1), ('cclauss', 1), ('GiantCrocodile', 1), ('eik3', 1), ('sonicdoe', 1), ('jasperroloff', 1), ('corny', 1), ('MrKrisKrisu', 1), ('manonthemat', 1), ('tzwenn', 1), ('cafischer', 1), ('fdsbot', 1), ('rugk', 1), ('xenein', 1)","[16, 'Peace, Justice and Strong Institutions']"
signdict/website,A sign language dictionary,"# SignDict.org

A crowdsourced sign language dictionary.

[![Build Status](https://github.com/signdict/website/actions/workflows/elixir.yml/badge.svg?branch=main)](https://github.com/signdict/website/actions/workflows/elixir.yml)
[![Coverage Status](https://coveralls.io/repos/github/signdict/website/badge.svg?branch=main)](https://coveralls.io/github/signdict/website?branch=main)

Here we will work on SignDict. A sign language dictionary
where registered users can add new signs using their webcam.
With the crowdsourcing approach we can create the most accurate
and largest German sign language dictionary together.

**You want to help?** Awesome. Scroll through the issues, open a new one, join our [Gitter Community](https://gitter.im/signdict/Lobby) or just send
a short notice using the [contact form](https://signdict.org/contact). We are happy about every person who wants to help.

We also offer an [API](https://github.com/signdict/website/wiki/API).

## Development setup

SignDict uses Elixir and Phoenix. Information on how
to install Elixir can be found [here](http://elixir-lang.org/install.html).

For the video transcoding it uses [redis](http://redis.io). On
mac, install it via `brew install --cask redis`, or `sudo apt install redis-server` for Linux and start it.

As database it uses [PostgreSQL](http://postgresql.org).

After you installed everything, the setup is as follows:

### Mac instructions:

```bash
mix ua_inspector.download
mix deps.get
mix ecto.setup
cd assets/ && yarn install && cd ..
mix phx.server
```

### Linux instructions:

Note: If you run into postgres password authentication errors, check out [this blogpost for help](https://juwondaniel.wordpress.com/2016/09/23/solve-mix-ecto-create-postgresql-password-issue-with-phoenix/).

```bash
mix deps.get
mix ecto.setup
sudo apt install npm
sudo npm install -g yarn
cd assets/ && yarn install && cd ..
mix phx.server
```

### Docker instructions:

First, make the PostgreSQL and redis servers point to the docker services. Change `hostname: ""localhost""` to `hostname: ""db""` in `config/dev.exs`, and add `host: ""redis""` to the `:exq` section in `config/config.exs`.

Then run

```bash
docker-compose up
```

to install all dependencies and start the PostgreSQL, redis and web server services (including code reloading). The website is available at http://localhost:4000.

With that you have a running system and a default admin user called
`admin@example.com` with the password `thepasswordisalie`.

Before you contribute code, please make sure to read the [CONTRIBUTING.md](CONTRIBUTING.md)

In development mode `Bamboo` will not sent emails. Instead you can see what
would have been sent out here: `http://localhost:4000/sent_emails`

This project is using [yarn](http://yarnjs.com/) for javascript dependency management.

You can also use the included `Procfile` to start redis and the phoenix server at
the same time. Install `foreman` with `gem install foreman` and execute `foreman start`
to have both started automatically.

### How to run the suite

```bash
mix test
```

## Deployment

The server is configured using [ansible](https://www.ansible.com/) with [this playbook](/ansible/playbook.yml) and can be updated with:

```bash
ansible-playbook ansible/playbook.yml -i ansible/hosts --extra-vars '{""username"": ""******""}'
```

The system is currently using [bootleg](https://github.com/labzero/bootleg) to
deploy the app. Simply call `bootleg_user=USERNAME mix bootleg.update` to
deploy it to the production environment.

## Importing files manually

You can import a file using a json file with `mix importer file.json`. The json should have [this](test/fixtures/videos/Zug.json) format.

## Funding

This project is government funded by the [German Federal Ministry of Education and Research](http://bmbf.de)
and is part of the 1st batch of the [prototype fund](http://prototypefund.de).

![Logo of the German Federal Ministry of Education and Research](images/support-bmbf.png)
![Prototype Fund Logo](images/support-prototype.png)
","'asl', 'dgs', 'education', 'elixir', 'phoenix', 'sign-language-dictionary', 'signlanguage'",2023-10-10T13:14:45Z,16,81,8,"('dependabot-previewbot', 536), ('dependabotbot', 401), ('bitboxer', 349), ('gulp21', 7), ('jann', 6), ('Rantastique', 4), ('alvarofernandoms', 4), ('gildesmarais', 3), ('EinLama', 2), ('berlintam', 2), ('timonegk', 2), ('jbaugh', 1), ('k-nut', 1), ('laurens', 1), ('manonthemat', 1), ('weiland', 1)","[3, 'Good Health and Well-Being']"
coralproject/talk,A better commenting experience from Vox Media,"
  



  A better commenting experience from Vox Media.



  
  
  
  


## Description

Online comments are broken. Our open-source commenting platform,
[Coral](https://coralproject.net), rethinks how moderation, comment display, and
conversation function, creating the opportunity for safer, smarter discussions
around your work.


## Documentation

If you're new to Coral, the [Coral documentation](https://docs.coralproject.net/)
is a great place to start running and developing with Coral.

You’ve installed Coral, and you’re preparing to launch it on your site. The real
community work starts now, before you go live. You have a unique opportunity
pre-launch to set your community up for success. Read our
[Community Guides](https://guides.coralproject.net/start-here/) to learn more.

## Support

We can help you set up Coral, migrate your comments from another system,
integrate your registration platform, pair with your programmers, and help you
with bespoke installs. To learn more, [contact us](https://coralproject.net/pricing/).

## Contributing

Coral is a Apache-2.0 licensed open-source project built with <3 by the Coral
team, a part of [Vox Media](https://product.voxmedia.com/).

If you are interested in contributing to Coral, check out our [Contributor's Guide](CONTRIBUTING.md).

## License

Coral is [Apache-2.0 licensed](LICENSE).

## Versioning

If you're packaging a release of Coral, there is a convenient script to update the version numbers for all the child projects in the mono-repo.

The below script will run `npm version x.y.z` against all the child repositories so you don't have to manually update them by hand!

```
sh scripts/version.sh MAJOR.MINOR.VERSION // i.e. 8.5.0
```
","'comments', 'comments-widget', 'graphql', 'javascript', 'journalism', 'nodejs', 'react', 'typescript'",2024-05-03T13:04:32Z,30,1864,52,"('nick-funk', 938), ('kabeaty', 799), ('cvle', 704), ('tessalt', 608), ('wyattjoh', 535), ('marcushaddon', 411), ('okbel', 288), ('iambrianchan', 42), ('kgardnr', 37), ('dkolkena', 30), ('dependabotbot', 24), ('cristiandean', 22), ('losowsky', 14), ('rmens', 13), ('nbachman-vox', 13), ('janslu', 6), ('bowser2010', 6), ('HectorNM', 5), ('samisafatli', 5), ('DimDimDimDimDimDim', 5), ('HenkoR', 4), ('oliver-dvorski', 4), ('artmsilva', 3), ('bennettp123', 3), ('Lammland', 3), ('immber', 3), ('jpkilpi', 3), ('ElijahPepe', 2), ('miroc', 2), ('Emptyfruit', 2)","[8, 'Decent Work and Economic Growth']"
jembi/bsis,"BSIS is a system to monitor blood inventory from collection to transfusion.  Started as a collaboration between the Computing For Good (C4G) program at Georgia Tech and the Centers for Diseases Control and Prevention (CDC), the application is now maintained, developed and managed by Jembi Health Systems (JHS), in partnership with Safe Blood for Africa (SBFA) and the Centers for Diseases Control (CDC).","[![Build Status](https://travis-ci.com/jembi/bsis.svg?token=FCDs4JFMycqVrLvQpU5A&branch=develop)](https://travis-ci.com/jembi/bsis) [![codecov.io](https://codecov.io/github/jembi/bsis/coverage.svg?branch=develop&token=hU3P8AQ1o0)](https://codecov.io/github/jembi/bsis/coverage.svg?branch=develop&token=hU3P8AQ1o0)

BSIS
====

BSIS (Blood Safety Information System) was a project started at Georgia Institute of Technology as part of the [Computing For Good (C4G)](http://www.cc.gatech.edu/about/advancing/c4g/) Program at Georgia Tech, under the name V2V (Vein to Vein). 
The project is now maintained by [Jembi Health Systems](http://www.jembi.org) in South Africa.

# BSIS Disclaimer
If you are considering forking this code and making any changes, please make sure that you read the Disclaimer below to ensure you are aware of the risks associated with this.
1. No warranties of any kind whatsoever are made as to the results that You will obtain from relying upon the covered code (or any information or content obtained by way of the covered code), including but not limited to compliance with privacy laws or regulations or laboratory and clinical care industry standards and protocols. Use of the covered code is not a substitute for appropriately-trained and registered professional blood service and healthcare providers, standard practice, quality assurance guidelines or professional judgment. Any decision with regard to the appropriateness of treatment, or the validity or reliability of information or content made available by the covered code, is the sole responsibility of the appropriately-trained and registered professional blood service personnel and health care providers.

2. Under no circumstances and under no legal theory, whether tort (including negligence), contract, or otherwise, shall any Contributor, or anyone who distributes Covered Software as permitted by the license, be liable to You for any indirect, special, incidental, consequential damages of any character including, without limitation, damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other damages or losses of any nature whatsoever (direct or otherwise) on account of or associated with the use or inability to use the covered content (including, without limitation, the use of information or content made available by the covered code, all documentation associated therewith, and the failure of the covered code to comply with privacy laws and regulations or clinical care industry standards and protocols), even if such party shall have been informed of the possibility of such damages.

# BSIS Contributors Agreement 
If you are planning to contribute in any way to the source code, please first read the [Contributors Agreement](https://github.com/jembi/bsis/blob/develop/CONTRIBUTORS-AGREEMENT.md).

# Background

BSIS is a software to manage blood related information from the point of donation through testing, storage to its eventual usage in a hospital or a clinic. 
Handling of blood is a sensitive operation and record of movement of blood as it goes through the following chain is critical to the effective functioning of blood processing centers. 
This solution is primary targeted for deployment in the developing countries of Africa where much of the blood inventory tracking is done on paper.

The software was developed at Georgia Tech as part of the C4G program in collaboration with Centers for Disease Control and Prevention (CDC), Safe Blood for Africa (SBFA) and the participating countries of Africa, and is now developed and maintained by Jembi Health Systems (JHS) in South Africa.

## Problems related to blood supply in Africa

* Unsafe blood can lead to disease transmissions
* Insufficient supply of Safe Blood in hospitals point-of-care
* Higher number of individuals living with HIV in Africa
* Shortage of blood
* Many countries of Africa depend on paper based systems to track the movement of blood through the blood processing chain

[This factsheet](http://www.who.int/mediacentre/factsheets/fs279/en/index.html) from WHO discusses the problems related to blood safety and availability specifically the developing countries of Africa in greater detail.

## Challenges

 * Transitioning from a paper based system to a computer based system can take time and experience
 * Varying blood processing practices across different countries
 * Unreliable Internet connectivity
 * Developing intuitive and easy-to-use admin. Cannot depend on an onsite admin to configure databases.

## Solution

The challenges listed above have influenced the design of BSIS and the following design features have resulted.
 * Develop a browser based but locally deployable solution to avoid dependence on Internet
 * Smaller upgrade package size to allow 
 * Make features configurable so that blood processing centers can turn them on/off based on their current practice
 * A single installer which installs and configures all dependencies like Java, MySQL, Apache Tomcat etc. to make first time configuration simple.

Many other design decisions in the future will be influenced by the same parameters.

## Learn about BSIS
Learn more about BSIS and the Blood Safety Strengthening Programme [here](https://www.jembi.org/Project/Blood-Safety-Strengthening-Programme-(BSSP)). Full Requirements Specifications are available.

# Developer Documentation

## Technology

### Production Code

* Language: [Java](http://www.oracle.com/technetwork/java/javase/overview/index.html)
* Application Server (Servlet Container): [Apache Tomcat](http://tomcat.apache.org/)
* Application and Web Framework: [Spring](https://spring.io/projects/spring-framework)
* Object Relational Mapper: [Hibernate](http://hibernate.org/)
* Database Management: [Liquibase](https://www.liquibase.org/)

### Test Code

* [Spring Test](https://docs.spring.io/spring/docs/4.1.x/spring-framework-reference/htmlsingle/#testing)

### Tooling

* Build: [Maven](https://maven.apache.org/)
* Containerization: [Docker](https://www.docker.com/)
* Database: [MySQL CE](https://www.mysql.com/products/community/)

## Build, Run and Test

### Build

* With Maven: `mvn compile`
* With Docker Compose: `sudo docker-compose build bsis`

### Run

* With Docker Compose: `sudo docker-compose --build up`
* With Bash Script: `./start.sh` (runs docker-compose internally)

### Test

* With Maven: `mvn test`

## Conventions

### Code Style

  * Use 2 spaces instead of tabs to indent your code. 
  * Use the appropriate [codestyles](codestyles) template for your IDE. These codestyles are based on the [Google Java Style Guide](https://google.github.io/styleguide/javaguide.html).
  * Only auto format the code you are working on.

### Git Commit Messages

Use the supplied Git commit message template to ensure that commit messages conform to the project standards. 
The first line of the commit message should contain a JIRA ticket reference and a short description of the commit (50 characters max).  
The following lines (72 characters max) should describe why the change is being made, what the problem was, and may contain external references if necessary.

In your BSIS Git repository folder, run the following command:

``` git config commit.template .git.template ```

This will set the git commit template for the local git repository only. 
If you'd like to set the template for all your Git repositories, then run the following command:

``` git config --global commit.template .git.template ```

## Getting Started

Starting work on a new code base even when it uses similar versions and tooling that you are used can be frustrating. 
Here are a few tips to help you get coding quickly, pain free!

### With Docker

1. Install the required tooling: [Maven](https://maven.apache.org/) and [Docker](https://www.docker.com/).
1. Change the host name part of the database url in [bsis.properties](src/main/resources/bsis.properties) from `localhost` to `mysql` the name of the docker container.
1. Start the application with the bash script `./start.sh` which will download the dependencies, start the application and create the database.
NB. Creating the database from scratch takes quite a long time (10s of minutes) so it might be easier to grab a database dump from another developer.
 
### Without Docker

1. This repository contains some Eclipse specific configuration. If you are comfortable using Eclipse then setting up your development environment will be relatively easier.
   You may use another IDE if you wish to.
    * [Install Eclipse Indigo 3.7 or Eclipse Juno 4.2](http://www.eclipse.org/downloads/).
    * Install [Apache Tomcat 7](http://tomcat.apache.org/) and [integrate with Eclipse](http://www.coreservlets.com/Apache-Tomcat-Tutorial/tomcat-7-with-eclipse.html).
    * Clone the repository into a local directory preferably housed in your eclipse workspace and checkout the develop branch.
    * The project dependencies are configured using [Maven](http://maven.apache.org/). Install Eclipse plugin [m2eclipse](http://maven.apache.org/eclipse-plugin.html) for easier integration of Maven with your development environment. In Eclipse, select Help>Install New Software and add the site http://download.eclipse.org/technology/m2e/releases to add the m2eclipse plugin.Setup m2eclipse so that it automatically downloads all the required dependencies, sources, javadocs.
    * Import the source code into Eclipse by using 'Import Existing project' option. The master branch contains the required eclipse project files (e.g. .project and .settings), so you should be able to start working right away. For other IDE's you will need to more work.
2. You will also need to setup a [MySQL CE](https://www.mysql.com/products/community/) database:
    *  Run the Maven build `mvn liquibase:update` which will call the liquibase scripts to create and initialise your database.
    * `src/main/resources/bsis.properties` contains your database connection information.
    * Note: Liquibase is executed on application startup, so this step is optional.
",,2022-12-16T10:01:26Z,20,3,7,"('timler', 988), ('lauravignoli', 748), ('iamrohitbanga', 674), ('danfuterman', 673), ('bausmeier', 572), ('tumijacob', 384), ('debonair', 249), ('jmuzinda', 236), ('tmvumbi2', 202), ('srikanthmalyala', 107), ('gtldevarshi', 49), ('gtlnikita', 49), ('trevorgowing', 47), ('gtldevang', 38), ('micnice', 15), ('gtlankur', 14), ('MattyJ007', 8), ('satayev', 3), ('ArunKumarGK', 1), ('yusufeen', 1)","[3, 'Good Health and Well-Being']"
learningequality/ka-lite,KA Lite: lightweight web server for serving core Khan Academy content (videos and exercises) without needing internet connectivity,"KA Lite
=======

by `Learning Equality `__

|Build Status| |Coverage Status| |Docs|

.. |Build Status| image:: https://circleci.com/gh/learningequality/ka-lite/tree/develop.svg?style=svg
   :target: https://circleci.com/gh/learningequality/ka-lite/tree/develop

.. |Coverage Status| image:: http://codecov.io/github/learningequality/ka-lite/coverage.svg?branch=develop
  :target: http://codecov.io/github/learningequality/ka-lite?branch=develop

.. |Docs| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat
   :target: http://ka-lite.readthedocs.org/

`Khan Academy `__'s core mission is to
""provide a free world-class education for anyone anywhere"", and as over `60%
of the world's population is without access to the
internet `__,
primarily in the developing world, providing an alternative delivery
mechanism for Khan Academy content is key to fulfilling this mission.

`KA Lite `__ is a lightweight
`Django `__ web app for serving core
Khan Academy content (videos and exercises) from a local server, with
points and progress-tracking, without needing internet connectivity.

Primary use cases include:
--------------------------

-  For servers/\ **computer labs located in remote schools**, which
   could be slowly syncing with a central server over a cell/satellite
   network or via USB keys.
-  In **correctional facilities** and other environments where providing
   educational materials is of value, but users cannot be given general
   internet access.
-  **Mobile school ""vans""**, which transport a server and multiple
   laptops/tablets between a number of schools (or orphanages, community
   centers, etc) in remote communities on a rotating basis, and syncing
   up with a central database (to download new content and upload
   analytics) when in an area with internet connectivity.

Get involved!
-------------

-  Learn how you can contribute code on our `KA Lite GitHub Wiki `__
-  Report bugs by `creating issues `__
-  Read more about the project's motivation at `Introducing KA Lite, an offline version of Khan
   Academy `__.

Roadmap
-------

Later in 2017, Learning Equality will be launching the successor of KA Lite. It's
called `Kolibri `__ and will have
very similar features to KA Lite, but will also be a platform for many other
educational resources besides Khan Academy's.

Because of the popularity of KA Lite, we are continuing
to support deployments by providing fixes to problems that
directly affect current usage. These include issues related to new
browsers, operating systems etc. We are also still optimizing regarding
performance issues.

If you are creating a new deployment at this very moment, feel assured that
KA Lite is still alive and will be maintained for the rest of 2017, after which
point we will be recommending that you migrate to Kolibri.

In the meantime, if you need new features in KA Lite, we welcome you to join
the community and contribute. In other words, we (Learning Equality) encourages
you (community members), to feel empowered and take responsibility for the
future of KA Lite.

Connect
^^^^^^^

- Community forums: `community.learningequality.org `__
- IRC: **#kalite** on Freenode
- Twitter: `@ka_lite `__

Contact Us
^^^^^^^^^^

Tell us about your project and experiences!

-  Email: info@learningequality.org
-  Add your project to the map: https://learningequality.org/ka-lite/map/

License information
-------------------

The KA Lite sourcecode itself is open-source `MIT
licensed `__, and the other included
software and content is licensed as described in the
`LICENSE `__
file. Please note that KA Lite is not officially affiliated with, nor
maintained by, Khan Academy, but rather makes use of Khan Academy's open
API and Creative Commons content, which may only be used for
non-commercial purposes.
",,2021-04-19T22:24:23Z,30,457,69,"('rtibbles', 2221), ('aronasorman', 1811), ('jamalex', 1614), ('MCGallaspy', 1060), ('benjaoming', 994), ('dylanjbarth', 582), ('cpauya', 192), ('66eli77', 173), ('mtorourk', 112), ('jtamiace', 105), ('heidimason', 103), ('wangguan59', 97), ('radinamatic', 85), ('ruimalheiro', 80), ('anuragkanungo', 61), ('tk21', 59), ('mrpau-richard', 56), ('arceduardvincent', 53), ('howdyitshelena', 44), ('mauk81', 40), ('sethachoi', 38), ('djallado', 36), ('mrpau-eugene', 33), ('christinepham', 31), ('almgong', 30), ('jaraniego', 30), ('mrshu', 29), ('PratikPramanik', 26), ('gimick', 23), ('mrpau-dev', 20)","[4, 'Quality Education']"
Sunbird-Obsrv/sunbird-data-pipeline,Repository for set of real-time streaming jobs to process and enrich the telemetry data generated by various user devices. The repository also consists of ansible provisioning playbooks to automate data pipeline related infrastructure provisioning and deployment playbooks to automate deployment of various components related to data analytics.,"# sunbird-data-pipeline

[![Codacy Badge](https://api.codacy.com/project/badge/Grade/737bb0af576e4f229c30d950c28c5c50)](https://www.codacy.com/app/project-sunbird/sunbird-data-pipeline?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=project-sunbird/sunbird-data-pipeline&amp;utm_campaign=Badge_Grade)

[![Build Status](https://travis-ci.org/project-sunbird/sunbird-data-pipeline.svg?branch=master)](https://travis-ci.org/project-sunbird/sunbird-data-pipeline)
",,2024-04-29T08:09:11Z,30,13,9,"('sowmya-dixit', 1159), ('manjudr', 1125), ('RevathiKotla', 677), ('sreeharikmarar', 403), ('anandp504', 322), ('utk14', 273), ('ishawakankar', 266), ('SanthoshVasabhaktula', 239), ('sunilpes', 230), ('Kaali09', 229), ('steotia', 204), ('kirang20', 145), ('gauravsinghania', 119), ('SMYALTAMASH', 98), ('amitB10', 80), ('beepdot', 64), ('kumarks1122', 62), ('G33tha', 61), ('navaneeth', 59), ('rjshrjndrn', 37), ('harshavardhanc', 35), ('jijeshmohan', 28), ('Sanjay-Krishnan93', 23), ('Shakthieshwari', 22), ('Ashwiniev95', 11), ('IStarzy', 10), ('raghupathiguduri', 8), ('maheshkumargangula', 6), ('gandham-santhosh', 6), ('surabhi-mahawar', 5)","[17, 'Partnerships for the Goals']"
Imisi3D/leVRn-beta,Beta version for leVRn development,"# leVRn-beta
leVRn is an open source VR solution aimed at transforming education in Nigeria.

## Short Description
It is a compilation of VR modules tailored after different topics in the Nigerian Secondary School Curriculum. These modules are then used by various secondary schools to enhance the learning experience of their students. It is currently a member of the UNICEF Innovation Fund cohort and more details on that can be found [here](https://www.unicef.org/innovation/stories/imisi-3d-providing-locally-tailored-interactive-content-using-vr-transform-education)

## Prerequisites
- Unity 2019.1.0f2

## Getting Started
To get a copy of this project working properly on your local machine, follow these simple steps:

- Download the project folder.
- Open the project with Unity 2019.1.0f2.

To keep the git repository small in size, a lot of the large assets used are not included in the repo but are hosted somewhere else as a .unitypackage.

- Download the .unitypackage here [leVRn-beta UnityPackage]()
- Import the leVRn-beta unitypackage into the project.
- Ensure the platform is set to Android in Build Settings
- Build the .apk, load it into an Oculus Go and test the experience to ensure everything works fine.

## Contributing
Please read [CONTRIBUTING.md](https://github.com/Imisi3D/leVRn-beta/blob/master/Contributing.md) for details on how best to contribute to this project.

## Authors
- **Tade Ajiboye** - [Github](https://github.com/Gazuntype/)

## License
This project is licensed under the GNU General Public License v3.0 - see the [LICENSE.md](https://github.com/Imisi3D/leVRn-beta/blob/master/LICENSE) file for detailsa
",,2019-07-04T17:39:05Z,1,0,3,"('gazuntype', 26)","[4, 'Quality Education']"
scratchfoundation/scratch-www,Standalone web client for Scratch,"# scratch-www
#### Standalone web client for Scratch

[![Build Status](https://travis-ci.org/LLK/scratch-www.svg)](https://travis-ci.org/LLK/scratch-www)
[![Coverage Status](https://coveralls.io/repos/github/LLK/scratch-www/badge.svg?branch=develop)](https://coveralls.io/github/LLK/scratch-www?branch=develop)
[![Greenkeeper badge](https://badges.greenkeeper.io/LLK/scratch-www.svg)](https://greenkeeper.io/)

## Overview

This is Scratch’s open source web client! This is the code for much of the [Scratch website](https://scratch.mit.edu).

In particular, this codebase includes code for:
* the ""project page"", which shows a playable version of the project, along with the project's title, description, comments, remixes and studios; this page operates in the background when you ""See inside"" a project
* the site's home page
* the Ideas page
* landing pages for various Scratch extensions, such as LEGO MINDSTORMS and micro:bit
* the info page for Scratch Desktop
* and other pages such as Credits and FAQ.

### How this fits in with other Scratch repos

The scratch-www project has lots of aspects of its design that are particular to our backend systems.
To use it for your own project, you would have to look at all the places it makes backend calls, and
create your own backend systems to perform those functions.

The [scratch-gui](https://github.com/scratchfoundation/scratch-gui) project, on the other hand, is designed to be
able to be used by anyone, without needing to create backend systems, though it also can support
backend systems for project and asset saving.

### Contributing

We welcome your contributions to this codebase! You may want to start by browsing [the current
list of open issues labeled ""help wanted""](https://github.com/scratchfoundation/scratch-www/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22).

Contributing to scratch-www can be more difficult than contributing to
[scratch-gui](https://github.com/scratchfoundation/scratch-gui). This is because scratch-gui can be run on its
own, without needing any other services to be running, while scratch-www needs to communicate
with several backend systems that the Scratch team runs (see ""How this fits in with other Scratch
repos"" above). If you are new to contributing to Scratch's source code, we suggest you start
by becoming familiar with scratch-gui and its [list of open issues labeled ""help
wanted""](https://github.com/scratchfoundation/scratch-gui/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22).

To contribute, please follow the [standard steps for contributing to a project on
GitHub](https://github.com/firstcontributions/first-contributions).

### License

See the [LICENSE](https://github.com/scratchfoundation/scratch-www/blob/master/LICENSE) file in this repo.

## Understanding this codebase

### Guides

Here are some resources to help you get acquainted with how we’re working on the Scratch codebase:

* [Contributor Guidelines](https://github.com/scratchfoundation/scratch-www/blob/develop/.github/CONTRIBUTING.md)
* [Style Guide](https://github.com/scratchfoundation/scratch-www/wiki/Style-Guide)
* [Testing Guide](https://github.com/scratchfoundation/scratch-www/wiki/Testing-Guide-for-Bugfixes)
* [Localization Guide](https://github.com/scratchfoundation/scratch-www/wiki/Localization-Guide)
* [Map of the repository](https://github.com/scratchfoundation/scratch-www/wiki/Repo-Map)

### Core technologies

Significant core technologies this codebase uses include:

#### Development technologies

* [Node](https://nodejs.org/)
* [Webpack](https://webpack.js.org/)
* [React](https://facebook.github.io/react/)
* [Redux](https://redux.js.org/)
* [Sass](http://sass-lang.com/documentation/file.SASS_REFERENCE.html)

#### Testing technologies

Our tests use:

* [Jest](https://jestjs.io/) (we are writing most new tests in Jest)
* [Tap](https://node-tap.org/) (we are moving away from using Tap, but many tests still use it)
* [Enzyme](https://airbnb.io/enzyme/)
* [Selenium](https://selenium.dev/)

## Developing scratch-www

### Before Getting Started

Make sure you have installed:
* [node](https://docs.npmjs.com/getting-started/installing-node): version 16
* npm (Node Package Manager): used to maintain and update packages required to build the site

### Update Packages

It's important to make sure that all of the dependencies are up to date because the
scratch-www code only works with specific versions of the dependencies.
You can update the packages by using the command:

```bash
npm install
```

#### Warnings during npm install

These warnings can be safely ignored:

```bash
npm WARN react-modal@0.6.1 requires a peer of react@^0.14.0 but none was installed.
npm WARN react-redux@4.4.0 requires a peer of react@^0.14.0 but none was installed.
npm WARN react-redux@4.4.0 requires a peer of redux@^2.0.0 || ^3.0.0 but none was installed.
npm WARN react-addons-test-utils@0.14.7 requires a peer of react@^0.14.7 but none was installed.
npm WARN react-dom@0.14.8 requires a peer of react@^0.14.8 but none was installed.
```

These currently exist in static/js/lib .

### To Build

To compile the source code into HTML and JavaScript bundles browsers can read, you can create
a temporary version of the site on your machine that you can access through your web browser.

You can either ""build"" the site a single time, by running:

```bash
npm run build
```

Or, you can run a server that rebuilds the files as you edit them, by running the commands:

```bash
npm run translate
npm start
```

*NOTE: `npm run translate` builds the intl directory. The site will build fine without it,
but translatable text strings will not show up correctly until you have built intl.*

During development, `npm start` watches any update you make to files in either
`./static` or `./src` and triggers a rebuild of the project.  In development,
the build is stored in memory, and not served from the `./build` directory.

### Viewing the local site

Once you have built the local site, using either `npm run build` or `npm start`,
the site hosted on your local machine can be accessed by a web browser by entering
`localhost:8333` into your browser's address bar.

### Troubleshooting

When running `npm start`, here are some important log messages to keep an eye out for:
* `webpack: bundle is now VALID.` – The bundle has been loaded into memory and is now viewable in the browser. This will show up both once `npm start` has completed its setup, and also once updates you make to files have been re-compiled for viewing in the browser.
* `webpack: bundle is now INVALID.` – If you see this, then it means you have made updates to files that are still being compiled for browser viewing. Pages will still be viewable, but they will not see any updates you made yet.

### To stop npm

To stop the `npm start` process which is making the site available to your web browser
(created above in ""To Build""), use `^C` (control-c) in the terminal.

#### Configuration

`npm start` can be configured with the following environment variables, by setting them in
the beginning of the command, before `npm start`:

| Variable        | Default                            | Description                                    |
| --------------- | ---------------------------------- | ---------------------------------------------- |
| `API_HOST`      | `https://api.scratch.mit.edu`      | Hostname for API requests                      |
| `ASSET_HOST`    | `https://assets.scratch.mit.edu`   | Hostname for asset requests                    |
| `BACKPACK_HOST` | `https://backpack.scratch.mit.edu` | Hostname for backpack requests                 |
| `PROJECT_HOST`  | `https://projects.scratch.mit.edu` | Hostname for project requests                  |
| `FALLBACK`      | `''`                               | Pass-through location for old site             |
| `GTM_ID`        | `''`                               | Google Tag Manager ID                          |
| `GTM_ENV_AUTH`  | `''`                               | Google Tag Manager env and auth info           |
| `NODE_ENV`      | `null`                             | If not `production`, app acts like development |
| `PORT`          | `8333`                             | Port for devserver (http://localhost:XXXX)     |

*NOTE: Because by default `API_HOST=https://api.scratch.mit.edu`, please be aware that, by default, you will be seeing and interacting with real data on the Scratch website.*

## Tests

### Unit tests

Most of our unit tests run using Jest, but older unit tests use the TAP framework.

#### Run all tests

To build the application and run all unit and localization tests, use the command:

```bash
npm test
```

#### Run one test

To run a single unit test file from the command-line using Jest, use the command:

```bash
node_modules/.bin/jest ./test/unit/PATH/TO/FILENAME.test.js
```

*NOTE: replace `PATH/TO/FILENAME` with the actual path to the file you wish to run.*

### Integration tests

Our integration tests assume that a larger environment is running than just scratch-www
on its own; for instance, many require that a test user be able to log in to the site,
which requires backend and database support.

By default, tests run against our Staging instance, but you can pass in a different
location with the ROOT_URL environment variable (see below) if you want to run the
tests against another location--for instance, your local build.

All of our integration tests use Jest as our testing framework.

#### Running the tests

To run all integration tests from the command-line:

```bash
SMOKE_USERNAME=username SMOKE_PASSWORD=password ROOT_URL=https://scratch.mit.edu UNOWNED_SHARED_PROJECT_ID=# UNOWNED_UNSHARED_PROJECT_ID=# OWNED_SHARED_PROJECT_ID=# OWNED_UNSHARED_PROJECT_ID=# npm run test:integration
```


#### Usernames/Password for the tests

The tests use multiple users with similar usernames and the same password.  They use the the username you pass in with SMOKE_USERNAME as well as the same username with a 1, 2, 3, 4, 5, and 6 (soon to be higher numbers as well) appended to the end of it.  So if you use the username ""test"" it will also use the username ""test1"", ""test2"", ""test3"", etc.  Make sure you have created accounts with this pattern and use the same password for all accounts involved.

You can use any set of usernames that fit this pattern.  Each account needs to share the same password, which is passed in as SMOKE_PASSWORD.

#### Environment Variables
Several environment variables need to be passed in for the tests to run.  Most of them have defaults that point to the staging server.

 * SMOKE_USERNAME             -   Root username used for tests that sign in. See the Usernames section above
 * SMOKE_PASSWORD             -   Password for all accounts used in the tests
 * UNOWNED_SHARED_PROJECT_ID  -   ID for a shared project owned by [testuser]2 This project should have at least one remix.  Remix it with another of the [testuser] accounts. Used in the project-page tests.
 * OWNED_SHARED_PROJECT_ID    -   ID for a shared project owned by [testuser]6.  Used in the project-page tests.
 * UNOWNED_UNSHARED_PROJECT_ID  - ID for an unshared project owned by [testuser]2.  It is used in tests where it is opened by [testuser]6 in the project-page tests.
 * OWNED_UNSHARED_PROJECT_ID  -   ID for an unshared project owned by [testuser]6.  It will be opened by its owner in the project-page tests.
 * UNOWNED_SHARED_SCRATCH2_PROJECT_ID - ID for a shared scratch2 project owned by [testuser]2.  It will be opened by [testuser]6.
 * OWNED_UNSHARED_SCRATCH2_PROJECT_ID - ID for an unshared scratch2 project owned by [testuser]6.  It will be opened by [testuser]6.
 * SAUCE_USERNAME             -   Username for a saucelabs account.  Only used when running tests remotely with test:integration:remote
 * SAUCE_ACCESS_KEY           -   Access token used by the saucelabs account included. Only used when running tests remotely with test:integration:remote
 * SMOKE_REMOTE               -   Boolean to set whether to use saucelabs to run tests remotely.  Set to true automatically when running tests with test:integration:remote, otherwise defaults to false.
 * RATE_LIMIT_CHECK           -   A URL that triggers clearing the studio creation rate limit for very specific accounts. This is needed for the my-stuff tests to test studio creation, ensuring they run the same every time.  This needs to be setup separately.

### Run a single test file
To run a single file from the command-line using Jest:

```bash
SMOKE_USERNAME=username SMOKE_PASSWORD=password ROOT_URL=https://scratch.mit.edu node_modules/.bin/jest ./test/integration/filename.test.js
```

To run a single file from the command-line using TAP:

```bash
SMOKE_USERNAME=username SMOKE_PASSWORD=password ROOT_URL=https://scratch.mit.edu node_modules/.bin/tap ./test/integration-legacy/smoke-testing/filename.js -R classic --no-coverage --timeout=3600
```

* the `-R classic` makes tap use the old reporting style, which avoids an error with the ""nyc"" package
* `--no-coverage` is because we do not use the coverage-tracking feature of tap
* the `timeout` argument is for the length of the entire tap test-suite; if you are getting a timeout error, you may need to adjust this value (some of the Selenium tests take a while to run)

#### Running Remote tests

Integration tests can be run using Saucelabs, an online service that can test multiple
browser/OS combinations remotely. (Currently, all tests are written for use for Chrome on Mac).

You will need a Saucelabs account in order to use it for testing. If you have one, you can
find your Access Key:
1. click your username
1. select ""User Settings"" from the dropdown menu
1. near the bottom of the page is your access key

To run tests using Saucelabs, run the command:

```bash
SMOKE_USERNAME=username SMOKE_PASSWORD=password SAUCE_USERNAME=saucelabsUsername SAUCE_ACCESS_KEY=saucelabsAccessKey ROOT_URL=https://scratch.mit.edu npm run test:integration:remote
```

*NOTE: Currently Jest tests will not run with Saucelabs.*

#### Configuration

| Variable      		| Default               | Description                                 			    |
| ---------------------	| --------------------- | --------------------------------------------------------- |
| `ROOT_URL`			| `scratch.ly`			| Location you want to run the tests against                |
| `SMOKE_USERNAME`    	| `None` 				| Username for Scratch user you're signing in with to test 	|
| `SMOKE_PASSWORD`  	| `None`                | Password for Scratch user you're signing in with to test  |
| `SMOKE_REMOTE`        | `false`               | Tests with Sauce Labs or not. True if running test:smoke:sauce |
| `SMOKE_HEADLESS`      | `false`               | Run browser in headless mode. Flaky at the moment         |
| `SAUCE_USERNAME`      | `None`                | Username for your Sauce Labs account                      |
| `SAUCE_ACCESS_KEY`    | `None`                | Access Key for Sauce Labs found under User Settings       |

## To Deploy

Deploying to staging or production will upload code to S3 and configure Fastly.

```bash
npm install
virtualenv ENV
. ENV/bin/activate
pip install -r requirements.txt
npm run build && npm run deploy
```

| Variable                 | Default | Description                                      |
| ------------------------ | ------- | ------------------------------------------------ |
| `FASTLY_SERVICE_ID`      | `''`    | Fastly service ID for `bin/configure-fastly.js`  |
| `FASTLY_API_KEY`         | `''`    | Fastly API key for `bin/configure-fastly.js`     |
| `FASTLY_ACTIVATE_CHANGES`| `false` | Activate changes and purge all after configuring |
| `AWS_ACCESS_KEY_ID`      | `''`    | AWS access key id for S3                         |
| `AWS_SECRET_ACCESS_KEY`  | `''`    | AWS secret access key for S3                     |
| `S3_BUCKET_NAME`         | `''`    | S3 bucket name to deploy into                    |

### Fastly deployment details

When deploying, Fastly's API is used to clone the active VCL configuration, update just the
relevant component with content from this repo's `routes.json` file, and activate the new VCL
configuration.

#### routes.json

Much of the routes.json file is straightforward, but some fields are not obvious in their purpose.

`routeAlias` helps us keep the overall length and complexity of the regex comparison code in
Fastly from getting too large. There is one large regex which we have Fastly test the incoming
request URL against to know if it can reply with a static file in S3; if no match is found, we
assume we need to pass the request on to scratchr2. We could test every single route `pattern`
regex in `routes.json`, but many are similar, so instead we just take the unique set of all
`routeAlias` entries, which is shorter and quicker.

## Windows

For development on Windows, you will probably need to use a program that provides you a Unix interface.

There are several options for doing this:

* Use the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10) to run Linux inside Windows
* Use [Cygwin](https://www.cygwin.com/)
* Use Wubi, a Windows Installer for Ubuntu that allows you to have Ubuntu and Windows on one disk, without the need of an extra partition. There is a [version for Windows XP, Vista, or 7](https://wiki.ubuntu.com/WubiGuide) and a [version for Windows 8 or higher](https://github.com/hakuna-m/wubiuefi).

In addition, you will need to install Node; [here are instructions for installing Node on WSL](https://docs.microsoft.com/en-us/windows/nodejs/setup-on-wsl2#install-nvm-nodejs-and-npm).

## Current issues with developing scratch-www

We're currently in the process of transitioning into this web client from Scratch's existing structure. As we transition, there are going to be some issues along the way that relate to how this client needs to interact with the existing infrastructure to work properly in production.

### FALLBACK

On top of migrating to using this as our web client, Scratch is also transitioning into using a new API backend, Scratch REST API (closed-source). As that is also currently in development and incomplete, we are set up to fall back to using existing Scratch endpoints if an API endpoint does not exist – which is where the `FALLBACK` comes in.

Most of the issues we have currently revolve around the use of `FALLBACK`. This variable is used to specify what URL to fall back onto should a request fail within the context of this web client, or when using the `API_HOST`. If not specified in the process, it will not be used, and any request that is not made through the web client or the API will be unreachable.

Setting `FALLBACK=https://scratch.mit.edu` allows the web client to retrieve data from the Scratch website in your development environment. However, because of security concerns, trying to send data to Scratch through your development environment won't work. This means the following things will be broken for the time being:

* Login on the splash page (*In the process of being fixed*)
* Some update attempts to production data made through a development version of the web client

Additionally, if you set `FALLBACK=https://scratch.mit.edu`, be aware that clicking on links to parts of the website not yet migrated over (currently such as `Discuss`, `Profile`, `My Stuff`, etc.) will take you to the Scratch website itself.
",,2024-05-03T15:08:17Z,76,1559,127,"('renovatebot', 2708), ('dependabot-previewbot', 1133), ('benjiwheeler', 1067), ('mewtaylor', 909), ('paulkaplan', 691), ('renovate-bot', 609), ('rschamp', 509), ('chrisgarrity', 442), ('picklesrus', 429), ('ericrosenbaum', 372), ('kchadha', 322), ('cwillisf', 248), ('greenkeeperbot', 209), ('thisandagain', 203), ('tomlum', 187), ('hyperobject', 177), ('LiFaytheGoblin', 138), ('fsih', 108), ('colbygk', 99), ('TheGrits', 81), ('apple502j', 65), ('carljbowman', 53), ('St19Galla', 50), ('technoboy10', 43), ('zoebentley', 38), ('morantsur', 36), ('jwzimmer-zz', 27), ('kyleplo', 21), ('Paul-Clue', 18), ('Kenny2github', 18), ('mxmou', 16), ('Sheshank-s', 16), ('seotts', 15), ('R4356th', 15), ('JoelGritter', 14), ('Accio1', 11), ('ChrisC', 9), ('jeffalo', 9), ('kerrtravers', 9), ('GrahamSH-LLK', 7), ('jakel181', 7), ('sehgalvibhor', 7), ('mrsrec', 6), ('deepankarmalhan', 6), ('edelfino11', 5), ('hlcolani', 5), ('AlexisGoodfellow', 5), ('towerofnix', 4), ('delasare', 4), ('kaaaaathy', 4), ('chen-robert', 4), ('ckilroy', 4), ('sjgllghr', 4), ('joker314', 3), ('aoneill01', 3), ('sclements', 3), ('BoomerScratch', 3), ('as-com', 3), ('LankyBox01', 2), ('ChampikaF', 2), ('webdev03', 2), ('julescubtree', 2), ('TimothyKeaveny', 2), ('nandedamana', 2), ('MasterOfTheTiger', 2), ('itta611', 2), ('EdwardGeneralov', 2), ('carlflor', 1), ('DeleteThisAcount', 1), ('doc-han', 1), ('Nambaseking01', 1), ('Purple-Hacker', 1), ('SeansC12', 1), ('tmickel', 1), ('hello-smile6', 1), ('southwolf', 1)","[4, 'Quality Education']"
OpenBankProject/OBP-API,"An open source RESTful API platform for banks that supports Open Banking, XS2A and PSD2 through access to accounts, transactions, counterparties, payments, entitlements and metadata - plus a host of internal banking and management APIs.","# README

The Open Bank Project API

## ABOUT

The Open Bank Project is an open source API for banks that enables account holders to interact with their bank using a wider range of applications and services.

The OBP API supports transparency options (enabling account holders to share configurable views of their transaction data with trusted individuals and even the public), data blurring (to preserve sensitive information) and data enrichment (enabling users to add tags, comments and images to transactions).

The OBP API abstracts away the peculiarities of each core banking system so that a wide range of apps can interact with  multiple banks on behalf of the account holder. We want to raise the bar of financial transparency and enable a rich ecosystem of innovative financial applications and services.

Our tag line is: Bank as a Platform. Transparency as an Asset.

The API supports [OAuth 1.0a](https://apiexplorer-ii-sandbox.openbankproject.com/glossary#OAuth%201.0a), [OAuth 2](https://apiexplorer-ii-sandbox.openbankproject.com/glossary#OAuth%202), [OpenID Connect OIDC](https://apiexplorer-ii-sandbox.openbankproject.com/glossary#OAuth%202%20with%20Google) and other authentication methods including [Direct Login](https://apiexplorer-ii-sandbox.openbankproject.com/glossary#Direct%20Login).

## DOCUMENTATION 

The API documentation is best viewed using the [OBP API Explorer](https://apiexplorer-ii-sandbox.openbankproject.com) or a third party tool that has imported the OBP Swagger definitions.

If you want to run your own copy of API Explorer II, see [here](https://github.com/OpenBankProject/API-Explorer-II)

## STATUS of API Versions

OBP instances support multiple versions of the API simultaniously (unless they are deactivated in config)
To see the status (DRAFT, STABLE or BLEEDING-EDGE) of an API version, look at the root endpoint e.g. /obp/v2.0.0/root or /obp/v3.0.0/root
```
24.01.2017, [V1.2.1](https://apisandbox.openbankproject.com/obp/v1.2.1/root) was marked as stable. 
24.01.2017, [V1.3.0](https://apisandbox.openbankproject.com/obp/v1.3.0/root) was marked as stable. 
08.06.2017, [V2.0.0](https://apisandbox.openbankproject.com/obp/v2.0.0/root) was marked as stable. 
27.10.2018, [V2.1.0](https://apisandbox.openbankproject.com/obp/v2.1.0/root) was marked as stable. 
27.10.2018, [V2.2.0](https://apisandbox.openbankproject.com/obp/v2.2.0/root) was marked as stable. 
18.11.2020, [V3.0.0](https://apisandbox.openbankproject.com/obp/v3.0.0/root) was marked as stable. 
18.11.2020, [V3.1.0](https://apisandbox.openbankproject.com/obp/v3.1.0/root) was marked as stable. 
16.12.2022, [V4.0.0](https://apisandbox.openbankproject.com/obp/v4.0.0/root) was marked as stable. 
16.12.2022, [V5.0.0](https://apisandbox.openbankproject.com/obp/v5.0.0/root) was marked as stable. 
```


## LICENSE
.
This project is dual licensed under the AGPL V3 (see NOTICE) and commercial licenses from TESOBE GmbH.

## SETUP

The project uses Maven 3 as its build tool.

To compile and run jetty, install Maven 3, create your configuration in obp-api/src/main/resources/props/default.props and execute:

     mvn install -pl .,obp-commons && mvn jetty:run -pl obp-api

In case that the above command fails try next one:

    export MAVEN_OPTS=""-Xss128m"" && mvn install -pl .,obp-commons && mvn jetty:run -pl obp-api

[Note: How to run via IntelliJ IDEA](obp-api/src/main/docs/glossary/Run_via_IntelliJ_IDEA.md)

## Run some tests.
  
* In obp-api/src/main/resources/props create a test.default.props for tests. Set connector=mapped

* Run a single test. For instance right click on obp-api/test/scala/code/branches/MappedBranchProviderTest and select Run Mapp...

* Run multiple tests: Right click on obp-api/test/scala/code and select Run. If need be:
    Goto Run / Debug configurations
    Test Kind: Select All in Package
    Package: Select code
    Add the absolute /path-to-your-OBP-API in the ""working directory"" field
    You might need to assign more memory via VM Options: e.g. -Xmx1512M -XX:MaxPermSize=512M

    or

    -Xmx2048m -Xms1024m -Xss2048k -XX:MaxPermSize=1024m
    
    Make sure your test.default.props has the minimum settings (see test.default.props.template)

    
    Right click obp-api/test/scala/code and select the Scala Tests in code to run them all.
    
    Note: You may want to disable some tests not relevant to your setup e.g.:
    set bank_account_creation_listener=false in test.default.props 


## Other ways to run tests

* See pom.xml for test configuration
* See http://www.scalatest.org/user_guide


## From the command line

Set memory options

    export MAVEN_OPTS=""-Xmx3000m -Xss2m""

Run one test

    mvn -DwildcardSuites=code.api.directloginTest test

## Ubuntu

If you use Ubuntu (or a derivate) and encrypted home directories (e.g. you have ~/.Private), you might run into the following error when the project is built:

    uncaught exception during compilation: java.io.IOException
    [ERROR] File name too long
    [ERROR] two errors found
    [DEBUG] Compilation failed (CompilerInterface)

The current workaround is to move the project directory onto a different partition, e.g. under /opt/ .

## Running the docker image

Docker images of OBP API can be found on Dockerhub: https://hub.docker.com/r/openbankproject/obp-api - pull with `docker pull openbankproject/obp-api`

Props values can be set as environment variables. Props need to be prefixed with `OBP_`, `.` replaced with `_`, and all upper-case, e.g.:

`openid_connect.enabled=true` becomes `OBP_OPENID_CONNECT_ENABLED=true`

## Databases:

The default database for testing etc is H2. PostgreSQL is used for the sandboxes (user accounts, metadata, transaction cache). List of databases fully tested is: PostgreSQL, MS SQL and H2. 

### Notes on using H2 web console in Dev and Test mode:

Set DB options in props file:

    db.driver=org.h2.Driver
    db.url=jdbc:h2:./obp_api.db;DB_CLOSE_ON_EXIT=FALSE
    
In order to start H2 web console go to http://127.0.0.1:8080/console and you will see a login screen.
Please use the following values:
Note: make sure the JDBC URL used matches your Props value!

    Driver Class: org.h2.Driver
    JDBC URL: jdbc:h2:./obp_api.db;AUTO_SERVER=FALSE
    User Name:
    Password:


### Notes on the basic ussage of Postgres:
Once postgres is installed: (On Mac use brew)

psql postgres

create database obpdb; (or any other name of your choosing)

create user obp; (this is the user that OBP-API will use to create and access tables etc)

alter user obp with password 'daniel.says'; (put this password in the OBP-API Props)

grant all on database obpdb to obp; (So OBP-API can create tables etc.)

Then set the db.url in your Props: 

db.driver=org.postgresql.Driver
db.url=jdbc:postgresql://localhost:5432/obpdb?user=obp&password=daniel.says

The restart OBP-API

### Notes on using Postgres with SSL:

Postgres needs to be compiled with SSL support.

Use openssl to create the files you need.

For the steps, see: https://www.howtoforge.com/postgresql-ssl-certificates

In short, edit postgresql.conf

ssl = on

ssl_cert_file = '/etc/YOUR-DIR/server.crt'

ssl_key_file = '/etc/YOUR-DIR/server.key'

And restart postgres.

Now, this should enable SSL (on the same port that Postgres normally listens on) - but it doesn't force it.
To force SSL, edit pg_hba.conf replacing the host entries with hostssl

Now in OBP-API Props, edit your db.url and add &ssl=true

 e.g.

 db.url=jdbc:postgresql://localhost:5432/my_obp_database?user=my_obp_user&password=the_password&ssl=true

Note: Your Java environment may need to be setup correctly to use SSL

Restart OBP-API, if you get an error, check your Java environment can connect to the host over SSL.

Note: You can copy the following example files to prepare your own configurations
 - /obp-api/src/main/resources/logback.xml.example -> /obp-api/src/main/resources/logback.xml (try TRACE or DEBUG)
 - /obp-api/src/main/resources/logback-test.xml.example -> /obp-api/src/main/resources/logback-test.xml (try TRACE or DEBUG)

There is a gist / tool which is useful for this. Search the web for SSLPoke. Note this is an external repository.

e.g. https://gist.github.com/4ndrej/4547029

or

git clone https://github.com/MichalHecko/SSLPoke.git .

gradle jar
cd ./build/libs/

java -jar SSLPoke-1.0.jar www.github.com 443

Successfully connected

java -jar SSLPoke-1.0.jar YOUR-POSTGRES-DATABASE-HOST PORT

You can add switches e.g. for debugging.

java -jar -Dhttps.protocols=TLSv1.1,TLSv1.2 -Djavax.net.debug=all SSLPoke-1.0.jar localhost 5432


To import a certificate:

keytool -import -storepass changeit -noprompt -alias localhost_postgres_cert -keystore /Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home/jre/lib/security/cacerts -trustcacerts -file /etc/postgres_ssl_certs/server/server.crt


To get certificate from the server / get further debug information:

openssl s_client -connect ip:port

The above section is work in progress.

## Administrator role / SuperUser

In the API's props file, add the ID of your user account to `super_admin_user_ids=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`. User Id can be retrieved via the ""Get User (Current)"" endpoint (e.g. /obp/v4.0.0/users/current) after login or via API Explorer (https://github.com/OpenBankProject/API-Explorer) at `/#OBPv3_0_0-getCurrentUser`.

Super users can give themselves any entitlement, but it is recommended to use this props only for bootstrapping (creating the first admin user). Use this admin user to create further priviledged users by granting them the ""CanCreateEntitlementAtAnyBank"" role. This, again, can be done via API Explorer (`/#OBPv2_0_0-addEntitlement`, leave `bank_id` empty) or, more conveniently, via API Manager (https://github.com/OpenBankProject/API-Manager).

## Sandbox data

To populate the OBP database with sandbox data:

1) In the API's props file, set `allow_sandbox_data_import=true`
2) Grant your user the role `CanCreateSandbox`. See previous section on how to do this
3) Now post the JSON data using the payload field at `/#2_1_0-sandboxDataImport`. An example of an import set of data (json) can be found [here](https://raw.githubusercontent.com/OpenBankProject/OBP-API/develop/obp-api/src/main/scala/code/api/sandbox/example_data/2016-04-28/example_import.json)
4) If successful you should see this result `{ ""success"": ""Success"" }` and no error message



## Production Options.

* set the status of HttpOnly and Secure cookie flags for production, uncomment the following lines of  ""webapp/WEB-INF/web.xml"" :

        
          
            true
            true
          
        

## Running the API in Production Mode

We use 9 to run the API in production mode.

1) Install java and jetty9

2) jetty configuration

* Edit the /etc/default/jetty9 file so that it contains the following settings:

        NO_START=0
        JETTY_HOST=127.0.0.1 #If you want your application to be accessed from other hosts, change this to your IP address
        JAVA_OPTIONS=""-Drun.mode=production -XX:PermSize=256M -XX:MaxPermSize=512M -Xmx768m -verbose -Dobp.resource.dir=$JETTY_HOME/resources -Dprops.resource.dir=$JETTY_HOME/resources""

* In obp-api/src/main/resources/props create a test.default.props file for tests. Set connector=mapped

* In obp-api/src/main/resources/props create a default.props file for development. Set connector=mapped

* In obp-api/src/main/resources/props create a production.default.props file for production. Set connector=mapped.

* This file could be similar to the default.props file created above, or it could include production settings, such as information about Postgresql server, if you are using one. For example, it could have the following line for postgresql configuration.

        db.driver=org.postgresql.Driver
        db.url=jdbc:postgresql://localhost:5432/yourdbname?user=yourdbusername&password=yourpassword

* Now, build the application to generate .war file which will be deployed on jetty server:

        cd OBP-API/
        mvn package

* This will generate OBP-API-1.0.war under OBP-API/target/

* Copy OBP-API-1.0.war to /usr/share/jetty9/webapps/ directory and rename it to root.war

* Edit the /etc/jetty9/jetty.conf file and comment out the lines:

        etc/jetty-logging.xml
        etc/jetty-started.xml

* Now restart jetty9:

        sudo service jetty9 restart

* You should now be able to browse to localhost:8080 (or yourIPaddress:8080)

## Using OBP-API in different app modes

1) `portal` => OBP-API as a portal i.e. without REST API 
2) `apis` => OBP-API as a apis app i.e. only REST APIs
3) `apis,portal`=> OBP-API as portal and apis i.e. REST APIs and web portal

* Edit your props file(s) to contain one of the next cases:
        
        1) server_mode=portal
        2) server_mode=apis
        3) server_mode=apis,portal
        In case is not defined default case is the 3rd one i.e. server_mode=apis,portal

## Using Akka remote storage

Most internal OBP model data access now occurs over Akka. This is so the machine that has JDBC access to the OBP database can be physically separated from the OBP API layer. In this configuration we run two instances of OBP-API on two different machines and they communicate over Akka. Please see README.Akka.md for instructions.

## Using SSL Encryption with kafka

For SSL encryption we use jks keystores.
Note that both the keystore and the truststore (and all keys within) must have the same password for unlocking, for which
the api will stop at boot up and ask for. 

* Edit your props file(s) to contain:
        
        kafka.use.ssl=true
        keystore.path=/path/to/api.keystore.jks
        truststore.path=/path/to/api.truststore.jks

## Using SSL Encryption with props file

For SSL encryption we use jks keystores.
Note that keystore (and all keys within) must have the same password for unlocking, for which the api will stop at boot up and ask for. 

* Edit your props file(s) to contain:
        
        jwt.use.ssl=true
        keystore.path=/path/to/api.keystore.jks
        keystore.alias=SOME_KEYSTORE_ALIAS
        
A props key value, XXX, is considered encrypted if has an encryption property (XXX.is_encrypted) in addition to the regular props key name in the props file e.g:

   *  db.url.is_encrypted=true
   *  db.url=BASE64URL(SOME_ENCRYPTED_VALUE)
   
The Encrypt/Decrypt workflow is :
1. Encrypt: Array[Byte]
2. Helpers.base64Encode(encrypted)
3. Props file: String
4. Helpers.base64Decode(encryptedValue)
5. Decrypt: Array[Byte]

1st, 2nd and 3rd step can be done using an external tool

### Encrypting props values with openssl on the commandline

1. Export the public certificate from the keystore:

    `keytool -export -keystore /PATH/TO/KEYSTORE.jks -alias CERTIFICATE_ALIAS -rfc -file apipub.cert`
2. Extract the public key from the public certificate

    `openssl x509 -pubkey -noout -in apipub.cert > PUBKEY.pub`
3. Get the encrypted propsvalue like in the following bash script (usage ./scriptname.sh /PATH/TO/PUBKEY.pub propsvalue)

```
#!/bin/bash
echo -n $2 |openssl pkeyutl -pkeyopt rsa_padding_mode:pkcs1 -encrypt  -pubin -inkey $1 -out >(base64)
```

## Using jetty password obfuscation with props file

You can obfuscate passwords in the props file the same way as for jetty:

1. Create the obfuscated value as described here: https://www.eclipse.org/jetty/documentation/9.3.x/configuring-security-secure-passwords.html

2. A props key value, XXX, is considered obfuscated if has an obfuscation property (XXX.is_obfuscated) in addition to the regular props key name in the props file e.g:

   *  db.url.is_obfuscated=true
   *  db.url=OBF:fdsafdsakwaetcetcetc

## Code Generation
Please refer to the [Code Generation](https://github.com/OpenBankProject/OBP-API/blob/develop/CONTRIBUTING.md##code-generation) for links

## Customize Portal WebPage
Please refer to the [Custom Webapp](obp-api/src/main/resources/custom_webapp/README.md) for links

## Using jetty password obfuscation with props file

You can obfuscate passwords in the props file the same way as for jetty:

1. Create the obfuscated value as described here: https://www.eclipse.org/jetty/documentation/9.3.x/configuring-security-secure-passwords.html

2. A props key value, XXX, is considered obfuscated if has an obfuscation property (XXX.is_obfuscated) in addition to the regular props key name in the props file e.g:

   *  db.url.is_obfuscated=true
   *  db.url=OBF:fdsafdsakwaetcetcetc

## Rate Limiting
We support rate limiting i.e functionality to limit calls per consumer key (App). Only `New Style Endpoins` support it. The list of they can be found at this file: https://github.com/OpenBankProject/OBP-API/blob/develop/obp-api/src/main/scala/code/api/util/NewStyle.scala. 
There are two supported modes:
   *  In-Memory
   *  Redis
   
It is assumed that you have some Redis instance if you wan to use the functionality in multi node architecture.


We apply Rate Limiting for two type of access:
   *  Authorized
   *  Anonymous

To set up Rate Limiting in case of the anonymous access edit your props file in next way:
```
user_consumer_limit_anonymous_access=100, In case isn't defined default value is 60
```
   
Te set up Rate Limiting in case of the authorized access use these endpoints
1. `GET ../management/consumers/CONSUMER_ID/consumer/call-limits` - Get Call Limits for a Consumer
2. `PUT ../management/consumers/CONSUMER_ID/consumer/call-limits` - Set Call Limits for a Consumer


In order to make it work edit your props file in next way:

```
use_consumer_limits=false, In case isn't defined default value is ""false""
redis_address=YOUR_REDIS_URL_ADDRESS, In case isn't defined default value is 127.0.0.1
redis_port=YOUR_REDIS_PORT, In case isn't defined default value is 6379
```


Next types are supported:
```
1. per second
2. per minute
3. per hour
4. per day
5. per week
6. per month
```    
If you exceed rate limit per minute for instance you will get the response:
```json
{
    ""error"": ""OBP-10018: Too Many Requests.We only allow 3 requests per minute for this Consumer.""
}
```
and response headers:
```
X-Rate-Limit-Limit → 3
X-Rate-Limit-Remaining → 0
X-Rate-Limit-Reset → 22
```
Description of the headers above:
1. `X-Rate-Limit-Limit` - The number of allowed requests in the current period
2. `X-Rate-Limit-Remaining` - The number of remaining requests in the current period
3. `X-Rate-Limit-Reset` - The number of seconds left in the current period

Please note that first will be checked `per second` call limit then `per minute` etc.

Info about rate limiting availability at some instance can be found over next API endpoint: https://apisandbox.openbankproject.com/obp/v3.1.0/rate-limiting. Response we are interested in looks lke:
```json
{
  ""enabled"": false,
  ""technology"": ""REDIS"",
  ""service_available"": false,
  ""is_active"": false
}
```

## Webhooks
Webhooks are used to call external URLs when certain events happen.
Account Webhooks focus on events around accounts.
For instance, a webhook could be used to notify an external service if a balance changes on an account.
This functionality is work in progress!

There are 3 API's endpoint related to webhooks:
1. `POST ../banks/BANK_ID/account-web-hooks` - Create an Account Webhook
2. `PUT ../banks/BANK_ID/account-web-hooks` - Enable/Disable an Account Webhook
3. `GET ../management/banks/BANK_ID/account-web-hooks` - Get Account Webhooks
---

## OpenID Connect
In order to enable an OIDC workflow at an instance of OBP-API portal app(login functionality) you need to set-up the following props:
```props
## Google as an identity provider
# openid_connect_1.client_secret=OYdWujJl******_NXzPlDI4T
# openid_connect_1.client_id=883**3244***-s4hi72j0rble0iiivq1gn09k7***tdci.apps.googleusercontent.com
# openid_connect_1.callback_url=http://127.0.0.1:8080/auth/openid-connect/callback
# openid_connect_1.endpoint.authorization=https://accounts.google.com/o/oauth2/v2/auth
# openid_connect_1.endpoint.userinfo=https://openidconnect.googleapis.com/v1/userinfo
# openid_connect_1.endpoint.token=https://oauth2.googleapis.com/token
# openid_connect_1.endpoint.jwks_uri=https://www.googleapis.com/oauth2/v3/certs
# openid_connect_1.access_type_offline=false
# openid_connect_1.button_text = Yahoo

## Yahoo as an identity provider
# openid_connect_2.client_secret=685d47412efd8b74891ad711876558189793e957
# openid_connect_2.client_id=zg0yJmk9WUEzaERzd1RtMU02JmQ9WVdrOU9FOHpTbXN5TkhNbWNHbzlNQS0tJnM9Y38uc3VtZXJzZWNyZXQmc3Y9MCZ4PWjW
# openid_connect_2.callback_url=https://1aaac045.ngrok.io/auth/openid-connect/callback-2
# openid_connect_2.endpoint.authorization=https://api.login.yahoo.com/oauth2/request_auth
# openid_connect_2.endpoint.userinfo=https://api.login.yahoo.com/openid/v1/userinfo
# openid_connect_2.endpoint.token=https://api.login.yahoo.com/oauth2/get_token
# openid_connect_2.endpoint.jwks_uri=https://api.login.yahoo.com/openid/v1/certs
# openid_connect_2.access_type_offline=true
# openid_connect_2.button_text = Yahoo
```
Please note in the example above you MUST run OBP-API portal at the URL: http://127.0.0.1:8080

## OAuth 2.0 Authentication
In order to enable an OAuth2 workflow at an instance of OBP-API backend app you need to set-up the following props:
```
# -- OAuth 2 ---------------------------------------------------------------------------------
# Enable/Disable OAuth 2 workflow at a server instance
# In case isn't defined default value is false
# allow_oauth2_login=false
# URL of Public server JWK set used for validating bearer JWT access tokens
# It can contain more than one URL i.e. list of uris. Values are comma separated.
# If MITREId URL is present it must be at 1st place in the list
# because MITREId URL can be an appropirate value and we cannot rely on it.
# oauth2.jwk_set.url=http://localhost:8080/jwk.json,https://www.googleapis.com/oauth2/v3/certs
# ------------------------------------------------------------------------------ OAuth 2 ------

OpenID Connect is supported.
Tested Identity providers: Google, MITREId.

```
### Example for Google's OAuth 2.0 implementation for authentication, which conforms to the OpenID Connect specification
```
allow_oauth2_login=true
oauth2.jwk_set.url=https://www.googleapis.com/oauth2/v3/certs
```
---

## Frozen APIs
API versions may be marked as ""STABLE"", if changes are made to an API which has been marked as ""STABLE"", then unit test `FrozenClassTest`  will fail.
### Changes to ""STABLE"" api cause the tests fail: 
* modify request or response body structure of apis
* add or delete apis
* change the apis versionStatus from or to ""STABLE""

If it is required for a ""STABLE"" api to be changed, then the class metadata must be regenerated using the FrozenClassUtil (see how to freeze an api)
### Steps to freeze an api
* Run the FrozenClassUtil to regenerate persist file of frozen apis information, the file is `PROJECT_ROOT_PATH/obp-api/src/test/resources/frozen_type_meta_data`
* push the file `frozen_type_meta_data` to github

There is a video about the detail: [demonstrate the detail of the feature](https://www.youtube.com/watch?v=m9iYCSM0bKA)

## Frozen Connector InBound OutBound types
The same as `Frozen APIs`, if related unit test fail, make sure whether the modify is required, if yes, run frozen util to re-generate frozen types metadata file. take `RestConnector_vMar2019` as example, the corresponding util is `RestConnector_vMar2019_FrozenUtil`, the corresponding unit test is `RestConnector_vMar2019_FrozenTest`

## Scala / Lift

* We use scala and liftweb http://www.liftweb.net/

* Advanced architecture: http://exploring.liftweb.net/master/index-9.html

* A good book on Lift: ""Lift in Action"" by Timothy Perrett published by Manning.

## Supported JDK Versions
* OracleJDK: 1.8, 13
* OpenJdk: 11

## Endpoint Request and Response Example
    ResourceDoc#exampleRequestBody and ResourceDoc#successResponseBody can be the follow type
* Any Case class
* JObject
* Wrapper JArray: JArrayBody(jArray)
* Wrapper String: StringBody(""Hello"")
* Wrapper primary type: IntBody(1), BooleanBody(true), FloatBody(1.2F)...
* Empty: EmptyBody

example: 
```
resourceDocs += ResourceDoc(
      exampleRequestBody= EmptyBody,
      successResponseBody= BooleanBody(true),
      ...
)
```
","'openbanking', 'openbanking-api', 'psd2', 'psd2-xs2a-interface', 'xs2a', 'xs2a-connector', 'xs2a-interface'",2024-05-02T13:44:59Z,27,1503,124,"('constantine2nd', 3507), ('hongwei1', 3291), ('simonredfern', 2620), ('everett-tesobe', 788), ('oldbig', 677), ('tawoe', 134), ('sebtesobe', 115), ('kjyv', 102), ('cur1576', 66), ('moule3053', 47), ('karmaking', 31), ('tgpfeiffer', 20), ('CristhTejada', 19), ('somanole', 18), ('Jianwei-Tesobe', 15), ('MhmdGhali', 7), ('slavisac', 6), ('dependabotbot', 5), ('florind', 5), ('chrisjsimpson', 4), ('kination', 2), ('andrisak', 2), ('Akendo', 1), ('spolnik', 1), ('hong-wei', 1), ('kassapo', 1), ('kernifex', 1)","[17, 'Partnerships for the Goals']"
code-dot-org/code-dot-org,The code powering code.org and studio.code.org,"# Code.org

Welcome! You've found the source code for [the Code.org website](https://code.org/) and [the Code Studio platform](https://studio.code.org/). Code.org is a non-profit dedicated to expanding access to computer science education. You can read more about our efforts at [code.org/about](https://code.org/about).

## Quick start

1. Follow our [setup guide](./SETUP.md) to configure your workstation.
2. `rake build` to build the application if you have not done so already
3. `bin/dashboard-server` to launch the development server.
4. Open your browser to [http://localhost-studio.code.org:3000/](http://localhost-studio.code.org:3000/).

To see a list of all build commands, run `rake` from the repository root.

## How to help

Wondering where to start?  See our [contribution guidelines](CONTRIBUTING.md).

## What's in this repo?
Here's a quick overview of the major landmarks:

### Documentation

* [SETUP](./SETUP.md): Instructions to get everything up and running.
* [TESTING](./TESTING.md): How to be sure nothing broke.
* [STYLEGUIDE](./STYLEGUIDE.md): Our code style conventions.
* Our [LICENSE](./LICENSE) and [NOTICE](./NOTICE).
* There are many more topical guides in the [docs](./docs) folder.
* In addition, several sections of the repository have their own documentation:
  * [apps/README](./apps/README.md)
  * [blockly/README](https://github.com/code-dot-org/blockly/blob/master/README.md)

### [dashboard](./dashboard)

The server for our [**Code Studio** learning platform](https://studio.code.org/), a [Ruby on Rails](http://rubyonrails.org/) application responsible for:

* Our courses, tutorials, and puzzle configurations
* User accounts
* Student progress and projects
* The ""levelbuilder"" content creation tools

### [pegasus](./pegasus)

The server for [the **Code.org** website](https://code.org/), a [Sinatra](http://www.sinatrarb.com/) application responsible for:

* [code.org](https://code.org)
* [hourofcode.com](https://hourofcode.com)
* [csedweek.org](https://csedweek.org)

### [apps](./apps)

The JavaScript 'engine' for all of our tutorials, puzzle types and online tools.  It gets built into a static package that we serve through dashboard. Though there are currently some exceptions, the goal is that all JS code ultimately lives here, so that it gets the benefit of linting/JSX/ES6/etc.
Start here if you are looking for:
* The Hour of Code tutorials: [Star Wars](https://code.org/starwars), [Minecraft](https://code.org/api/hour/begin/mc), [Frozen](https://studio.code.org/s/frozen) and [Classic Maze](http://studio.code.org/hoc/1)
* Tools like [Artist](https://studio.code.org/projects/artist), [Play Lab](https://studio.code.org/projects/playlab) and [App Lab](https://code.org/educate/applab)
* Other core puzzle types: Maze, Farmer, Bee, Bounce, Calc, Eval
* Other JS code consumed by dashboard and pegasus.

### Everything else

* **aws**: Configuration and scripts that manage our deployments.
* **bin**: Developer utilities.
* **cookbooks**: Configuration management through [Chef](https://www.chef.io/).
* **shared**: Source and assets used by many parts of our application.
* **tools**: Git commit hooks.


","'blockly', 'code', 'coding', 'computer', 'educational', 'learning', 'online', 'science', 'studio'",2024-05-03T15:40:28Z,160,813,81,"('deploy-code-org', 43252), ('islemaster', 9764), ('Hamms', 9448), ('davidsbailey', 9147), ('breville', 7988), ('dmcavoy', 7823), ('joshlory', 5874), ('Bjvanminnen', 5844), ('Erin007', 4161), ('tanyaparker', 4098), ('ashercodeorg', 4081), ('bethanyaconnor', 3451), ('wjordan', 2993), ('bencodeorg', 2846), ('mehalshah', 2619), ('bcjordan', 2472), ('ajpal', 2443), ('caleybrock', 2327), ('laurelfan', 2126), ('maddiedierker', 2089), ('molly-moen', 1979), ('cpirich', 1977), ('sureshc', 1742), ('jmkulwik', 1529), ('nkiruka', 1431), ('hacodeorg', 1404), ('maureensturgeon', 1219), ('philbogle', 1144), ('aoby', 1030), ('pcardune', 995), ('hannahbergam', 990), ('mikeharv', 986), ('daynew', 864), ('ewjordan', 774), ('JillianK', 699), ('geoffrey-elliott', 560), ('kelbyhawn', 555), ('sanchitmalhotra126', 552), ('dju90', 530), ('balderdash', 526), ('rlhawk', 518), ('tim-dot-org', 502), ('cat5inthecradle', 431), ('TurnerRiley', 417), ('cforkish', 405), ('pablo-code-org', 405), ('jamescodeorg', 384), ('fisher-alice', 337), ('smusoke', 324), ('annaxuphoto', 314), ('wilkie', 305), ('artem-vavilov', 266), ('marybarnes37', 202), ('snickell', 178), ('mgc1194', 172), ('larrypo', 171), ('dmantonyuk', 157), ('levadadenys', 150), ('sfilman', 149), ('etaderhold', 144), ('lfryemason', 138), ('drew-beckmen', 136), ('juanmanzojr', 135), ('KatieShipley', 130), ('drewsamnick', 127), ('mehals', 111), ('kobryan0619', 109), ('kimj42', 103), ('katleiahramos', 102), ('ebeastlake', 97), ('kozzi', 96), ('rshipp', 96), ('MeyAyre', 96), ('mrjoshida', 92), ('stephenliang', 88), ('poorvasingal', 87), ('nicklathe', 84), ('carl-codeorg', 77), ('ryansloan', 75), ('elf-code', 68), ('bakerfranke', 66), ('tshaffercodeorg', 64), ('made-line', 64), ('LizGauthier', 62), ('indhu-gunda', 61), ('ericfershtman', 61), ('mvkski', 58), ('KylieModen', 51), ('asherkach', 51), ('mcatullo', 44), ('vijayamanohararaj', 43), ('AfifahK', 42), ('thomasoniii', 41), ('Nokondi', 40), ('Fikayoz', 35), ('dependabotbot', 35), ('dabbler0', 34), ('marcd123', 31), ('SteveEisner', 24), ('cearachew', 23), ('cnbrenci', 20), ('brickware', 20), ('audreyclark', 19), ('Sam-Hutchins', 18), ('dostos', 18), ('WilliamHarmonMS', 16), ('kakiha11', 14), ('rvarshney', 14), ('beezwaxz', 13), ('jakebrbell', 13), ('shadinaif', 13), ('aaronwaggener', 13), ('tessawiedmann', 12), ('castro-jorge', 11), ('salvillalon45', 11), ('LeoLe101', 11), ('pickettd', 10), ('PhantomMike', 10), ('hemabatra', 10), ('onlinecsteacher', 9), ('ty-po', 9), ('simplycloud', 7), ('dancodedotorg', 7), ('rhc2104', 6), ('davetchen', 6), ('stevekrenzel', 5), ('abrenner79', 5), ('DrewPerlman', 5), ('katiejofr', 5), ('fisheali', 4), ('jordan-springer', 4), ('unlox775-code-dot-org-contrib', 3), ('FirePieman', 3), ('rezasafi', 3), ('dlittle1', 3), ('davidwufer', 3), ('davidwu-sfdc', 3), ('unlox775-code-dot-org', 3), ('dillia23', 3), ('aaka3207', 3), ('raboof', 2), ('brook-osborne', 2), ('polinavishnev', 2), ('NeverMin', 1), ('simonguest', 1), ('simonjtyler', 1), ('sukykang', 1), ('amy-b', 1), ('angD13', 1), ('FelixNgFender', 1), ('hbbatra', 1), ('lauraschute', 1), ('Naggi-Goishi', 1), ('oldfritter', 1), ('joshk', 1), ('hamidzr', 1), ('ElanHasson', 1), ('davidmfoley', 1), ('bpcb', 1), ('alivira', 1)","[4, 'Quality Education']"
Welthungerhilfe/cgm-scanner,Child Growth Monitor Scanner App,"# Child Growth Monitor Scanner App
Mothers and governmental frontline workers often fail to detect severe malnutrition of children. As a result, they do not help the child in the right way. The magnitude of a nutrition crises (both in emergencies and chronic hunger situations) is often blurred. This hinders a determined response by emergency workers as well as policy makers.

We provide a game-changer in measurement and data processing for malnourished children under the age of 5 years. It is a fool proof solution based on a mobile app using augmented reality in combination with artificial intelligence. By determining weight and height through a 3D scan of children, the app can instantly detect malnutrition.

- [Child Growth Monitor Website](https://childgrowthmonitor.org)
- [GitHub main project](https://github.com/Welthungerhilfe/ChildGrowthMonitor/)
- info@childgrowthmonitor.org

## Branch policy

#### master branch
- Master branch contains source codes of the version on Google Play in Closed Alpha, Public Beta and Production tracks
- We merge testing into master after using by users for some period of time

#### testing branch
- Testing branch contains source codes of the version on Google Play in Internal Testing and Demo/QA tracks
- We merge develop into testing after passing all tests

#### develop branch
- All features merged into develop are finished and tested
- Developers do all pull requests into develop
",,2024-04-28T16:35:34Z,7,25,13,"('lvonasek', 341), ('jaydesai111', 238), ('lou-seniordev', 84), ('mmatiaschek', 49), ('prajurock', 24), ('sblisztomaniac', 7), ('lev-stupka', 6)","[2, 'Zero Hunger']"
GliaX/Stethoscope,A research-validated stethoscope whose plans are available Freely and openly. The cost of the entire stethoscope is between $2.5 to $5 to produce,"Stethoscope
===========

This project aims to create a research-validated stethoscope whose plans are 
available freely and openly. The goal is for the bell to cost ~USD$1-2 to produce, 
and the rest of the stethoscope to cost approximately the same. You can see the peer-reviewed publication relating to this stethoscope's validation here:

http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193087

Currently, the stethoscope resulting from this project functions as well as the 
market gold standard, the Littmann Cardiology III.


Bill of Materials
=================

**Printed parts:**
* 1 stethoscope head (head.stl)
* 2 ear tubes (eartube.stl)
* 1 Y-piece (y_piece.stl)
* 1 Spring (spring.stl)
* 1 Ring (ring.stl)

**Other hardware:**
Some vendors are suggested.
* 40cm - 50cm Silicone 13mm (preferred) or 12mm OD, 8mm ID, 50 durometer
  * [8MM I.D X 13MM O.D NGP60 Clear Translucent Silicone Hose Pipe Tubing](https://www.advancedfluidsolutions.co.uk/8mm-id-x-13mm-od-clear-transulcent-silicone-hose-pipe-tubing-2482-p.asp) (USD$7.34/meter)

* 20cm Silicone 8mm (preferred) or 6mm OD, 4mm ID (cut into 10cm pieces), 60 durometer
  * [4mm ID x 8mm OD](https://www.advancedfluidsolutions.co.uk/4mm-id-x-8mm-od-clear-transulcent-silicone-hose-pipe-tubing-2454-p.asp)

* Diaphragm: 40mm diameter cut from a report cover with approx 0.35mm plastic sheet
  * [In Business Report Covers](https://www.amazon.com/dp/B0CJCBC86X?_encoding=UTF8&psc=1&ref_=cm_sw_r_ud_dp_CY0Q8HRSQEBTA39H9VF2)(USD$8.99)
  * You can cut the diaphragm by hand or with a stamp like [this one](https://www.amazon.ca/Karujimu-ki-jumbo-craft-circle-CN45004/dp/B001CBY41W)

* Earbuds / Eartips: Any large-sized standard earbuds will do.
  * [Silicone Earbuds 7 Pairs - Large size](https://www.amazon.ca/gp/product/B006VELFJY)
 
**Optional hardware**:
* Metal spring for holding ear tubes together


Print Instructions
==================
**INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%**

* Use PETG or ABS
* Layer height: 0.2mm
* Use PrusaSlicer 2.0 or above to import 3MF file
* Modify filament and printer settings as needed.
* **DO NOT MODIFY PRINT SETTINGS**
* Export and print


Troubleshooting
===============

* If the spring and eartubes do not fit well, go ahead and scale the spring as needed
* If the head and ring do not fit well, use caution as you may be modifying the acoustics. You can scale the head a little.


Notes
=====

* We do not use PLA due to deformation in heat and poor plastic quality in the spring causing early failure. PLA may be used, but the lifetime of the stethoscope will decrease significantly.

* We do not use brims, but you may print the eartubes and Y-pieces with a brim of 2mm to ensure that none of the parts lift off.

**INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%.** Otherwise, the stethoscope will not produce a correct sound.


Assembly Instructions
=====================

See [this instructional video](https://www.youtube.com/watch?v=u-KNTc0POLA) for assembly instructions.

* Attach the diaphragm (40mm) to the stethoscope head.
* Attach the stethoscope head to the silicone tube.
* Attach the silicone tube to the Y-Piece.
* Attach spring to ear tubes.
* Attach the Y-piece to the ear tubes.
* Attach the ear tubes to the eartips / earbuds.
* Test the stethoscope as per the validation instructions.


Printing the inserts
====================
The inserts are included in the `manual` directory. Using 8.5 x 11 (Legal) paper,
they can be printed at 8.25"" page width with 95% scale for the top print and 
90% scale for the bottom print using GIMP.

The top insert is cut at 14.5cm and again at 1cm creating two labels of 13.5cm height.
These inserts are printed on adhesive material.

The bottom insert is cut at a width of 20cm with a height cuts at 25cm, 15cm, 
13.5cm and 3.5cm creating 10cm x 20cm inserts.


Changing and creating SCAD files
================================

[CrystalSCAD](https://github.com/Joaz/CrystalScad) and [OpenSCAD](http://www.openscad.org/) 
were used to create all STL files. To recreate the stethoscope head, simply do:

``` shell
gem install crystalscad
ruby source_files/stethoscope_head/stethoscope_head.rb
```

The SCAD files output from CrystalSCAD are found in `source_files/stethoscope_head/output` and are named as follows:
* PrintableStethoscopeHead1Assembly_output.scad - The head


Mass Manufacturing
==================
We generally print 4 stethoscopes per plate to ensure that each stethoscope is created out of the same material.

Our serial numbering system consists of two parts. The last number part is the total number of unique stethoscopes created since day 1. All numbers before that dash are spool identifiers involved in that stethoscope. For example:

001-010 would be the tenth stethoscope made with the first spool in our inventory. If the first spool makes a total of 15 stethoscopes, then the first stethoscope of the second spool would be 002-016. If the twentieth stethoscope uses plastic from spool 002 and spool 003, it would be 002-003-020.


Other stethoscopes
==================
Others have made 3D printed stethoscopes too. See:
* https://www.youmagine.com/designs/stethoscope-chestpiece


Licensing notes
===============
As per our understanding, hardware is not covered by copyright. However, we present
our work under the TAPR OHL license insofar as it applies.
","'free-software', 'glia', 'medicine', 'open-hardware', 'open-source', 'stethoscope'",2023-11-09T16:23:08Z,13,728,93,"('tareko', 28), ('alawx', 17), ('awpavlos', 8), ('SpencerChambers', 7), ('kliment', 3), ('Mohammed-Khdair', 3), ('shakersh', 3), ('carewake', 2), ('ShugLab', 1), ('EpidemiAngie', 1), ('MarkRijckenberg', 1), ('jenwilsonglia', 1), ('mef51', 1)","[3, 'Good Health and Well-Being']"
CodeForAfrica/HealthTools.KE-theStarHealth,"HealthTools is a suite of data-driven web and SMS-based tools that help citizens check everything from medicine prices and hospital services, to whether their doctor is a quack or not. The toolkit was pioneered in Kenya, and has since also been deployed in Ghana, Nigeria, and South Africa. The original project can be accessed at: https://www.the-star.co.ke/health/","
  
    
  


HealthTools.KE - theStar Health

HealthTools is a suite of data-driven web and SMS-based tools that help citizens check everything from medicine prices and hospital services, to whether their doctor is a quack or not. The toolkit was pioneered in Kenya, and has since also been deployed in Ghana, Nigeria, and South Africa.

Explore Project Here &raquo;

---

## Table of contents

- [Installation](#installation)
- [Testing](#testing)
- [Contributing](#contributing)
- [License and Copyright](#license-and-copyright)

---

## Installation

This is a [Jekyll](https://jekyllrb.com) powered website hosted on [Github Pages](https://pages.github.com/). To install:

1. Install Ruby globally
    - `sudo apt-get install ruby` For Linux Users
    - `brew install ruby` For Mac Users 
    - Download the RubyInstaller - For Windows Users

2. Install Jekyll and Bundler gems through RubyGems
    - `gem install jekyll bundler`

3. Inside the cloned repo directory, run the application
    - `jekyll serve`
 
4. You should now be able to access the application on http://localhost:4000

---

## Testing



...

---

## Contributing

Contributions are always welcomed to the project. If you are interested in enhancing the features in the project, follow these simple steps:
 * Fork the project to your repository then clone it to your local machine.
 * Create a new branch and make the necessary enhancement to the features.
 * If the you wish to update an existing enhancement submit a pull request.
 * Pull requests should follow the standard format adopted for this project.
 * If you are unsure about certain areas in the project feel to ask for assistance.

---

## License and Copyright

The MIT License (MIT)

Copyright (c) 2017 Code for Africa

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
","'africa', 'civic-apps', 'civictech', 'code-for-africa', 'code-for-kenya', 'doctor', 'health', 'healthtools', 'hospital-services', 'kenya', 'media', 'medicine-prices', 'newstools', 'sms'",2023-09-07T05:34:18Z,9,28,24,"('mevey', 41), ('DavidLemayian', 41), ('gregweston', 3), ('andela-johia', 3), ('Tawakalt', 2), ('thepsalmist', 2), ('andela-mabdussalam', 2), ('andela-ookoro', 1), ('andela-mmakinde', 1)","[3, 'Good Health and Well-Being']"
MSH/RxSolution,"RxSolution is an electronic pharmaceutical management system used to manage inventory, purchase and issue stocks, and dispense medication. This tool was created by the USAID-funded Systems for Improved Access to Pharmaceutical and Services (SIAPS) Program implemented by Management Sciences for Health. See README below for more info.","﻿ &nbsp;&nbsp; 

# RxSolution
The SIAPS Program is funded by the U.S. Agency for International Development (USAID) under cooperative agreement AID-OAA-A-11-00021 and implemented by Management Sciences for Health. The information provided on this web site is not official U.S. Government information and does not represent the views or positions of the U.S. Agency for International Development or the U.S. Government. 

Disclaimer of warranties and limitation of liability

The RxSolution software, documentation and other products, information, materials and services provided by Management Sciences for Health (MSH or Licensor) are provided “as is.” Licensor hereby disclaims all warranties, whether express, implied, statutory or other (including all warranties arising from course of dealing, usage or trade practice), and specifically disclaims all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement. Without limiting the foregoing, licensor makes no warranty of any kind that this software or documentation, or any other licensor or third-party goods, services, technologies or materials (including any software or hardware), or any products or results of the use of any of them, will meet the users’ or other persons' requirements, operate without interruption, achieve any intended result, be compatible or work with any other goods, services, technologies or materials (including any software, hardware, system or network), or be secure, accurate, complete, free of harmful code or error-free. Licensor is not responsible for further development or any future versions of RxSolution.

Special note for developers: The source code was developed using Delphi 7 and Plugins. The QuantumGrid v3.22 is required but no longer available. This means that a manual conversion of tables from Tdx to Tcx has to be performed. Please follow the advise here from the plugin developers: https://www.devexpress.com/Support/Center/Question/Details/DQ13868/convert-from-tdxdbgrid-v3-to-the-new-v5

Copyright Management Sciences for Health.
",,2018-08-16T11:18:13Z,2,5,3,"('khoppenworth', 5), ('JulieFrye', 2)","[17, 'Partnerships for the Goals']"
WorldHealthOrganization/app,COVID-19 App,"# COVID-19 App

[![Client CI](https://github.com/WorldHealthOrganization/app/workflows/Client%20CI/badge.svg?event=push)](https://github.com/WorldHealthOrganization/app/actions?query=workflow%3A""Client+CI""+branch%3Amaster+event%3Apush)
[![Server CI](https://github.com/WorldHealthOrganization/app/workflows/Server%20CI/badge.svg?event=push)](https://github.com/WorldHealthOrganization/app/actions?query=workflow%3A""Server+CI""+branch%3Amaster+event%3Apush)

![Flutter](https://img.shields.io/badge/Framework-Flutter-3cc6fd?logo=flutter)
![Dart](https://img.shields.io/badge/Language-Dart-0c458b?logo=dart)
![Java](https://img.shields.io/badge/Language-Java-ed8217?logo=java)
![App Engine](https://img.shields.io/badge/Cloud-App%20Engine-3469ee?logo=Google%20Cloud)
![Firebase](https://img.shields.io/badge/Cloud-Firebase-f5ba23?logo=Firebase)

COVID-19 app



| Goal                                                     | Where to go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| -------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Where&nbsp;do&nbsp;I&nbsp;get&nbsp;the&nbsp;app?         | This app is currently only available in Nigeria. If you are from elsewhere you should consult the [WHO's COVID-19 website](https://www.who.int/emergencies/diseases/novel-coronavirus-2019).                                                                                                                                                                                                                                                                                                                                                                            |
| How can I help out?                                      | We are humbled at the outpouring of support we have received from all over the world and are eager to have you contribute and apologize that we are still in the early process of figuring out how to manage all the contributions. Please bear with us while we organize into smaller teams working in parallel so we can then on-board more volunteers. Until then, please keep filing issues, and fill out the [volunteer form here](https://forms.gle/FUugWvUVvMcV3dLJA). We’ll get back in touch. You can then read the [onboarding documentation](ONBOARDING.md). |
| There's a problem with the app - help!                   | File a report to help us improve. Choose ""Bug Report"" [here](https://github.com/WorldHealthOrganization/app/issues/new/choose).                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| I have an idea for the app!                              | Choose ""Feature Request"" [here](https://github.com/WorldHealthOrganization/app/issues/new/choose).                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| How do I build the app myself?                           | See the [development guide](ONBOARDING.md#development).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Who can I talk to about product, engineering, or design? | Reach out to our [functional leads](ONBOARDING.md#point-people).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| How does the app work?                                   | See our [engineering design documentation](devdesign/README.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |

## Development Builds

Please note that if you run the app from the GitHub repository, you are using a development-only build of this app not intended for public use. You agree that you have no expectation of privacy when using this build and understand that the content of the app may not have been reviewed by the World Health Organization.
","'appengine-java', 'coronavirus', 'covid-19', 'dart', 'epidemiology', 'firebase', 'flutter'",2024-02-23T22:29:52Z,59,2123,106,"('advayDev1', 187), ('brunobowden', 148), ('theswerd', 75), ('hspinks', 59), ('dependabotbot', 47), ('britannio', 38), ('patniemeyer', 33), ('epicfaace', 30), ('SamMousa', 26), ('matthewblain', 16), ('crazybob', 14), ('clementmouchet', 9), ('jasonTelanoff', 9), ('myiremark', 8), ('luigi-rosso', 6), ('deanhach', 5), ('rjhuijsman', 4), ('kassim', 4), ('sapte91', 4), ('davidkaneda', 3), ('jamesblasco', 3), ('creativecreatorormaybenot', 3), ('Moosphan', 3), ('dnfield', 3), ('cbgrey', 3), ('snyk-bot', 3), ('bezfeng', 2), ('shankari', 2), ('ShivamSinghania', 2), ('nightlark', 2), ('pjayathissa', 2), ('PieterAelse', 2), ('Henrymarks1', 2), ('ayushr2', 2), ('marctan', 1), ('mjohnsullivan', 1), ('oashrafouad', 1), ('pwicherski', 1), ('RohanTalip', 1), ('sstur', 1), ('stevan-milovanovic', 1), ('champ96k', 1), ('darcien', 1), ('anikaraghu', 1), ('areille', 1), ('devamhdz', 1), ('purpledrosophila', 1), ('Samaritan1011001', 1), ('Luccasoli', 1), ('yukuairoy', 1), ('kieranbehn', 1), ('jpelgrim', 1), ('guillermo-varela', 1), ('dhruvilp', 1), ('devoncarew', 1), ('darish', 1), ('orchardbirds', 1), ('AyushBherwani1998', 1), ('avkvirtru', 1)","[3, 'Good Health and Well-Being']"
Prescrypto/rxchain,A lightweight blockchain implementation,"
# rxchain - a Python - SQL based lightweight, cryptographically enabled, centralised blockchain implementation – Originally forked from NaiveChain and ported into Python, but no original code remains.

### Motivation
All the current implementations of blockchains are tightly coupled with the larger context and problems they (e.g. Bitcoin or Ethereum) are trying to solve. This leaves little room to implement different solutions. Especially source-code-wisely. This project is an attempt to provide a lightweight concise and simple implementation of a blockchain as possible, completely designed around electronic medical prescriptions.


### What is blockchain
[From Wikipedia](https://en.wikipedia.org/wiki/Blockchain_(database)) : Blockchain is a new database technology that maintains a continuously-growing list of records called blocks secured from tampering and revision. I encourage the reader to thoroughly understand the different key aspects of Blockchain technology form this article: https://medium.com/@sbmeunier/blockchain-technology-a-very-special-kind-of-distributed-database-e63d00781118


### Key concepts of rxchain
 *rxchain* is focused on the specifics of cryptography (which can be linked to electronic identities) and immutability, achieved by Blocks that couple prescription's merkle trees and can verify integrity easily.
* HTTP interface to control the node
* At the moment it is a centralised chain of blocks, the block's merkle root can be anchored with Proof of Existence to any particular distributed Blockchain (in a similar way to Factom's white paper) (https://github.com/FactomProject/FactomDocs/blob/master/whitepaper.md)
* At the moment data is persisted in an SQL implementation
* Access to the database is enabled by Asymetric Cryptography
* Proof-of-work or proof-of-stake: This is the next step, a proof of work is a useful way to stop fake data from being created
* After proof of work has been developed, a distributed version can be built

### Quick start
(set up node and mine 1 block)
```
vagrant up
get server running and start creating stuff
vagrant ssh

$ cd /vagrant/prescryptchain
$ python manage.py migrate
$ python manage.py loaddata ./fixtures/initial_data.json
$ python manage.py runserver [::]:8000
```


### HTTP API
##### Get blockchain
```
curl http://localhost:8080/api/v1/block
```
##### Create block
```
# Public Key is an binari exadecimal representation  of publick_key made by rsa python library
curl -X POST \
  http://127.0.0.1:8000/api/v1/rx-endpoint/ \
  -H 'Content-Type: application/json' \
  -d '{
  ""diagnosis"": ""Diagnostico de Ojo Irritado"",
  ""location"": ""México, CDMX"",
  ""medic_cedula"": ""465713"",
  ""medic_hospital"": ""Privado"",
  ""medic_name"": ""Juan Alberto Torres García"",
  ""medications"": [
    {
      ""instructions"": ""Artelac RDules"",
      ""presentation"": ""DUSTALOX (KETOROLACO TROMETAMINA  5 mg / ml 1 SOL 5 ml)""
    }
  ],
  ""patient_age"": 29,
  ""patient_name"": ""Jesus"",
  ""public_key"": ""63636f70795f7265670a5f7265636f6e7374727563746f720a70310a28637273612e6b65790a5075626c69634b65790a70320a635f5f6275696c74696e5f5f0a6f626a6563740a70330a4e745270340a284c373435313530383630343332393237323237393336343532383430323735313630383337373839333331383033363932383838383034323630323635393130383336383335353931353533323533343238353732343832333830373537333939343637313337383133363633313537303432363933373330313136353533373433333638333830333634393839383937363238373033343934394c0a4936353533370a74622e"",
  ""timestamp"": ""2018-02-01T21:59:19.454752""
}'

```
",,2018-10-29T20:44:57Z,9,8,4,"('xtornasol512', 350), ('ebarojas', 68), ('mevangelista-alvarado', 35), ('lhartikk', 5), ('lukaswelte', 3), ('valerybriz', 3), ('imoutaharik', 2), ('vash512', 2), ('elahmo', 1)","[3, 'Good Health and Well-Being']"
project-callisto/callisto-core,"Report intake, escrow, matching and secure delivery code for Callisto, an online reporting system for sexual assault.","# callisto-core

| Status | Support |
| --- | --- |
| [![Build Status][build-image]][build-url] | ![python 3.6][python36] |
| [![PyPI Version][pypi-image]][pypi-url] | ![django 1.11][django111] |
| [![Code Climate](https://api.codeclimate.com/v1/badges/eed2b78a9c9cbf80e7af/maintainability)](https://codeclimate.com/github/project-callisto/callisto-core/maintainability) | - |
| [![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://dashboard.heroku.com/new?template=https://github.com/project-callisto/callisto-core/&env\[DJANGO_SETTINGS_MODULE\]=callisto_core.utils.settings_live) | - |

[build-image]: https://travis-ci.org/project-callisto/callisto-core.svg?branch=master
[build-url]: https://travis-ci.org/project-callisto/callisto-core

[pypi-image]: https://img.shields.io/pypi/v/callisto-core.svg
[pypi-url]: https://pypi.python.org/pypi/callisto-core

[climate-image]:https://codeclimate.com/github/project-callisto/callisto-core/badges/gpa.svg
[climate-url]:https://codeclimate.com/github/project-callisto/callisto-core

[python36]: https://img.shields.io/badge/python-3.6-green.svg
[django111]: https://img.shields.io/badge/django-1.11-yellowgreen.svg

This is the report intake, escrow, matching and secure delivery code for [Callisto](https://www.projectcallisto.org), an online reporting system for sexual assault.

 Learn more about how Callisto works

Learn more about information escrows

### Support us

[Donate](https://www.projectcallisto.org/donate/) to Sexual Health Innovations, the organization behind Callisto.

Our current list of issues is available here https://github.com/SexualHealthInnovations/callisto-core/issues

There's documentation on [running a developement version](https://github.com/SexualHealthInnovations/callisto-core/blob/master/docs/DEVELOPEMENT.md) of this repository, and on [using callisto-core as a python package](https://github.com/SexualHealthInnovations/callisto-core/blob/master/docs/USAGE.md).

Finally there's a live heroku testing application to give you a feeling for how Callisto operates https://callisto-core.herokuapp.com/

Read more about this on our [contribution guide](https://github.com/SexualHealthInnovations/callisto-core/blob/master/docs/CONTRIBUTING.md).

Want to talk about extending the code to other applications, how Callisto works, or schedule a demo of Callisto for your campus? Contact us at [tech@sexualhealthinnovations.org](mailto:tech@sexualhealthinnovations.org).
","'django', 'escrow', 'report-intake'",2023-06-30T22:18:41Z,23,138,22,"('coilysiren', 2532), ('kelseyq', 335), ('e-lisa', 329), ('requires', 236), ('divergentdave', 48), ('asabine', 20), ('kevindaum', 12), ('ALawley', 11), ('Not-A-Sith-Lord', 9), ('compwron', 5), ('mstbbs', 4), ('lojikil', 4), ('dodgeblaster', 4), ('sunahfloship', 3), ('agude', 2), ('willingc', 2), ('jisantuc', 2), ('justingeeslin', 2), ('keisetsu', 1), ('scott-callisto', 1), ('gitter-badger', 1), ('callisto-bot', 1), ('hellerve', 1)","[5, 'Gender Equality']"
PopulateTools/gobierto,Plataforma de gobierno abierto open source,"[![CircleCI](https://circleci.com/gh/PopulateTools/gobierto.svg?style=svg)](https://circleci.com/gh/PopulateTools/gobierto)
[![codecov](https://codecov.io/gh/PopulateTools/gobierto/branch/master/graph/badge.svg)](https://codecov.io/gh/PopulateTools/gobierto)




_This README is available [in English](README_EN.md)_

Gobierto es una plataforma de gobierno abierto de código libre. Ofrece herramientas de transparencia, participación y rendición de cuentas para administraciones públicas y otras organizaciones que quieran abrirse.

* [Filosofía y principios de diseño](#filosofía-y-principios-de-diseño)
* [Qué ofrece](#qué-ofrece)
* [Características técnicas](#características-técnicas)
* [Quién lo usa](#quién-lo-usa)
* [Quiénes somos](#quiénes-somos)
* [Comienza a usarlo](#comienza-a-usarlo)
* [Síguenos](#síguenos)
* [Manuales](#manuales)
* [Instalación, desarrollo y contribuciones](#instalación-desarrollo-y-contribuciones)
* [Licencia](#licencia)

## Filosofía y principios de diseño

- **Modular y eficiente**: uno de los objetivos de Gobierto es conseguir una implementación eficiente - puedes arrancar usando un solo módulo, e ir activando y personalizando nuevos módulos poco a poco. No necesitas instalar una aplicación diferente para cada servicio que quieras poner en marcha, con el consiguiente ahorro de tiempo y dinero y la mejora de la experiencia del usuario.
- **Foco en el uso**: Los fundadores del proyecto tenemos una larga trayectoria en el diseño de productos digitales y aplicamos
- **Abierto e interoperable**: Gobierto está diseñado para conectarse con sistemas externos - ej. portales de open data para reutilizar la información presupuestaria, sistemas de SSO para identificar y verificar a los usuarios en procesos participativos, las agendas de cargos y directivos, etc.
- **Transparencia legalista vs. transparencia útil**: Gobierto da respuesta a las exigencias de la Ley de Transparencia, pero no se para en cumplir una lista de requisitos, si no de ofrecer utilidad real para los ciudadanos y permitir a las administraciones conectar con ellos.
- **Involucración progresiva**: todavía no existe un modelo mental  sobre transparencia y participación. No hay expectativas claras ni sabemos qué esperar si participamos. Tan importante es poner en marcha iniciativas, como conseguir comunicarlas e involucrar a los destinatarios. Y pensamos que esto se consigue ofreciendo mecanismos de involucración progresiva.
- **Transición off-on**: La participación offline especialmente a nivel local tiene muchos años de trayectoria. Pensamos que las herramientas online deben ser una extensión, apoyo y complemento de procesos y metodologías que ya pueden estar en marcha.
- **Largo plazo**: pensamos que el camino hacia la transparencia y la participación online es un largo recorrido que apenas hemos comenzado. Las iniciativas a poner en marcha ahora no serán las mismas que dentro de 10 años. Nos configuramos para estar aquí entonces.

## Qué ofrece

- **Visualización de presupuestos**: haz tus presupuestos comprensibles por la ciudadanía y permite seguirlos.
- **Personas y agendas**: muestra el organigrama de tu organización y los datos clave de las personas (biografía, curriculum, declaraciones de bienes y actividades...). Sincroniza las agenda de los cargos, modera el contenido, y permite hacer un seguimiento de los eventos.
- **Indicadores estadísticos - Tu entidad en cifras**: Visualiza de forma comprensible los principales indicadores estadísticos de tu municipio - población, empleo, economía, criminalidad...
- **Consultas sobre presupuestos**: consulta a tus vecinos y consigue opiniones sobre los temas que más les preocupan para _informar_ tus decisiones presupuestarias y aumentar tu legitimidad.
- **Participación**: Diseña procesos de participación para que el _online_ sea una extensión y complemento de tus iniciativas actuales. Herramientas de debate, consultas, votación, anotación de textos...
- **Legislación colaborativa**: involucra a los ciudadanos en la redacción o modificación de normativa.
- **Plan de gobierno y objetivos**: Permite explorar tu programa de gobierno en proyecto clasificados por áreas, ámbitos... Haz rendición de cuentas informando periódicamente sobre el progreso  en su implementación.
- **Mapas**: a veces no es fácil disponer de un visor GIS simple de usar. Gobierto se conecta a tu open data para ofrecerte un visor web GIS que se puede usar.
- **Comparador de presupuestos**: compara, contextualiza y analiza el presupuesto de entidades equivalentes (de un conjunto de municipios, por ejemplo)
- ... y más que irán llegando. ¿Tienes algo en la cabeza? ¡Hablemos!


Funcionalidades transversales:

- **CRM (Citizen Relationship Manager)**: Permite a los usuarios suscribirse a cualquier contenido y recibir resumen de las novedades: cuando se actualicen los presupuestos, cuando haya nuevos eventos en las agendas, cuando un cargo actualice su declaración de bienes, cuando se publiquen resultados de un plan de acción... Alertas personalizables por temáticas y ámbitos geográficos.
- **SSO (Single Sign On) y conexión con censo**
- **Gestión de usuarios**
- **Personalización de sitios**


## Características técnicas

- Desarrollado con Ruby on Rails, Postgresql, SASS
- Multientidad: una misma instalación da soporte a sites independientes.
- Responsive: diseñado para funcionar en desktop, móvil, tableta...
- Modo SAAS: como parte de nuestra oferta de servicios profesionales, te lo ofrecemos en modo servicio para hacer todavía más eficiente y económica la puesta en marcha.

## Quién lo usa

Estas son algunas de las organizaciones que usan Gobierto:

* [Diputación de Valencia](https://altoscargos.go.dival.es/cargos-y-agendas)
* [Sencelles](https://gobierto.es/blog/20170314-sencelles-consultas.html)
* [Generalitat de Catalunya](https://gobierto.es/blog/20170126-generalitat-catalunya.html)
* [Sant Feliu de Llobregat](https://pressupost.santfeliu.cat/)
* [Mataró](https://pressupost.mataro.cat/)
* [Sant Boi de Llobregat](https://agendes.santboi.cat/)
* [Getafe](https://gobiernoabierto.getafe.es/)

## Quiénes somos

Gobierto es un producto de Populate, un estudio de diseño y tecnología entorno al civic engagement. Más información en nuestra web: [populate.tools](https://populate.tools)

## Comienza a usarlo

* Puedes instalar Gobierto y personalizarlo para tu organización: [Instalación](#instalación) (inglés). Si te encuentras con alguna dificultad, [crea una _issue_](issues/new).
* Populate ofrece servicios comerciales para la implementación de Gobierto. Si nos necesitas, [danos un toque](https://populate.tools/es/about/#como-trabajamos).


## Síguenos

- Twitter: [@gobierto](https://twitter.com/gobierto) & [@populate_](https://twitter.com/populate_)
- Web + Blog: [gobierto.es](https://gobierto.es)

## Manuales y documentación

Toda la documentación está disponible en https://gobierto.readme.io

## Desarrollo

- Contribuciones: Si quieres realizar aportaciones a Gobierto, lee [cómo colaborar](CONTRIBUTING_EN.md)


## Licencia

Software publicado bajo la licencia de código abierto AFFERO GPL v3 (ver [LICENSE-AGPLv3.txt](https://github.com/PopulateTools/gobierto/blob/master/LICENSE-AGPLv3.txt))
","'citizen', 'civic-tech', 'data-visualisation', 'open-data', 'open-government', 'transparency'",2024-05-01T05:38:01Z,19,71,11,"('entantoencuanto', 2970), ('Crashillo', 1764), ('jorgeatgu', 1670), ('ferblape', 1555), ('apradillap', 930), ('depfubot', 918), ('amiedes', 832), ('danguita', 429), ('martgnz', 404), ('furilo', 374), ('eltercero', 168), ('dependabotbot', 88), ('stbnrivas', 71), ('amaia', 34), ('bertocq', 7), ('javierarce', 4), ('aramollis', 3), ('lilxelo', 1), ('flachica', 1)","[16, 'Peace, Justice and Strong Institutions']"
openfoodfacts/openfoodfacts-server,"Open Food Facts database, API server and web interface - 🐪🦋 Perl, CSS and JS coders welcome 😊 For helping in Python, see Robotoff or taxonomy-editor","
  
  
  



# Open Food Facts - Product Opener (Web Server)

[![Project Status](http://opensource.box.com/badges/active.svg)](http://opensource.box.com/badges)
[![Crowdin](https://d322cqt584bo4o.cloudfront.net/openfoodfacts/localized.svg)](https://translate.openfoodfacts.org/)
[![Open Source Helpers](https://www.codetriage.com/openfoodfacts/openfoodfacts-server/badges/users.svg)](https://www.codetriage.com/openfoodfacts/openfoodfacts-server)
[![Backers on Open Collective](https://opencollective.com/openfoodfacts-server/backers/badge.svg)](#backers)
[![Sponsors on Open Collective](https://opencollective.com/openfoodfacts-server/sponsors/badge.svg)](#sponsors)
![GitHub language count](https://img.shields.io/github/languages/count/openfoodfacts/openfoodfacts-server)
![GitHub top language](https://img.shields.io/github/languages/top/openfoodfacts/openfoodfacts-server)
![GitHub last commit](https://img.shields.io/github/last-commit/openfoodfacts/openfoodfacts-server)
![Github Repo Size](https://img.shields.io/github/repo-size/openfoodfacts/openfoodfacts-server)

## Tests

[![Pull Requests](https://github.com/openfoodfacts/openfoodfacts-server/actions/workflows/pull_request.yml/badge.svg)](https://github.com/openfoodfacts/openfoodfacts-server/actions/workflows/pull_request.yml)

## What is Product Opener?

**Product Opener** is the server software for **Open Food Facts** and **Open Beauty Facts**. It is released under the AGPL license and is being developed in Perl, HTML and JavaScript as [Free and Open-Source Software](https://en.wikipedia.org/wiki/Free_and_open-source_software).

It works together with [Robotoff](https://github.com/openfoodfacts/robotoff), Open Food Facts' AI system (in Python, which can also be installed locally) and the [Open Food Facts App](https://github.com/openfoodfacts/smooth-app) (which can work with your local instance after enabling dev mode)

## What is Open Food Facts?

### A food product database

Open Food Facts is a database of food products with ingredients, allergens, nutritional facts and all the tidbits of information that is available on various product labels.

### Made by everyone

Open Food Facts is a non-profit association of volunteers. 25,000+ contributors like you have added 1.7 million+ products from 150 countries using our Android, iPhone and Windows Phone apps or their camera to scan barcodes and upload pictures of products and their labels.

### For everyone

Data about food is of public interest and has to be open (i.e available to everyone). The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own!


Visit the [website](https://world.openfoodfacts.org) for more info.

## Weekly Meetings

- We e-meet on Mondays at 17:00 Paris Time (Europe/Paris), which is CET (UTC+1) or CEST (UTC+2 during Daylight Saving Time). For easy conversion in local time, check [17:00 in Paris](https://time.is/1700_in_Paris).
- ![Google Meet](https://img.shields.io/badge/Google%20Meet-00897B?logo=google-meet&logoColor=white) Video call link: https://meet.google.com/nnw-qswu-hza
- Join by phone: https://tel.meet/nnw-qswu-hza?pin=2111028061202
- Add the event to your calendar by [adding the Open Food Facts community calendar to your calendar](https://wiki.openfoodfacts.org/Events).
- [Weekly agenda](https://drive.google.com/open?id=1LL8-aiSF482xaJ1o0AKmhXB5QWfVE0_jzvYakq3VXys): please add the Agenda items as early as you can. 
- Make sure to check the agenda items in advance of the meeting, so that we have the most informed discussions possible. 
- The meeting will handle agenda items first, and if time permits, collaborative bug triage.
- We strive to timebox the core of the meeting (decision making) to 30 minutes, with an optional free discussion/live debugging afterwards.
- We take comprehensive notes in the weekly agenda of agenda item discussions and of decisions taken.

## Feature Sprint 
- We use feature-based sprints, [tracked here](https://github.com/orgs/openfoodfacts/projects/32)

## User Interface
-  [![Figma](https://img.shields.io/badge/figma-%23F24E1E.svg?logo=figma&logoColor=white) Mockups on the current design and future plans to discuss](https://www.figma.com/file/Qg9URUyrjHgYmnDHXRsTTB/New-website-design-(2022)-(Quentin)?t=00ZMlgxe590W8TRY-0)

## Priorities
- [Top issues](https://github.com/openfoodfacts/openfoodfacts-server/issues/7374)
- [P1 problems](https://github.com/openfoodfacts/openfoodfacts-server/labels/P0,P1)
- [P1 candidates](https://github.com/openfoodfacts/openfoodfacts-server/labels/P1%20candidate)

## How do I get started?
- Join us on slack at  in the channels: `#api`, `#productopener`, `#dev`.
- [API v2 documentation (WIP)](https://openfoodfacts.github.io/openfoodfacts-server/api/ref-v2/) ([source](https://github.com/openfoodfacts/openfoodfacts-server/tree/main/docs/api/ref/api.yml))
- Developer documentation:
  - To start coding, head to the [Quick start guide (docker)](./docs/dev/how-to-quick-start-guide.md)
  - Additional documentation
    - [Server documentation](https://openfoodfacts.github.io/openfoodfacts-server/)
    - [Developer guide (docker)](./docs/dev/how-to-develop-using-docker.md)
    - [Developer guide (gitpod)](./docs/dev/how-to-use-gitpod.md)
    - Configuration [TBA]
    - Dependencies [TBA]
    - Database configuration [TBA]
    - How to run tests [TBA]
    - [Perl modules documentation (POD)](https://openfoodfacts.github.io/dev/ref-perl/)


**Note:** Documentation follows the [Diátaxis Framework](https://diataxis.fr/).

## Contribution Guidelines

If you're new to Open-Source, we recommend you to check out our [_Contributing Guidelines_](https://github.com/openfoodfacts/openfoodfacts-server/blob/master/CONTRIBUTING.md). Feel free to fork the project and send us a pull request.

- Writing tests
- Code review

Please add new features to the `CHANGELOG.md` file before or after merge to make testing easier

## Reporting problems or asking for a feature

Have a bug or a feature request? Please search for existing and closed issues. If your problem or idea is not addressed yet, please [open a new issue](https://github.com/openfoodfacts/openfoodfacts-server/issues). You can ask directly in the discussion room if you're not sure.

## Translate Open Food Facts in your language

You can help translate the Open Food Facts web version and the app at:
 (no technical knowledge required, takes a minute to signup).

## Helping with HTML and CSS

We have [templatized](https://github.com/openfoodfacts/openfoodfacts-server/tree/master/templates) Product Opener, we use Gulp and NPM, but you'll need to run the Product Opener docker to be able to see the result (see the How do I get set started? section).
In particular, you can [help with issues on the new design](https://github.com/openfoodfacts/openfoodfacts-server/issues?q=is%3Aissue+is%3Aopen+label%3A%22new+design%22).

## Who do I talk to?

Join our discussion room at . Make sure to join the `#productopener` and `#productopener-alerts` channels. Stéphane, Pierre, Charles or Hangy will be around to help you get started.

## Contributors

This project exists thanks to all the people who contribute.


## Backers

Thank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/openfoodfacts-server#backer)]



## Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor](https://opencollective.com/openfoodfacts-server#sponsor)














Open Food Facts Personal Search project was funded through the NGI0 Discovery Fund,
a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme.
","'allergens', 'allergies', 'crowdsourcing', 'database', 'environment', 'food', 'food-products', 'hacktoberfest', 'nutrition', 'perl', 'recycling'",2024-05-03T15:01:47Z,186,599,33,"('teolemon', 19093), ('stephanegigandet', 5533), ('aleene', 2910), ('hangy', 1092), ('moon-rabbitOFF', 557), ('svensven', 460), ('dependabotbot', 399), ('alexgarel', 260), ('VaiTon', 210), ('AcuarioCat', 204), ('roshnaeem', 171), ('github-actionsbot', 163), ('Ban3', 148), ('benbenben2', 144), ('dependabot-previewbot', 129), ('dipietroR', 122), ('CharlesNepote', 118), ('areeshatariq', 112), ('raphael0202', 68), ('lan33', 56), ('jolesh', 53), ('Roto15', 48), ('mrmd8', 48), ('NerOcrO', 45), ('Syronanm', 34), ('bradfora', 32), ('openfoodfacts-bot', 31), ('OpenChris', 29), ('yuktea', 28), ('MonalikaPatnaik', 26), ('duhow', 24), ('mahlzahn', 24), ('pauamma', 18), ('laralem', 18), ('john-gom', 16), ('blazern', 15), ('Roxie-32', 15), ('BathoryPeter', 15), ('DanieliusAsm', 14), ('CloCkWeRX', 14), ('ArturLange', 13), ('rbournhonesque', 13), ('maddingue', 13), ('syl10100', 13), ('albatrousse', 12), ('yarons', 12), ('GendelfLugansk', 11), ('Teiron', 9), ('jnsereko', 9), ('TaciteOFF', 8), ('crowdin-bot', 8), ('K13b3r', 7), ('himanshisrestha', 7), ('bernardlemoullec', 6), ('manon-corneille', 6), ('Naruyoko', 6), ('pratyaksh1610', 6), ('Valimp', 6), ('mariali1', 6), ('offvince', 6), ('Steinhagen', 5), ('muskangarg21', 5), ('TheSussex', 5), ('pradumna-gautam', 5), ('KommX', 5), ('deveshidwivedi', 5), ('fuddl', 4), ('zond', 4), ('tur-ium', 4), ('sebeeek', 4), ('sandy9999', 4), ('Strubbl', 4), ('MaximeLaurenty', 4), ('Lunique', 4), ('jayaddison', 4), ('eric-nguyen-cs', 4), ('Annieieieie', 4), ('sultanowski', 4), ('Makitsu', 3), ('teleboas', 3), ('alemit', 3), ('TCatinaud', 3), ('sumit-158', 3), ('zigouras', 3), ('diivi', 3), ('astagi', 3), ('tkbremnes', 2), ('crapitea', 2), ('alexouille123', 2), ('bertrand-amaraggi', 2), ('borrokk', 2), ('fmarghi', 2), ('k2s', 2), ('sgtSeme4ki', 2), ('vanshikaarora', 2), ('alex-marty', 2), ('damil', 2), ('snyk-bot', 2), ('aadarsh-ram', 2), ('aadak99', 2), ('Aman-Jamshed', 2), ('Vicam', 2), ('BryanH01', 2), ('kant', 2), ('Etatdesprit', 2), ('Jecrivaine', 2), ('monkeywithacupcake', 2), ('leventgunay', 2), ('M123-dev', 2), ('pilou-', 2), ('priyanka0906', 2), ('tchen0125', 1), ('aleksejrs', 1), ('Victor-Hermes', 1), ('ditsuke', 1), ('TechShooter', 1), ('Tanujgarg37', 1), ('smonff', 1), ('simonj2', 1), ('SantosSi', 1), ('shivangi3001', 1), ('shamun-khatri', 1), ('Sakshamgupta90', 1), ('PrajwalM2212', 1), ('archanox', 1), ('petervdv', 1), ('maxymus-1', 1), ('Payne680', 1), ('Cypresslin', 1), ('oyenuga17', 1), ('omerfaruk-cakmak', 1), ('omahs', 1), ('odin-h', 1), ('noatime', 1), ('monsieurtanuki', 1), ('leonidlednev', 1), ('laulowen', 1), ('k3lyan', 1), ('jusfla', 1), ('jasmeet0817', 1), ('jalanyash', 1), ('helloworldtest123', 1), ('gospodin55', 1), ('emmapeel2', 1), ('danwyk', 1), ('code-a1', 1), ('anaritadauane', 1), ('andrewhuanggg', 1), ('HimajPatil', 1), ('tachylatus', 1), ('goverdhan07', 1), ('hrabkin', 1), ('GabrielBeFr', 1), ('srflp', 1), ('ferb7o2', 1), ('FemmeNoire', 1), ('aquilax', 1), ('eseyman', 1), ('g123k', 1), ('DhruvAwasthi', 1), ('dennisahlqvist', 1), ('davidoskky', 1), ('Cutypareek', 1), ('c-schuhmann', 1), ('cquest', 1), ('lucaa', 1), ('advaitathreya', 1), ('olivier5741', 1), ('nicolasleger', 1), ('ms10398', 1), ('ItshMoh', 1), ('mike-lu1', 1), ('Lampone', 1), ('elbeho', 1), ('Luzifer', 1), ('Kezxo', 1), ('Karljoones', 1), ('jhutchings1', 1), ('JulienPalard', 1), ('JulieMaricicDetweiler', 1), ('JohnNilsson', 1), ('prometheas', 1), ('JieDiscovery', 1), ('jaens', 1), ('JBelcoco', 1), ('HummingBrid', 1)","[12, 'Responsible Consumption and Production']"
openclimatedata/global-carbon-budget,Global Carbon Budget Data Package,"The Global Carbon Budget is an annual living data publication of carbon cycle
sources and sinks, generated from multiple data sources and by multiple
organisations and research groups.

This [Data Package](http://frictionlessdata.io/specs/data-package/) makes the data from the 2019 Global Carbon Budget and National Emissions [Excel files](https://www.icos-cp.eu/GCP/2019) v1.0 available as CSV files. For updates of the original data and further information refer to the
[Global Carbon Budget](http://www.globalcarbonproject.org/carbonbudget/index.htm) website.

Maintainer of this Data Package is Robert Gieseke ().
See below for [license information](#license)

## Data

### Global Carbon Budget

[Notes and Methods](doc/global-carbon-budget.md)

[global-carbon-budget.csv](data/global-carbon-budget.csv)


### Fossil fuel and cement production emissions by fuel type

[Notes and Methods](doc/fossil-fuel-cement.md)

[fossil-fuel-cement.csv](data/fossil-fuel-cement.csv)

[fossil-fuel-cement-per-capita.csv](data/fossil-fuel-cement-per-capita.csv)


### Land-use change emissions

[Notes and Model References](doc/land-use-change.md)

[land-use-change.csv](data/land-use-change.csv)


### Ocean CO2 sink (positive values represent a flux from the atmosphere to the ocean)

[Notes and Model References](doc/ocean-sink.md)

[ocean-sink.csv](data/ocean-sink.csv)


### Terrestrial CO2 sink (positive values represent a flux from the atmosphere to the land)

[Notes and Model References](doc/terrestrial-sink.md)

[terrestrial-sink.csv](data/terrestrial-sink.csv)


### Historical CO2 budget

[Notes and References](doc/historical-budget.md)

[historical-budget.csv](data/historical-budget.csv)


### Territorial Emissions

[Notes and Methods](doc/territorial-emissions.md)

[territorial-emissions.csv](data/territorial-emissions.csv)


### Consumption Emissions GCB

[Notes and Methods](doc/consumption-emissions.md)

[consumption-emissions.csv](data/consumption-emissions.csv)


### Emissions Transfers GCB

[Notes and Methods](doc/emissions-transfers.md)

[emissions-transfers.csv](data/emissions-transfers.csv)



### Country Definitions

Details of the geographical information corresponding to countries and regions used in this database for Consumption and Transfer emissions

[country-definitions.csv](data/country-definitions.csv)

## Preparation

To update or regenerate the CSV files the following steps need to be run:

```
make clean
```

```
make
```

To validate the Data Package:
```
make validate
```


## Notes

The *Global Carbon Budget* data is written to CSV files using the
accuracy used for display in the original Excel, or one digit more files
assuming this to be the implied precision.

The *National Emissions* are written to CSV files with three significant digits
as this is the accuracy used for the CDIAC data in the Excel file, thus
rounding the numbers derived from splitting up countries or using trend data as
with BP emissions data.

If other accuracy is needed adjust the processing scripts accordingly.

## License

The Global Carbon Budget [data page](http://www.globalcarbonproject.org/carbonbudget/19/data.htm) states:

> The use of data is conditional on citing the original data sources. Full details on how to cite the data are given at the top of each page. For research projects, if the data are essential to the work, or if an important result or conclusion depends on the data, co-authorship may need to be considered. The Global Carbon Project facilitates access to data to encourage its use and promote a good understanding of the carbon cycle. Respecting original data sources is key to help secure the support of data providers to enhance, maintain and update valuable data.

The primary reference for the full Global Carbon Budget 2019 is:

Global Carbon Budget 2019, by Pierre Friedlingstein, Matthew W. Jones, Michael O’Sullivan, Robbie M. Andrew, Judith Hauck, Glen P. Peters, Wouter Peters, Julia Pongratz, Stephen Sitch, Corinne Le Quéré, Dorothee C. E. Bakker, Josep G. Canadell, Philippe Ciais, Rob Jackson, Peter  Anthoni, Leticia Barbero, Ana Bastos, Vladislav Bastrikov, Meike Becker, Laurent Bopp, Erik Buitenhuis, Naveen Chandra, Frédéric Chevallier, Louise P. Chini, Kim I. Currie, Richard A. Feely, Marion Gehlen, Dennis Gilfillan, Thanos Gkritzalis, Daniel S. Goll, Nicolas Gruber, Sören Gutekunst, Ian Harris, Vanessa Haverd, Richard A. Houghton, George Hurtt, Tatiana Ilyina, Atul K. Jain, Emilie Joetzjer, Jed O. Kaplan, Etsushi Kato, Kees Klein Goldewijk, Jan Ivar Korsbakken, Peter Landschützer, Siv K. Lauvset, Nathalie Lefèvre, Andrew Lenton, Sebastian Lienert, Danica Lombardozzi, Gregg Marland, Patrick C. McGuire, Joe R. Melton, Nicolas Metzl, David R. Munro, Julia E. M. S. Nabel, Shin-Ichiro Nakaoka, Craig Neill, Abdirahman M. Omar, Tsuneo Ono, Anna Peregon, Denis Pierrot, Benjamin Poulter, Gregor Rehder, Laure Resplandy, Eddy Robertson, Christian Rödenbeck, Roland Séférian, Jörg Schwinger, Naomi Smith, Pieter P. Tans, Hanqin Tian, Bronte Tilbrook, Francesco N Tubiello, Guido R. van der Werf, Andrew J. Wiltshire, and Sönke Zaehle (2019), Earth System Science Data, 11, 1783-1838, 2019, 

Otherwise please refer as:

Global Carbon Project. (2019). Supplemental data of Global Carbon Budget 2019 (Version 1.0) [Data set]. Global Carbon Project. https://doi.org/10.18160/gcp-2019

or

Global Carbon Project (2019) Carbon budget and trends 2019.  published on 4 December 2019, along with any other original peer-reviewed papers and data sources as appropriate.

See also the [Global Carbon Budget Publications](http://www.globalcarbonproject.org/carbonbudget/19/publications.htm) page.

The source code in `scripts` and the metadata in this Data Package itself are released under a
[CC0 Public Dedication License](https://creativecommons.org/publicdomain/zero/1.0/).
",'data-package',2021-04-10T06:48:12Z,2,57,11,"('rgieseke', 92), ('swillner', 1)","[13, 'Climate Action']"
Coinsence/coinsence-monorepo,Coinsence project monorepo - includes our smart contracts,"# coinsence-monorepo ![GitHub](https://img.shields.io/github/license/Coinsence/coinsence-monorepo.svg) [![CircleCI](https://circleci.com/gh/Coinsence/coinsence-monorepo/tree/master.svg?style=svg)](https://circleci.com/gh/Coinsence/coinsence-monorepo/tree/master) [![Build Status](https://travis-ci.org/Coinsence/coinsence-monorepo.svg?branch=master)](https://travis-ci.org/Coinsence/coinsence-monorepo) [![Coverage Status](https://coveralls.io/repos/github/Coinsence/coinsence-monorepo/badge.svg?branch=master)](https://coveralls.io/github/Coinsence/coinsence-monorepo?branch=master)
Coinsence project monorepo - includes our smart contracts

This repository contains the following apps:

- **[Space](apps/space)**: Deploy and manage coinsence spaces
- **[Coin](apps/coin)**: Deploy and manage coinsence coins
- **[Member](apps/member)**: Manage coinsence members accounts
- **[CoinsenceKit](apps/coinsence-kit)**: Deploy and manage coinsence DAO

## Local development chain

For local development it is recommended to use 
[ganache](http://truffleframework.com/ganache/) to run a local development 
chain. Using the ganache simulator no full Ethereum node is required.

We use the default aragon-cli devchain command to confgure and run a local 
development ganache.

    $ npm run devchain (or aragon devchain)

To clear/reset the chain use: 

    $ npm run devchain -- --reset (or aragon devchain --reset)

### Development Setup

Node.js LTS or greater required.

- Note: @aragon/cli and truffle npm deps are automatically installed when bootstrapping.

```bash
# Bootstrap project dependencies:
$ npm i

# Run an Ethereum node and ipfs
$ npm run devchain

# Deploy each app to the devchain
$ npm run deploy:apps

# Deploy a new CoinsenceKit and create a new DAO with the latest app versions
$ npm run deploy:kit
$ npm run deploy:dao

# Run all tests
$ npm run test

# Run single app tests
$ npm run test:space

# Run coverage
$ npm run coverage

# current app name aliases: {space, coin, member}
```

## Contracts architecture

Contracts are organized in independent apps (see `/apps`) and are developed 
and deployed independently. Each app has a version and can be ""installed"" 
on the Coinsence DAO independently.

![](docs/architecture/draw-io-dao-architecture-2.png)

A DAO can be deployed using the `apps/coinsence-kit/scripts/deploy-kit.js` script or with the 
`npm run deploy:dao` command. This deploys a new Kredits DAO, installs
the latest app versions and sets the required permissions.

See each app in `/apps/*` for details.

## Helper scripts

`scripts/` contains some helper scripts to interact with the contracts from the
CLI. _At some point these should be moved into a real nice CLI._

To run these scripts use `truffle exec`. For example: `truffle exec
scripts/add-proposal.js`.

### current-address.js

Prints all known DAO addresses and the DAO address for the current network

    $ truffle exec scripts/current-address.js
    or
    $ npm run dao:address

### deploy-kit.js

Deploys a new CoinsenceKit that allows to create a new DAO

    $ cd apps/coinsence-kit && truffle exec scripts/deploy-kit.js
    or
    $ npm run deploy:kit

`ENS` address is required as environment variable.  

### new-dao.js

Creates and configures a new DAO instance.

    $ cd apps/coinsence-kit && aragon contracts exec scripts/new-dao.js 
      Arguments:
        --spaceName     Space name
        --ipfs          IPFS hash for space description
        --coinName      Coin name
        --coinSymbol    Coin symbol         
        --coinDecimals  Coin decimals
        --root          Owner address
    or
    $ npm run deploy:dao

CoinsenceKit address is load from `lib/addresses/CoinsenceKit.json` or can be 
configured through the `COINSENCE_KIT` environment variable.

### deploy-apps.sh

Runs `npm install` for each app and publishes a new version.

    $ ./scripts/deploy-apps.sh
    or
    $ npm run deploy:apps

## Deployment

### Apps deployment

To deploy a new app version run:

    $ aragon apm publish major --environment=ENVIRONMENT_TO_DEPLOY

### CoinsenceKit

deploy the CoinsenceKit as Kit to create new DAOs

    $ aragon contracts exec apps/coinsence-kit/scripts/deploy-kit.js --environment=ENVIRONMENT_TO_DEPLOY

### Creating a new DAO

make sure all apps and the CoinsenceKit are deployed, then create a new DAO:

    $ aragon contracts exec apps/coinsence-kit/scripts/new-dao.js --environment=ENVIRONMENT_TO_DEPLOY
    --spaceName=SPACE_NAME --ipfs=IPFS_HASH --coinName=COIN_NAME --coinSymbol=COIN_SYMBOL --coinDecimals=COIN_DECIMALS --root=OWNER_ADDRESS

## ACL / Permissions

## Upgradeable contracts

We use aragonOS for upgradeablity of the different contracts.
Refer to the [aragonOS upgradeablity documentation](https://hack.aragon.org/docs/upgradeability-intro) 
for more details.

### Example

1. Setup (see ###Development Setup)
    1. Deploy each contract/apps (see `/apps/*`)
    2. Create a new DAO (see /apps/coinsence-kit/scripts/deploy-kit.js)
2. Update
    1. Deploy a new Version of the contract/app (see `/apps/*`)
    2. Use the `aragon dao upgrade` command to ""install"" the new version for the DAO
      (`aragon dao upgrade  `)

## Issues

If you come across an issue with Coinsence apps, do a search in the [Issues](https://github.com/Coinsence/coinsence-monorepo/issues) tab of this repo to make sure it hasn't been reported before. Follow these steps to help us prevent duplicate issues and unnecessary notifications going to the many people watching this repo:

- If the issue you found has been reported and is still open, and the details match your issue, give a ""thumbs up"" to the relevant posts in the issue thread to signal that you have the same issue. No further action is required on your part.
- If the issue you found has been reported and is still open, but the issue is missing some details, you can add a comment to the issue thread describing the additional details.
- If the issue you found has been reported but has been closed, you can comment on the closed issue thread and ask to have the issue reopened because you are still experiencing the issue. Alternatively, you can open a new issue, reference the closed issue by number or link, and state that you are still experiencing the issue. Provide any additional details in your post so we can better understand the issue and how to fix it.
","'aragon', 'ethereum', 'smart-contracts', 'solidity'",2021-08-17T21:54:28Z,2,5,5,"('haythemsellami', 269), ('bumi', 46)","[17, 'Partnerships for the Goals']"
openml/openml-python,Python module to interface with OpenML,"# OpenML-Python

[![All Contributors](https://img.shields.io/badge/all_contributors-2-orange.svg?style=flat-square)](#contributors-)


A python interface for [OpenML](http://openml.org), an online platform for open science collaboration in machine learning.
It can be used to download or upload OpenML data such as datasets and machine learning experiment results.

## General

* [Documentation](https://openml.github.io/openml-python).
* [Contribution guidelines](https://github.com/openml/openml-python/blob/develop/CONTRIBUTING.md).

[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

## Citing OpenML-Python

If you use OpenML-Python in a scientific publication, we would appreciate a reference to the
following paper:

[Matthias Feurer, Jan N. van Rijn, Arlind Kadra, Pieter Gijsbers, Neeratyoy Mallik, Sahithya Ravi, Andreas Müller, Joaquin Vanschoren, Frank Hutter
**OpenML-Python: an extensible Python API for OpenML**
Journal of Machine Learning Research, 22(100):1−5, 2021](https://www.jmlr.org/papers/v22/19-920.html)

Bibtex entry:
```bibtex
@article{JMLR:v22:19-920,
  author  = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas Müller and Joaquin Vanschoren and Frank Hutter},
  title   = {OpenML-Python: an extensible Python API for OpenML},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {100},
  pages   = {1--5},
  url     = {http://jmlr.org/papers/v22/19-920.html}
}
```

## Contributors ✨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):





  
    a-moadel📖 💡
    Neeratyoy Mallik💻 📖 💡
  






This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!
","'hacktoberfest', 'machine-learning', 'meta-learning', 'openml', 'python'",2024-04-29T20:33:41Z,45,274,22,"('mfeurer', 573), ('janvanrijn', 380), ('PGijsbers', 177), ('amueller', 159), ('Neeratyoy', 104), ('sahithyaravi', 46), ('joaquinvanschoren', 44), ('zardaloop', 40), ('ArlindKadra', 28), ('LennartPurucker', 17), ('dependabotbot', 10), ('eddiebergman', 9), ('pre-commit-cibot', 9), ('glemaitre', 7), ('a-moadel', 4), ('v-parmar', 3), ('rhiever', 3), ('engelen', 3), ('allcontributorsbot', 2), ('prabhant', 2), ('toon-vc', 2), ('Mirkazemi', 1), ('rth', 1), ('Rong-Inspur', 1), ('tashay', 1), ('TwsThomas', 1), ('timandrews335', 1), ('willcmartin', 1), ('WilliamRaynaut', 1), ('chadmarchand', 1), ('m7142yosuke', 1), ('marcoslbueno', 1), ('22quinn', 1), ('minoriinoue', 1), ('waffle-iron', 1), ('konrad', 1), ('scratchmex', 1), ('ledell', 1), ('varshneydevansh', 1), ('brylie', 1), ('Bilgecelik', 1), ('chouhanaryan', 1), ('anatolfernandez', 1), ('hinchliff', 1), ('fsabr', 1)","[4, 'Quality Education']"
SORMAS-Foundation/SORMAS-Project,"SORMAS (Surveillance, Outbreak Response Management and Analysis System) is an early warning and management system to fight the spread of infectious diseases.","
  
    <img
      alt=""SORMAS - Surveillance, Outbreak Response Management and Analysis System""
      src=""https://raw.githubusercontent.com/sormas-foundation/SORMAS-Project/development/logo.png""
      height=""200""
    />
  
  
  
  
  



# SORMAS

**SORMAS** (Surveillance Outbreak Response Management and Analysis System) is an open source eHealth system - consisting of separate web and mobile apps - that is geared towards optimizing the processes used in monitoring the spread of infectious diseases and responding to outbreak situations.

## FAQ (Frequently Asked Questions)

### How Does it Work?
You can give SORMAS a try on our demo server at !

### How Can I Get Involved?
Read through our [*Contributing Readme*](docs/CONTRIBUTING.md) and contact us at info@sormas.org to learn how you can help to drive the development of SORMAS forward,
or check out our [Discussions](https://github.com/sormas-foundation/SORMAS-Project/discussions) to get development support from the core developers and other community members.
SORMAS is a community-driven project, and we'd love to have you on board!

If you want to contribute to the code, please strictly adhere to the [*Development Environment*](docs/DEVELOPMENT_ENVIRONMENT.md) guide to ensure that everything is set up correctly.
Please also make sure that you've read the [*Development Contributing Guidelines*](docs/CONTRIBUTING.md#development-contributing-guidelines) before you start to develop.

### How Can I Report a Bug or Request a Feature?
If you want to report a **security issue**, please read and follow our [*Security Policies*](docs/SECURITY.md). For bugs without security implications, change and feature requests, please [create a new issue](https://github.com/sormas-foundation/SORMAS-Project/issues/new/choose) and
read the [*Submitting an Issue*](docs/CONTRIBUTING.md#submitting-an-issue) guide for more detailed instructions. We appreciate your help!

### Which Browsers and Android Versions are Supported?
SORMAS officially supports and is tested on **Chromium-based browsers** (like Google Chrome) and **Mozilla Firefox**, and all Android versions starting from **Android 7.0** (Nougat). In principle, SORMAS should be usable with all web browsers that are supported by Vaadin 8 (Chrome, Firefox, Safari, Edge, Internet Explorer 11; see ).

Making use of the SORMAS web application through a mobile device web browser is possible and acceptable also in countries that are subject to the General Data Protection Regulation (GDPR) as enforced by the European Union. However, in such countries that are subject to the GDPR, the Android application (.apk file) for SORMAS should not be used on mobile devices until further notice.

### Is there a ReST API documentation?
Yes! Please download the [latest release](https://github.com/sormas-foundation/SORMAS-Project/releases/latest) and copy the content of /deploy/openapi/sormas-rest.yaml to an editor that generates a visual API documentation(e.g. ).
A runtime Swagger documentation of the External Visits Resource (used by external symptom journals such as CLIMEDO or PIA) is available at ``>/sormas-rest/openapi.json`` or ``>/sormas-rest/openapi.yaml``

### Who is responsible for Data Protection and Data Security?
We herewith explicitly would like to draw your attention to the fact, that the respective public health agency running SORMAS is in charge of data security and data protection and has to ensure compliance with national data protection and data security regulations in their respective jurisdiction.
It has to ensure that state-of-the art requirements for data protection and data security are fulfilled. All those prerequisites and examinations have to be done in the context of the country and its respective legal framework.
For these reasons, HZI cannot take the responsibility from the respective public health agency running the SORMAS systems and is not liable for any violation of data protection of the agency as the data generated by SORMAS belong to that very agency.



## Guidelines and Resources
If you want to learn more about the development and contribution process, setting up or customizing your own system, or technical details, please consider the following guides and resources available in this repository. You can also view this readme and all guides outside the Wiki with a full table of content and search functionality here: 

* **[GitHub Wiki](https://github.com/sormas-foundation/SORMAS-Project/wiki) - Our wiki contains additional guides for server customization and development instructions. Please have a look at it if you need information on anything that this readme does not contain.**
* [Contributing Guidelines](docs/CONTRIBUTING.md) - These are mandatory literature if you want to contribute to this repository in any way (e.g. by submitting issues, developing code, or translating SORMAS into new languages).
* [Development Environment Setup Instructions](docs/DEVELOPMENT_ENVIRONMENT.md) - If you want to get involved with development, this guide tells you how to correctly set up your system in order to contribute to the code in adherence with codestyle guidelines, development practices, etc.
* [Troubleshooting](docs/TROUBLESHOOTING.md) - A collection of solutions to common (mostly development) problems. Please consult this readme when encountering issues before issuing a support request.
* [Server Customization](docs/SERVER_CUSTOMIZATION.md) - If you are maintaining a SORMAS server or are a developer, this guide explains core concepts such as turning features on or off, importing infrastructure data or adjusting the configuration file.
* [Internationalization](docs/I18N.md) - SORMAS can be translated in any language by using the open source tool [Crowdin](https://crowdin.com/project/sormas); this resource explains how this process is working.
* [Disease Definition Instructions](docs/SOP_DISEASES.md) - We already support a large number of diseases, but not all of them are fully configured for case-based surveillance, and some might not be part of SORMAS at all yet; if you need SORMAS to support a specific disease, please use these instructions to give us all the information we need in order to extend the software with your requested disease.
* [Sormas2Sormas](docs/SORMAS2SORMAS.md) - The Sormas2Sormas API is used to share entities between SORMAS instances.

* [Security Policies](docs/SECURITY.md) - These contain important information about how to report security problems and the processes we are using to take care of them.
* [Third-Party License Acknowledgement](docs/3RD_PARTY_ACK.md) - This resource contains the names and license copies of external resources that SORMAS is using.

If you want to set up a SORMAS instance for production, testing or development purposes, please refer to the following guides:
* [Installing a SORMAS Server](docs/SERVER_SETUP.md)
* [Updating a SORMAS Server](docs/SERVER_UPDATE.md)
* [Setup Development environment](docs/DEVELOPMENT_ENVIRONMENT.md)
* [Creating a Demo Android App](docs/DEMO_APP.md)

## Project Structure
The project consists of the following modules:

- [**sormas-api:**](/sormas-api) General business logic and definitions for data exchange between app and server
- [**sormas-app:**](/sormas-app) The Android app
- [**sormas-backend:**](/sormas-backend) Server entity services, facades, etc.
- [**sormas-base:**](/sormas-base) Base project that also contains build scripts
- [**sormas-cargoserver:**](/sormas-cargoserver) Setup for a local dev server using maven-cargo
- [**sormas-e2e-performance-tests:**](/sormas-e2e-performance-tests) Automated performance tests addressing the ReST interface (sormas-rest)
- [**sormas-e2e-tests:**](/sormas-e2e-tests) Automated frontend tests addressing sormas-ui **and** API tests against sormas-rest. The API steps are partly used to prepare data for UI tests.
- [**sormas-ear:**](/sormas-ear) The ear needed to build the application
- [**sormas-keycloak-service-provider:**](/sormas-keycloak-service-provider) Custom Keycloak SPI for SORMAS
- [**sormas-rest:**](/sormas-rest) The REST interface; see [`sormas-rest/README.md`](sormas-rest/README.md)
- [**sormas-serverlibs:**](/sormas-serverlibs) Dependencies to be deployed with the payara server
- [**sormas-ui:**](/sormas-ui) The web application
- [**sormas-widgetset:**](/sormas-widgetset) The GWT widgetset generated by Vaadin
- [**sormas-e2e-tests:**](/sormas-e2e-tests) Automated tests addressing the sormas-ui, and the ReST interface
","'android', 'contact-tracing', 'covid-19', 'diseases', 'epidemics', 'infectious-diseases', 'low-bandwidth', 'offline-first', 'outbreak', 'sdg-3', 'surveillance', 'web'",2024-05-03T13:28:18Z,81,291,28,"('fhauptmann', 7252), ('MateStrysewske', 3580), ('sergiupacurariu', 1154), ('rdutu-vg', 1132), ('pk-sgent', 826), ('hms-sgent', 816), ('BarnaBartha', 769), ('JonasCir', 684), ('mk-sgent', 579), ('mp-sgent', 452), ('ciSymeda', 442), ('leventegal-she', 373), ('StefanKock', 371), ('syntakker', 337), ('dinua', 327), ('alexcaruntu-vita', 326), ('MartinWahnschaffe', 317), ('carina29', 315), ('DavidBaldsiefen', 309), ('ChristopherRiedel', 261), ('faatihi', 247), ('vidi42', 236), ('sormas-vitagroup', 204), ('StefanSzczesny', 171), ('HolgerReiseVSys', 163), ('lb-sgent', 125), ('FredrikSchaefer', 120), ('jp-sgent', 99), ('legolas38', 95), ('Alexandru-Pirvu', 84), ('dependabotbot', 80), ('raghupola-vg', 76), ('sampsonorson', 75), ('danRosca', 68), ('alinapopaiustina', 61), ('xavier-calland', 58), ('andrei7bal', 56), ('cazacmarin', 49), ('aponcon-atolcd', 45), ('richardbartha', 43), ('marius2301', 36), ('pmoscode', 31), ('dmarius23', 29), ('BojteTamas', 29), ('alexandre-gille', 28), ('popadriangeo', 25), ('jeremiahpslewis', 25), ('user-yormen', 23), ('ssferrazza', 21), ('valentinmikleuvg', 14), ('jabrandes', 11), ('tabotkevin', 10), ('tamasvlad', 9), ('razvancornita', 9), ('MichaelSp', 8), ('af-fwa', 7), ('fuatse', 6), ('Mihai-Bota', 6), ('opatajoshua', 6), ('AxelNennker', 5), ('VGitVlad', 5), ('mario-kuehne', 5), ('Candice-Louw', 5), ('tbroyer', 4), ('mathieu-petit-atol', 4), ('GouthamVadivelmurugan', 4), ('tkaefer', 3), ('hkweku', 2), ('georgeginguta', 2), ('daniel-schulze3', 2), ('arrol', 2), ('obinna-h-n', 2), ('schwzr', 2), ('polatluca', 1), ('mouloud-mala', 1), ('horatiu-cosma', 1), ('Toap777', 1), ('obdulia-losantos', 1), ('SeriousMartin', 1), ('jbellmann', 1), ('Anselmoo', 1)","[17, 'Partnerships for the Goals']"
TransparencyToolkit/LookingGlass,Intuitive and configurable search interface for document archives.,"LookingGlass
============

Search, filter, and browse any set of documents. LookingGlass includes full
text search, category filters, and date queries all through a nice search
interface with an **Elasticsearch** backend. LookingGlass also supports
customizable
[themes](https://github.com/TransparencyToolkit/LookingGlass/blob/master/THEMES.md)
and flexible document view pages for browsing and embedding a variety of
document types.

LookingGlass requires
[DocManager](https://github.com/TransparencyToolkit/DocManager) so that it can
interact with Elasticsearch. LookingGlass can be used in combination with
[Harvester](https://github.com/TransparencyToolkit/Harvester) for crawling,
parsing, and loading documents and automatically turning them into a
searchable archive. However, it also works well as a standalone archiving
tool.


# Installation

## Dependencies

* [DocManager](https://github.com/TransparencyToolkit/DocManager) and all of
  its dependencies
* ruby 2.4.1
* rails 5
* (optionally) [Harvester](https://github.com/TransparencyToolkit/Harvester)
* libmagic-dev

## Setup Instructions

1. Install the dependencies

* Download elasticsearch (https://www.elastic.co/downloads/elasticsearch)
* Download rvm (https://rvm.io/rvm/install)
* Install Ruby: Run `rvm install 2.4.1` and `rvm use 2.4.1`
* Install Rails: `gem install rails`
* Follow the installation instructions for [DocManager](https://github.com/TransparencyToolkit/DocManager)

2. Get LookingGlass

* Clone repo: `git clone --recursive git@github.com:TransparencyToolkit/LookingGlass.git`
* Go into the LookingGlass directory: `cd LookingGlass`
* Install the Rubygems LookingGlass uses: `bundle install`
* Generate simple form data: `rails generate simple_form:install --bootstrap`
* Precompile assets: `rake assets:precompile`

3. Run LookingGlass

* Start DocManager: Follow the instructions on the
  [DocManager](https://github.com/TransparencyToolkit/DocManager) repo
* Configure Project: Edit the file in `config/initializers/project_config` so
  that the PROJECT_INDEX value is the name of the index in the
  [DocManager](https://github.com/TransparencyToolkit/DocManager) project
  config LookingGlass should use
* Start LookingGlass: Run `rails server -p 3001`
* Use LookingGlass: Go to [http://0.0.0.0:3001](http://0.0.0.0:3001) in your
  browser
 

# Features

LookingGlass is a frontend for searchable document archives. Previously, it
also included the backend that interacted with Elasticsearch, but this has
since been split out into
[DocManager](https://github.com/TransparencyToolkit/DocManager). The key
features are described below.

## Display of Document Sets

LookingGlass shows document sets from multiple data sources. It displays a
list of documents on the main page. The fields displayed for each document on
the index page and the order the documents are displayed in (sorted by date or
another numerical field) are customizable in
[DocManager](https://github.com/TransparencyToolkit/DocManager)'s data source
config files.

Each individual document set is then displayed on its own page for easy
reading. The document page includes a sidebar with the document's categorical
field and a customizable set of tabs that can display the document text, embed
the document itself (which is stored remotely, locally, or on document cloud),
offer document downloads, or load links.

## Search

LookingGlass allows full text of document sets using the Elasticsearch
backend. It can be used to search documents in most languages. LookingGlass
supports searching all fields or individual fields, and a variety of non-text
fields like dates. Results are sorted by relevance with text matching the
query highlighted.

## Categorical Filters

Many document sets have categorical fields that are common across documents,
either in the original data or that can be extracted with a tool like
[Catalyst](https://github.com/transparencytoolkit/catalyst). For example,
countries mentioned in a document, file format, hashtags, and topic-specific
keywords are common types of categories. LookingGlass allows filtering
document sets by one or more categories by clicking links on the sidebar to
get, say, all the documents that are about a particular country.

The category sidebar also displays the number of documents for each value in
each category that matches the current query. This is great for getting an
overview of the content in the document set.

## Document View Templates

On both the search results/document index and individual document pages, the
way the document is displayed is highly customizable. It is possible to add
new templates to display different types of data sources however you want and
even thread together multiple documents when needed (in email datasets, for
example).

These view templates are defined in app/views/docs/show/tabs/panes (for the
document view page) and app/views/docs/index/results/result_templates (for the
index/result view). The fields to use as a thread ID and view templates to
used are specified per-source in the
[DocManager](https://github.com/TransparencyToolkit/DocManager) data source
config files.

## Version Tracking

LookingGlass can be used to track which documents change over time and
how. Documents that are changed are specified in categories on the sidebar and
the document view page has a tool that allows users to view the exact
difference between two documents over time.

The fields used to check if a document has changed are specified per-source in
the [DocManager](https://github.com/TransparencyToolkit/DocManager) data
source config files.

## Custom Themes

LookingGlass supports custom theming. The color scheme, fonts, logo, text, and
links are all entirely customizable.

Some of these settings, like the theme used, project title, and logo are defined in the
[DocManager](https://github.com/TransparencyToolkit/DocManager) project config
file. The colors and fonts can then be set by [creating a
theme](https://github.com/TransparencyToolkit/LookingGlass/blob/master/THEMES.md).
","'archive', 'document', 'interface', 'search'",2019-11-11T23:33:56Z,4,198,30,"('Shidash', 467), ('bnvk', 412), ('iliasbartolini', 4), ('ageis', 1)","[17, 'Partnerships for the Goals']"
Ecohackerfarm/powerplant,Optimize and assist planning your garden,"# powerplant

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE-OF-CONDUCT.md)



Optimize your garden plan, and learn about permaculture in the process! powerplant provides intelligent planting suggestions which maximize positive crop interaction for the mutual benefit of all your crops. It also helps you track the progress of your garden with planting schedules and customizable tasks.

## Status

Currently in development. You may try a demo of a recent development snapshot online [here](http://demo.powerplant.software).

## Setup

Basically you need to have [Node.js](https://nodejs.org/en/) installed, then follow the instructions in the [development documentation](https://ecohackerfarm.github.io/powerplant/generated/index.html#toc1__anchor).

## Contributing

If you'd like to contribute a complete feature to powerplant, or are working on related projects, or want to discuss something, join #powerplant @ freenode. For other communication options see the [wiki](https://wiki.ecohackerfarm.org/powerplant:start).
","'crop', 'garden', 'javascript', 'permaculture', 'pouchdb', 'powerplant', 'react', 'redux'",2023-01-03T21:45:37Z,12,80,15,"('petteripitkanen', 225), ('JustTB', 156), ('severhale', 32), ('jsorribes82', 15), ('gljivar', 11), ('TobbaT', 9), ('l0n3star', 4), ('aimeejulia', 2), ('hwalia282', 1), ('liuhx1027', 1), ('s3krit', 1), ('dependabotbot', 1)","[12, 'Responsible Consumption and Production']"
drnikki/open-demographics,Open Demographics Initiative: An open standard for collecting identity/demographic data in open source communities. ,"# Open demographics

## What is this?
Open Demographics is a recommended set of questions that anyone can use to ask community members about their demographics.

## Why  did this start?
When talking about  the makeup of the people in their (open source) community, I hear project leaders say a variation of ""I'd like to ask about it but I don't know the right way and I don't want to mess up.""

So it felt like a logical next step to standardize the ways that we ask questions about peoples' identities and demographic data.  

The content and format of these questions were initially crowdsourced as part of the [demographics of the web survey](https://github.com/drupaldiversity/diversity-of-the-web), run with support from Pantheon.

## What is the point?
Part of the challenge with measuring ""diversity"" in an open-source community is the very poor data from community to community.  If we can standardize the questions that we use, the answers (or range of answers) can be valid across communities.

## What do we need?
This project isn't ethically complete - I have concerns about storing this data alongside a user's federated identity. As one example, it's possible that people identify as LGBTQIA but are not or cannot be public about that for  Reasons.  Thus, in its current state, it is not a recommendation for storage, only for the text of questions and some variations depending on level of detail.

**Further plans:**
- Recommend field name and data storage formats
- Recommend linking or not to user's identity
- Recommend workflows for anonymous storage
- Make this survey less US-centric and provide translations and additional questions appropriate for other countries.
- Get more contributors


Please get involved in improving our [website](https://drnikki.github.io/open-demographics/) via our [github repository](https://github.com/drnikki/open-demographics/)!
",,2021-08-25T10:13:50Z,11,105,22,"('drnikki', 45), ('sagesharp', 7), ('lee0c', 6), ('germonprez', 5), ('emmairwin', 3), ('sarahsharp', 2), ('0xreid', 1), ('ktopham', 1), ('NickDickinsonWilde', 1), ('patcon', 1), ('wyclem', 1)","[5, 'Gender Equality']"
GeoNode/geonode,"GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data.","![GeoNode](https://raw.githubusercontent.com/GeoNode/documentation/master/about/img/geonode-logo_for_readme.gif ""GeoNode"")
![OSGeo Project](https://www.osgeo.cn/qgis/_static/images/osgeoproject.png)

Table of Contents
=================

- [Table of Contents](#table-of-contents)
  - [What is GeoNode?](#what-is-geonode)
  - [Try out GeoNode](#try-out-geonode)
  - [Quick Docker Start](#quick-docker-start)
  - [Learn GeoNode](#learn-geonode)
  - [Development](#development)
  - [Contributing](#contributing)
  - [Roadmap](#roadmap)
  - [Showcase](#showcase)
  - [Most useful links](#most-useful-links)
  - [Licensing](#licensing)

What is GeoNode?
----------------

GeoNode is a geospatial content management system, a platform for the
management and publication of geospatial data. It brings together mature
and stable open-source software projects under a consistent and
easy-to-use interface allowing non-specialized users to share data and
create interactive maps.

Data management tools built into GeoNode allow for integrated creation
of data, metadata, and map visualization. Each dataset in the system can
be shared publicly or restricted to allow access to only specific users.
Social features like user profiles and commenting and rating systems
allow for the development of communities around each platform to
facilitate the use, management, and quality control of the data the
GeoNode instance contains.

It is also designed to be a flexible platform that software developers
can extend, modify or integrate against to meet requirements in their
own applications.

Try out GeoNode
---------------

If you just want to try out GeoNode visit our official Demo online at:
[https://development.demo.geonode.org](https://development.demo.geonode.org). After your registration, you will be able
to test all basic functionalities like uploading layers, creation of
maps, editing metadata, styles, and much more. To get an overview what
GeoNode can do we recommend having a look at the [Users
Workshop](https://docs.geonode.org/en/master/usage/index.html).

Quick Docker Start
------------------

  ```bash
    python create-envfile.py
  ```
`create-envfile.py` accepts the following arguments:

- `--https`: Enable SSL. It's disabled by default
- `--env_type`: 
   - When set to `prod` `DEBUG` is disabled and the creation of a valid `SSL` is requested to Letsencrypt's ACME server
   - When set to `test`  `DEBUG` is disabled and a test `SSL` certificate is generated for local testing
   - When set to `dev`  `DEBUG` is enabled and no `SSL` certificate is generated
- `--hostname`: The URL that will serve GeoNode (`localhost` by default)
- `--email`: The administrator's email. Notice that a real email and valid SMPT configurations are required if  `--env_type` is set to `prod`. Letsencrypt uses email for issuing the SSL certificate 
- `--geonodepwd`: GeoNode's administrator password. A random value is set if left empty
- `--geoserverpwd`: GeoNode's administrator password. A random value is set if left empty
- `--pgpwd`: PostgreSQL's administrator password. A random value is set if left empty
- `--dbpwd`: GeoNode DB user role's password. A random value is set if left empty
- `--geodbpwd`: GeoNode data DB user role's password. A random value is set if left empty
- `--clientid`: Client id of Geoserver's GeoNode Oauth2 client. A random value is set if left empty
- `--clientsecret`: Client secret of Geoserver's GeoNode Oauth2 client. A random value is set if left empty

```bash
  docker compose build
  docker compose up -d
```

Learn GeoNode
-------------

After you´ve finished the setup process make yourself familiar with the
general usage and settings of your GeoNodes instance. - the [User
Training](https://docs.geonode.org/en/master/usage/index.html)
is going in depth into what we can do. - the [Administrators
Workshop](https://docs.geonode.org/en/master/admin/index.html)
will guide you to the most important parts regarding management commands
and configuration settings.

Development
-----------

GeoNode is a web-based GIS tool, and as such, in order to do development
on GeoNode itself or to integrate it into your own application, you
should be familiar with basic web development concepts as well as with
general GIS concepts.

For development, GeoNode can be run in a 'development environment'. In
contrast to a 'production environment' development differs as it uses
lightweight components to speed up things.

To get started visit the [Developer
workshop](https://docs.geonode.org/en/master/devel/index.html)
for a basic overview.

If you're planning to customize your GeoNode instance or to extend
its functionalities it's not advisable to change core files in any
case. In this case, it's common to setup a [GeoNode Project
Template](https://github.com/GeoNode/geonode-project).

Contributing
------------

GeoNode is an open source project and contributors are needed to keep
this project moving forward. Learn more on how to contribute on our
[Community
Bylaws](https://github.com/GeoNode/geonode/wiki/Community-Bylaws).

Roadmap
-------

GeoNode's development roadmap is documented in a series of GeoNode
Improvement Projects (GNIPS). They are documented at [GeoNode Wiki](https://github.com/GeoNode/geonode/wiki/GeoNode-Improvement-Proposals).

GNIPS are considered to be large undertakings that will add a large
number of features to the project. As such they are the topic of
community discussion and guidance. The community discusses these on the
developer mailing list: http://lists.osgeo.org/pipermail/geonode-devel/

Showcase
--------

A handful of other Open Source projects extend GeoNode’s functionality
by tapping into the re-usability of Django applications. Visit our
gallery to see how the community uses GeoNode: [GeoNode
Showcase](https://geonode.org/gallery/).

The development community is very supportive of new projects and
contributes ideas and guidance for newcomers.

Most useful links
-----------------


**General**

- Project homepage: https://geonode.org
- Repository: https://github.com/GeoNode/geonode
- Official Demos: https://demo.geonode.org
- GeoNode Wiki: https://github.com/GeoNode/geonode/wiki
- Issue tracker: https://github.com/GeoNode/geonode-project/issues

    In case of sensitive bugs like security vulnerabilities, please
    contact a GeoNode Core Developer directly instead of using an issue
    tracker. We value your effort to improve the security and privacy of
    this project!

**Related projects**

- GeoNode Project: https://github.com/GeoNode/geonode-project
- GeoNode at Docker: https://hub.docker.com/u/geonode
- GeoNode OSGeo-Live: https://live.osgeo.org/en/


**Support**

- User Mailing List: https://lists.osgeo.org/cgi-bin/mailman/listinfo/geonode-users
- Developer Mailing List: https://lists.osgeo.org/cgi-bin/mailman/listinfo/geonode-devel
- Gitter Chat: https://gitter.im/GeoNode/general


Licensing
---------

GeoNode is Copyright 2018 Open Source Geospatial Foundation (OSGeo).

GeoNode is free software: you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation, either version 3 of the License, or (at your
option) any later version. GeoNode is distributed in the hope that it
will be useful, but WITHOUT ANY WARRANTY; without even the implied
warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License along
with GeoNode. If not, see http://www.gnu.org/licenses.
","'django', 'gis', 'python', 'sdi', 'spatial-data-infrastructure'",2024-05-02T07:59:18Z,240,1389,111,"('ingenieroariel', 1780), ('simod', 1574), ('dependabotbot', 1165), ('capooti', 486), ('dependabot-previewbot', 448), ('tomkralidis', 408), ('afabiani', 368), ('github-actionsbot', 308), ('dwins', 274), ('cezio', 262), ('garnertb', 260), ('sbenthall', 250), ('ppasq', 250), ('jj0hns0n', 234), ('t-book', 216), ('mattiagiupponi', 214), ('giohappy', 214), ('francbartoli', 169), ('ahocevar', 162), ('b-angerer', 151), ('mekanix', 142), ('marthamareal', 124), ('lukeman', 123), ('iwillig', 117), ('mdiener21', 106), ('mbertrand', 84), ('ischneider', 82), ('state-hiu-dev', 82), ('frafra', 81), ('mtnorthcott', 75), ('kalxas', 70), ('cspanring', 69), ('etj', 67), ('mwengren', 57), ('pjdufour', 56), ('gannebamm', 53), ('Coop56', 44), ('ahmednoureldeen', 41), ('groldan', 41), ('sarasafavi', 40), ('travislbrundage', 40), ('petrus7', 36), ('bieganowski', 36), ('davelowe', 34), ('ricardogsilva', 34), ('karakostis', 28), ('menegon', 27), ('mweisman', 26), ('teamgeode', 25), ('nrb', 24), ('italogsfernandes', 24), ('nathanhilbert', 24), ('snyk-bot', 24), ('olivierdalang', 23), ('chrisjones-brack3t', 21), ('lucacasagrande', 20), ('jean', 20), ('matthewhanson', 16), ('sebastianlach', 15), ('ccosse', 13), ('vdeparday', 13), ('lucernae', 12), ('chrispatterson', 12), ('mwallschlaeger', 11), ('jondoig', 11), ('markiliffe', 11), ('aaime', 9), ('maabdelghaffar', 9), ('RegisSinjari', 9), ('MINDoSOFT', 8), ('allyoucanmap', 8), ('Gpetrak', 8), ('abulojoshua1', 8), ('ict4eo', 8), ('drwelby', 8), ('benmccall', 8), ('meomancer', 8), ('al3mon', 7), ('ismailsunni', 7), ('Vampouille', 7), ('waybarrios', 7), ('ilpise', 7), ('madi', 7), ('gonrial', 7), ('eg-novelt', 6), ('DingDingFan0207', 6), ('DanielJDufour', 6), ('lukerees', 6), ('mishravikas', 6), ('burner761', 5), ('eomwandho', 5), ('sbsimo', 5), ('glennvorhes', 5), ('mikefedak', 5), ('kikislater', 5), ('ndufrane', 5), ('jmwenda', 5), ('ebradbury', 5), ('davisc', 5), ('ewsterrenburg', 4), ('calvinmetcalf', 4), ('amefad', 4), ('rmarianski', 4), ('milafrerichs', 4), ('jonnyforestGIS', 4), ('gsolat', 4), ('auvipy', 4), ('federicogodan', 3), ('lpasquali', 3), ('assefay', 3), ('twelch', 3), ('tschaub', 3), ('senoadiw', 3), ('daniviga', 3), ('EHJ-52n', 3), ('fvicent', 3), ('hfs', 3), ('hishamkaram', 3), ('pmdias', 3), ('luorlandini', 3), ('alaw005', 2), ('smellman', 2), ('tomtl', 2), ('vasuse7en', 2), ('ahmdthr', 2), ('binkiesbane', 2), ('liyingben', 2), ('matthesrieke', 2), ('niamh-av', 2), ('npmiller', 2), ('numahell', 2), ('warex03', 2), ('d3netxer', 2), ('boney-bun', 2), ('brylie', 2), ('DBlasby', 2), ('artists974', 2), ('code-geek', 2), ('cristianzamar', 2), ('DavidQuartz', 2), ('Gustry', 2), ('firnasnadirman', 2), ('gdreich', 2), ('JivanAmara', 2), ('JJediny', 2), ('Gnafu', 2), ('luizvital', 2), ('rukarangi', 2), ('nastasi-oq', 2), ('micattoc', 2), ('Miro-Polc', 2), ('uniomni', 2), ('tlpinney', 1), ('cuttlefish', 1), ('vanbremer', 1), ('Yann-J', 1), ('jacksonrakena', 1), ('breadexperience', 1), ('crabtree', 1), ('cristinao', 1), ('dhlambert', 1), ('drumbsd', 1), ('frippe12573', 1), ('AnatolBu', 1), ('timlinux', 1), ('smesdaghi', 1), ('iamsarin', 1), ('ridhodwid', 1), ('batje', 1), ('pieterprovoost', 1), ('paltman', 1), ('cityhubla', 1), ('nloira', 1), ('JonDHo', 1), ('roicort', 1), ('zoran995', 1), ('yuribit', 1), ('trendspotter', 1), ('sharon-tickell', 1), ('stefmec', 1), ('stan4gis', 1), ('sebastianclarke', 1), ('prbordon', 1), ('nicokant', 1), ('mmyrda', 1), ('NyakudyaA', 1), ('machakux', 1), ('lsntdev', 1), ('kamalhg', 1), ('jwood106', 1), ('jonas-veselka', 1), ('joebocop', 1), ('collaer', 1), ('jannefleischer', 1), ('ezzine', 1), ('gamesbook', 1), ('fvanderbiest', 1), ('fpennica', 1), ('Fanevanjanahary', 1), ('MaxiReglisse', 1), ('edsonflavio', 1), ('dimitri-justeau', 1), ('dwilhelm89', 1), ('drnextgis', 1), ('samperd', 1), ('davicustodio', 1), ('trepagnier', 1), ('dkastl', 1), ('theduckylittle', 1), ('cstephen', 1), ('frewsxcv', 1), ('chrismayer', 1), ('autermann', 1), ('benjwadams', 1), ('allilou', 1), ('alesarrett', 1), ('randomorder', 1), ('jayeshguru99', 1), ('malnajdi', 1), ('mapplus', 1), ('takispanagiotopoulos', 1), ('mikesname', 1), ('maurimiranda', 1), ('oware', 1), ('mdsnor', 1), ('MalteIwanicki', 1), ('LiliGuimaraes', 1), ('khaledboka', 1), ('Kanahiro', 1), ('kaloyan13', 1), ('jsta', 1), ('JorgeMartinezG', 1), ('archaeogeek', 1), ('ridoo', 1), ('HaydenElza', 1), ('GuidoS', 1), ('KEERATIPONG-PET', 1), ('glarrain', 1), ('GeospatialPython', 1), ('george-silva', 1)","[17, 'Partnerships for the Goals']"
